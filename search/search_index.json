{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ALCF User Guides","text":"<p>The ALCF user-facing documentation source material is hosted on GitHub, in order to facilitate contributions and issue reporting from the community.</p> <p>Our user guides contain information for: </p> <ul> <li>Account and Project Management: Information and instructions on how to manage your ALCF account and awarded project.  </li> <li>Data Management: Information on our file systems that are mounted globally across all of our production systems.</li> <li>Aurora: Information on getting your code ready for Aurora, Argonne's exacale supercomputer.</li> <li>Polaris: Information on how to get started our newest supercomputer.</li> <li>AI Testbed: Information on how to use our AI Accelerators.</li> <li>Services: Information on how to use various services provided across clusters.</li> <li>Facility Policies: Information on our policies and procedures.</li> </ul>"},{"location":"#how-to-get-access","title":"How to Get Access","text":"<p>Researchers interested in using the ALCF systems (including Polaris and the AI Testbed\u2019s Cerebras CS-2 and SambaNova DataScale platforms) can now submit project proposals via the ALCF\u2019s Director\u2019s Discretionary program. Calls for proposals for additional allocation programs will be open at a later date.</p> <p>Submit your proposal requests at: Allocation Request Page</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>If you'd like to get started using our ALCF resources, our Getting Started page provides information on what you need to do in order to get time on our systems, get an account, and how to start running jobs.</p> <p>If you have an account and an award for Polaris, we suggest visiting Getting on Polaris.</p> <p>If you'd like to use our AI accelerators, visit Getting Started on AI Testbed.</p> <p>Please send feedback to support@alcf.anl.gov</p>"},{"location":"issues/","title":"Questions/Issues on ALCF Docs","text":"<p>The ALCF documentation source code is hosted on GitHub to make it easier to contribute and collaborate.</p> <p>If you find any issues within these pages, please open an Issue at ALCF GitHub user guide webpage. Provide us with a brief description of the issue, page URL, section heading, and mark the appropriate label type on the right pane. </p> <p>For any other issues kindly email support@alcf.anl.gov</p>"},{"location":"account-project-management/accounts-and-access/MyALCF/","title":"Getting Started on MyALCF","text":""},{"location":"account-project-management/accounts-and-access/MyALCF/#access-myalcf","title":"Access MyALCF","text":"<p>To find MyALCF, either use the myALCF link at the ALCF homepage or visit the MyALCF homepage</p>"},{"location":"account-project-management/accounts-and-access/MyALCF/#logging-in","title":"Logging In","text":"<p>Log in using your ALCF username and CRYPTOcard passcode. For those who already have an ALCF account, this set of credentials isn\u2019t changed. For those who do not have an ALCF account, you will need to request a new account in the section below the login. For more about accounts and passcodes, see Passcode Tokens</p>"},{"location":"account-project-management/accounts-and-access/MyALCF/#home-screen-dashboard","title":"Home Screen Dashboard","text":"<p>Once logged in to MyALCF, a home screen is presented with useful information pertaining to an individual's projects and the facility. The navigation menu links to get to more in depth tools and information. Simple icons show the current status of ALCF systems. Up-to-date data about current compute project allocations are shown as a high-level view of current activity (if there are no current allocations, there will be a link to request a new allocation). Links for support help, training event information, and facility news updates complete the home screen dashboard.</p> <p></p> <p>Example appearance of home screen dashboard.</p>"},{"location":"account-project-management/accounts-and-access/MyALCF/#navigation","title":"Navigation","text":"<p>Navigation menu items link to pages to update account and project information, use the sbank accounting tool, view training and documentation, and use any other tools your role allows you to access.</p>"},{"location":"account-project-management/accounts-and-access/MyALCF/#system-status","title":"System Status","text":"<p>ALCF machine status is visible with a green up arrow showing a running system and a red down arrow showing a system down for scheduled or unscheduled maintenance. Clicking the system name will take you to the resource status page for that machine if available. Machine usage, active jobs, and more is shown on the resource status page.</p>"},{"location":"account-project-management/accounts-and-access/MyALCF/#current-allocations","title":"Current Allocations","text":"<p>Collection of data showing status and usage of compute allocations. If there are no current compute allocations, a link to request a new discretionary allocation is visible. Data for each allocation is presented in rows with five columns per row:</p> <ul> <li>Compute Allocation: The project name, system, award type, allocation ID and dates.</li> <li>On-track Trend: Comparison of project activity to a linear usage of hours. Expanding the graph shows a more detailed version of the graph with node hour percentage used and months specified. A second graph shows job sizes needed to use all node hours available in the remaining time. </li> </ul> <p> </p> <p>Screenshot of example expanded on-track graph.</p> <ul> <li>Node Hour Usage: Total node hours available, used, and remaining in the allocation.</li> <li>14 Day Jobs Activity: show a simple graph of the last two weeks of activity. Expanding it gives a more detailed view with node hour counts per day and per month.</li> </ul> <p> </p> <p>Screenshot of example expanded activity graph.</p> <ul> <li>Hours by Size: How the jobs in the allocation are split in terms of machine nodes used. Expanding this graph will give a visualization of how that split has occurred over time.</li> </ul> <p> </p> <p>Screenshot of example expanded jobs by size graph.</p>"},{"location":"account-project-management/accounts-and-access/MyALCF/#links-training-facility-updates","title":"Links, Training, Facility Updates","text":"<p>The bottom portion of the MyALCF home screen shows quick links to training events, support, and current facility news.</p>"},{"location":"account-project-management/accounts-and-access/MyALCF/#navigation-menu","title":"Navigation Menu","text":"<p>The navigation menu is personalized and shows sections available based on the user role. As more MyALCF features are developed, this menu will update to keep the site easy to navigate. All active account users will be able to access the following:</p>"},{"location":"account-project-management/accounts-and-access/MyALCF/#update-account","title":"Update Account","text":"<p>View account details and update necessary fields here. View project memberships and UNIX groups.</p>"},{"location":"account-project-management/accounts-and-access/MyALCF/#projects-resources","title":"Projects &amp; Resources","text":"<p>View and update project memberships, join projects, view and edit UNIX groups and request an allocation from this section.</p>"},{"location":"account-project-management/accounts-and-access/MyALCF/#sbank","title":"sbank","text":"<p>sbank is the compute node hour accounting system at ALCF. It has historically been accessed via the command line but this interface allows for accessing information via graphic-based interactions. The basic capabilities and tools within the graphic interface are the following:</p> <ul> <li>Command Builder: The Command Builder is a point-and-click style interface that allows for typing commands into the top command bar and/or selecting options from input boxes and/or dropdowns to create specific sbank commands. The default command is \u2018sbank-list-allocations\u2019. You can switch to sbank-list-jobs, sbank-detail-allocations, etc. under the top command bar and refine the command below via the input fields. Dynamic help is offered when a field is highlighted to explain the command and give examples. The output commands can be altered in the \u2018view options\u2019 panel accessed via the \"View Options\" button.</li> </ul> <p></p> <p>Screenshot of command builder portion of sbank page.</p> <ul> <li>View Options: View Options is shown as a set of three lists with selectable items that affect how the output is presented.<ul> <li>Display Fields: The column titles that will show in the output. The column titles are listed in the order they will be viewed in the output (1 is the column shown to the left extent, followed to the right by 2, 3, etc.) The column titles are selectable/deselectable and can be reordered via the arrow buttons or by dragging and dropping them to the desired order. \"Field Width\" is available if a character limit is desired in the column's output.</li> </ul> </li> <li>Filters: Allows for toggling between different sets of rows available. By default, output is for active allocations.</li> <li> <p>Display Options: Allows for toggling items shown in the output.</p> <p> </p> <p>Screenshot of the view options portion of sbank page.</p> </li> <li> <p>Output: The output can be either text based or an html table that is filterable. This is done by toggling between \u2018text output\u2019 and \u2018html output\u2019 next to the output window. Output can be saved as .csv by clicking the \u2018Save Output\u2019 button next to the output window. Commands can also be saved by clicking the \u2018save command\u2019 button.</p> </li> <li>Popular Commands: Popular commands are a set of preconfigured commands that will output useful information with no modification necessary.  \u2018Popular Commands\u2019 can be accessed by clicking the \u2018Quick Commands\u2019 bar. Each command can be run as is or can be copied to the clipboard for use in the command line or copied to the command builder.</li> <li>Saved Commands: Commands that are likely to be repeated can be saved for later use with the \u2018save command\u2019 feature. \u2018Save command\u2019 buttons can be found next to the command bar or next to the output window. When you want to save a command, enter it via the command bar or by selecting options through the interface, click the save command button, and then enter a name and a brief description of the function. Once saved, the command will be accessible in the saved commands window unless deleted. The saved command can be run as is, copied to either the command builder or clipboard, edited (name and description), or deleted.</li> </ul>"},{"location":"account-project-management/accounts-and-access/MyALCF/#training","title":"Training","text":"<p>Link to ALCF training events.</p>"},{"location":"account-project-management/accounts-and-access/MyALCF/#user-guides","title":"User Guides","text":"<p>Link to ALCF User Guides.</p>"},{"location":"account-project-management/accounts-and-access/accounts-and-access-faqs/","title":"Accounts and Access FAQ","text":""},{"location":"account-project-management/accounts-and-access/accounts-and-access-faqs/#i-am-unable-to-sign-in-to-the-accounts-website-what-do-i-do","title":"I am unable to sign in to the Accounts website. What do I do?","text":"<p>Only users with active ALCF accounts can sign in to the Account and Project Management website. If you have an active account, verify that you are using the correct ALCF username. Note that the username is case-sensitive. If you forgot your username, contact support@alcf.anl.gov. For passcode token issues, please review the troubleshooting information on this page: Passcode Tokens.</p> <p>If your account is inactive, please submit a reactivation request here: https://my.alcf.anl.gov/accounts/#/accountReactivate.</p> <p>If you never had an ALCF account, please apply for one here: https://my.alcf.anl.gov/accounts/#/accountRequest. Note that all ALCF accounts must be associated with a project with an active allocation.</p>"},{"location":"account-project-management/accounts-and-access/accounts-and-access-faqs/#how-do-i-request-a-new-projectallocation","title":"How do I request a new project/allocation?","text":"<p>There are three allocation opportunities at ALCF. Please see \"How to Get an Allocation\" on how to get time on our systems.</p>"},{"location":"account-project-management/accounts-and-access/accounts-and-access-faqs/#who-do-i-contact-if-my-discretionary-project-allocation-expires-or-if-i-need-to-request-additional-hours","title":"Who do I contact if my Discretionary Project Allocation expires or if I need to request additional hours?","text":"<p>To request an extension of your existing discretionary allocation or to request additional hours, please email support@alcf.anl.gov with answers to the following or fill out the form to request an extension/additional hours: - What have you accomplished with your original allocation?   - Please include a brief description of any publications or major presentations that were (or will be) generated in full or in part because of this allocation. - What will you do with the extra time? - What are you requesting as your new expiration date? - How many additional hours are you requesting?</p>"},{"location":"account-project-management/accounts-and-access/accounts-and-access-faqs/#how-do-i-join-a-project","title":"How do I join a project?","text":"<p>To join a project, please go to https://my.alcf.anl.gov/, login then click the \"Project &amp; Resources\" dropdown on the left sidebar. Next, select \"Join project\". Once there, search and scroll down to the project you want to join and click on the. At the bottom of the next page, please click on the \"Request Membership\" button. Once we receive approval from the PI regarding your membership request, we will provide you with access to the necessary resources.</p>"},{"location":"account-project-management/accounts-and-access/accounts-and-access-faqs/#how-do-i-request-a-reservation","title":"How do I request a reservation?","text":"<p>Reservation requests must include information detailed here: </p> <ul> <li>Machine Reservations: Please email the completed reservation request to support@alcf.anl.gov. We will contact you after your request is reviewed by our reservations committee.</li> </ul>"},{"location":"account-project-management/accounts-and-access/accounts-and-access-faqs/#how-do-i-apply-for-a-new-account","title":"How do I apply for a new account?","text":"<p>Note: All ALCF accounts must be associated with an allocated project.</p> <ul> <li>Request a new account: https://my.alcf.anl.gov/accounts/#/accountRequest</li> <li>ALCF Accounts: https://my.alcf.anl.gov/</li> </ul>"},{"location":"account-project-management/accounts-and-access/accounts-and-access-faqs/#what-do-i-do-when-my-alcf-account-expires","title":"What do I do when my ALCF account expires?","text":"<p>Please forward your account expiry email to your Sponsor. As soon as we receive an approval email from your Sponsor, we'll proceed with your account renewal process as needed.</p>"},{"location":"account-project-management/accounts-and-access/accounts-and-access-faqs/#what-do-i-do-when-i-receive-a-warning-that-my-593-has-expired-is-about-to-expire","title":"What do I do when I receive a warning that my 593 has expired / is about to expire?","text":"<p>If you are planning to extend this assignment/computer user account, please let us know, so a new 593 (Foreign Visit &amp; Assignment Request form) will be filed for you using the information from before. In case any other documents are needed from your end, you'll be contacted as necessary. In order to allow sufficient time for an indices check, it is recommended that your response be submitted as soon as possible.</p> <p>If you are not planning to extend your account, also let us know so that we may close out your records.</p>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/","title":"ALCF Passcode Tokens","text":"<p>Please note: An account can be associated with a single token only (Mobile or Physical token). Please contact accounts@alcf.anl.gov to change your token preference.</p>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/#mobile-token","title":"Mobile Token","text":"<p>The SafeNet MobilePass+ Mobile Token allows access to ALCF systems. This security mobile token uses one-time passwords combined with your PIN for controlled access to the login systems. The mobile token utilizes an app that is keyed to your user account and for which you are responsible on your Android, iPhone, or Windows mobile device. Please safeguard your phone as you would your credit cards or house keys: Do not store username, PIN, or other account-related records with the token. Sharing of mobile tokens is strictly forbidden. A mobile token can be associated with a single device only.</p>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/#step-1-download-the-safenet-mobilepass-app-for-your-device","title":"Step 1. Download the SafeNet MobilePass+ app for your device:","text":"<p>The SafeNet MobilePASS+ app turns your device into a two-factor authentication device, removing the need to carry an additional hardware token. As a SafeNet MobilePASS+ user, you can generate passcodes on your mobile device and use those passcodes to authenticate on ALCF computing resources. See supported OS and platforms for more information.</p>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/#step-2-enroll-your-mobilepass-mobile-token","title":"Step 2. Enroll your MobilePass+ mobile token:","text":"<p>After you\u2019ve been provisioned a mobile token, you will receive a notification email with the subject line \"ALCF Mobile Token Self-Enrollment,\" which you must access from the device on which you wish to install the token.</p> <p>Auto-Enrollment (to enroll SafeNet MobilePass+ token automatically):</p> <ol> <li>Click on the <code>http://</code> link in the email. The SafeNet Authentication Service Self-Enrollment will open.</li> <li>Click enroll your SafeNet MobilePass+ token.</li> <li>When prompted to open in MobilePass+, tap Open.</li> <li>You will now be prompted to enter a 6-digit all-numeric PIN.</li> <li>Enter your PIN in the Token PIN field and repeat in the Confirm PIN field.</li> <li>You will be taken to the Enrollment Complete screen to name the token.</li> <li>Insert the desired name in the Token Name field or leave it as is. This name is not utilized by the server; it is for you only.</li> <li>The newly enrolled SafeNet MobilePass+ token is now displayed in the SafeNet MobilePass+ app.</li> </ol> <p>Manual Enrollment:</p> <ol> <li>Copy the activation string from the SafeNet provision email.</li> <li>Open the SafeNet MobilePass+ app and tap the manual option.</li> <li>Paste the enrollment string into the field provided and tap the Enroll button.</li> <li>You will now be prompted to enter a 6-digit all-numeric PIN.</li> <li>Enter your PIN in the Token PIN field and repeat in the Confirm PIN field.</li> <li>You will be taken to the Enrollment Complete screen to name the token.</li> <li>Insert the desired name in the Token Name field or leave it as is. This name is not utilized by the server; it is for you only.</li> </ol>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/#logging-in-to-an-alcf-system-using-a-mobile-token","title":"Logging in to an ALCF System using a Mobile Token","text":"<ol> <li> <p>Open the MobilePASS+ app on your device. Then initiate an SSH session and type the following:</p> <pre><code>ssh &lt;ALCF username&gt;@&lt;system_name&gt;.alcf.anl.gov\n</code></pre> </li> <li> <p>When prompted for a password, click the SafeNet MobilePASS+ app on your phone. Click on the token name listed within the app, and enter your PIN.</p> </li> <li> <p>The app will display your passcode immediately. Enter the passcode as the login password for the system within the SSH session. Please Note: You do NOT have to enter the PIN on the SSH screen when logging into a resource. This only needs to be done to access the passcode within the SafeNet MobilePASS+ app.</p> </li> <li> <p>Each generated passcode is valid on the SafeNet MobilePass+ app window until your mobile device screen times out.</p> </li> </ol>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/#troubleshooting-your-mobile-token","title":"Troubleshooting your Mobile Token","text":"<p>Case 1: Forgotten PIN: If you enter a PIN for your mobile token and you get an invalid PIN, you will be asked to re-enter your PIN. After 6 failed attempts, your token will be deleted, and you will need to call the ALCF help desk or send an email to ALCF support to have a new mobile token provisioned.</p> <p>Case 2: Account Lockout: If you fail to enter the correct password 6 times, you will get a permission denied error on the SSH screen. Upon 4 more failed attempts, your IP will be blocked. You will need to call the ALCF help desk and submit a ticket to have the IP unblocked.</p> <p>Case 3: PIN Change: While logged in to the mobile token, click on token settings, then tap change PIN. Enter the current PIN followed by the new PIN and confirm.</p> <p>Case 4: Re-Sync: If you are unable to log in to a resource after entering the correct PIN and passcode, your token may be out of sync with the server. Please email ALCF Service Desk at accounts@alcf.anl.gov for assistance.</p> <p>Case 5: New Mobile Device: If you have a new mobile device, please email the ALCF Service Desk at accounts@alcf.anl.gov to have a new mobile token provisioned.</p>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/#physical-token","title":"Physical Token","text":"<p>The physical token allows access to the ALCF systems. This security token uses one-time passwords combined with your PIN for controlled access to the login systems. The physical token is a tracked asset for which you are responsible and is keyed to your use. Please safeguard your token as you would your credit cards or house keys: Do not store username, PIN, or other account-related records with the token. Sharing of tokens is strictly forbidden. Please do not mark on the token or alter it in any way.</p>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/#enabling-your-alcf-physical-token","title":"Enabling Your ALCF Physical Token","text":"<p>Upon receipt of the CRYPTOcard token, contact accounts@alcf.anl.gov to verify your identity and activate the token. If this step is not performed, the CRYPTOcard token will not be able to log on to the ALCF resource.</p> <p>ALCF Accounts Service Desk Info Hours: Monday-Friday 9 a.m. - 5 p.m. (Central time); Email: accounts@alcf.anl.gov</p>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/#logging-in-to-an-alcf-system-using-a-physical-token","title":"Logging in to an ALCF System using a Physical Token","text":"<p>When the physical token is activated, an initial PIN will be provided. This will be a four-digit number that will prepend to the one-time password string generated by the token.</p> <p>Upon INITIAL login (to one of the ALCF machines), a prompt to change the PIN will appear. PINs must be at least four characters long and must only contain numbers.</p> <ol> <li> <p>Initiate an SSH session using:</p> <pre><code>ssh &lt;ALCF username&gt;@&lt;system_name&gt;.alcf.anl.gov\n</code></pre> </li> <li> <p>A password prompt will be received. At this point, push the button on the physical token once.</p> </li> <li> <p>An eight-character, one-time password made up of letters and numbers will appear on the token\u2019s display. This one-time password is case-sensitive.</p> </li> <li> <p>Type your PIN followed immediately by the one-time password at the SSH password prompt.</p> </li> </ol> <p>For example, if your PIN is 1234 and you received the one-time password string ABCD9876, you would type 1234ABCD9876 at the password prompt.</p>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/#troubleshooting-your-physical-token","title":"Troubleshooting Your Physical Token","text":"<p>Case 1: It says \"locked\": The physical token may be locked due to too many failed attempts. Please contact the ALCF Help Desk to return the defective token so a replacement can be sent.</p> <p>Case 2: You have a PIN for your physical token: Once a PIN has been set for your physical token, you will need to prepend your PIN to the token password. Otherwise, you will not be able to log in. If you do not remember your PIN, please email us so we can verify your identity and reset your initial PIN.</p> <p>Case 3: It does not say \"locked\" but still does not work: It is likely that your token has fallen out of sync with the server. If you have pushed the button on your physical token more than 10 times without successfully logging in, it will fail to authenticate because it has lost synchronization with the server. Please try connecting to Polaris first. If it still fails, please follow the re-sync instructions below.</p>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/#re-sync-instructions","title":"Re-Sync Instructions","text":"<p>If you have pushed the button on your physical token more than 10 times, it will fail to authenticate because it has lost synchronization with the server. You can re-synchronize your token using the following procedure:</p> <ol> <li> <p>Have your physical token ready.</p> </li> <li> <p>Obtain a challenge sequence:</p> <ul> <li>Initiate an SSH session to a host that allows token authentication (such as <code>polaris.alcf.anl.gov</code>). At the password prompt, just hit 'Enter'. This will cause the CRYPTOcard service to produce a challenge string consisting of 8 numbers.</li> </ul> </li> <li> <p>Hold down the button on your token for a few seconds until the display says \"Init,\" then let go.</p> </li> <li> <p>The token will scroll through a series of menu options. When it displays \"ReSync,\" hit the button again.</p> </li> <li> <p>The display will say</p> <p>Resync?0</p> </li> <li> <p>The number at the end will start cycling from 0 to 9, over and over.</p> </li> <li> <p>Look at the numbers in your challenge string. When the number displayed on your token changes to the first number of the challenge string, press the button. The display will now show this number, and the second digit will start cycling.</p> </li> <li> <p>Enter each of the numbers from your challenge string in the same manner, until the display on your token matches the entire challenge string. Choose the \"&lt;\" to backspace and re-enter the previous number if necessary.</p> </li> <li> <p>Once you've entered all 8 digits, re-check to make sure they're accurate. Then, while all 8 digits are displayed on the token, press the button to generate a new password.</p> </li> <li> <p>Enter your PIN followed by the new password, and hit 'Enter'. If successful, you will be logged in to the resource. You're now back in sync with the authentication server.</p> </li> </ol> <p>If you are unsuccessful, you will be presented with another challenge string. At this point, you may need to perform the re-sync instructions again.</p> <p>If there are still problems after completing the re-synchronization procedures, please email us at accounts@alcf.anl.gov so we can run a test on the physical token to determine if it is defective.</p> <p>If it is found to be defective, we will promptly replace it. Physical tokens are the property of Argonne National Laboratory.</p> <p>Please return them to us at:</p> <pre><code>ALCF Help Desk\nArgonne National Laboratory\n9700 S. Cass Ave.\nBldg. 240, Rm. 2129\nLemont, IL 60439\n</code></pre>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/#resetting-the-physical-token-pin","title":"Resetting the Physical Token PIN","text":"<p>Please email us at support@alcf.anl.gov for PIN resets. Once your identity has been verified, we will provide you with a new PIN for your CRYPTOcard token.</p>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/#returning-a-physical-token","title":"Returning a Physical Token","text":"<p>If you no longer need your physical token, please return it to this address:</p> <pre><code>ALCF Help Desk\nArgonne National Laboratory\n9700 S. Cass Ave.\nBldg. 240, Rm. 2129\nLemont, IL 60439\n</code></pre>"},{"location":"account-project-management/accounts-and-access/user-account-overview/","title":"ALCF User Account Overview","text":"<p>All computing carried out on the ALCF systems is associated with a user \"account.\" This account is used to log onto the login servers and run jobs on the resources. If someone has a user account, then they have a login name that is recorded in the user database. This web page describes the process that users will need to understand to manage account details, including policies and procedures.</p> <p>If you need an account, visit the Accounts and Project Management website: Request an account</p> <p>If you want to learn how to get started, visit the Get Started Guide</p>"},{"location":"account-project-management/accounts-and-access/user-account-overview/#who-can-get-an-account","title":"Who Can Get an Account","text":"<p>Those who are interested in having an account on an ALCF resource must first request an allocation and provide a detailed description of the work, including computational requirements and coding capabilities for the applicable ALCF resources. Alternatively, one may be part of a project team that already has an active allocation. Once an allocation has been granted, new users should complete an account request. A project\u2019s Principal Investigator (PI) must sponsor these accounts\u2014if the PI is the user, an ALCF staff member must serve as sponsor. Sponsors are asked annually to evaluate the accounts they have sponsored to determine whether or not these accounts should be kept active.</p>"},{"location":"account-project-management/accounts-and-access/user-account-overview/#account-abilities","title":"Account Abilities","text":"<p>A user with an active account can log in to the ALCF login servers (e.g., polaris.alcf.anl.gov). This account will have some home directory space, where file transfer can occur from that space via the login nodes, and where development activities, such as editing and compiling, can also occur.</p>"},{"location":"account-project-management/accounts-and-access/user-account-overview/#account-states","title":"Account States","text":"<p>Accounts are classified in one of the following categories:</p> <ul> <li>Pending: An account that has been requested but has not yet been created.</li> <li>Active: An account that can be used to interact with the ALCF Login Servers. This is the normal state for all accounts.</li> <li>Inactive: An account that still exists on the system (that is, the account continues to be registered in the database and the user's files exist on disk) but the user cannot interact with the ALCF Login Servers. An account might be disabled due to misuse, security concerns, or because it is no longer allocated or the approval period has expired.</li> <li>Deleted: An account that existed on the system and is thus in the records and backups, but whose user no longer has access to the systems or files on disk. Typically, an account that is inactive for 90 days is flagged as deleted.</li> </ul>"},{"location":"account-project-management/accounts-and-access/user-account-overview/#more-information","title":"More Information","text":"<ul> <li>Account Policy</li> <li>User Authentication Policy</li> <li>Account Sponsorship and Retention Policy</li> </ul>"},{"location":"account-project-management/allocation-management/allocation-management/","title":"Managing Your Allocations","text":"<p>Allocations require management \u2013 balance checks, resource allocation, requesting more time, etc.</p>"},{"location":"account-project-management/allocation-management/allocation-management/#checking-for-an-active-allocation","title":"Checking for an Active Allocation","text":"<p>To determine if there is an active allocation, check Job Submission.</p> <p>For information on how to run the query, look at our documentation on our sbank Allocations Accounting System or email support@alcf.anl.gov and ask for all active allocations.</p>"},{"location":"account-project-management/allocation-management/allocation-management/#using-sbank-to-determine-the-balance-of-an-allocation","title":"Using sbank to Determine the Balance of an Allocation","text":"<p>To determine which platforms have an active balance, check our allocation accounting system sbank.</p> <ul> <li>To obtain the allocation balance, check the <code>sbank</code> command sbank-list-allocations.</li> <li>DD projects with a negative balance will not be able to run jobs until they have requested additional time; see the \"Getting More Time\" subsection below.</li> <li>INCITE and ALCC PIs automatically receive an email summary of project usage. If this is a DD project, please email support@alcf.anl.gov.</li> </ul>"},{"location":"account-project-management/allocation-management/allocation-management/#allocation-expiration","title":"Allocation Expiration","text":"<p>Projects and allocations at the ALCF are different. A particular project might have multiple allocations of time. For example, a discretionary project that has been approved more than three times will have three allocations (two are probably expired) but just one project. Projects will not expire -- allocations will. If allocations are expired, or have no hours left, jobs will not be able to run. Consult the two above sections to determine active allocations.</p>"},{"location":"account-project-management/allocation-management/allocation-management/#getting-more-time","title":"Getting More Time","text":"<p>To request an extension of your existing discretionary allocation or to request additional hours, please email support@alcf.anl.gov with answers to the following:</p> <ul> <li>What have you accomplished with your original allocation?</li> <li>Please include a brief description of any publications or major presentations that were (or will be) generated in full or in part because of this allocation.</li> <li>What will you do with the extra time?</li> <li>What are you requesting as your new expiration date?</li> <li>How many additional hours are you requesting?</li> </ul>"},{"location":"account-project-management/allocation-management/allocation-management/#sub-allocations","title":"Sub-allocations","text":"<p>Tip</p> <p>See <code>sbank new suballocation -h</code> for all the options.</p> <p>Suballocations let PIs control who in their team can run jobs, how much they are allowed to consume (allocation amount), and when they are allowed to run jobs (start and end dates).</p> <p>Step 1: Create Suballocations (Project PI):</p> <p>PI creates suballocations </p> <pre><code>sbank new sub &lt;allocationid&gt; --name &lt;nameofsuballoc&gt;\n</code></pre> <p>Step 2: Manage Suballocations (Project PI)</p> <p>PI adds users to suballocations</p> <pre><code>sbank e sub &lt;projectname&gt;::&lt;nameofsuballoc&gt; --add-user=\"&lt;username1&gt; &lt;username2&gt; ...\"\n</code></pre> <p>PI can change the name of a suballocation </p> <pre><code>sbank e sub &lt;suballocationID&gt; --name=&lt;new_name_of_suballocation&gt;\n</code></pre> <p>By default, the primary suballocation (which is the default suballocation created when the allocation is created by ALCF) is unrestricted, i.e., enabled for all project members. That means all project members can submit jobs against the primary suballocation by default. All other suballocations are restricted by default, and users have to be added for each of them.</p> <p>To change the default for the primary suballocation to restrict usage, PI must first edit the suballocation:</p> <pre><code>sbank-edit-suballocation --restrict &lt;primary suballocation id&gt;\n</code></pre> <p>Then add users with this command:</p> <pre><code>sbank e sub &lt;primary suballocation id&gt; --add-user=\"&lt;username1&gt; &lt;username2&gt; ...\"\n</code></pre> <p>PI changes start and end dates for a suballocation:</p> <pre><code>sbank e sub &lt;suballocationID&gt; -S &lt;start_date&gt; -E &lt;end_date&gt;\n</code></pre> <p>PI adds hours to a suballocation:</p> <pre><code>sbank e sub &lt;projectname&gt;::&lt;nameOfSourceSuballoc&gt; --hours-to-move &lt;hours&gt; --to-suballocation &lt;projectname&gt;::&lt;nameOfDestSuballoc&gt;\n</code></pre> <p>Note</p> <p><code>hours</code> must be less than or equal to the available balance for the suballocation <code>nameOfSourceSuballoc</code>.</p> <p>Tip</p> <p>See <code>sbank e suballocation -h</code> for all the options.</p> <p>Step 3: Submit Jobs (Project team)</p> <p>Submit jobs to a suballocation. Note that the user should be on the suballocation\u2019s user list.</p> <p>Example:</p> <pre><code>qsub -l select=10,walltime=30:00,filesystems=eagle:home -A &lt;suballocationID&gt; -q demand test.sh\n</code></pre> <p>Note: Once submanagement is enabled for a project allocation, all job submissions must specify the <code>suballocationID</code>.</p> <p>Useful commands:</p> <p>List all suballocations for a project that shows the number of jobs run, charges, allocation balance, suballocation name, and list of users:</p> <pre><code>sbank-list-allocations -r polaris -p &lt;projectname&gt; -f \"+subname users_list\"\n</code></pre> <p>Tip</p> <p>See <code>sbank l a -h</code> for all the options and <code>sbank \u2013f\\?</code> for a list of fields that can be displayed.</p>"},{"location":"account-project-management/allocation-management/overview/","title":"Allocations on ALCF Computing Resources","text":""},{"location":"account-project-management/allocation-management/overview/#getting-an-allocation-award","title":"Getting an Allocation Award","text":""},{"location":"account-project-management/allocation-management/overview/#incite-alcc-and-adsp","title":"INCITE, ALCC, and ADSP","text":"<p>Researchers gain access to ALCF systems for computational science and engineering projects\u2014typically with awards of millions of core-hours\u2014through competitive, peer-reviewed allocation programs supported by the DOE and Argonne. Our peer-reviewed award programs consist of the INCITE, ALCC, and ADSP programs. More information about the programs, including dates for our CFPs, can be found on their web pages.</p>"},{"location":"account-project-management/allocation-management/overview/#directors-discretionary","title":"Director's Discretionary","text":"<p>Alternatively, ALCF offers a Director's Discretionary allocation award program for leadership computing preparation, INCITE and ALCC scaling, and application performance to maximize scientific application efficiency and productivity on leadership computing platforms. See the Director's Discretionary (DD) Program page for more information.</p>"},{"location":"account-project-management/allocation-management/overview/#initializing-your-awarded-allocation","title":"Initializing Your Awarded Allocation","text":"<p>Projects with INCITE, ALCC, and ADSP awards will be contacted directly by the ALCF staff with information on creating accounts.</p> <p>Director's Discretionary awards will receive information in the award confirmation email. </p>"},{"location":"account-project-management/allocation-management/overview/#allocation-resources","title":"Allocation Resources","text":"<p>While requesting an allocation, users can choose from:</p> <p>Compute: * Polaris</p> <p>File System:  * Eagle (Community Sharing)</p>"},{"location":"account-project-management/allocation-management/overview/#policy-information-related-to-allocations","title":"Policy Information Related to Allocations","text":"<p>Pullback Policy</p>"},{"location":"account-project-management/allocation-management/overview/#requesting-additional-allocation-hours","title":"Requesting Additional Allocation Hours","text":"<p>If you are a PI of a Director's Discretionary project that has an active allocation, you can request additional time or an extension using the allocation request form.</p> <p> </p> To request more hours, renew your project using the allocation request form."},{"location":"account-project-management/allocation-management/sbank-allocation-accounting-system/","title":"sbank Allocation Accounting System","text":"<p>sbank is the accounting system used within the ALCF. It tracks project allocations, usage charges, and refunds. sbank allows queries about the balance and expiration of project allocations and has replaced the outdated cbank accounting system.</p> <p>The sbank accounting system helps users manage their allocations and usage per job. It gives the PIs the ability to monitor their allocation usage by user, job, and machine. It also allows the user to monitor their usage per allocation and provides insight into how many hours are left on the project.</p>"},{"location":"account-project-management/allocation-management/sbank-allocation-accounting-system/#getting-started-with-sbank","title":"Getting Started with sbank","text":"<p>sbank Example Commands provides a set of example commands on how to use the most common commands.</p>"},{"location":"account-project-management/allocation-management/sbank-allocation-accounting-system/#sbank-man-pages","title":"sbank Man Pages","text":"<p>Use these sbank man pages to get information on how to use the commands.</p> <ul> <li>sbank</li> <li>sbank-detail</li> <li>sbank-detail-allocations</li> <li>sbank-detail-jobs</li> <li>sbank-detail-projects</li> <li>sbank-detail-transactions</li> <li>sbank-detail-users</li> <li>sbank-list</li> <li>sbank-list-allocations</li> <li>sbank-list-jobs</li> <li>sbank-list-projects</li> <li>sbank-list-transactions</li> <li>sbank-list-users</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/","title":"Manpage for sbank-detail-allocations","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#sbank-detail-allocations-options","title":"sbank-detail-allocations [options] [ ... ] <p>Detail allocation information.</p> <p>NOTE:  1. The list of <code>&lt;allocation id&gt;</code> arguments is optional. 2. You can also enter <code>&lt;allocation id&gt;</code> list by using the <code>-a</code> option multiple times. 3. Regardless, both are optional, and you can get detailed allocation info using the option filters below.</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#options","title":"OPTIONS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-version","title":"--version","text":"<p>Show the program's version number and exit.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-h-help","title":"-h, --help","text":"<p>Show this help message and exit.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-a-allocation_id-allocation-idallocation_id","title":"-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID","text":"<p>Filter on allocation id.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-e-event_id-event-idevent_id","title":"-e EVENT_ID, --event-id=EVENT_ID","text":"<p>Filter on event id.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-f-field_info-field-to-displayfield_info","title":"-f FIELD_INFO, --field-to-display=FIELD_INFO","text":"<p><code>FIELD_INFO</code> is <code>&lt;FIELD&gt;[:&lt;WIDTH&gt;]</code>. For available fields, enter <code>-f?</code> or <code>-f \"?\"</code>. To add fields, enter <code>-f \"+ &lt;FIELD&gt;[:&lt;WIDTH&gt;] ...\"</code>.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-j-jobid-jobidjobid","title":"-j JOBID, --jobid=JOBID","text":"<p>Filter on jobid.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-n-num_fields_to_display-num-fields-to-displaynum_fields_to_display","title":"-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY","text":"<p>Set the number of fields to display.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-p-project-projectproject","title":"-p PROJECT, --project=PROJECT","text":"<p>Filter on name or id. DO NOT MIX. Enter 'all' to get all. Wildcards '*' are allowed but only on names.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-r-resource-resourceresource","title":"-r RESOURCE, --resource=RESOURCE","text":"<p>Filter on name or id. DO NOT MIX. Enter 'all' to get all. Wildcards '*' are allowed but only on names.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-t-transaction_id-transaction-idtransaction_id","title":"-t TRANSACTION_ID, --transaction-id=TRANSACTION_ID","text":"<p>Filter on transaction id.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-u-user-useruser","title":"-u USER, --user=USER","text":"<p>Filter on name or id. DO NOT MIX. Enter 'all' to get all. Wildcards '*' are allowed but only on names.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-w-field_info-field-width","title":"-w \"FIELD_INFO\", --field-width=","text":"<p><code>\"FIELD_INFO\"</code> is <code>&lt;FIELD&gt;:&lt;WIDTH&gt;</code>. For available fields, enter <code>-w?</code> or <code>-w \"?\"</code>.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-e-end-endend","title":"-E END, --end=END","text":"<p><code>[OPER1]&lt;UTC_DATE1&gt;[...[OPER2]&lt;UTC_DATE2&gt;]</code>, where the operators <code>OPER1</code> and <code>OPER2</code> can be one of the following:</p> <ul> <li><code>ge</code>, <code>gt</code>, <code>le</code>, <code>lt</code>, <code>eq</code> or <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>==</code>.</li> </ul> <p>Operator Defaults:</p> <ul> <li><code>OPER1</code> is 'ge' for single date entry.</li> <li><code>OPER1</code> and <code>OPER2</code> are 'ge' and 'lt', respectively, for range date entry.</li> </ul> <p>Date Parsing Precedence:</p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012.</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-h-human-readable","title":"-H, --human-readable","text":"<p>Abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions), ...</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-i-get-inactive","title":"-I, --get-inactive","text":"<p>Also get inactive allocations.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-o-get-only-inactive","title":"-O, --get-only-inactive","text":"<p>Only inactive allocations.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-s-start-startstart","title":"-S START, --start=START","text":"<p><code>[OPER1]&lt;UTC_DATE1&gt;[...[OPER2]&lt;UTC_DATE2&gt;]</code>, where the operators <code>OPER1</code> and <code>OPER2</code> can be one of the following:</p> <ul> <li><code>ge</code>, <code>gt</code>, <code>le</code>, <code>lt</code>, <code>eq</code> or <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>==</code>.</li> </ul> <p>Operator Defaults:</p> <ul> <li><code>OPER1</code> is 'ge' for single date entry.</li> <li><code>OPER1</code> and <code>OPER2</code> are 'ge' and 'lt', respectively, for range date entry.</li> </ul> <p>Date Parsing Precedence:</p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012.</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-t-transaction_type-transaction-typetransaction_type","title":"-T TRANSACTION_TYPE, --transaction-type=TRANSACTION_TYPE","text":"<p>Transaction types: CHARGE, REFUND, PULLBACK, DEPOSIT, VOID.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-award-type-nameaward_type_name","title":"--award-type-name=AWARD_TYPE_NAME","text":"<p>Filter on award type name.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-award-categoryaward_category","title":"--award-category=AWARD_CATEGORY","text":"<p>Filter on award category.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-cbank-refcbank_ref","title":"--cbank-ref=CBANK_REF","text":"<p>Filter on Clusterbank reference id.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-createdcreated_timestamp","title":"--created=CREATED_TIMESTAMP","text":"<p><code>[OPER1]&lt;UTC_DATE1&gt;[...[OPER2]&lt;UTC_DATE2&gt;]</code>, where the operators <code>OPER1</code> and <code>OPER2</code> can be one of the following:</p> <ul> <li><code>ge</code>, <code>gt</code>, <code>le</code>, <code>lt</code>, <code>eq</code> or <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>==</code>.</li> </ul> <p>Operator Defaults:</p> <ul> <li><code>OPER1</code> is 'ge' for single date entry.</li> <li><code>OPER1</code> and <code>OPER2</code> are 'ge' and 'lt', respectively, for range date entry.</li> </ul> <p>Date Parsing Precedence:</p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012.</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-debugdebug_level","title":"--debug=DEBUG_LEVEL","text":"<p>SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-get-deleted","title":"--get-deleted","text":"<p>Also get deleted objects.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-get-only-deleted","title":"--get-only-deleted","text":"<p>Only deleted objects.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-all-charges","title":"--all-charges","text":"<p>Only show list info that have charges regardless of project/user relationship.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-history-date-rangeend","title":"--history-date-range=END","text":"<p><code>[OPER1]&lt;UTC_DATE1&gt;[...[OPER2]&lt;UTC_DATE2&gt;]</code>, where the operators <code>OPER1</code> and <code>OPER2</code> can be one of the following:</p> <ul> <li><code>ge</code>, <code>gt</code>, <code>le</code>, <code>lt</code>, <code>eq</code> or <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>==</code>.</li> </ul> <p>Operator Defaults:</p> <ul> <li><code>OPER1</code> is 'ge' for single date entry.</li> <li><code>OPER1</code> and <code>OPER2</code> are 'ge' and 'lt', respectively, for range date entry.</li> </ul> <p>Date Parsing Precedence:</p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012.</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-last-updatedlast_updated_timestamp","title":"--last-updated=LAST_UPDATED_TIMESTAMP","text":"<p><code>[OPER1]&lt;UTC_DATE1&gt;[...[OPER2]&lt;UTC_DATE2&gt;]</code>, where the operators <code>OPER1</code> and <code>OPER2</code> can be one of the following:</p> <ul> <li><code>ge</code>, <code>gt</code>, <code>le</code>, <code>lt</code>, <code>eq</code> or <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>==</code>.</li> </ul> <p>Operator Defaults:</p> <ul> <li><code>OPER1</code> is 'ge' for single date entry.</li> <li><code>OPER1</code> and <code>OPER2</code> are 'ge' and 'lt', respectively, for range date entry.</li> </ul> <p>Date Parsing Precedence:</p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012.</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-no-commas","title":"--no-commas","text":"<p>Remove commas from comma-separated thousands.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-no-header","title":"--no-header","text":"<p>Do not display the header.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-no-history","title":"--no-history","text":"<p>Do not show history information.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-no-rows","title":"--no-rows","text":"<p>Do not display the row data.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-no-sys-msg","title":"--no-sys-msg","text":"<p>Do not display system messages.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-no-totals","title":"--no-totals","text":"<p>Do not display the totals.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-jobs/","title":"sbank-detail-jobs","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-jobs/#sbank-detail-jobs-options","title":"sbank-detail-jobs [options] [  |  ...  | ] <p>Detail job information. NOTE: </p> <ol> <li>The arguments <code>&lt;jobid.resource&gt;</code> or <code>&lt;event_id&gt;</code> are NOT REQUIRED.  </li> <li><code>event_id</code> is the JOB DATABASE ID.  </li> <li><code>&lt;jobid&gt;</code> is the SCHEDULER CREATED ID, such as Cobalt.  </li> <li><code>&lt;jobid&gt;</code> can also be entered using the option <code>-j &lt;jobid&gt;</code>.  </li> <li><code>&lt;event_id&gt;</code> can also be entered using the option <code>-e &lt;event_id&gt;</code>.  </li> <li><code>&lt;resource&gt;</code> can also be entered using the option <code>-r &lt;resource&gt;</code>.  </li> <li>Regardless, you can use options or arguments to get detailed job information.</li> </ol>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-jobs/#options","title":"OPTIONS","text":"<p>--version</p> <p>Show the program's version number and exit.</p> <p>-h, --help</p> <p>Show this help message and exit.</p> <p>-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID</p> <p>Filter on allocation ID.</p> <p>-e EVENT_ID, --event-id=EVENT_ID</p> <p>Filter on event ID.</p> <p>-f FIELD_INFO, --field-to-display=FIELD_INFO</p> <p>FIELD_INFO is <code>&lt;FIELD&gt;[:&lt;WIDTH&gt;]</code>. For available fields, enter <code>-f?</code> or <code>-f \"?\"</code>. To add fields, enter <code>-f \"+ &lt;FIELD&gt;[:&lt;WIDTH&gt;] ...\"</code>.</p> <p>-j JOBID, --jobid=JOBID</p> <p>Filter on job ID.</p> <p>-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY</p> <p>Set the number of fields to display.</p> <p>-p PROJECT, --project=PROJECT</p> <p>Filter on name or ID. DO NOT MIX. Enter 'all' to get all. Wildcards '*' are allowed but only on names.</p> <p>-r RESOURCE, --resource=RESOURCE</p> <p>Filter on name or ID. DO NOT MIX. Enter 'all' to get all. Wildcards '*' are allowed but only on names.</p> <p>-t TRANSACTION_ID, --transaction-id=TRANSACTION_ID</p> <p>Filter on transaction ID.</p> <p>-u USER, --user=USER</p> <p>Filter on name or ID. DO NOT MIX. Enter 'all' to get all. Wildcards '*' are allowed but only on names.</p> <p>-w \"FIELD_INFO\", --field-width</p> <p>\"FIELD_INFO\" is <code>&lt;FIELD&gt;:&lt;WIDTH&gt;</code>. For available fields, enter <code>-w?</code> or <code>-w \"?\"</code>.</p> <p>-E END, --end=END</p> <p><code>[OPER1]&lt;UTC_DATE1&gt;[...[OPER2]&lt;UTC_DATE2&gt;]</code>, where the operators OPER1 and OPER2 can be one of the following: <code>ge</code>, <code>gt</code>, <code>le</code>, <code>lt</code>, <code>eq</code> or <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>==</code>. Operator Defaults: OPER1 is 'lt' for a single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012.</p> <p>-H, --human-readable</p> <p>Abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...</p> <p>-S START, --start=START</p> <p><code>[OPER1]&lt;UTC_DATE1&gt;[...[OPER2]&lt;UTC_DATE2&gt;]</code>, where the operators OPER1 and OPER2 can be one of the following: <code>ge</code>, <code>gt</code>, <code>le</code>, <code>lt</code>, <code>eq</code> or <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>==</code>. Operator Defaults: OPER1 is 'ge' for a single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012.</p> <p>-T TRANSACTION_TYPE, --transaction-type=TRANSACTION_TYPE</p> <p>Transaction types: CHARGE, REFUND, PULLBACK, DEPOSIT, VOID.</p> <p>--created=CREATED_TIMESTAMP</p> <p><code>[OPER1]&lt;UTC_DATE1&gt;[...[OPER2]&lt;UTC_DATE2&gt;]</code>, where the operators OPER1 and OPER2 can be one of the following: <code>ge</code>, <code>gt</code>, <code>le</code>, <code>lt</code>, <code>eq</code> or <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>==</code>. Operator Defaults: OPER1 is 'ge' for a single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012.</p> <p>--debug=DEBUG_LEVEL</p> <p>SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2.</p> <p>--eligible=ELIGIBLE_TIMESTAMP</p> <p><code>[OPER1]&lt;UTC_DATE1&gt;[...[OPER2]&lt;UTC_DATE2&gt;]</code>, where the operators OPER1 and OPER2 can be one of the following: <code>ge</code>, <code>gt</code>, <code>le</code>, <code>lt</code>, <code>eq</code> or <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>==</code>. Operator Defaults: OPER1 is 'ge' for a single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012.</p> <p>--get-not-charged</p> <p>Only un-charged jobs.</p> <p>--history-date-range=END</p> <p><code>[OPER1]&lt;UTC_DATE1&gt;[...[OPER2]&lt;UTC_DATE2&gt;]</code>, where the operators OPER1 and OPER2 can be one of the following: <code>ge</code>, <code>gt</code>, <code>le</code>, <code>lt</code>, <code>eq</code> or <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>==</code>. Operator Defaults: OPER1 is 'ge' for a single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012.</p> <p>--last-updated=LAST_UPDATED_TIMESTAMP</p> <p><code>[OPER1]&lt;UTC_DATE1&gt;[...[OPER2]&lt;UTC_DATE2&gt;]</code>, where the operators OPER1 and OPER2 can be one of the following: <code>ge</code>, <code>gt</code>, <code>le</code>, <code>lt</code>, <code>eq</code> or <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>==</code>. Operator Defaults: OPER1 is 'gt' for a single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012.</p> <p>--no-commas</p> <p>Remove commas from comma-separated thousands.</p> <p>--no-header</p> <p>Do not display the header.</p> <p>--no-history</p> <p>Do not show history information.</p> <p>--no-rows</p> <p>Do not display the row data.</p> <p>--no-sys-msg</p> <p>Do not display system messages.</p> <p>--no-totals</p> <p>Do not display the totals.</p> <p>--queued=QUEUED_TIMESTAMP</p> <p><code>[OPER1]&lt;UTC_DATE1&gt;[...[OPER2]&lt;UTC_DATE2&gt;]</code>, where the operators OPER1 and OPER2 can be one of the following: <code>ge</code>, <code>gt</code>, <code>le</code>, <code>lt</code>, <code>eq</code> or <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>==</code>. Operator Defaults: OPER1 is 'ge' for a single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/","title":"Manpage for sbank-detail-projects","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#sbank-detail-projects-options","title":"sbank-detail-projects [options] [ ... ] <p>Detail project information. </p> <p>NOTE:    1. The list of  arguments are optional   2. you can also enter  list by using the -p option multiple times   3. regardless, both are optional, and you can get detail project info using the option filters below","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#options","title":"OPTIONS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-version","title":"--version","text":"<p>show program's version number and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-h-help","title":"-h, --help","text":"<p>show this help message and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-a-allocation_id-allocation-idallocation_id","title":"-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID","text":"<p>filter on allocation id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-f-field_info-field-to-displayfield_info","title":"-f FIELD_INFO, --field-to-display=FIELD_INFO","text":"<p>FIELD_INFO is [:], for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [:] ...\""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-n-num_fields_to_display-num-fields-to-displaynum_fields_to_display","title":"-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY","text":"<p>set number of fields to display</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-p-project-projectproject","title":"-p PROJECT, --project=PROJECT","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-r-resource-resourceresource","title":"-r RESOURCE, --resource=RESOURCE","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-u-user-useruser","title":"-u USER, --user=USER","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-w-field_info-field-width","title":"-w \"FIELD_INFO\", --field-width","text":"<p>\"FIELD_INFO\" FIELD_INFO is :, for available fields enter -w? or -w \"?\""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-e-end-endend","title":"-E END, --end=END","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:  <ul> <li>ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==. </li> </ul> <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence:    - YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-h-human-readable","title":"-H, --human-readable","text":"<p>abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-i-get-inactive","title":"-I, --get-inactive","text":"<p>get inactive allocations</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-s-start-startstart","title":"-S START, --start=START","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:  <ul> <li>ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==. </li> </ul> <p>Operator Defaults:</p> <ul> <li>OPER1 is 'ge' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-debugdebug_level","title":"--debug=DEBUG_LEVEL","text":"<p>SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-all-charges","title":"--all-charges","text":"<p>only show list info that have charges regardless of project/user relationship</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-no-commas","title":"--no-commas","text":"<p>remove commas from comma separated thousands</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-no-header","title":"--no-header","text":"<p>do not display the header</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-no-rows","title":"--no-rows","text":"<p>do not display the row data</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-no-sys-msg","title":"--no-sys-msg","text":"<p>do not display system message</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-no-totals","title":"--no-totals","text":"<p>do not display the totals</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/","title":"Manpage for sbank-detail-transactions","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#sbank-detail-transactions-options","title":"sbank-detail-transactions [options] [ ... ] <p>Detail transaction information.</p>  <p>Note</p> <ol> <li>The list of <code>&lt;transaction id&gt;</code> arguments is optional.</li> <li>You can also enter the <code>&lt;transaction id&gt;</code> list by using the <code>-t</code> option multiple times.</li> <li>Regardless, both are optional, and you can get detailed transaction info using the option filters below.</li> </ol>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#options","title":"OPTIONS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-version","title":"--version","text":"<p>Show the program's version number and exit.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-h-help","title":"-h, --help","text":"<p>Show this help message and exit.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-a-allocation_id-allocation-idallocation_id","title":"-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID","text":"<p>Filter on allocation id.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-c-comment","title":"-c, --comment","text":"<p>Display comment.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-e-event_id-event-idevent_id","title":"-e EVENT_ID, --event-id=EVENT_ID","text":"<p>Filter on event id.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-f-field_info-field-to-displayfield_info","title":"-f FIELD_INFO, --field-to-display=FIELD_INFO","text":"<p><code>FIELD_INFO</code> is <code>&lt;FIELD&gt;[:&lt;WIDTH&gt;]</code>. For available fields, enter <code>-f?</code> or <code>-f \"?\"</code>. To add fields, enter <code>-f \"+ &lt;FIELD&gt;[:&lt;WIDTH&gt;] ...\"</code>.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-j-jobid-jobidjobid","title":"-j JOBID, --jobid=JOBID","text":"<p>Filter on jobid.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-n-num_fields_to_display-num-fields-to-displaynum_fields_to_display","title":"-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY","text":"<p>Set the number of fields to display.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-p-project-projectproject","title":"-p PROJECT, --project=PROJECT","text":"<p>Filter on name or id. DO NOT MIX. Enter 'all' to get all. Wildcards '*' are allowed but only on names.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-r-resource-resourceresource","title":"-r RESOURCE, --resource=RESOURCE","text":"<p>Filter on name or id. DO NOT MIX. Enter 'all' to get all. Wildcards '*' are allowed but only on names.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-t-transaction_id-transaction-idtransaction_id","title":"-t TRANSACTION_ID, --transaction-id=TRANSACTION_ID","text":"<p>Filter on transaction id.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-u-user-useruser","title":"-u USER, --user=USER","text":"<p>Filter on name or id. DO NOT MIX. Enter 'all' to get all. Wildcards '*' are allowed but only on names.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-w-field_info-field-width","title":"-w \"FIELD_INFO\", --field-width=","text":"<p><code>\"FIELD_INFO\"</code> is <code>&lt;FIELD&gt;:&lt;WIDTH&gt;</code>. For available fields, enter <code>-w?</code> or <code>-w \"?\"</code>.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-e-job_end-endjob_end","title":"-E JOB_END, --end=JOB_END","text":"<p><code>[OPER1]&lt;UTC_DATE1&gt;[...[OPER2]&lt;UTC_DATE2&gt;]</code>, where the operators <code>OPER1</code> and <code>OPER2</code> can be one of the following:</p> <ul> <li><code>ge</code>, <code>gt</code>, <code>le</code>, <code>lt</code>, <code>eq</code> or <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>==</code>.</li> </ul> <p>Operator Defaults:</p> <ul> <li><code>OPER1</code> is 'ge' for a single date entry.</li> <li><code>OPER1</code> and <code>OPER2</code> are 'ge' and 'lt', respectively, for range date entry.</li> </ul> <p>Date Parsing Precedence:</p> <ul> <li>YEAR then MONTH then DAY, i.e., <code>121101</code> is parsed as YYMMDD, hence Nov 1, 2012.</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-h-human-readable","title":"-H, --human-readable","text":"<p>Abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-s-job_start-startjob_start","title":"-S JOB_START, --start=JOB_START","text":"<p><code>[OPER1]&lt;UTC_DATE1&gt;[...[OPER2]&lt;UTC_DATE2&gt;]</code>, where the operators <code>OPER1</code> and <code>OPER2</code> can be one of the following:</p> <ul> <li><code>ge</code>, <code>gt</code>, <code>le</code>, <code>lt</code>, <code>eq</code> or <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>==</code>.</li> </ul> <p>Operator Defaults:</p> <ul> <li><code>OPER1</code> is 'ge' for a single date entry.</li> <li><code>OPER1</code> and <code>OPER2</code> are 'ge' and 'lt', respectively, for range date entry.</li> </ul> <p>Date Parsing Precedence:</p> <ul> <li>YEAR then MONTH then DAY, i.e., <code>121101</code> is parsed as YYMMDD, hence Nov 1, 2012.</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-t-transaction_type-transaction-typetransaction_type","title":"-T TRANSACTION_TYPE, --transaction-type=TRANSACTION_TYPE","text":"<p>Transaction types: CHARGE, REFUND, PULLBACK, DEPOSIT, VOID.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-attransaction_at_timestamp","title":"--at=TRANSACTION_AT_TIMESTAMP","text":"<p><code>[OPER1]&lt;UTC_DATE1&gt;[...[OPER2]&lt;UTC_DATE2&gt;]</code>, where the operators <code>OPER1</code> and <code>OPER2</code> can be one of the following:</p> <ul> <li><code>ge</code>, <code>gt</code>, <code>le</code>, <code>lt</code>, <code>eq</code> or <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>==</code>.</li> </ul> <p>Operator Defaults:</p> <ul> <li><code>OPER1</code> is 'ge' for a single date entry.</li> <li><code>OPER1</code> and <code>OPER2</code> are 'ge' and 'lt', respectively, for range date entry.</li> </ul> <p>Date Parsing Precedence:</p> <ul> <li>YEAR then MONTH then DAY, i.e., <code>121101</code> is parsed as YYMMDD, hence Nov 1, 2012.</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-cbank-refcbank_ref","title":"--cbank-ref=CBANK_REF","text":"<p>Filter on Clusterbank reference id.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-createdjob_created_timestamp","title":"--created=JOB_CREATED_TIMESTAMP","text":"<p><code>[OPER1]&lt;UTC_DATE1&gt;[...[OPER2]&lt;UTC_DATE2&gt;]</code>, where the operators <code>OPER1</code> and <code>OPER2</code> can be one of the following:</p> <ul> <li><code>ge</code>, <code>gt</code>, <code>le</code>, <code>lt</code>, <code>eq</code> or <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>==</code>.</li> </ul> <p>Operator Defaults:</p> <ul> <li><code>OPER1</code> is 'ge' for a single date entry.</li> <li><code>OPER1</code> and <code>OPER2</code> are 'ge' and 'lt', respectively, for range date entry.</li> </ul> <p>Date Parsing Precedence:</p> <ul> <li>YEAR then MONTH then DAY, i.e., <code>121101</code> is parsed as YYMMDD, hence Nov 1, 2012.</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-debugdebug_level","title":"--debug=DEBUG_LEVEL","text":"<p>SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-no-commas","title":"--no-commas","text":"<p>Remove commas from comma-separated thousands.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-no-header","title":"--no-header","text":"<p>Do not display the header.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-no-rows","title":"--no-rows","text":"<p>Do not display the row data.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-no-sys-msg","title":"--no-sys-msg","text":"<p>Do not display system messages.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-no-totals","title":"--no-totals","text":"<p>Do not display the totals.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-queuedjob_queued_timestamp","title":"--queued=JOB_QUEUED_TIMESTAMP","text":"<p><code>[OPER1]&lt;UTC_DATE1&gt;[...[OPER2]&lt;UTC_DATE2&gt;]</code>, where the operators <code>OPER1</code> and <code>OPER2</code> can be one of the following:</p> <ul> <li><code>ge</code>, <code>gt</code>, <code>le</code>, <code>lt</code>, <code>eq</code> or <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>==</code>.</li> </ul> <p>Operator Defaults:</p> <ul> <li><code>OPER1</code> is 'ge' for a single date entry.</li> <li><code>OPER1</code> and <code>OPER2</code> are 'ge' and 'lt', respectively, for range date entry.</li> </ul> <p>Date Parsing Precedence:</p> <ul> <li>YEAR then MONTH then DAY, i.e., <code>121101</code> is parsed as YYMMDD, hence Nov 1, 2012.</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/","title":"Manpage for sbank-detail-users","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#sbank-detail-users-options","title":"sbank-detail-users [options] [ ... ] <p>Detail user information.</p>  <p>Note</p> <ol> <li>Use <code>-I</code> to include inactive allocations.</li> <li>The list of <code>&lt;user&gt;</code> arguments is optional.</li> <li>You can also enter the <code>&lt;user&gt;</code> list by using the <code>-u</code> option multiple times.</li> <li>Regardless, both are optional, and you can get detailed user info using the option filters below.</li> </ol>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#options","title":"OPTIONS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-version","title":"--version","text":"<p>Show the program's version number and exit.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-h-help","title":"-h, --help","text":"<p>Show this help message and exit.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-a-allocation_id-allocation-idallocation_id","title":"-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID","text":"<p>Filter on allocation ID.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-f-field_info-field-to-displayfield_info","title":"-f FIELD_INFO, --field-to-display=FIELD_INFO","text":"<p><code>FIELD_INFO</code> is <code>&lt;FIELD&gt;[:&lt;WIDTH&gt;]</code>. For available fields, enter <code>-f?</code> or <code>-f \"?\"</code>. To add fields, enter <code>-f \"+ &lt;FIELD&gt;[:&lt;WIDTH&gt;] ...\"</code>.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-n-num_fields_to_display-num-fields-to-displaynum_fields_to_display","title":"-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY","text":"<p>Set the number of fields to display.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-p-project-projectproject","title":"-p PROJECT, --project=PROJECT","text":"<p>Filter on name or ID. DO NOT MIX. Enter 'all' to get all. Wildcards '*' are allowed but only on names.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-r-resource-resourceresource","title":"-r RESOURCE, --resource=RESOURCE","text":"<p>Filter on name or ID. DO NOT MIX. Enter 'all' to get all. Wildcards '*' are allowed but only on names.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-u-user-useruser","title":"-u USER, --user=USER","text":"<p>Filter on name or ID. DO NOT MIX. Enter 'all' to get all. Wildcards '*' are allowed but only on names.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-w-field_info-field-width","title":"-w \"FIELD_INFO\", --field-width","text":"<p><code>\"FIELD_INFO\"</code> is <code>&lt;FIELD&gt;:&lt;WIDTH&gt;</code>. For available fields, enter <code>-w?</code> or <code>-w \"?\"</code>.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-e-end-endend","title":"-E END, --end=END","text":"<p><code>[OPER1]&lt;UTC_DATE1&gt;[...[OPER2]&lt;UTC_DATE2&gt;]</code>, where the operators <code>OPER1</code> and <code>OPER2</code> can be one of the following: - <code>ge</code>, <code>gt</code>, <code>le</code>, <code>lt</code>, <code>eq</code> or <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>==</code>.</p> <p>Operator Defaults:</p> <ul> <li><code>OPER1</code> is 'lt' for a single date entry. <code>OPER1</code> and <code>OPER2</code> are 'ge' and 'lt', respectively, for range date entry.</li> </ul> <p>Date Parsing Precedence:</p> <ul> <li>YEAR then MONTH then DAY, i.e., <code>121101</code> is parsed as YYMMDD, hence Nov. 1, 2012.</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-h-human-readable","title":"-H, --human-readable","text":"<p>Abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-i-get-inactive","title":"-I, --get-inactive","text":"<p>Get inactive allocations.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-s-start-startstart","title":"-S START, --start=START","text":"<p><code>[OPER1]&lt;UTC_DATE1&gt;[...[OPER2]&lt;UTC_DATE2&gt;]</code>, where the operators <code>OPER1</code> and <code>OPER2</code> can be one of the following: - <code>ge</code>, <code>gt</code>, <code>le</code>, <code>lt</code>, <code>eq</code> or <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>==</code>.</p> <p>Operator Defaults:</p> <ul> <li><code>OPER1</code> is 'lt' for a single date entry. <code>OPER1</code> and <code>OPER2</code> are 'ge' and 'lt', respectively, for range date entry.</li> </ul> <p>Date Parsing Precedence:</p> <ul> <li>YEAR then MONTH then DAY, i.e., <code>121101</code> is parsed as YYMMDD, hence Nov. 1, 2012.</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-debugdebug_level","title":"--debug=DEBUG_LEVEL","text":"<p><code>SILENT</code>, <code>MUCH_LESS</code>, <code>LESS</code>, <code>MORE</code>, <code>VERBOSE</code>, <code>DEBUG</code>, <code>DEBUG1</code>, <code>DEBUG2</code>.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-all-charges","title":"--all-charges","text":"<p>Only show list info that has charges regardless of project/user relationship.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-no-commas","title":"--no-commas","text":"<p>Remove commas from comma-separated thousands.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-no-header","title":"--no-header","text":"<p>Do not display the header.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-no-rows","title":"--no-rows","text":"<p>Do not display the row data.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-no-sys-msg","title":"--no-sys-msg","text":"<p>Do not display system messages.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-no-totals","title":"--no-totals","text":"<p>Do not display the totals.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/","title":"Manpage for sbank-detail","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#sbank-detail-options","title":"sbank-detail  [options] <p>Detail Meta Command</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#commands","title":"COMMANDS","text":"<ul> <li>allocations [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-I|-O|-S|-T|...] (DEFAULT) </li> <li>categories [-f|-n|-w|...] </li> <li>messages [-f|-n|-w|...] </li> <li>names [-f|-n|-w|...] </li> <li>jobs [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...] </li> <li>projects [-a|-f|-n|-p|-r|-u|-w|-E|-H|-I|-S|...] </li> <li>transactions [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...] </li> <li>users [-a|-f|-n|-p|-r|-u|-w|-E|-H|-S|...]</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#options","title":"OPTIONS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-a-allocation","title":"-a --allocation","text":"<p>Enter allocation ID.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-c-comment","title":"-c --comment","text":"<p>Enter a comment for new or edit commands, display comment for list commands.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-e-event-id","title":"-e --event-id","text":"<p>Enter event DB ID; event DB ID is an internal ID created by the charging system.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-f-field","title":"-f --field","text":"<p>Enter [:], width is optional; enter -f? or -f \"?\" for available fields, + to add fields."},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-h-help","title":"-h --help","text":"<p>Command line help.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-j-jobid","title":"-j --jobid","text":"<p>Enter job ID; job ID is created by the scheduler and is not unique.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-n-num-field","title":"-n --num-field","text":"<p>Enter the number of fields to display.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-p-project","title":"-p --project","text":"<p>Enter name or ID, DO NOT MIX, enter 'all' to get all, wildcards '*' are allowed, but only on names.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-r-resource","title":"-r --resource","text":"<p>Enter name or ID, DO NOT MIX, enter 'all' to get all, wildcards '*' are allowed but only on names.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-s-suballocation","title":"-s --suballocation","text":"<p>Enter suballocation ID.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-t-transaction","title":"-t --transaction","text":"<p>Enter transaction ID.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-u-user","title":"-u --user","text":"<p>Enter name or ID, DO NOT MIX, enter 'all' to get all, wildcards '*' are allowed, but only on names.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-w-field-width","title":"-w --field-width","text":"<p>Enter the field width as follows: :, enter -w? or -w \"?\" for available fields."},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-e-end","title":"-E --end","text":"<p>Enter end datetime filter.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-h-human-readable","title":"-H --human-readable","text":"<p>Abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions), etc.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-i-get-inactive","title":"-I --get-inactive","text":"<p>Include inactive allocations.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-o-get-only-inactive","title":"-O --get-only-inactive","text":"<p>Get only inactive allocations.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-s-start","title":"-S --start","text":"<p>Enter start datetime filter.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-t-type","title":"-T --Type","text":"<p>Enter type of transaction.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-all-charges","title":"--all-charges","text":"<p>For list allocations | projects | users, only show info with charges.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-at","title":"--at","text":"<p>Enter transaction created datetime filter.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-award-category","title":"--award-category","text":"<p>Enter allocation award category.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-award-type-name","title":"--award-type-name","text":"<p>Enter allocation award-type name.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-created","title":"--created","text":"<p>Enter created datetime filter.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-debug","title":"--debug","text":"<p>Enter debug level.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-get-deleted","title":"--get-deleted","text":"<p>Get deleted objects.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-get-not-charged","title":"--get-not-charged","text":"<p>Get jobs that have not been charged.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-get-only-deleted","title":"--get-only-deleted","text":"<p>Get only deleted objects.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-history-date-range","title":"--history-date-range","text":"<p>Enter history datetime filter.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-last-updated","title":"--last-updated","text":"<p>Enter last updated datetime filter.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-no-commas","title":"--no-commas","text":"<p>Remove commas from comma-separated thousands.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-no-header","title":"--no-header","text":"<p>Do not display header.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-no-history","title":"--no-history","text":"<p>Do not display history information.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-no-rows","title":"--no-rows","text":"<p>Do not display rows.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-no-sys-msg","title":"--no-sys-msg","text":"<p>Do not display system message.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-no-totals","title":"--no-totals","text":"<p>Do not display totals.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-queued","title":"--queued","text":"<p>Enter queued datetime filter.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-examples/","title":"sbank Example Commands","text":"<p>Below is a set of helpful commands to help you better manage the projects you have running at the ALCF.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-examples/#view-your-projects-allocations","title":"View your project's allocations","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-examples/#command-sbank-list-allocations","title":"Command: sbank-list-allocations","text":"<p>Use this command to list all of your active allocations for a specific project [Project-X]. This is useful when you need to provide this information in a report.</p> <pre><code>&gt; sbank-list-allocations -p ProjectX -r all\n Id         Start       End         Resource   Project          Jobs        Charged          Available Balance \n ---------  ----------  ----------  ---------  ---------------  ----------  ---------------  ----------------- \n 2106       2016-01-04  2017-01-01  cooley     ProjectX              1,139          6,032.8           43,967.2 \n 2146       2016-01-14  2017-01-10  theta      ProjectX                983      1,084,770.3       25,483,927.5\n 6438       2020-09-22  2022-01-01  thetagpu   ProjectX                  3              0.0            2,000.0 \n\nTotals:\n  Rows: 3\n  Cooley:\n    Available Balance: 43,967.2 node hours\n    Charged          : 6,032.8 node hours\n    Jobs             : 1,139 \n Theta:\n    Available Balance: 25,483,927.5 node hours \n    Charged          : 1,084,770.3 node hours \n    Jobs             : 983 \n Thetagpu:\n    Available Balance: 2,000.0 node hours\n    Charged          : 0.0 node hours\n    Jobs             : 3 \n</code></pre>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-examples/#list-your-projects-quota-on-eagle-file-system","title":"List your project's quota on Eagle File system","text":"<pre><code>&gt; sbank-list-allocations -p ProjectX -r eagle\n Allocation  Suballocation  Start       End         Resource  Project      Quota\n ----------  -------------  ----------  ----------  --------  -----------  -----\n 6687        6555           2020-12-16  2022-01-01  eagle     ProjectX    1.0\n\nTotals:\n  Rows: 1\n   Eagle: \n    Quota: 1.0 TB\n\n&gt; sbank-list-allocations -p ProjectX -r eagle\n Allocation  Suballocation  Start       End         Resource  Project      Quota\n ----------  -------------  ----------  ----------  --------  -----------  -----\n 6688        6556           2020-12-16  2022-01-01  eagle     ProjectX    1.0\n\nTotals:\n  Rows: 1\n  Eagle:\n    Quota: 1.0 TB\n</code></pre>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-examples/#list-only-the-created-timestamp-field-for-all-allocations-that-were-created-before-01-01-2015-for-projectx-across-all-resources","title":"List only the created timestamp field for all allocations that were created before 01-01-2015 for ProjectX across all resources","text":"<pre><code>&gt; sbank-list-allocations  --created \"&lt;20150101\" -r all -p ProjectX \"-f created\"\n Created    \n ---------- \n 2016-01-04 \n 2016-01-14 \n 2016-01-15 \n\nTotals:\n  Rows: 3\nDate filters (UTC): created &lt; \"2015-01-01 00:00:00\",  \n</code></pre>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-examples/#list-all-active-allocations-for-all-resources-for-project-projectx-and-add-the-field-created-to-the-display-list","title":"List all active allocations for all resources for project ProjectX and add the field Created to the display list","text":"<pre><code>shrubbery~ &gt; sbank-list-allocations -r all  -p ProjectX -f \"+created\"\n Id         Start       End         Resource   Project          Jobs        Charged          Available Balance  Created    \n ---------  ----------  ----------  ---------  ---------------  ----------  ---------------  -----------------  ---------- \n 279        2011-08-30  2020-01-01  theta      ProjectX              6,361     12,332,699.9      -12,332,699.9  2013-02-22 \n 2106       2016-01-04  2017-01-01  cooley     ProjectX              1,150          6,080.9           43,919.1  2016-01-04  \n\nTotals:\n  Rows: 2\n  Theta:\n    Available Balance: -12,332,699.9 node hours\n    Charged          : 12,332,699.9 node hours\n    Jobs             : 6,361 \n  Cooley:\n    Available Balance: 43,919.1 node hours\n    Charged          : 6,080.9 node hours\n    Jobs             : 1,150 \n</code></pre>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-examples/#list-all-available-fields-for-the-sbank-list-allocations-command","title":"List all available fields for the sbank-list-allocations command","text":"<pre><code>&gt; sbank-list-allocations  -f \"?\"\navailable fields:\n id\n start_timestamp\n end_timestamp\n resource\n project_name\n jobs_count\n charged_sum\n available_balance_sum\n created_timestamp\n award_category\n award_type_name\n admin_name\n cbank_ref\n comment\n</code></pre>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-examples/#view-your-projects-users","title":"View your project's users","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-examples/#command-sbank-list-users","title":"Command: sbank-list-users","text":"<p>List all charges for userx on theta on project ProjectX</p> <pre><code>&gt; sbank-list-users -p ProjectX -r theta -u userx\n User             Jobs        Charged         \n ---------------  ----------  --------------- \n userx                 1,814          9,884.5\n\nTotals:\n  Rows: 1\n  Resources: theta\n  Charged: 9,884.5 node hours\n  Jobs   : 1,814 \n</code></pre>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-examples/#list-charges-for-all-users-in-projectx-on-cooley","title":"List charges for all users in ProjectX on Cooley.","text":"<p>This works for project leads (i.e., PIs, Co-PIs, Proxies), since they can see everything in their own projects.</p> <pre><code>&gt; sbank-list-users -p ProjectX -r theta\n User             Jobs        Charged         \n ---------------  ----------  --------------- \n user1                   120          4,243.7 \n user2                     0              0.0 \n user3                     0              0.0 \n user4                   181          1,195.5 \n user5                     0              0.0 \n user6                 2,560         10,868.7 \n user7                     0              0.0 \n user8                     0              0.0 \n user9                     0              0.0 \n user10                    7              3.5 \n user11                    0              0.0 \n\nTotals:\n  Rows: 11\n  Resources: theta\n  Charged: 16,311.4 node hours\n  Jobs   : 2,868 \n</code></pre>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-examples/#view-your-projects-jobs","title":"View your project's jobs","text":"<p>List jobs for user \"userx\" for jobs that started in the range 2016-02-15 &lt;= started &lt; 2016-02-29 and add the transactions related to the job</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-examples/#command-sbank-list-jobs","title":"Command: sbank-list-jobs","text":"<p>Note: The job with the refund <code>transaction_ids_list</code> field can be shortened all the way to \"t\" in the <code>-f \"+ t\"</code></p> <pre><code>shrubbery~ &gt; sbank-list-jobs -u userx -f \"+ t\" -S \"2016-02-15...2016-02-29\"\n Id         Jobid      Resource   Project          Allocation  User       Duration   Charged          Transaction Ids \n ---------  ---------  ---------  ---------------  ----------  ---------  ---------  ---------------  --------------- \n 1013857    730417     theta       ProjectX         1740        userx      1:53:07           61,776.8  CHARGE-1011230  \n 1013860    730558     theta       ProjectX         1740        userx      1:53:07           61,776.8  CHARGE-1011233  \n 1014168    730668     theta       ProjectX         1740        userx      1:53:25           61,940.6  CHARGE-1011541  \n\nTotals:\n  Rows: 3\n  Theta:\n    Charged      : 185,494.2 node hours\n    Duration     : 6:44:00 \nDate filters (UTC): \"2016-02-15 00:00:00\" &lt;= start &lt; \"2016-02-29 00:00:00\",\n</code></pre>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-examples/#list-the-nodes-used-runtime-and-start-timestamp-for-cooley-job-744160","title":"List the nodes used, runtime, and start timestamp for Cooley job 744160","text":"<p>Note: To display the date and time, we increased the number of characters of start_timestamp to 19</p> <pre><code>catapult~ &gt; sbank l j -r theta -j 50576 -f \"jobid nodes_used runtime start_timestamp:19\"\n Jobid Nodes Used Runtime Start \n --------- ---------- --------- ------------------- \n 50576 512 1:00:49 2013-01-16 21:49:30 \n\nTotals: \n Rows: 1\n</code></pre>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-examples/#view-your-projects-transactions","title":"View your project's transactions","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-examples/#command-sbank-list-transactions","title":"Command: sbank-list-transactions","text":"<p>List of transactions that were at or after 2016-02-29 for ProjectX add fields: job_duration, nodes_used, and hosts</p> <p>Note:  - job_duration, nodes_used, and hosts are shortened, but they are still uniquely identified - host has the left justified width of 20, specified as \"h:-20\"</p> <pre><code>catapult~ &gt; sbank-list-transactions -p ProjectX --at \"ge 2016-02-29\" -f \"+ job_d nodes_u h:-20\" -r theta\n Id         Resource   Project          Allocation  At          User             Transaction Type  Amount           Jobid      Job Duration  Nodes Used  Hosts                \n ---------  ---------  ---------------  ----------  ----------  ---------------  ----------------  ---------------  ---------  ------------  ----------  -------------------- \n 1025426    theta       ProjectX         2147        2016-02-29  userx            CHARGE                   48,005.1  740587     1:27:54       2048        MIR-00800-33BF1-2048 \n 1028046    theta       ProjectX         2147        2016-03-01  userx            CHARGE                  147,647.1  742090     4:30:21       2048        MIR-40000-733F1-2048 \n 1028755    theta       ProjectX         2147        2016-03-02  userx            CHARGE                1,576,068.0  742126     6:00:44       16384       MIR-04000-77FF1-1638 \n\nTotals:\n  Rows: 3\n  Theta:\n    Charges Amount: 1,771,720.2 node hours\n    Job Duration  : 11:58:98 \nDate filters (UTC) : at &gt;= \"2016-02-29 00:00:00\",  \n</code></pre>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/","title":"Manpage for sbank-list-allocations","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#sbank-list-allocations-options","title":"sbank-list-allocations [options]","text":"<p>Generate allocation list report.</p> <p>Notes:  1. Use <code>-I</code> to include inactive allocations. 2. Enter <code>-r all</code> to get information for all resources.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#options","title":"OPTIONS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-version","title":"--version","text":"<p>Show program's version number and exit.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-h-help","title":"-h, --help","text":"<p>Show this help message and exit.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-a-allocation_id-allocation-idallocation_id","title":"-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID","text":"<p>Filter on allocation ID.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-c-comment","title":"-c, --comment","text":"<p>Display comment.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-e-event_id-event-idevent_id","title":"-e EVENT_ID, --event-id=EVENT_ID","text":"<p>Filter on event ID.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-f-field_info-field-to-displayfield_info","title":"-f FIELD_INFO, --field-to-display=FIELD_INFO","text":"<p><code>FIELD_INFO</code> is <code>&lt;FIELD&gt;[:&lt;WIDTH&gt;]</code>. For available fields, enter <code>-f?</code> or <code>-f \"?\"</code>. To add fields, enter <code>-f \"+ &lt;FIELD&gt;[:&lt;WIDTH&gt;] ...\"</code>.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-j-jobid-jobidjobid","title":"-j JOBID, --jobid=JOBID","text":"<p>Filter on job ID.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-n-num_fields_to_display-num-fields-to-displaynum_fields_to_display","title":"-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY","text":"<p>Set number of fields to display.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-p-project-projectproject","title":"-p PROJECT, --project=PROJECT","text":"<p>Filter on name or ID. DO NOT MIX. Enter <code>all</code> to get all. Wildcards <code>*</code> are allowed, but only on names.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-r-resource-resourceresource","title":"-r RESOURCE, --resource=RESOURCE","text":"<p>Filter on name or ID. DO NOT MIX. Enter <code>all</code> to get all. Wildcards <code>*</code> are allowed, but only on names.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-t-transaction_id-transaction-idtransaction_id","title":"-t TRANSACTION_ID, --transaction-id=TRANSACTION_ID","text":"<p>Filter on transaction ID.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-u-user-useruser","title":"-u USER, --user=USER","text":"<p>Filter on name or ID. DO NOT MIX. Enter <code>all</code> to get all. Wildcards <code>*</code> are allowed, but only on names.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-w-field_info-field-width","title":"-w \"FIELD_INFO\", --field-width","text":"<p><code>FIELD_INFO</code> is <code>&lt;FIELD&gt;:&lt;WIDTH&gt;</code>. For available fields, enter <code>-w?</code> or <code>-w \"?\"</code>.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-e-end-endend","title":"-E END, --end=END","text":"<p><code>[OPER1]&lt;UTC_DATE1&gt;[...[OPER2]&lt;UTC_DATE2&gt;]</code>, where the operators OPER1 and OPER2 can be one of the following:    - <code>ge</code>, <code>gt</code>, <code>le</code>, <code>lt</code>, <code>eq</code> or <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>==</code>. </p> <p>Operator Defaults: </p> <ul> <li>OPER1 is <code>lt</code> for single date entry. OPER1 and OPER2 are <code>ge</code> and <code>lt</code>, respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., <code>121101</code> is parsed as YYMMDD, hence Nov. 1, 2012.</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-h-human-readable","title":"-H, --human-readable","text":"<p>Abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-i-get-inactive","title":"-I, --get-inactive","text":"<p>Get inactive allocations.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-o-get-only-inactive","title":"-O, --get-only-inactive","text":"<p>Only inactive allocations.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-s-start-startstart","title":"-S START, --start=START","text":"<p><code>[OPER1]&lt;UTC_DATE1&gt;[...[OPER2]&lt;UTC_DATE2&gt;]</code>, where the operators OPER1 and OPER2 can be one of the following:    - <code>ge</code>, <code>gt</code>, <code>le</code>, <code>lt</code>, <code>eq</code> or <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>==</code>. </p> <p>Operator Defaults: </p> <ul> <li>OPER1 is <code>lt</code> for single date entry. OPER1 and OPER2 are <code>ge</code> and <code>lt</code>, respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., <code>121101</code> is parsed as YYMMDD, hence Nov. 1, 2012.</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-t-transaction_type-transaction-typetransaction_type","title":"-T TRANSACTION_TYPE, --transaction-type=TRANSACTION_TYPE","text":"<p>Transaction types: CHARGE, REFUND, PULLBACK, DEPOSIT, VOID.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-award-type-nameaward_type_name","title":"--award-type-name=AWARD_TYPE_NAME","text":"<p>Filter on award-type name.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-award-categoryaward_category","title":"--award-category=AWARD_CATEGORY","text":"<p>Filter on award category.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-cbank-refcbank_ref","title":"--cbank-ref=CBANK_REF","text":"<p>Filter on Clusterbank reference ID.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-createdcreated_timestamp","title":"--created=CREATED_TIMESTAMP","text":"<p><code>[OPER1]&lt;UTC_DATE1&gt;[...[OPER2]&lt;UTC_DATE2&gt;]</code>, where the operators OPER1 and OPER2 can be one of the following:    - <code>ge</code>, <code>gt</code>, <code>le</code>, <code>lt</code>, <code>eq</code> or <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>==</code>. </p> <p>Operator Defaults: </p> <ul> <li>OPER1 is <code>lt</code> for single date entry. OPER1 and OPER2 are <code>ge</code> and <code>lt</code>, respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., <code>121101</code> is parsed as YYMMDD, hence Nov. 1, 2012.</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-debugdebug_level","title":"--debug=DEBUG_LEVEL","text":"<p>SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-get-deleted","title":"--get-deleted","text":"<p>Get deleted objects.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-get-only-deleted","title":"--get-only-deleted","text":"<p>Get only deleted objects.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-all-charges","title":"--all-charges","text":"<p>Only show list info that have charges regardless of project/user relationship.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-last-updatedlast_updated_timestamp","title":"--last-updated=LAST_UPDATED_TIMESTAMP","text":"<p><code>[OPER1]&lt;UTC_DATE1&gt;[...[OPER2]&lt;UTC_DATE2&gt;]</code>, where the operators OPER1 and OPER2 can be one of the following:    - <code>ge</code>, <code>gt</code>, <code>le</code>, <code>lt</code>, <code>eq</code> or <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>==</code>. </p> <p>Operator Defaults: </p> <ul> <li>OPER1 is <code>lt</code> for single date entry. OPER1 and OPER2 are <code>ge</code> and <code>lt</code>, respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., <code>121101</code> is parsed as YYMMDD, hence Nov. 1, 2012.</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-no-commas","title":"--no-commas","text":"<p>Remove commas from comma-separated thousands.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-no-header","title":"--no-header","text":"<p>Do not display the header.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-no-rows","title":"--no-rows","text":"<p>Do not display the row data.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-no-sys-msg","title":"--no-sys-msg","text":"<p>Do not display system message.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-no-totals","title":"--no-totals","text":"<p>Do not display the totals.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/","title":"Manpage for sbank-list-jobs","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#sbank-list-jobs-options","title":"sbank-list-jobs [options]","text":"<p>Generate job list report. Note: To get information for all resources, enter \"-r all\".</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#options","title":"OPTIONS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-version","title":"--version","text":"<p>Show the program's version number and exit.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-h-help","title":"-h, --help","text":"<p>Show this help message and exit.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-a-allocation_id-allocation-idallocation_id","title":"-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID","text":"<p>Filter on allocation ID.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-e-event_id-event-idevent_id","title":"-e EVENT_ID, --event-id=EVENT_ID","text":"<p>Filter on event ID.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-f-field_info-field-to-displayfield_info","title":"-f FIELD_INFO, --field-to-display=FIELD_INFO","text":"<p><code>FIELD_INFO</code> is <code>&lt;FIELD&gt;[:&lt;WIDTH&gt;]</code>. For available fields, enter <code>-f?</code> or <code>-f \"?\"</code>. To add fields, enter <code>-f \"+ &lt;FIELD&gt;[:&lt;WIDTH&gt;] ...\"</code>.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-j-jobid-jobidjobid","title":"-j JOBID, --jobid=JOBID","text":"<p>Filter on job ID.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-n-num_fields_to_display-num-fields-to-displaynum_fields_to_display","title":"-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY","text":"<p>Set the number of fields to display.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-p-project-projectproject","title":"-p PROJECT, --project=PROJECT","text":"<p>Filter on name or ID. DO NOT MIX. Enter 'all' to get all. Wildcards '*' are allowed but only on names.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-r-resource-resourceresource","title":"-r RESOURCE, --resource=RESOURCE","text":"<p>Filter on name or ID. DO NOT MIX. Enter 'all' to get all. Wildcards '*' are allowed but only on names.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-t-transaction_id-transaction-idtransaction_id","title":"-t TRANSACTION_ID, --transaction-id=TRANSACTION_ID","text":"<p>Filter on transaction ID.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-u-user-useruser","title":"-u USER, --user=USER","text":"<p>Filter on name or ID. DO NOT MIX. Enter 'all' to get all. Wildcards '*' are allowed but only on names.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-w-field_info-field-width","title":"-w \"FIELD_INFO\", --field-width","text":"<p><code>\"FIELD_INFO\"</code> is <code>&lt;FIELD&gt;:&lt;WIDTH&gt;</code>. For available fields, enter <code>-w?</code> or <code>-w \"?\"</code>.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-e-end-endend","title":"-E END, --end=END","text":"<p><code>[OPER1]&lt;UTC_DATE1&gt;[...[OPER2]&lt;UTC_DATE2&gt;]</code>, where the operators <code>OPER1</code> and <code>OPER2</code> can be one of the following:    - <code>ge</code>, <code>gt</code>, <code>le</code>, <code>lt</code>, <code>eq</code> or <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>==</code>. </p> <p>Operator Defaults: </p> <ul> <li><code>OPER1</code> is 'lt' for a single date entry. <code>OPER1</code> and <code>OPER2</code> are 'ge' and 'lt', respectively, for a range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., <code>121101</code> is parsed as YYMMDD, hence Nov. 1, 2012.</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-h-human-readable","title":"-H, --human-readable","text":"<p>Abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions), etc.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-s-start-startstart","title":"-S START, --start=START","text":"<p><code>[OPER1]&lt;UTC_DATE1&gt;[...[OPER2]&lt;UTC_DATE2&gt;]</code>, where the operators <code>OPER1</code> and <code>OPER2</code> can be one of the following:    - <code>ge</code>, <code>gt</code>, <code>le</code>, <code>lt</code>, <code>eq</code> or <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>==</code>. </p> <p>Operator Defaults: </p> <ul> <li><code>OPER1</code> is 'lt' for a single date entry. <code>OPER1</code> and <code>OPER2</code> are 'ge' and 'lt', respectively, for a range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., <code>121101</code> is parsed as YYMMDD, hence Nov. 1, 2012.</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-t-transaction_type-transaction-typetransaction_type","title":"-T TRANSACTION_TYPE, --transaction-type=TRANSACTION_TYPE","text":"<p>Transaction types: CHARGE, REFUND, PULLBACK, DEPOSIT, VOID.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-createdcreated_timestamp","title":"--created=CREATED_TIMESTAMP","text":"<p><code>[OPER1]&lt;UTC_DATE1&gt;[...[OPER2]&lt;UTC_DATE2&gt;]</code>, where the operators <code>OPER1</code> and <code>OPER2</code> can be one of the following:    - <code>ge</code>, <code>gt</code>, <code>le</code>, <code>lt</code>, <code>eq</code> or <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>==</code>. </p> <p>Operator Defaults: </p> <ul> <li><code>OPER1</code> is 'lt' for a single date entry. <code>OPER1</code> and <code>OPER2</code> are 'ge' and 'lt', respectively, for a range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., <code>121101</code> is parsed as YYMMDD, hence Nov. 1, 2012.</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-debugdebug_level","title":"--debug=DEBUG_LEVEL","text":"<p>SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-eligibleeligible_timestamp","title":"--eligible=ELIGIBLE_TIMESTAMP","text":"<p><code>[OPER1]&lt;UTC_DATE1&gt;[...[OPER2]&lt;UTC_DATE2&gt;]</code>, where the operators <code>OPER1</code> and <code>OPER2</code> can be one of the following:    - <code>ge</code>, <code>gt</code>, <code>le</code>, <code>lt</code>, <code>eq</code> or <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>==</code>. </p> <p>Operator Defaults: </p> <ul> <li><code>OPER1</code> is 'lt' for a single date entry. <code>OPER1</code> and <code>OPER2</code> are 'ge' and 'lt', respectively, for a range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., <code>121101</code> is parsed as YYMMDD, hence Nov. 1, 2012.</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-get-not-charged","title":"--get-not-charged","text":"<p>Get only jobs that have not been charged.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-last-updatedlast_updated_timestamp","title":"--last-updated=LAST_UPDATED_TIMESTAMP","text":"<p><code>[OPER1]&lt;UTC_DATE1&gt;[...[OPER2]&lt;UTC_DATE2&gt;]</code>, where the operators <code>OPER1</code> and <code>OPER2</code> can be one of the following:    - <code>ge</code>, <code>gt</code>, <code>le</code>, <code>lt</code>, <code>eq</code> or <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>==</code>. </p> <p>Operator Defaults: </p> <ul> <li><code>OPER1</code> is 'lt' for a single date entry. <code>OPER1</code> and <code>OPER2</code> are 'ge' and 'lt', respectively, for a range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., <code>121101</code> is parsed as YYMMDD, hence Nov. 1, 2012.</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-no-commas","title":"--no-commas","text":"<p>Remove commas from comma-separated thousands.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-no-header","title":"--no-header","text":"<p>Do not display the header.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-no-rows","title":"--no-rows","text":"<p>Do not display the row data.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-no-sys-msg","title":"--no-sys-msg","text":"<p>Do not display system messages.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-no-totals","title":"--no-totals","text":"<p>Do not display the totals.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-queuedqueued_timestamp","title":"--queued=QUEUED_TIMESTAMP","text":"<p><code>[OPER1]&lt;UTC_DATE1&gt;[...[OPER2]&lt;UTC_DATE2&gt;]</code>, where the operators <code>OPER1</code> and <code>OPER2</code> can be one of the following:    - <code>ge</code>, <code>gt</code>, <code>le</code>, <code>lt</code>, <code>eq</code> or <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>==</code>. </p> <p>Operator Defaults: </p> <ul> <li><code>OPER1</code> is 'lt' for a single date entry. <code>OPER1</code> and <code>OPER2</code> are 'ge' and 'lt', respectively, for a range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., <code>121101</code> is parsed as YYMMDD, hence Nov. 1, 2012.</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/","title":"Manpage for sbank-list-projects","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#sbank-list-projects-options","title":"sbank-list-projects [options]","text":"<p>Generate project list report.</p> <p>Notes:</p> <ol> <li>Use <code>-I</code> to include inactive allocations.</li> <li>To get information for all resources, enter <code>-r all</code>.</li> </ol>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#options","title":"OPTIONS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-version","title":"--version","text":"<p>Show the program's version number and exit.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-h-help","title":"-h, --help","text":"<p>Show this help message and exit.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-a-allocation_id-allocation-idallocation_id","title":"-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID","text":"<p>Filter on allocation ID.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-f-field_info-field-to-displayfield_info","title":"-f FIELD_INFO, --field-to-display=FIELD_INFO","text":"<p><code>FIELD_INFO</code> is <code>&lt;FIELD&gt;[:&lt;WIDTH&gt;]</code>. For available fields, enter <code>-f?</code> or <code>-f \"?\"</code>. To add fields, enter <code>-f \"+ &lt;FIELD&gt;[:&lt;WIDTH&gt;] ...\"</code>.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-n-num_fields_to_display-num-fields-to-displaynum_fields_to_display","title":"-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY","text":"<p>Set the number of fields to display.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-p-project-projectproject","title":"-p PROJECT, --project=PROJECT","text":"<p>Filter on name or ID. DO NOT MIX. Enter 'all' to get all. Wildcards '*' are allowed but only on names.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-r-resource-resourceresource","title":"-r RESOURCE, --resource=RESOURCE","text":"<p>Filter on name or ID. DO NOT MIX. Enter 'all' to get all. Wildcards '*' are allowed, but only on names.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-u-user-useruser","title":"-u USER, --user=USER","text":"<p>Filter on name or ID. DO NOT MIX. Enter 'all' to get all. Wildcards '*' are allowed, but only on names.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-w-field_info-field-width","title":"-w \"FIELD_INFO\", --field-width","text":"<p><code>\"FIELD_INFO\"</code> is <code>&lt;FIELD&gt;:&lt;WIDTH&gt;</code>. For available fields, enter <code>-w?</code> or <code>-w \"?\"</code>.</p> <p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==. <p>Operator Defaults:</p> <ul> <li>OPER1 is 'lt' for a single date entry. OPER1 and OPER2 are 'ge' and 'lt', respectively, for a range date entry.</li> </ul> <p>Date Parsing Precedence:</p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012.</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-h-human-readable","title":"-H, --human-readable","text":"<p>Abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-i-get-inactive","title":"-I, --get-inactive","text":"<p>Get inactive allocations.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-s-start-startstart","title":"-S START, --start=START","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==. <p>Operator Defaults:</p> <ul> <li>OPER1 is 'lt' for a single date entry. OPER1 and OPER2 are 'ge' and 'lt', respectively, for a range date entry.</li> </ul> <p>Date Parsing Precedence:</p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012.</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-debugdebug_level","title":"--debug=DEBUG_LEVEL","text":"<p>SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-all-charges","title":"--all-charges","text":"<p>Only show list info that has charges regardless of project/user relationship.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-no-commas","title":"--no-commas","text":"<p>Remove commas from comma-separated thousands.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-no-header","title":"--no-header","text":"<p>Do not display the header.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-no-rows","title":"--no-rows","text":"<p>Do not display the row data.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-no-sys-msg","title":"--no-sys-msg","text":"<p>Do not display system messages.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-no-totals","title":"--no-totals","text":"<p>Do not display the totals.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/","title":"Manpage for sbank-list-transactions","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#sbank-list-transactions-options","title":"sbank-list-transactions [options]","text":"<p>Generate a transaction list report.</p> <p>Note: To get information for all resources, enter <code>-r all</code>.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#options","title":"OPTIONS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-version","title":"--version","text":"<p>Show the program's version number and exit.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-h-help","title":"-h, --help","text":"<p>Show this help message and exit.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-a-allocation_id-allocation-idallocation_id","title":"-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID","text":"<p>Filter on allocation ID.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-c-comment","title":"-c, --comment","text":"<p>Display comment.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-e-event_id-event-idevent_id","title":"-e EVENT_ID, --event-id=EVENT_ID","text":"<p>Filter on event ID.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-f-field_info-field-to-displayfield_info","title":"-f FIELD_INFO, --field-to-display=FIELD_INFO","text":"<p><code>FIELD_INFO</code> is <code>&lt;FIELD&gt;[:&lt;WIDTH&gt;]</code>. For available fields, enter <code>-f?</code> or <code>-f \"?\"</code>. To add fields, enter <code>-f \"+ &lt;FIELD&gt;[:&lt;WIDTH&gt;] ...\"</code>.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-j-jobid-jobidjobid","title":"-j JOBID, --jobid=JOBID","text":"<p>Filter on job ID.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-n-num_fields_to_display-num-fields-to-displaynum_fields_to_display","title":"-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY","text":"<p>Set the number of fields to display.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-p-project-projectproject","title":"-p PROJECT, --project=PROJECT","text":"<p>Filter on name or ID. Do not mix. Enter 'all' to get all. Wildcards '*' are allowed but only on names.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-r-resource-resourceresource","title":"-r RESOURCE, --resource=RESOURCE","text":"<p>Filter on name or ID. Do not mix. Enter 'all' to get all. Wildcards '*' are allowed but only on names.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-t-transaction_id-transaction-idtransaction_id","title":"-t TRANSACTION_ID, --transaction-id=TRANSACTION_ID","text":"<p>Filter on transaction ID.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-u-user-useruser","title":"-u USER, --user=USER","text":"<p>Filter on name or ID. Do not mix. Enter 'all' to get all. Wildcards '*' are allowed but only on names.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-w-field_info-field-width","title":"-w \"FIELD_INFO\", --field-width","text":"<p><code>\"FIELD_INFO\"</code> is <code>&lt;FIELD&gt;:&lt;WIDTH&gt;</code>. For available fields, enter <code>-w?</code> or <code>-w \"?\"</code>.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-e-job_end-endjob_end","title":"-E JOB_END, --end=JOB_END","text":"<p><code>[OPER1]&lt;UTC_DATE1&gt;[...[OPER2]&lt;UTC_DATE2&gt;]</code>, where the operators <code>OPER1</code> and <code>OPER2</code> can be one of the following:    - <code>ge</code>, <code>gt</code>, <code>le</code>, <code>lt</code>, <code>eq</code> or <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>==</code>. </p> <p>Operator Defaults: </p> <ul> <li><code>OPER1</code> is 'lt' for a single date entry. <code>OPER1</code> and <code>OPER2</code> are 'ge' and 'lt', respectively, for a range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012.</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-h-human-readable","title":"-H, --human-readable","text":"<p>Abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-s-job_start-startjob_start","title":"-S JOB_START, --start=JOB_START","text":"<p><code>[OPER1]&lt;UTC_DATE1&gt;[...[OPER2]&lt;UTC_DATE2&gt;]</code>, where the operators <code>OPER1</code> and <code>OPER2</code> can be one of the following:    - <code>ge</code>, <code>gt</code>, <code>le</code>, <code>lt</code>, <code>eq</code> or <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>==</code>. </p> <p>Operator Defaults: </p> <ul> <li><code>OPER1</code> is 'lt' for a single date entry. <code>OPER1</code> and <code>OPER2</code> are 'ge' and 'lt', respectively, for a range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012.</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-t-transaction_type-transaction-typetransaction_type","title":"-T TRANSACTION_TYPE, --transaction-type=TRANSACTION_TYPE","text":"<p>Transaction types: CHARGE, REFUND, PULLBACK, DEPOSIT, VOID.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-attransaction_at_timestamp","title":"--at=TRANSACTION_AT_TIMESTAMP","text":"<p><code>[OPER1]&lt;UTC_DATE1&gt;[...[OPER2]&lt;UTC_DATE2&gt;]</code>, where the operators <code>OPER1</code> and <code>OPER2</code> can be one of the following:    - <code>ge</code>, <code>gt</code>, <code>le</code>, <code>lt</code>, <code>eq</code> or <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>==</code>. </p> <p>Operator Defaults: </p> <ul> <li><code>OPER1</code> is 'lt' for a single date entry. <code>OPER1</code> and <code>OPER2</code> are 'ge' and 'lt', respectively, for a range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012.</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-cbank-refcbank_ref","title":"--cbank-ref=CBANK_REF","text":"<p>Filter on Clusterbank reference ID.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-createdjob_created_timestamp","title":"--created=JOB_CREATED_TIMESTAMP","text":"<p><code>[OPER1]&lt;UTC_DATE1&gt;[...[OPER2]&lt;UTC_DATE2&gt;]</code>, where the operators <code>OPER1</code> and <code>OPER2</code> can be one of the following:    - <code>ge</code>, <code>gt</code>, <code>le</code>, <code>lt</code>, <code>eq</code> or <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>==</code>. </p> <p>Operator Defaults: </p> <ul> <li><code>OPER1</code> is 'lt' for a single date entry. <code>OPER1</code> and <code>OPER2</code> are 'ge' and 'lt', respectively, for a range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012.</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-debugdebug_level","title":"--debug=DEBUG_LEVEL","text":"<p>SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-no-commas","title":"--no-commas","text":"<p>Remove commas from comma-separated thousands.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-no-header","title":"--no-header","text":"<p>Do not display the header.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-no-rows","title":"--no-rows","text":"<p>Do not display the row data.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-no-sys-msg","title":"--no-sys-msg","text":"<p>Do not display system messages.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-no-totals","title":"--no-totals","text":"<p>Do not display the totals.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-queuedjob_queued_timestamp","title":"--queued=JOB_QUEUED_TIMESTAMP","text":"<p><code>[OPER1]&lt;UTC_DATE1&gt;[...[OPER2]&lt;UTC_DATE2&gt;]</code>, where the operators <code>OPER1</code> and <code>OPER2</code> can be one of the following:    - <code>ge</code>, <code>gt</code>, <code>le</code>, <code>lt</code>, <code>eq</code> or <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>==</code>. </p> <p>Operator Defaults: </p> <ul> <li><code>OPER1</code> is 'lt' for a single date entry. <code>OPER1</code> and <code>OPER2</code> are 'ge' and 'lt', respectively, for a range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012.</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/","title":"Manpage for sbank-list-users","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#sbank-list-users-options","title":"sbank-list-users [options]","text":"<p>Generate user list report.</p> <p>Notes:</p> <ol> <li>Use <code>-I</code> to include inactive allocations.</li> <li>For information on all resources, use <code>-r all</code>.</li> </ol>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#options","title":"OPTIONS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-version","title":"--version","text":"<p>Show the program's version number and exit.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-h-help","title":"-h, --help","text":"<p>Show this help message and exit.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-a-allocation_id-allocation-idallocation_id","title":"-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID","text":"<p>Filter on allocation ID.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-f-field_info-field-to-displayfield_info","title":"-f FIELD_INFO, --field-to-display=FIELD_INFO","text":"<p><code>FIELD_INFO</code> is <code>&lt;FIELD&gt;[:&lt;WIDTH&gt;]</code>. For available fields, enter <code>-f?</code> or <code>-f \"?\"</code>. To add fields, enter <code>-f \"+ &lt;FIELD&gt;[:&lt;WIDTH&gt;] ...\"</code>.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-n-num_fields_to_display-num-fields-to-displaynum_fields_to_display","title":"-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY","text":"<p>Set the number of fields to display.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-p-project-projectproject","title":"-p PROJECT, --project=PROJECT","text":"<p>Filter on name or ID. DO NOT MIX. Enter 'all' to get all. Wildcards '*' are allowed, but only on names.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-r-resource-resourceresource","title":"-r RESOURCE, --resource=RESOURCE","text":"<p>Filter on name or ID. DO NOT MIX. Enter 'all' to get all. Wildcards '*' are allowed, but only on names.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-u-user-useruser","title":"-u USER, --user=USER","text":"<p>Filter on name or ID. DO NOT MIX. Enter 'all' to get all. Wildcards '*' are allowed, but only on names.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-w-field_info-field-width","title":"-w \"FIELD_INFO\", --field-width","text":"<p><code>\"FIELD_INFO\"</code> is <code>&lt;FIELD&gt;:&lt;WIDTH&gt;</code>. For available fields, enter <code>-w?</code> or <code>-w \"?\"</code>.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-e-end-endend","title":"-E END, --end=END","text":"<p><code>[OPER1]&lt;UTC_DATE1&gt;[...[OPER2]&lt;UTC_DATE2&gt;]</code>, where the operators <code>OPER1</code> and <code>OPER2</code> can be one of the following:    - <code>ge</code>, <code>gt</code>, <code>le</code>, <code>lt</code>, <code>eq</code> or <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>==</code>.</p> <p>Operator Defaults:</p> <ul> <li><code>OPER1</code> is 'lt' for a single date entry. <code>OPER1</code> and <code>OPER2</code> are 'ge' and 'lt', respectively, for a range date entry.</li> </ul> <p>Date Parsing Precedence:</p> <ul> <li>YEAR then MONTH then DAY, i.e., <code>121101</code> is parsed as YYMMDD, hence Nov. 1, 2012.</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-h-human-readable","title":"-H, --human-readable","text":"<p>Abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-i-get-inactive","title":"-I, --get-inactive","text":"<p>Also get inactive allocations.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-s-start-startstart","title":"-S START, --start=START","text":"<p><code>[OPER1]&lt;UTC_DATE1&gt;[...[OPER2]&lt;UTC_DATE2&gt;]</code>, where the operators <code>OPER1</code> and <code>OPER2</code> can be one of the following:    - <code>ge</code>, <code>gt</code>, <code>le</code>, <code>lt</code>, <code>eq</code> or <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>==</code>.</p> <p>Operator Defaults:</p> <ul> <li><code>OPER1</code> is 'lt' for a single date entry. <code>OPER1</code> and <code>OPER2</code> are 'ge' and 'lt', respectively, for a range date entry.</li> </ul> <p>Date Parsing Precedence:</p> <ul> <li>YEAR then MONTH then DAY, i.e., <code>121101</code> is parsed as YYMMDD, hence Nov. 1, 2012.</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-debugdebug_level","title":"--debug=DEBUG_LEVEL","text":"<p>SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-all-charges","title":"--all-charges","text":"<p>Only show list info that have charges regardless of project/user relationship.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-no-commas","title":"--no-commas","text":"<p>Remove commas from comma-separated thousands.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-no-header","title":"--no-header","text":"<p>Do not display the header.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-no-rows","title":"--no-rows","text":"<p>Do not display the row data.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-no-sys-msg","title":"--no-sys-msg","text":"<p>Do not display system messages.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-no-totals","title":"--no-totals","text":"<p>Do not display the totals.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/","title":"Manpage for sbank-list","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#sbank-list-options","title":"sbank-list  [options] <p>List Meta Command</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#commands","title":"COMMANDS","text":"<ul> <li>allocations [-a|-c|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-I|-O|-S|-T|...] (DEFAULT)</li> <li>categories [-f|-n|-w|...]</li> <li>messages [-f|-n|-w|...]</li> <li>names [-f|-n|-w|...]</li> <li>jobs [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...]</li> <li>projects [-a|-f|-n|-p|-r|-u|-w|-E|-H|-I|-S|...]</li> <li>transactions [-a|-c|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...]</li> <li>users [-a|-f|-n|-p|-r|-u|-w|-E|-H|-S|...]</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#options","title":"OPTIONS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-a-allocation","title":"-a --allocation","text":"<p>Enter allocation ID.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-c-comment","title":"-c --comment","text":"<p>Enter comment for new or edit commands, display comment for list commands.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-e-event-id","title":"-e --event-id","text":"<p>Enter event DB ID; event DB ID is an internal ID created by the charging system.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-f-field","title":"-f --field","text":"<p>Enter <code>&lt;field&gt;[:&lt;width&gt;]</code>, width is optional; enter <code>-f?</code> or <code>-f \"?\"</code> for available fields, <code>+</code> to add fields.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-h-help","title":"-h --help","text":"<p>Command line help.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-j-jobid","title":"-j --jobid","text":"<p>Enter job ID; job ID is created by the scheduler and is not unique.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-n-num-field","title":"-n --num-field","text":"<p>Enter number of fields to display.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-p-project","title":"-p --project","text":"<p>Enter name or ID, DO NOT MIX, enter 'all' to get all, wildcards '*' are allowed, but only on names.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-r-resource","title":"-r --resource","text":"<p>Enter name or ID, DO NOT MIX, enter 'all' to get all, wildcards '*' are allowed, but only on names.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-s-suballocation","title":"-s --suballocation","text":"<p>Enter suballocation ID.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-t-transaction","title":"-t --transaction","text":"<p>Enter transaction ID.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-u-user","title":"-u --user","text":"<p>Enter name or ID, DO NOT MIX, enter 'all' to get all, wildcards '*' are allowed, but only on names.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-w-field-width","title":"-w --field-width","text":"<p>Enter the field width as follows: <code>&lt;field&gt;:&lt;width&gt;</code>, enter <code>-w?</code> or <code>-w \"?\"</code> for available fields.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-e-end","title":"-E --end","text":"<p>Enter end datetime filter.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-h-human-readable","title":"-H --human-readable","text":"<p>Abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions), etc.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-i-get-inactive","title":"-I --get-inactive","text":"<p>Include inactive allocations.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-o-get-only-inactive","title":"-O --get-only-inactive","text":"<p>Include only inactive allocations.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-s-start","title":"-S --start","text":"<p>Enter start datetime filter.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-t-type","title":"-T --Type","text":"<p>Enter type of transaction.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-all-charges","title":"--all-charges","text":"<p>For list allocations | projects | users, only show info with charges.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-at","title":"--at","text":"<p>Enter transaction-created datetime filter.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-award-category","title":"--award-category","text":"<p>Enter allocation award category.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-award-type-name","title":"--award-type-name","text":"<p>Enter allocation award-type name.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-created","title":"--created","text":"<p>Enter created datetime filter.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-debug","title":"--debug","text":"<p>Enter debug level.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-get-deleted","title":"--get-deleted","text":"<p>Get deleted objects.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-get-not-charged","title":"--get-not-charged","text":"<p>Get jobs that have not been charged.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-get-only-deleted","title":"--get-only-deleted","text":"<p>Get only deleted objects.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-history-date-range","title":"--history-date-range","text":"<p>Enter history datetime filter.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-last-updated","title":"--last-updated","text":"<p>Enter last updated datetime filter.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-no-commas","title":"--no-commas","text":"<p>Remove commas from comma-separated thousands.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-no-header","title":"--no-header","text":"<p>Do not display header.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-no-history","title":"--no-history","text":"<p>Do not display history information.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-no-rows","title":"--no-rows","text":"<p>Do not display rows.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-no-sys-msg","title":"--no-sys-msg","text":"<p>Do not display system message.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-no-totals","title":"--no-totals","text":"<p>Do not display totals.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-queued","title":"--queued","text":"<p>Enter queued datetime filter.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/","title":"Manpage for sbank Commands","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#sbank-options","title":"sbank   [options]","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#description","title":"DESCRIPTION","text":"<p>HPC Accounting System Command Line Interface</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#detail-meta-command","title":"detail meta command","text":"<p>\"detail\" meta command displays information in a long format with history updates, where appropriate.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#list-meta-command","title":"list meta command","text":"<p>\"list\" meta command displays information in a table format, but no history updates are displayed.</p> <p>IMPORTANT NOTES   1. All dates entered shall be interpreted as UTC   2. non-admin users will only be able to see their content (jobs, charges, etc.)   3. project admin users will be able to see all of the content for their projects   4. staff admin users will be able to see all the content   5. --help and -h are the help options.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#meta-commands","title":"META COMMANDS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-detail-options","title":"- detail  [options]","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-list-options-default","title":"- list  [options] (DEFAULT) <p>DETAIL COMMANDS   * allocations [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-I|-O|-S|-T|...] [ ... ] (DEFAULT)    * jobs [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...] [ ... ]    * projects [-a|-f|-n|-p|-r|-u|-w|-E|-H|-I|-S|...] [ ... ]    * transactions [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...] [ ... ]    * users [-a|-f|-n|-p|-r|-u|-w|-E|-H|-S|...] [ ... ] <p>LIST COMMANDS   * allocations [-a|-c|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-I|-O|-S|-T|...] (DEFAULT)    * jobs [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...] projects [-a|-f|-n|-p|-r|-u|-w|-E|-H|-I|-S|...]    * transactions [-a|-c|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...]    * users [-a|-f|-n|-p|-r|-u|-w|-E|-H|-S|...]</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#options","title":"OPTIONS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-a-allocation","title":"-a --allocation <p>enter allocation id</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-c-comment","title":"-c --comment <p>enter comment for new or edit commands, display comment for list commands</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-e-event-id","title":"-e --event-id <p>enter event db id; event db id is an internal id created by the charging system</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-f-field","title":"-f --field <p>enter [:], width is optional; enter -f? or -f \"?\" for available fields, + to add fields","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-h-help","title":"-h --help <p>command line help</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-j-jobid","title":"-j --jobid <p>enter jobid; jobid is created by the scheduler and is not unique</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-n-num-field","title":"-n --num-field <p>enter number of fields to display</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-p-project","title":"-p --project <p>enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-r-resource","title":"-r --resource <p>enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-s-suballocation","title":"-s --suballocation <p>enter suballocation id</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-t-transaction","title":"-t --transaction <p>enter transaction id</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-u-user","title":"-u --user <p>enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-w-field-width","title":"-w --field-width <p>enter the field width as follows: :, enter -w? or -w \"?\" for available fields","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-e-end","title":"-E --end <p>enter end datetime filter</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-h-human-readable","title":"-H --human-readable <p>abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-i-get-inactive","title":"-I --get-inactive <p>include inactive allocations</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-o-get-only-inactive","title":"-O --get-only-inactive <p>get only inactive allocations</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-s-start","title":"-S --start <p>enter start datetime filter</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-t-type","title":"-T --Type <p>enter type of transaction</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-all-charges","title":"--all-charges <p>for list allocations | projects | users, only show info with charges</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-at","title":"--at <p>enter transaction-created datetime filter</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-award-category","title":"--award-category <p>enter allocation award category</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-award-type-name","title":"--award-type-name <p>enter allocation award-type name</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-created","title":"--created <p>enter created datetime filter</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-debug","title":"--debug <p>enter debug level</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-get-deleted","title":"--get-deleted <p>get deleted objects</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-get-not-charged","title":"--get-not-charged <p>get jobs that have not been charged</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-get-only-deleted","title":"--get-only-deleted <p>get only deleted objects</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-history-date-range","title":"--history-date-range <p>enter history datetime filter</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-home-dir","title":"--home-dir <p>enter the directory to store the pbs meta file</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-ignore-pbs-files","title":"--ignore-pbs-files <p>all new pbs files will be ignored and marked as processed</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-last-updated","title":"--last-updated <p>enter last updated datetime filter</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-no-commas","title":"--no-commas <p>remove commas from comma-separated thousands</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-no-header","title":"--no-header <p>do not display header</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-no-history","title":"--no-history <p>do not display history information</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-no-rows","title":"--no-rows <p>do not display rows</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-no-sys-msg","title":"--no-sys-msg <p>do not display system message</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-no-totals","title":"--no-totals <p>do not display totals</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-queued","title":"--queued <p>enter queued datetime filter</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#more-option-explanations","title":"MORE OPTION EXPLANATIONS","text":"<p>For -a, -e, -f, -w, -j, -p, -r, -t, -u, -T, --award-categories, --award_type_names, --cbank_refs options:</p> <p>These options can be entered multiple times for different values or entered once for multiple values. </p> <p>Examples: </p> <ol> <li> <p>sbank-list-allocation -u \"pershey rojas allcock\" or &gt; sbank-list-allocation -u pershey -u rojas -u allcock </p> </li> <li> <p>sbank-list-allocation -f \"id p avail\" or &gt; sbank-list-allocation -f id -f p -f avail For -u, -p and -r the use of wild card \"*\" is allowed, but only on names, not ids: </p> </li> </ol> <p>Examples: </p> <ol> <li>The following command will find allocations for users whose names start with \"pers\" and also users rojas and allcock. &gt; sbank-list-allocation -u \"pers* rojas allcock\" </li> <li>The following command will find allocations for projects that contain \"ratio\" in the name. &gt; sbank-list-allocation -p ratio </li> <li>The following command will find allocations for projects that end with \"tion\" in the name. &gt; sbank-list-allocation -p *tion </li> <li>The following command will find allocations for projects that start with \"ab\" and end with \"ng\" in the name. &gt; sbank-list-allocation -p ab*ng</li> </ol> <p>For -f option: This option is the display field option. </p> <p>To get the available fields enter -f? or -f \"?\". Default fields columns will be displayed if no field option is specified. </p> <p>To replace the current fields to display, enter:  <pre><code>&gt; sbank-list-allocations ... -f \"FIELD[:WIDTH]...FIELD[:WIDTH]\" or &gt; sbank-list-allocations ... -f FIELD[:WIDTH] ... -f FIELD[:WIDTH] \n</code></pre></p> <p>If you wish to add fields to the default fields, enter one + symbol anywhere in the quoted string:  <pre><code>&gt; sbank-list-allocations ... -f \"+ FIELD[:WIDTH]...FIELD[:WIDTH]\", only one + symbol is needed.\n</code></pre></p> <p>The fields will be displayed in table format and in the order entered in the command line. You can specify the field width, where WIDTH can be positive or negative value. Left alignment use -, right alignment use + or nothing.</p> <p>For -w option:</p> <p>FIELD:WIDTH, if the field is displayed it will change the width for the specified field. </p> <p>NOTE: This will not add the field as in -f option, only change the width. To get available fields you can also use -w? or -w \"?\" as in -f option.</p> <p>For -S, -E, --created, --queued, --last-updated, --history-date-range options:</p> <p>These are the date filter options. All dates are treated as UTC. </p> <p>You can use any reasonable date string that resembles a date Ambiguous dates will be parsed with the following parsing precedence: **YEAR then MONTH then DAY **</p> <p>For example, 10-11-12 or 101112 will be the following date: Oct. 11, 2012 Not: Nov. 12, 2010 or Nov. 10, 2012 </p> <p>Or you can specify a single date as follows:  <pre><code>\"[OPER]UTC_DATE\" You can specify a date range as follows: \n\"[OPER1]UTC_DATE1...[OPER2]UTC_DATE2\" Where OPER can be one of the following operators: \"==\", \"&gt;=\", \"&lt;=\", \"&gt;\", \"&lt;\" or \"eq\", \"ge\", \"le\", \"gt\", \"lt\" \n</code></pre></p> <p>Note: The following defaults for OPER, OPER1, OPER2 for the following options:  <pre><code>Options OPER OPER1 OPER2 ------------------------- ---- ----- ----- -E, &lt; &gt;= &lt; -S, &gt;= &gt;= &lt; --at &gt;= &gt;= &lt; --created &gt;= &gt;= &lt; --eligible &gt;= &gt;= &lt; --last-updated &gt;= &gt;= &lt; --queued &gt;= &gt;= &lt; \n</code></pre></p> <p>You can also use the following key letters \"n\", \"t\", \"d\", \"w\", \"y\" as follows:  <pre><code>KEY SYNTAX DEFINITIONS ---------- ----------- n[ow] now, where \"now\" is current-date current-time UTC t[oday] today, where \"today\" is current-date 00:00:00 UTC [+/-]d specified \"number\" of +/- days from \"today\" in UTC [+/-]w specified \"number\" of +/- weeks from \"today\" in UTC [+/-]y specified \"number\" of +/- years from \"today\" in UTC\n</code></pre></p> <p>For -T option:</p> <p>Transaction type option. The following are the valid transaction types and their explanation: CHARGE filter on job charges PULLBACK filter on allocation pullbacks DEPOSIT filter on allocation deposits REFUND filter on job refunds VOID filter on void transactions</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#invocation","title":"INVOCATION","text":"<p>sbank sbank sbank sbank-detail sbank detail sbank d sbank-detail-allocations sbank detail allocations sbank d a sbank-detail-jobs sbank detail jobs sbank d j sbank-detail-projects sbank detail project sbank d p sbank-detail-transactions sbank detail transactions sbank d t sbank-detail-users sbank detail users sbank d u sbank-list sbank list sbank l sbank-list-allocations sbank list allocations sbank l a sbank-list-jobs sbank list jobs sbank l j sbank-list-projects sbank list projects sbank l p sbank-list-transactions sbank list transactions sbank l t sbank-list-users sbank list users sbank l u</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#environment-variables","title":"ENVIRONMENT VARIABLES","text":"<p>Command line default options: Define the following environment variables as you would in the command line. Once the environment variable is defined, it will be used as the default options and arguments for the specific command. Command line options will take precedence.</p> <p>sbank_DETAIL_ALLOCATIONS_ARGS</p> <p>Default arguments and options for sbank-detail-allocations.</p> <p>sbank_DETAIL_CATEGORIES_ARGS</p> <p>Default arguments and options for sbank-detail-categories.</p> <p>sbank_DETAIL_NAMES_ARGS</p> <p>Default arguments and options for sbank-detail-names.</p> <p>sbank_DETAIL_MESSAGES_ARGS</p> <p>Default arguments and options for sbank-detail-messages.</p> <p>sbank_DETAIL_JOBS_ARGS</p> <p>Default arguments and options for sbank-detail-jobs.</p> <p>sbank_DETAIL_PROJECTS_ARGS</p> <p>Default arguments and options for sbank-detail-projects.</p> <p>sbank_DETAIL_TRANSACTIONS_ARGS</p> <p>Default arguments and options for sbank-detail-transactions.</p> <p>sbank_DETAIL_USERS_ARGS</p> <p>Default arguments and options for sbank-detail-users.</p> <p>sbank_LIST_ALLOCATIONS_ARGS</p> <p>Default arguments and options for sbank-list-allocations.</p> <p>sbank_LIST_JOBS_ARGS</p> <p>Default arguments and options for sbank-list-jobs.</p> <p>sbank_LIST_PROJECTS_ARGS</p> <p>Default arguments and options for sbank-list-projects.</p> <p>sbank_LIST_TRANSACTIONS_ARGS</p> <p>Default arguments and options for sbank-list-transactions.</p> <p>sbank_LIST_USERS_ARGS</p> <p>Default arguments and options for sbank-list-users.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#examples","title":"EXAMPLES <p>Example 1: -f, --field <pre><code>&gt; sbank-list-transactions ... -f field1:-20 -f field2:20 -f field3 or &gt; sbank-list-transactions ... -f \"field1:-20 field2:20 field3\" \n</code></pre> Explanation: Fields will be displayed in order of appearance, where field1:-20 means 20 characters long, left align; where field2:20 means 20 characters long, right align; where field3 uses default sizes. Number fields default to right aligned. Text fields default to left aligned.</p> <p>Example 2: -S, -E, --created, --queued, --last-updated, --history-start, --history-end</p> <p>Single date-string examples: </p> <ul> <li>  <p>sbank-list-allocations -S \"&gt;=Oct 11, 2014\" start dates that are &gt;= \"2014-10-11 00:00:00\" </p>  </li> <li>  <p>sbank-list-allocations -S \"&lt;=2014-11-10\" start dates that are &lt;= \"2014-11-10 00:00:00\" </p>  </li> <li>  <p>sbank-list-allocations -E \"&lt;20141110\" end dates that are &lt; \"2014-11-10 00:00:00\" </p>  </li> <li>  <p>sbank-list-allocations -E \"22:30:10\" end dates that are &lt; \" 22:30:10\"    <li>  <p>sbank-list-allocations -S \"&gt;today\" start dates that are &gt; \" 00:00:00\"    <li>  <p>sbank-list-allocations -E t end dates that are &lt; \" 00:00:00\"    <li>  <p>sbank-list-allocations -S gtnow start dates that are &gt; \" \"    <li>  <p>sbank-list-allocations -E len end dates that are &lt;= \" \"    <li>  <p>sbank-list-allocations -S \"1d\" start dates that are &gt;= \"today +1 day\" </p>  </li> <li>  <p>sbank-list-allocations -E \"-2w\" end dates that are &lt; \"today -2 weeks\" </p>  </li> <li>  <p>sbank-list-allocations -S \"&gt;=1y\" start dates that are &gt;= \"today +1 year\" </p>  </li> <li>  <p>sbank-list-allocations -S \"&gt;2012\" start dates that are &gt; \"2012-- 00:00:00\"     <p>Range date-string examples: </p> <ul> <li>  <p>sbank-list-allocations -S \"2013-01-01...2014-01-01\" \"2013-01-01\" &lt;= DATES &lt; \"2014-01-01\" </p>  </li> <li>  <p>sbank-list-allocations -S \"-1y...t\" \"today -1 year\" &lt;= DATES &lt; \"today\" </p>  </li> <li>  <p>sbank-list-allocations -E \"2013...t\"\" \"2013--\" &lt;= DATES &lt; \"today\"    <li>  <p>sbank-list-allocations -E \"&gt;2013...&lt;=t\"\" \"2013--\" &lt; DATES &lt;= \"today\"    <p>Example 3: Command invocation examples</p> <ul> <li>  <p>sbank-list-projects list projects full command invocation </p>  </li> <li>  <p>sbank list projects list projects meta command invocation</p>  </li> <li>  <p>sbank s p list projects partial meta command invocation </p>  </li> <li>  <p>sbank p list projects where \"list\" is the default</p>  </li> <li>  <p>sbank list allocations is the default </p>  </li> <li>  <p>sbank a list allocations \"list\" is the default </p>  </li> <li>  <p>sbank s a list allocations partial meta command invocation</p>  </li> </ul> <p>Example 4: -h, --help</p> <ul> <li>  <p>sbank -h will give you help summary on all of sbank </p>  </li> <li>  <p>sbank list --help will give you help on all the \"list\" commands </p>  </li> <li>  <p>sbank list allocations -h will give you help on the \"list allocations\" command</p>  </li> <li>  <p>sbank-list-allocations -h will give you help on the \"list allocations\" command </p>  </li> <li>  <p>sbank l a --help will give you help on the \"list allocations\" command</p>  </li> </ul>","text":""},{"location":"account-project-management/project-management/project-reports/","title":"Quarterly and Year-End Reporting","text":"<p>The Argonne Leadership Computing Facility (ALCF) is required to report the progress and scientific accomplishments of all peer-reviewed projects.</p> <p>PIs of INCITE, ALCC, and ADSP projects are required to complete quarterly reports and a final end-of-project (EOY/EOP) report.</p>"},{"location":"account-project-management/project-management/project-reports/#due-dates","title":"Due dates","text":""},{"location":"account-project-management/project-management/project-reports/#due-dates-for-the-2024-incite-quarterly-eoy-and-the-eop-reports","title":"Due dates for the 2024 INCITE quarterly, EOY, and the EOP reports:","text":"<ul> <li>April 1, 2024 (CY2024 - Q1)</li> <li>July 1, 2024 (CY2024 - Q2)</li> <li>October 1, 2024 (CY2024 - Q3)</li> <li>January 1, 2025 (CY2025 - EOY) or February 15, 2025 (entire allocation period - EOP)</li> </ul>"},{"location":"account-project-management/project-management/project-reports/#due-dates-for-the-2023-2024-alcc-quarterly-and-the-eop-reports","title":"Due dates for the 2023-2024 ALCC quarterly and the EOP reports:","text":"<ul> <li>October 1, 2023 (CY2023 - Q3)</li> <li>January 1, 2024 (CY2024 - Q4)</li> <li>April 1, 2024 (CY2024 - Q1)</li> <li>August 15, 2024 (CY2024 - EOP)</li> </ul>"},{"location":"account-project-management/project-management/project-reports/#penalties","title":"Penalties","text":"<p>If a quarterly report is more than 30 days late:</p> <ul> <li>The ability to submit jobs for the PI and users of the late project will be disabled.</li> </ul> <p>If a quarterly report is more than 90 days late:</p> <ul> <li>The PI and users of the late project will have their accounts disabled.</li> </ul> <p>These penalties will be removed within three business days after the late quarterly or EOY report is submitted.</p>"},{"location":"account-project-management/project-management/project-reports/#alcc-specific-penalties","title":"ALCC Specific Penalties:","text":"<p>A similar penalty will also be applied to new ALCC projects with the same PI or co-PIs that have failed to submit the EOP report for a previous ALCC project. If the EOP report is more than 15 days late:</p> <ul> <li>The new ALCC project will be blocked. For a currently active ALCC project, the ability to submit jobs will be disabled for the project and all sub-projects. For a project that has not been created yet, the process for new project creation will be halted.</li> </ul>"},{"location":"account-project-management/project-management/project-reports/#appeals","title":"Appeals","text":"<p>A PI or user may appeal a project or account suspension to the ALCF Director by a request to support at alcf.anl.gov.</p>"},{"location":"account-project-management/project-management/project-reports/#report-templates","title":"Report Templates","text":"<p>Templates for the quarterly and the EOY reports can be found at the links at the bottom of this page.</p> <p>Please modify the filename to replace PINAME with the last name of the PI of the INCITE/ALCC project, ALLOCATION to INCITE/ALCC, and YEAR to the corresponding calendar year. For quarterly reports, please replace the X in the filename with the quarter number.</p> <p>For example, for a project with PI 'Joe Smith' that is submitting the quarterly report for the first quarter in the 2023-2024 cycle for ALCC, the filename will be Smith_ALCC_Q1.docx.</p> <p>For an EOY report, replace YEARS with the years associated with your allocation. For example, an ALCC 2023-2024 project with PI 'Joe Smith' would have a filename of Smith_ALCC_2023-2024_EOY.docx.</p>"},{"location":"account-project-management/project-management/project-reports/#templates-for-incite-and-alcc","title":"Templates for INCITE and ALCC:","text":"<ul> <li>Quarterly Report Template</li> <li>End of Project Report Template</li> <li>End of Year Report Template</li> </ul>"},{"location":"account-project-management/project-management/starting-alcf-award/","title":"Starting Your ALCF Award","text":"<p>The following guide is for PIs and Proxies to get insight into managing projects and teams for ALCF awards. Please submit questions or trouble tickets to support@alcf.anl.gov.</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#get-started-with-alcfs-systems","title":"Get Started with ALCF\u2019s Systems","text":"<p>To get started using our resources, please visit:  Connect &amp; Login</p> <p>We also encourage you to take full advantage of ALCF's training programs and user services. Some useful introductory materials and videos are listed below:</p> <ul> <li>Running on Polaris </li> <li>Lustre File Striping Basics</li> <li>Community Data Sharing with ACDC (using Eagle)</li> </ul>"},{"location":"account-project-management/project-management/starting-alcf-award/#project-terminology","title":"Project Terminology","text":"<p>Before your project begins, you will receive an email with the following project information:</p> <ul> <li>Project Short Name: The assigned, shortened name for your project. This will be the name that you\u2019ll use to access your project on the systems.</li> <li>Project Proxies: Project members designated by PIs that are authorized to add or renew project members on your behalf.</li> <li>Allocation System(s) and Allocation Amount: The approved system(s) and amount of your award in node hours.</li> <li>Approved Quota: The approved amount of disk space for your project directory.</li> <li>File System: The file system where your project directory will reside. For information on the Eagle file system, see Storage and Networking.</li> <li>Assigned Catalyst: INCITE projects will have ALCF staff members that are assigned to the projects who are available to assist the team throughout the duration of the INCITE allocation.</li> <li>Allocation Start Date: The start date of your award.</li> <li>Allocation End Date: The end date of your award.</li> </ul>"},{"location":"account-project-management/project-management/starting-alcf-award/#account-setup","title":"Account Setup","text":"<p>If you do not have an ALCF account: You will need to request one at https://my.alcf.anl.gov/accounts/#/accountRequest. When prompted for the project name, please select the project short name you were given in your award email from support@alcf.anl.gov.</p> <p>If you have an active ALCF account: Submit a request to join the newly awarded project at https://my.alcf.anl.gov/.</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#information-for-foreign-national-access","title":"Information for Foreign National Access","text":"<p>The U.S. Department of Energy has guidelines and requirements for foreign nationals who access its facilities and sites. This guidance is issued in DOE Order 142.3, which is part of Argonne's contract; therefore, all foreign nationals (non-U.S. Citizens) must obtain authorization prior to using ALCF resources.</p> <p>If you are a foreign national and do not have current authorization credentials, you are required to submit an ANL-593 (Foreign National Access Request) form. It is critical that identity documentation requests sent by ALCF staff are completed as early as possible to facilitate timely processing for your account approval.</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#user-agreement-for-incite-alcc-and-adsp","title":"User Agreement for INCITE, ALCC, and ADSP","text":"<p>Note: This does not apply to Director's Discretionary awards.</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#institution-master-agreement-for-incite-alcc-and-adsp","title":"Institution Master Agreement for INCITE, ALCC, and ADSP","text":"<p>If you are not an employee of Argonne National Laboratory, a user agreement must be signed by your home institution to perform research at Argonne\u2019s user facilities. This policy applies to every member of the project team who will be conducting research on ALCF resources.</p> <p>A list of home institutions that have master agreements in place is located on this webpage: https://www.aps.anl.gov/Users-Information/Legal-Financial/Argonne-User-Facility-Agreements</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#alcf-user-agreement-for-incite-alcc-and-adsp","title":"ALCF User Agreement for INCITE, ALCC, and ADSP","text":"<p>Note: This does not apply to Director's Discretionary awards.</p> <p>Every project team member who requests an ALCF account must sign and return an acknowledgment form, stating that they agree to the terms in the user agreement.</p> <p>The form is located at: https://www.alcf.anl.gov/files/Acknowledgement_Form.pdf. Please print, sign, scan, and email it to accounts@alcf.anl.gov.</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#managing-project-team-membership","title":"Managing Project Team Membership","text":"<p>As a PI, you can add members to your project. You can assign proxies who are project members authorized to add or renew project members on your behalf.</p> <p>A project PI or proxy has the authority to:</p> <ul> <li>Approve and renew accounts</li> <li>Add and delete users to/from the project</li> <li>Approve Foreign Assignment/Visit Request form renewals for project members who are foreign nationals</li> </ul> <p>During your project setup, the ALCF Support Team will request the following information to establish your project members:</p> <ul> <li>The names, email addresses, and/or ALCF usernames (if already existing) of up to two proxies and all project members.</li> </ul>"},{"location":"account-project-management/project-management/starting-alcf-award/#about-project-and-unix-group-membership","title":"About Project and UNIX Group Membership","text":"<p>All project members have the ability to run jobs against your allocation. There is no limit to the number of project members you may authorize. Project members are automatically added to the project UNIX group, giving them the ability to write to the project directory and to access project data. When a project member is added or removed from a project, this will automatically be reflected in the project UNIX group membership.</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#adding-project-members","title":"Adding Project Members","text":"<p>The PI or a proxy must approve each team member to access ALCF resources and run jobs on their project. PI/proxies can respond to emails from ALCF for account access approval with a \"yes\" or \"no\".</p> <p>PI/proxies with active ALCF accounts can also approve new account requests, project membership requests, account reactivation requests, and add existing active ALCF users to the project by logging into the ALCF Account and Project Management application.</p> <p>Note: If PI/proxies need to request an ALCF account, see the section below for instructions on \"how to apply\" for an account.</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#accounts-and-access-for-your-project-members","title":"Accounts and Access for your Project Members","text":"<p>All project members will need an ALCF user account to access project data and to run jobs on ALCF systems.</p> <p>Members that do not have an ALCF account should request one at: https://my.alcf.anl.gov/accounts/#/accountRequest. When prompted for the project name, they should select your project short name.</p> <p>Members with ALCF accounts that are no longer active should submit a reactivation request here: https://my.alcf.anl.gov/accounts/#/accountReactivate. When prompted for the project name, they should select your project short name.</p> <p>Members with active ALCF accounts but have not been added to your project should submit a request to join your project by going to this page: https://my.alcf.anl.gov/. They should search for your project and click the \"Request Membership\" button for that project.</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#moving-your-data","title":"Moving Your Data","text":"<p>We encourage you to use Globus to move your project data to your ALCF project directory before your allocation begins. For details, see Using Globus.</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#project-status-reports-for-incite-alcc-and-adsp","title":"Project Status Reports for INCITE, ALCC, and ADSP","text":"<p>Note: PIs that are awarded a Director's Discretionary will not receive weekly status project reports.</p> <p>Shortly after your allocation begins, we will begin sending you a weekly project status report via support@alcf.anl.gov to keep you informed of your award progress.</p> <p>Look for an email from us with the subject line: ALCF [ALLOCATION PROGRAM] Project Status Report for [PROJECT SHORT NAME]</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#reporting-requirements-for-incite-alcc-and-adsp","title":"Reporting Requirements for INCITE, ALCC, and ADSP","text":"<p>Note: PIs that are awarded a Director's Discretionary allocation are not required to submit project reports.</p> <p>If you received an INCITE, ALCC, or ADSP allocation award, quarterly reporting is required to keep DOE informed of progress related to your allocation.</p> <p>The ALCF will send you a report template at the end of each quarter. Please complete the report promptly and submit it via email to support@alcf.anl.gov. For more information, see the Quarterly Report webpage.</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#policies","title":"Policies","text":""},{"location":"account-project-management/project-management/starting-alcf-award/#pullback-policy","title":"Pullback Policy","text":"<p>Please be aware that we will periodically monitor, and could potentially adjust, your project allocation if a large portion of it goes unused. You may view: Pullback Policy</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#allocation-overburn-policy","title":"Allocation Overburn Policy","text":"<p>Please see this page for overburn/overuse eligibility for INCITE projects that have exhausted their allocation in the first 11 months of its allocation year: Allocation Overburn</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#acknowledgment-in-publications","title":"Acknowledgment In Publications","text":"<p>Please follow the guidelines provided on the ALCF Acknowledgement Policy page to properly acknowledge the use of ALCF resources in all of your publications, both online and print.</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#facility-policies","title":"Facility Policies","text":"<p>Facility policies have been established to provide consistent and reliable services. Please read about our ALCF Facility Policies.</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#useful-allocation-and-quota-commands","title":"Useful Allocation and Quota Commands","text":"<p>We have an allocation management tool called sbank, and below are a few helpful sbank commands.  </p> <ul> <li><code>myprojectquotas</code>: Log into Polaris and type this command to view the project directory quotas for all your projects.</li> <li><code>myquota</code>: Log into Polaris and type this command to view your home directory quota.</li> </ul> <p>You can use the following command to check your project balance on Polaris: - <code>sbank-list-allocations -p &lt;Project Shortname&gt; -r &lt;system name&gt;</code></p> <p>For more command examples and details, see sbank.</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#how-can-we-help","title":"How Can We Help?","text":"<p>We can also help resolve any issues or needs that may be delaying the start of your scientific campaign. - Are you in need of high-throughput software? - Are you having difficulty compiling your application? - Does your code have limited restart capabilities?</p> <p>If your project allocation usage is being held back for reasons due to one of our systems, please contact us for assistance by emailing support@alcf.anl.gov.</p>"},{"location":"account-project-management/project-management/team-management/","title":"Managing Your Team Members","text":"<p>The PI or Proxy must approve each member of the team to gain access and to run project jobs on the ALCF's resources. If you have an active ALCF account, you can manage your project team by logging into the ALCF account and project management website and navigating to https://my.alcf.anl.gov/</p> <p>Project members will need to have an active ALCF user account to access project data and to run jobs on ALCF systems. See Accounts and Access for your Project Members for information on how team members can get an account, reactivate an account, or request to join your project.</p>"},{"location":"account-project-management/project-management/team-management/#accessing-your-projects","title":"Accessing your project(s)","text":"<ol> <li>Log in at https://my.alcf.anl.gov/ using your credentials (ALCF username and Physical/Mobile token passcode one-time passcode).</li> <li>You will see a list of projects of which you are the Primary Investigator (PI).</li> <li>Click on the desired project to view information and management options for the selected project.</li> </ol>"},{"location":"account-project-management/project-management/team-management/#modifying-project-information","title":"Modifying project information","text":"<p>Some project information cannot be modified, but as the PI, you can modify the following: project title, institutions, and associated funding.</p> <p>Your project can be associated with multiple institutions, but you must specify a primary institution.</p>"},{"location":"account-project-management/project-management/team-management/#managing-project-members-with-an-existing-alcf-account","title":"Managing project members with an Existing ALCF Account","text":"<ol> <li>You can manage the membership for your project by clicking on the desired project from the Project Management screen.</li> <li>Add and/or remove proxies and team members by clicking on the red \"Remove\" button to the right of each member or clicking on \"Add new user.\"</li> <li>You can view account information for each user as it relates to the project:</li> <li>Account Status</li> <li>Project Role</li> <li>Proxy Permissions</li> <li> <p>Membership Status</p> </li> <li> <p>Proxies are individuals authorized to add or renew user accounts for the project PI. You have the ability to upgrade a user from a member to a Proxy, by clicking on the \"Proxy\" radio button that corresponds with the desired member.</p> </li> </ol>"},{"location":"ai-testbed/getting-started/","title":"ALCF AI Testbed","text":"<p>The ALCF AI Testbed houses some of the most advanced AI accelerators for scientific research. </p> <p>The goal of the testbed is to enable explorations into next-generation machine learning applications and workloads, enabling the ALCF and its user community to help define the role of AI accelerators in scientific computing and how to best integrate such technologies with supercomputing resources.</p> <p>The AI accelerators complement the ALCF's current and next-generation supercomputers to provide a state-of-the-art computing environment that supports pioneering research at the intersection of AI, big data, and high performance computing (HPC). </p> <p>The platforms are equipped with architectural features that support AI and data-centric workloads, making them well suited for research tasks involving the growing deluge of scientific data produced by powerful tools, such as supercomputers, light sources, telescopes, particle accelerators, and sensors. In addition, the testbed will allow researchers to explore novel workflows that combine AI methods with simulation and experimental science to accelerate the pace of discovery.</p>"},{"location":"ai-testbed/getting-started/#how-to-get-access","title":"How to Get Access","text":"<p>Researchers interested in using the AI Testbed\u2019s <code>Cerebras CS-2</code>, <code>SambaNova DataScale SN30</code>, <code>Graphcore Bow Pod64</code> and <code>GroqRack</code> platforms can now submit project proposals via the ALCF\u2019s Director\u2019s Discretionary program. Access to additional testbed resources, including <code>Habana</code> accelerators, will be announced at a later date. </p> <p>Submit your proposal requests at: Allocation Request Page</p>"},{"location":"ai-testbed/getting-started/#getting-started","title":"Getting Started","text":"<ol> <li> <p>Request a Director's Discretionary project on SambaNova/Cerebras/Graphcore/Groq.</p> </li> <li> <p>Apply for an ALCF account after the project request is approved. Choose the SambaNova/Cerebras/Graphcore/Groq project that your PI has created at ALCF. If you have an active ALCF account, request to join the project after your project is approved.</p> </li> <li> <p>Transfer data to ALCF using Globus after your account has been created.</p> <p>a. The endpoint for your data in ALCF is <code>alcf#ai_testbed_projects</code> with the path to your project being  <code>/&lt;project name&gt;</code>. </p> <p>b. The endpoint for your home directory on the AI Testbeds in ALCF is <code>alcf#ai_testbed_home</code>.</p> </li> <li> <p>Add/invite team members to your ALCF project on SambaNova/Cerebras/Graphcore/Groq. </p> </li> </ol>"},{"location":"ai-testbed/getting-started/#how-to-contribute-to-documentation","title":"How to Contribute to Documentation","text":"<p>The documentation is based on MkDocs and source files are on GitHub. You can contribute to the documentation by creating a pull request. </p> <p>Learn more on how to contribute to documentation.</p>"},{"location":"ai-testbed/cerebras/customizing-environment/","title":"Customizing Environments","text":""},{"location":"ai-testbed/cerebras/customizing-environment/#using-virtual-python-environments","title":"Using virtual Python environments","text":""},{"location":"ai-testbed/cerebras/customizing-environment/#to-make-a-pytorch-virtual-environment-for-cerebras","title":"To make a PyTorch virtual environment for Cerebras","text":"<pre><code>mkdir ~/R_2.4.0\ncd ~/R_2.4.0\n# Note: \"deactivate\" does not actually work in scripts.\ndeactivate\nrm -r venv_cerebras_pt\n/software/cerebras/python3.8/bin/python3.8 -m venv venv_cerebras_pt\nsource venv_cerebras_pt/bin/activate\npip install --upgrade pip\npip install cerebras_pytorch==2.4.0\npip install --editable git+https://github.com/Cerebras/modelzoo#egg=cerebras_modelzoo 'murmurhash==1.0.10' 'thinc==8.2.2' 'cymem&lt;2.0.10'\n</code></pre>"},{"location":"ai-testbed/cerebras/customizing-environment/#activation-and-deactivation","title":"Activation and deactivation","text":"<p>To activate a virtual environments</p> <pre><code>source ~/R_2.4.0/venv_cerebras_pt/bin/activate\n</code></pre> <p>To deactivate a virtual environment,</p> <pre><code>deactivate\n</code></pre>"},{"location":"ai-testbed/cerebras/example-programs/","title":"Example Programs","text":""},{"location":"ai-testbed/cerebras/example-programs/#use-a-local-copy-of-the-model-zoo","title":"Use a local copy of the model zoo","text":"<p>Make a working directory and a local copy of the Cerebras modelzoo and anl_shared repository, if not previously done, as follows.</p> <pre><code>mkdir ~/R_2.4.0\ncd ~/R_2.4.0\ngit clone https://github.com/Cerebras/modelzoo.git\ncd modelzoo\ngit tag\ngit checkout Release_2.4.0\n</code></pre>"},{"location":"ai-testbed/cerebras/example-programs/#bert-pytorch","title":"BERT - PyTorch","text":"<p>The modelzoo/modelzoo/transformers/pytorch/bert directory is a PyTorch implementation of BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding This BERT-large msl128 example uses a single sample dataset for both training and evaluation. See the README.md in the source directory for details on how to build a dataset from text input. First, source a Cerebras PyTorch virtual environment and make sure that the requirements are installed:</p> <pre><code>source ~/R_2.4.0/venv_cerebras_pt/bin/activate\npip install -r ~/R_2.4.0/modelzoo/requirements.txt\n</code></pre> <p>Then</p> <p><pre><code>cd ~/R_2.4.0/modelzoo/src/cerebras/modelzoo/models/nlp/bert\ncp /software/cerebras/dataset/bert_large/bert_large_MSL128_sampleds.yaml configs/bert_large_MSL128_sampleds.yaml\nexport MODEL_DIR=model_dir_bert_large_pytorch\nif [ -d \"$MODEL_DIR\" ]; then rm -Rf $MODEL_DIR; fi\npython run.py CSX --job_labels name=bert_pt --params configs/bert_large_MSL128_sampleds.yaml --num_workers_per_csx=1 --mode train --model_dir $MODEL_DIR --mount_dirs /home/ /software/ --python_paths /home/$(whoami)/R_2.4.0/modelzoo/src --compile_dir $(whoami) |&amp; tee mytest.log\n</code></pre> Note: the vocabulary file referenced in <code>/software/cerebras/dataset/bert_large/bert_large_MSL128_sampleds.yaml</code> is the same as the one at <code>/home/$(whoami)/R_2.4.0/modelzoo/modelzoo/transformers/vocab/google_research_uncased_L-12_H-768_A-12.txt</code>. </p> <p>The last parts of the output should resemble the following, with messages about cuda that should be ignored and are not shown.</p> <pre><code>2023-11-29 20:07:49,284 INFO:   Beginning appliance run\n2023-11-29 20:08:14,365 INFO:   | Train Device=CSX, Step=100, Loss=9.50000, Rate=4088.28 samples/sec, GlobalRate=4088.26 samples/sec\n2023-11-29 20:08:39,820 INFO:   | Train Device=CSX, Step=200, Loss=8.37500, Rate=4048.91 samples/sec, GlobalRate=4055.21 samples/sec\n2023-11-29 20:09:05,356 INFO:   | Train Device=CSX, Step=300, Loss=7.96875, Rate=4025.61 samples/sec, GlobalRate=4040.05 samples/sec\n2023-11-29 20:09:30,626 INFO:   | Train Device=CSX, Step=400, Loss=7.56250, Rate=4041.61 samples/sec, GlobalRate=4043.10 samples/sec\n2023-11-29 20:09:56,022 INFO:   | Train Device=CSX, Step=500, Loss=7.50000, Rate=4035.92 samples/sec, GlobalRate=4040.90 samples/sec\n2023-11-29 20:10:21,410 INFO:   | Train Device=CSX, Step=600, Loss=7.37500, Rate=4034.41 samples/sec, GlobalRate=4039.65 samples/sec\n2023-11-29 20:10:46,690 INFO:   | Train Device=CSX, Step=700, Loss=7.37500, Rate=4044.10 samples/sec, GlobalRate=4041.20 samples/sec\n2023-11-29 20:11:12,004 INFO:   | Train Device=CSX, Step=800, Loss=7.25000, Rate=4044.75 samples/sec, GlobalRate=4041.70 samples/sec\n2023-11-29 20:11:37,196 INFO:   | Train Device=CSX, Step=900, Loss=7.21875, Rate=4056.77 samples/sec, GlobalRate=4044.25 samples/sec\n2023-11-29 20:12:02,285 INFO:   | Train Device=CSX, Step=1000, Loss=7.12500, Rate=4071.60 samples/sec, GlobalRate=4047.95 samples/sec\n2023-11-29 20:12:02,286 INFO:   Saving checkpoint at step 1000\n2023-11-29 20:12:37,079 INFO:   Saved checkpoint model_dir_bert_large_pytorch/checkpoint_1000.mdl\n2023-11-29 20:13:25,683 INFO:   Heartbeat thread stopped for wsjob-gfi2baioyfduozkmgsc6a7.\n2023-11-29 20:13:25,691 INFO:   Training completed successfully!\n2023-11-29 20:13:25,691 INFO:   Processed 1024000 sample(s) in 336.373620536 seconds.\n</code></pre>"},{"location":"ai-testbed/cerebras/example-programs/#gpt-j-pytorch","title":"GPT-J PyTorch","text":"<p>GPT-J [github] is an auto-regressive language model created by EleutherAI. This PyTorch GPT-J 6B parameter pretraining sample uses 2 CS2s.</p> <p>First, source a Cerebras PyTorch virtual environment and make sure that the requirements are installed:</p> <pre><code>source ~/R_2.4.0/venv_cerebras_pt/bin/activate\npip install -r ~/R_2.4.0/modelzoo/requirements.txt\n</code></pre> <p>Then</p> <pre><code>cd ~/R_2.4.0/modelzoo/src/cerebras/modelzoo/models/nlp/gptj\ncp /software/cerebras/dataset/gptj/params_gptj_6B_sampleds.yaml configs/params_gptj_6B_sampleds.yaml\nexport MODEL_DIR=model_dir_gptj\nif [ -d \"$MODEL_DIR\" ]; then rm -Rf $MODEL_DIR; fi\npython run.py CSX --job_labels name=gptj_pt --params configs/params_gptj_6B_sampleds.yaml --num_csx=2 --mode train --model_dir $MODEL_DIR --mount_dirs /home/ /software --python_paths /home/$(whoami)/R_2.4.0/modelzoo/src --compile_dir $(whoami) |&amp; tee mytest.log\n</code></pre> <p>The last parts of the output should resemble the following:</p> <pre><code>2023-11-29 20:59:19,223 INFO:   Beginning appliance run\n2023-11-29 21:03:53,875 INFO:   | Train Device=CSX, Step=100, Loss=8.43750, Rate=43.70 samples/sec, GlobalRate=43.70 samples/sec\n2023-11-29 21:08:28,779 INFO:   | Train Device=CSX, Step=200, Loss=8.12500, Rate=43.67 samples/sec, GlobalRate=43.67 samples/sec\n2023-11-29 21:08:28,781 INFO:   Saving checkpoint at step 200\n2023-11-29 21:13:56,695 INFO:   Saved checkpoint model_dir_gptj/checkpoint_200.mdl\n2023-11-29 21:14:30,135 INFO:   Heartbeat thread stopped for wsjob-kd4olqkhu6ya8qqzt88utd.\n2023-11-29 21:14:30,142 INFO:   Training completed successfully!\n2023-11-29 21:14:30,142 INFO:   Processed 24000 sample(s) in 910.883781998 seconds.\n</code></pre>"},{"location":"ai-testbed/cerebras/example-programs/#llama2-7b","title":"Llama2-7B","text":"<p>The Cerebras llama2 7B model implementation can be found at modelzoo/modelzoo/transformers/pytorch/llama and its overview at https://github.com/Cerebras/modelzoo/blob/main/src/cerebras/modelzoo/models/nlp/llama/README.md#configs-included-for-this-model. This set up will use a subset of pile data (preprocessed at path /software/datasets/llama_data_32K/) to train with a 32K vocab size. </p> <p>First, source a Cerebras PyTorch virtual environment and make sure that the requirements are installed: <pre><code>source ~/R_2.4.0/venv_cerebras_pt/bin/activate\npip install -r ~/R_2.4.0/modelzoo/requirements.txt\n</code></pre> Instructions for training: <pre><code>cd ~/R_2.4.0/modelzoo/src/cerebras/modelzoo/models/nlp/llama\ncp /software/cerebras/dataset/params_llama2_7b.yaml configs/params_llama2_7b.yaml\nexport MODEL_DIR=model_dir_llama2_7b\nif [ -d \"$MODEL_DIR\" ]; then rm -Rf $MODEL_DIR; fi\npython run.py CSX --job_labels name=llama2_7b --params configs/params_llama2_7b.yaml --num_csx=1 --mode train --model_dir $MODEL_DIR --mount_dirs /projects /home/ /software --python_paths /home/$(whoami)/R_2.4.0/modelzoo/src  --compile_dir $(whoami) |&amp; tee mytest.log\n</code></pre></p> <p>Please find a sample output <pre><code>2024-03-21 14:40:57,949 INFO:   Effective batch size is 99.\n2024-03-21 14:40:57,970 INFO:   Checkpoint autoloading is enabled. Looking for latest checkpoint in \"/srv/projects/datascience/vsastry/model_dir_llama/\" directory with the following naming convention: `checkpoint_(step)(_timestamp)?.mdl`.\n2024-03-21 14:40:57,971 INFO:   No checkpoints were found in \"/srv/projects/datascience/vsastry/model_dir_llama/\".\n2024-03-21 14:40:57,971 INFO:   No checkpoint was provided. Using randomly initialized model parameters.\n2024-03-21 14:40:59,419 INFO:   Saving checkpoint at step 0\n2024-03-21 14:48:46,988 INFO:   Saved checkpoint /srv/projects/datascience/vsastry/model_dir_llama/checkpoint_0.mdl\n2024-03-21 14:49:05,547 INFO:   Compiling the model. This may take a few minutes.\n2024-03-21 14:49:05,550 INFO:   Defaulted to use the job-operator namespace as the usernode config /opt/cerebras/config_v2 only has access to that namespace.\n2024-03-21 14:49:06,819 INFO:   Initiating a new image build job against the cluster server.\n2024-03-21 14:49:06,898 INFO:   Custom worker image build is disabled from server.\n2024-03-21 14:49:06,911 INFO:   Defaulted to use the job-operator namespace as the usernode config /opt/cerebras/config_v2 only has access to that namespace.\n2024-03-21 14:49:07,143 INFO:   Initiating a new compile wsjob against the cluster server.\n2024-03-21 14:49:07,226 INFO:   compile job id: wsjob-pg4gslxvgsalvh6ppdvydb, remote log path: /n1/wsjob/workdir/job-operator/wsjob-pg4gslxvgsalvh6ppdvydb\n2024-03-21 14:49:17,259 INFO:   Poll ingress status: Waiting for job running, current job status: Queueing, msg: job is queueing. Job queue status: current job is top of queue but likely blocked by running jobs, 1 compile job(s) running using 67Gi memory. For more information, please run 'csctl get jobs'.\n2024-03-21 15:02:07,673 INFO:   Poll ingress status: Waiting for job running, current job status: Queueing, msg: job is queueing. Job queue status: current job is top of queue but likely blocked by running jobs, 1 execute job(s) running using 1 system(s), 1 compile job(s) running using 67Gi memory. For more information, please run 'csctl get jobs'.\n2024-03-21 15:02:17,683 INFO:   Poll ingress status: Waiting for job service readiness.\n2024-03-21 15:02:47,717 INFO:   Ingress is ready: Job ingress ready, poll ingress success.\n2024-03-21 15:02:58,509 INFO:   Pre-optimization transforms...\n2024-03-21 15:03:14,815 INFO:   Optimizing layouts and memory usage...\n2024-03-21 15:03:14,839 INFO:   Gradient accumulation enabled\n2024-03-21 15:03:14,840 WARNING:   Gradient accumulation will search for an optimal micro batch size based on internal performance models, which can lead to an increased compile time. Specify `micro_batch_size` option in the 'train_input/eval_input' section of your .yaml parameter file to set the gradient accumulation microbatch size, if an optimal microbatch size is known.\n\n2024-03-21 15:03:14,842 INFO:   Gradient accumulation trying sub-batch size 3...\n2024-03-21 15:03:21,632 INFO:   Exploring floorplans\n2024-03-21 15:03:30,198 INFO:   Exploring data layouts\n2024-03-21 15:03:50,589 INFO:   Optimizing memory usage\n2024-03-21 15:05:23,008 INFO:   Gradient accumulation trying sub-batch size 33...\n2024-03-21 15:05:30,532 INFO:   Exploring floorplans\n2024-03-21 15:05:37,304 INFO:   Exploring data layouts\n2024-03-21 15:06:11,327 INFO:   Optimizing memory usage\n2024-03-21 15:11:37,204 INFO:   Gradient accumulation trying sub-batch size 9...\n2024-03-21 15:11:44,383 INFO:   Exploring floorplans\n2024-03-21 15:11:50,639 INFO:   Exploring data layouts\n2024-03-21 15:12:16,120 INFO:   Optimizing memory usage\n2024-03-21 15:15:59,788 INFO:   Gradient accumulation trying sub-batch size 11...\n2024-03-21 15:16:06,314 INFO:   Exploring floorplans\n2024-03-21 15:16:12,563 INFO:   Exploring data layouts\n2024-03-21 15:16:40,965 INFO:   Optimizing memory usage\n2024-03-21 15:21:03,938 INFO:   Exploring floorplans\n2024-03-21 15:21:10,918 INFO:   Exploring data layouts\n2024-03-21 15:22:03,953 INFO:   Optimizing memory usage\n2024-03-21 15:30:35,456 INFO:   No benefit from gradient accumulation expected. Compile will proceed at original per-box batch size 99 with 9 lanes\n\n2024-03-21 15:30:35,540 INFO:   Post-layout optimizations...\n2024-03-21 15:32:11,639 INFO:   Allocating buffers...\n2024-03-21 15:32:18,023 INFO:   Code generation...\n2024-03-21 15:32:53,573 INFO:   Compiling image...\n2024-03-21 15:32:53,578 INFO:   Compiling kernels\n2024-03-21 15:34:39,222 INFO:   Compiling final image\n2024-03-21 15:36:54,995 INFO:   Compile artifacts successfully written to remote compile directory. Compile hash is: cs_2599085507768189065\n2024-03-21 15:36:55,146 INFO:   Heartbeat thread stopped for wsjob-pg4gslxvgsalvh6ppdvydb.\n2024-03-21 15:36:55,160 INFO:   Compile was successful!\n2024-03-21 15:36:55,171 INFO:   Programming Cerebras Wafer Scale Cluster for execution. This may take a few minutes.\n2024-03-21 15:36:56,403 INFO:   Defaulted to use the job-operator namespace as the usernode config /opt/cerebras/config_v2 only has access to that namespace.\n2024-03-21 15:36:56,659 INFO:   Initiating a new execute wsjob against the cluster server.\n2024-03-21 15:36:56,758 INFO:   execute job id: wsjob-bdcvvsrwely3kbfwduefqx, remote log path: /n1/wsjob/workdir/job-operator/wsjob-bdcvvsrwely3kbfwduefqx\n2024-03-21 15:37:06,789 INFO:   Poll ingress status: Waiting for job running, current job status: Scheduled, msg: job is scheduled. \n2024-03-21 15:37:16,793 INFO:   Poll ingress status: Waiting for job service readiness.\n2024-03-21 15:37:36,838 INFO:   Poll ingress status: Waiting for job ingress readiness.\n2024-03-21 15:37:46,861 INFO:   Ingress is ready: Job ingress ready, poll ingress success.\n2024-03-21 15:37:47,052 INFO:   Preparing to execute using 1 CSX\n2024-03-21 15:38:33,999 INFO:   About to send initial weights\n2024-03-21 15:40:01,150 INFO:   Finished sending initial weights\n2024-03-21 15:40:01,154 INFO:   Finalizing appliance staging for the run\n2024-03-21 15:40:01,203 INFO:   Waiting for device programming to complete\n2024-03-21 15:41:26,576 INFO:   Device programming is complete\n2024-03-21 15:41:27,888 INFO:   Using network type: ROCE\n2024-03-21 15:41:27,890 INFO:   Waiting for input workers to prime the data pipeline and begin streaming ...\n2024-03-21 15:41:27,942 INFO:   Input workers have begun streaming input data\n2024-03-21 15:41:45,009 INFO:   Appliance staging is complete\n2024-03-21 15:41:45,021 INFO:   Beginning appliance run\n2024-03-21 15:49:45,474 INFO:   | Train Device=CSX, Step=100, Loss=9.84375, Rate=20.61 samples/sec, GlobalRate=20.61 samples/sec\n2024-03-21 15:57:49,616 INFO:   | Train Device=CSX, Step=200, Loss=8.35938, Rate=20.51 samples/sec, GlobalRate=20.53 samples/sec\n2024-03-21 16:05:53,769 INFO:   | Train Device=CSX, Step=300, Loss=8.26562, Rate=20.47 samples/sec, GlobalRate=20.50 samples/sec\n2024-03-21 16:13:58,078 INFO:   | Train Device=CSX, Step=400, Loss=7.02344, Rate=20.45 samples/sec, GlobalRate=20.49 samples/sec\n2024-03-21 16:22:02,644 INFO:   | Train Device=CSX, Step=500, Loss=7.07812, Rate=20.44 samples/sec, GlobalRate=20.48 samples/sec\n2024-03-21 16:30:06,513 INFO:   | Train Device=CSX, Step=600, Loss=7.34375, Rate=20.45 samples/sec, GlobalRate=20.47 samples/sec\n2024-03-21 16:38:10,737 INFO:   | Train Device=CSX, Step=700, Loss=7.19531, Rate=20.45 samples/sec, GlobalRate=20.47 samples/sec\n2024-03-21 16:46:15,052 INFO:   | Train Device=CSX, Step=800, Loss=6.52344, Rate=20.44 samples/sec, GlobalRate=20.47 samples/sec\n2024-03-21 16:54:19,448 INFO:   | Train Device=CSX, Step=900, Loss=6.46875, Rate=20.44 samples/sec, GlobalRate=20.46 samples/sec\n2024-03-21 17:02:24,111 INFO:   | Train Device=CSX, Step=1000, Loss=5.98438, Rate=20.43 samples/sec, GlobalRate=20.46 samples/sec\n2024-03-21 17:10:28,632 INFO:   | Train Device=CSX, Step=1100, Loss=6.17188, Rate=20.43 samples/sec, GlobalRate=20.46 samples/sec\n2024-03-21 17:18:32,943 INFO:   | Train Device=CSX, Step=1200, Loss=6.04688, Rate=20.44 samples/sec, GlobalRate=20.46 samples/sec\n2024-03-21 17:26:37,241 INFO:   | Train Device=CSX, Step=1300, Loss=5.54688, Rate=20.44 samples/sec, GlobalRate=20.45 samples/sec\n2024-03-21 17:34:41,491 INFO:   | Train Device=CSX, Step=1400, Loss=5.92188, Rate=20.44 samples/sec, GlobalRate=20.45 samples/sec\n2024-03-21 17:42:45,646 INFO:   | Train Device=CSX, Step=1500, Loss=5.68750, Rate=20.45 samples/sec, GlobalRate=20.45 samples/sec\n2024-03-21 17:50:50,110 INFO:   | Train Device=CSX, Step=1600, Loss=5.85938, Rate=20.44 samples/sec, GlobalRate=20.45 samples/sec\n</code></pre></p>"},{"location":"ai-testbed/cerebras/example-programs/#esm-2","title":"ESM-2","text":"<p>Evolutionary Scale Modeling (ESM-2) is a transformer protein language models from the Meta Fundamental AI Research Protein Team (FAIR).  The Cerebras ESM-2 model implementation can be found at <code>modelzoo/src/cerebras/modelzoo/models/nlp/esm2</code>. Configs available are listed at https://github.com/Cerebras/modelzoo/tree/main/src/cerebras/modelzoo/models/nlp/esm2#configs-included-for-this-model. This example will use the Uniref 50 dataset, preprocessed at path /software/datasets/ESM-2/, to train a small 35M parameter model.</p> <p>First, source a Cerebras PyTorch virtual environment and make sure that the requirements are installed: <pre><code>source ~/R_2.4.0/venv_cerebras_pt/bin/activate\npip install -r ~/R_2.4.0/modelzoo/requirements.txt\n</code></pre> Instructions for training (for 400 steps): <pre><code>cd ~/R_2.4.0/modelzoo/src/cerebras/modelzoo/models/nlp/esm2\nexport MODEL_DIR=model_dir_esm2\nif [ -d \"$MODEL_DIR\" ]; then rm -Rf $MODEL_DIR; fi\ncp /software/cerebras/dataset/ESM-2/params_esm2_t12_35M_UR50D_modified.yaml configs/params_esm2_t12_35M_UR50D_modified.yaml\npython run.py CSX --job_labels name=esm2_t12_35m --params configs/params_esm2_t12_35M_UR50D_modified.yaml --num_csx=1 --mode train --model_dir $MODEL_DIR --mount_dirs /home/$(whoami)/ /software --python_paths /home/$(whoami)/R_2.4.0/modelzoo/src --compile_dir /$(whoami) |&amp; tee mytest.log\n</code></pre></p> <p>Sample output <pre><code>2024-08-02 20:53:25,927 INFO:   Checkpoint autoloading is enabled. Looking for latest checkpoint in \"model_dir_esm2\" directory with the following naming convention: `checkpoint_(step)(_timestamp)?.mdl`.\n2024-08-02 20:53:25,928 INFO:   No checkpoints were found in \"model_dir_esm2\".\n2024-08-02 20:53:25,928 INFO:   No checkpoint was provided. Using randomly initialized model parameters.\n2024-08-02 20:53:25,930 INFO:   Starting training loop 1, from global step 0 to 400\n2024-08-02 20:53:26,257 WARNING:   Passing an absolute path as the compile directory may lead to undesirably long paths as the directory is used on the server side, not on the client side. Please consider passing in a relative directory instead.\n2024-08-02 20:53:26,488 INFO:   Saving checkpoint at step 0\n2024-08-02 20:53:35,010 INFO:   Saved checkpoint model_dir_esm2/checkpoint_0.mdl\n2024-08-02 20:53:45,962 INFO:   Compiling the model. This may take a few minutes.\n2024-08-02 20:53:47,482 INFO:   Initiating a new image build job against the cluster server.\n2024-08-02 20:53:47,488 INFO:   Custom worker image build is disabled from server.\n2024-08-02 20:53:47,492 WARNING:   Passing an absolute path as the compile directory may lead to undesirably long paths as the directory is used on the server side, not on the client side. Please consider passing in a relative directory instead.\n2024-08-02 20:53:47,658 INFO:   Initiating a new compile wsjob against the cluster server.\n2024-08-02 20:53:47,672 INFO:   Compile job id: wsjob-4jm7wrbl6lfnjf9hc2qukx, remote log path: /n1/wsjob/workdir/job-operator/wsjob-4jm7wrbl6lfnjf9hc2qukx\n2024-08-02 20:53:57,709 INFO:   Poll ingress status: Waiting for job running, current job status: Initializing, msg: job initializing with config generation. \n2024-08-02 20:54:07,710 INFO:   Poll ingress status: Waiting for all Coordinator pods to be running, current running: 0/1. \n2024-08-02 20:54:17,724 INFO:   Ingress is ready: Job ingress ready, poll ingress success.\n2024-08-02 20:54:20,821 INFO:   Pre-optimization transforms...\n2024-08-02 20:54:24,168 INFO:   Optimizing layouts and memory usage...\n2024-08-02 20:54:24,181 INFO:   Gradient accumulation enabled\n2024-08-02 20:54:24,188 INFO:   Gradient accumulation trying micro batch size 64...\n2024-08-02 21:06:59,403 INFO:   Exploring floorplans\n2024-08-02 21:07:13,655 INFO:   Exploring data layouts\n2024-08-02 21:08:06,235 INFO:   Optimizing memory usage\n2024-08-02 21:09:37,526 INFO:   Gradient accumulation showed a benefit\n2024-08-02 21:09:37,658 INFO:   Post-layout optimizations for &lt;microbatch=64, lanes=7&gt;...\n2024-08-02 21:09:37,679 INFO:   Post-layout optimizations for &lt;microbatch=64, lanes=10&gt;...\n2024-08-02 21:09:37,680 INFO:   Post-layout optimizations for &lt;microbatch=64, lanes=8&gt;...\n2024-08-02 21:09:37,681 INFO:   Post-layout optimizations for &lt;microbatch=64, lanes=11&gt;...\n2024-08-02 21:09:37,682 INFO:   Post-layout optimizations for &lt;microbatch=64, lanes=5&gt;...\n2024-08-02 21:09:40,830 INFO:   Allocating buffers for &lt;microbatch=64, lanes=7&gt;...\n2024-08-02 21:09:41,943 INFO:   Allocating buffers for &lt;microbatch=64, lanes=8&gt;...\n2024-08-02 21:09:41,977 INFO:   Allocating buffers for &lt;microbatch=64, lanes=11&gt;...\n2024-08-02 21:09:42,011 INFO:   Allocating buffers for &lt;microbatch=64, lanes=10&gt;...\n2024-08-02 21:09:42,014 INFO:   Allocating buffers for &lt;microbatch=64, lanes=5&gt;...\n2024-08-02 21:09:42,657 INFO:   Code generation for &lt;microbatch=64, lanes=7&gt;...\n2024-08-02 21:09:43,821 INFO:   Code generation for &lt;microbatch=64, lanes=8&gt;...\n2024-08-02 21:09:43,986 INFO:   Code generation for &lt;microbatch=64, lanes=5&gt;...\n2024-08-02 21:09:44,039 INFO:   Code generation for &lt;microbatch=64, lanes=11&gt;...\n2024-08-02 21:09:44,059 INFO:   Code generation for &lt;microbatch=64, lanes=10&gt;...\n2024-08-02 21:09:52,882 INFO:   Gradient accumulation picked micro batch size 64\n2024-08-02 21:10:08,744 INFO:   Compile artifacts successfully written to remote compile directory. Compile hash is: cs_3997308062121820062\n2024-08-02 21:10:08,780 INFO:   Compile was successful!\n2024-08-02 21:10:08,780 INFO:   Waiting for weight initialization to complete\n2024-08-02 21:10:08,781 INFO:   Programming Cerebras Wafer Scale Cluster for execution. This may take a few minutes.\n2024-08-02 21:10:08,946 INFO:   Initiating a new execute wsjob against the cluster server.\n2024-08-02 21:10:08,968 INFO:   Execute job id: wsjob-bzlmwdcyywzfu7bttr9gz9, remote log path: /n1/wsjob/workdir/job-operator/wsjob-bzlmwdcyywzfu7bttr9gz9\n2024-08-02 21:10:18,994 INFO:   Poll ingress status: Waiting for job running, current job status: Initializing, msg: job initializing with config generation. \n2024-08-02 21:10:29,016 INFO:   Poll ingress status: Waiting for all Worker pods to be running, current running: 0/1. \n2024-08-02 21:10:39,024 INFO:   Poll ingress status: Waiting for all Activation pods to be running, current running: 48/59. \n2024-08-02 21:10:49,036 INFO:   Poll ingress status: Waiting for all Weight pods to be running, current running: 17/20. \n2024-08-02 21:10:59,048 INFO:   Poll ingress status: Waiting for all Activation pods to be running, current running: 54/59. \n2024-08-02 21:11:09,060 INFO:   Poll ingress status: Waiting for all Weight pods to be running, current running: 18/20. \n2024-08-02 21:11:19,067 INFO:   Poll ingress status: Waiting for all Activation pods to be running, current running: 54/59. \n2024-08-02 21:11:49,095 INFO:   Poll ingress status: Waiting for all Weight pods to be running, current running: 18/20. \n2024-08-02 21:11:59,105 INFO:   Poll ingress status: Waiting for all Activation pods to be running, current running: 54/59. \n2024-08-02 21:12:09,117 INFO:   Poll ingress status: Waiting for all Weight pods to be running, current running: 18/20. \n2024-08-02 21:12:19,138 INFO:   Ingress is ready: Job ingress ready, poll ingress success.\n2024-08-02 21:12:19,380 INFO:   Preparing to execute using 1 CSX\n2024-08-02 21:12:52,993 INFO:   About to send initial weights\n2024-08-02 21:12:59,491 INFO:   Finished sending initial weights\n2024-08-02 21:12:59,492 INFO:   Finalizing appliance staging for the run\n2024-08-02 21:13:20,532 INFO:   Waiting for device programming to complete\n2024-08-02 21:14:35,817 INFO:   Device programming is complete\n2024-08-02 21:14:36,774 INFO:   Using network type: ROCE\n2024-08-02 21:14:36,775 INFO:   Waiting for input workers to prime the data pipeline and begin streaming ...\n2024-08-02 21:14:36,792 INFO:   Input workers have begun streaming input data\n2024-08-02 21:14:38,044 INFO:   Appliance staging is complete\n2024-08-02 21:14:38,044 INFO:   Beginning appliance run\n2024-08-02 21:20:22,200 INFO:   | Train Device=CSX, Step=100, Loss=4.71332, Rate=595.23 samples/sec, GlobalRate=595.23 samples/sec\n2024-08-02 21:26:13,253 INFO:   | Train Device=CSX, Step=200, Loss=10.34700, Rate=588.13 samples/sec, GlobalRate=589.25 samples/sec\n2024-08-02 21:26:13,260 INFO:   Saving checkpoint at step 200\n2024-08-02 21:26:29,525 INFO:   Saved checkpoint model_dir_esm2/checkpoint_200.mdl\n2024-08-02 21:32:04,197 INFO:   | Train Device=CSX, Step=300, Loss=4.17420, Rate=585.39 samples/sec, GlobalRate=587.34 samples/sec\n2024-08-02 21:37:56,370 INFO:   | Train Device=CSX, Step=400, Loss=4.12672, Rate=583.08 samples/sec, GlobalRate=585.88 samples/sec\n2024-08-02 21:37:56,377 INFO:   Saving checkpoint at step 400\n2024-08-02 21:38:13,224 INFO:   Saved checkpoint model_dir_esm2/checkpoint_400.mdl\n2024-08-02 21:38:42,917 INFO:   Training completed successfully!\n2024-08-02 21:38:42,923 INFO:   Processed 819200 training sample(s) in 2716.994790088 seconds.\n</code></pre></p>"},{"location":"ai-testbed/cerebras/example-programs/#vision-transformer","title":"Vision Transformer","text":"<p>The cerebras transformer based vision classifier model implementation can be found at <code>modelzoo/models/vision/vision_transformer</code>. Configs for base and huge model of the vision transformer can be found at <code>modelzoo/models/vision/vision_transformer/configs</code>. This examples uses the ImageNet dataset preprocessed at path <code>/software/datasets/imagenet/</code>. </p> <p>First, source a Cerebras PyTorch virtual environment and make sure that the requirements are installed: <pre><code>source ~/R_2.4.0/venv_cerebras_pt/bin/activate\npip install -r ~/R_2.4.0/modelzoo/requirements.txt\n</code></pre> Instructions for training (for 400 steps): <pre><code>cd ~/R_2.4.0/modelzoo/src/cerebras/modelzoo/models/vision/vision_transformer\nexport MODEL_DIR=model_dir_vt\nif [ -d \"$MODEL_DIR\" ]; then rm -Rf $MODEL_DIR; fi\ncp  /software/cerebras/dataset/vision_transformer/params_vit_base_patch_16_imagenet_1k.yaml configs/params_vit_base_patch_16_imagenet_1k.yaml\npython run.py CSX --job_labels name=vision_transformer --params configs/params_vit_base_patch_16_imagenet_1k.yaml --num_csx=1 --mode train --model_dir $MODEL_DIR --mount_dirs /home/$(whoami)/ /software --python_paths /home/$(whoami)/R_2.4.0/modelzoo/src --compile_dir /$(whoami) |&amp; tee mytest.log\n</code></pre></p> <p>Sample output <pre><code>2024-12-21 00:40:15,426 INFO:   No need to use DLS for loss when half dtype is bfloat16. Disabling gradient scaling.\n2024-12-21 00:40:15,600 INFO:   Checkpoint autoloading is enabled. Looking for latest checkpoint in \"model_dir_vt\" directory with the following naming convention: `checkpoint_(step)(_timestamp)?.mdl`.\n2024-12-21 00:40:15,601 INFO:   No checkpoints were found in \"model_dir_vt\".\n2024-12-21 00:40:15,601 INFO:   No checkpoint was provided. Using randomly initialized model parameters.\n2024-12-21 00:40:15,602 INFO:   Effective batch size is 2850.\n2024-12-21 00:40:15,605 INFO:   The following sequence is used to transform data:\nCompose(\n    Resize(size=[256, 256], interpolation=bilinear, max_size=None, antialias=None)\n    RandomResizedCrop(size=[224, 224], scale=(0.08, 1.0), ratio=(0.75, 1.33), interpolation=bilinear, antialias=True)\n    RandomHorizontalFlip(p=0.5)\n    ToTensor()\n    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n    LambdaWithParam(args=(torch.bfloat16,), kwargs={})\n)\n2024-12-21 00:40:55,243 INFO:   Starting training loop 1, from global step 0 to 132225\n...\n2024-12-21 00:41:25,467 INFO:   Compiling the model. This may take a few minutes.\n...\n2024-12-21 00:45:49,911 INFO:   Compiling at original per-box batch size 2850\n2024-12-21 00:46:00,124 INFO:   Compiling image...\n2024-12-21 00:46:00,273 INFO:   Compiling kernels\n2024-12-21 00:48:10,561 INFO:   Compiling final image\n2024-12-21 00:51:53,676 INFO:   Compile artifacts successfully written to remote compile directory. Compile hash is: cs_9892963798577744835\n2024-12-21 00:51:59,530 INFO:   Compile was successful!\n2024-12-21 00:51:59,531 INFO:   Waiting for weight initialization to complete\n2024-12-21 00:51:59,531 INFO:   Programming Cerebras Wafer Scale Cluster for execution. This may take a few minutes.\n2024-12-21 00:51:59,919 INFO:   Initiating a new execute wsjob against the cluster server.\n...\n2024-12-21 00:53:37,788 INFO:   Finished sending initial weights\n2024-12-21 00:53:37,789 INFO:   Finalizing appliance staging for the run\n2024-12-21 00:53:58,206 INFO:   Waiting for device programming to complete\n2024-12-21 00:55:57,939 INFO:   Device programming is complete\n2024-12-21 00:55:58,865 INFO:   Using network type: ROCE\n2024-12-21 00:55:58,866 INFO:   Waiting for input workers to prime the data pipeline and begin streaming ...\n2024-12-21 00:55:58,883 INFO:   Input workers have begun streaming input data\n2024-12-21 00:56:00,346 INFO:   Appliance staging is complete\n2024-12-21 00:56:00,346 INFO:   Beginning appliance run\n2024-12-21 00:56:01,022 INFO:   | Train Device=CSX, Step=1, Loss=7.00964, Rate=4626.81 samples/sec, GlobalRate=4626.69 samples/sec\n2024-12-21 00:56:01,773 INFO:   | Train Device=CSX, Step=2, Loss=7.02971, Rate=4128.64 samples/sec, GlobalRate=4170.71 samples/sec\n2024-12-21 00:56:04,670 INFO:   | Train Device=CSX, Step=3, Loss=7.03938, Rate=2241.66 samples/sec, GlobalRate=2005.18 samples/sec\n2024-12-21 00:56:05,276 INFO:   | Train Device=CSX, Step=4, Loss=7.02248, Rate=3718.21 samples/sec, GlobalRate=2340.86 samples/sec\n2024-12-21 00:56:08,100 INFO:   | Train Device=CSX, Step=5, Loss=7.02704, Rate=2092.96 samples/sec, GlobalRate=1852.26 samples/sec\n</code></pre></p>"},{"location":"ai-testbed/cerebras/example-programs/#diffusion-transformer","title":"Diffusion Transformer","text":"<p>The Cerebras Diffusion Transformer[1] model implementation can be found at <code>modelzoo/src/cerebras/modelzoo/models/vision/dit</code>. Three configs, for the large and xlarge models in the paper, and for a larger model, can be found in <code>modelzoo/src/modelzoo/models/vision/dit/configs</code>. This example uses the ImageNet dataset, preprocessed at path <code>/software/cerebras/datasets/dit/</code>, and the config for the largest model.</p> <p>First, source a Cerebras PyTorch virtual environment and make sure that the requirements are installed: <pre><code>source ~/R_2.4.0/venv_cerebras_pt/bin/activate\npip install -r ~/R_2.4.0/modelzoo/requirements.txt\n</code></pre></p> <p>Instructions for training (for 400 steps): <pre><code>cd ~/R_2.4.0/modelzoo/src/cerebras/modelzoo/models/vision/dit\nexport MODEL_DIR=model_dir_dit\nif [ -d \"$MODEL_DIR\" ]; then rm -Rf $MODEL_DIR; fi\ncp  /software/cerebras/dataset/params_dit_2B_patchsize_2x2_modified.yaml configs/params_dit_2B_patchsize_2x2_modified.yaml\npython run.py CSX --job_labels name=DiT --mode train --params configs/params_dit_2B_patchsize_2x2_modified.yaml --model_dir ${MODEL_DIR} |&amp; tee mytest.log\n</code></pre></p> Example output: <pre><code>2025-01-24 21:53:05,710 INFO:   | Train Device=CSX, Step=397, Loss=0.18575, Rate=405.81 samples/sec, \nGlobalRate=405.41 samples/sec\n2025-01-24 21:53:08,405 INFO:   | Train Device=CSX, Step=398, Loss=0.18720, Rate=407.14 samples/sec, \nGlobalRate=405.42 samples/sec\n2025-01-24 21:53:11,080 INFO:   | Train Device=CSX, Step=399, Loss=0.18482, Rate=409.63 samples/sec, \nGlobalRate=405.44 samples/sec \n2025-01-24 21:53:13,749 INFO:   | Train Device=CSX, Step=400, Loss=0.18625, Rate=411.09 samples/sec, \nGlobalRate=405.45 samples/sec\n2025-01-24 21:53:13,761 INFO:   Saving checkpoint at step 400\nTransferring weights from server: 4556 tensors [02:34, 29.52 tensors/s]                                                                                                                         \n2025-01-24 21:56:05,648 INFO:   Saved checkpoint dit_model_dir/checkpoint_400.mdl\n2025-01-24 21:56:28,429 INFO:   Training completed successfully!\n2025-01-24 21:56:28,435 INFO:   Processed 440000 training sample(s) in 1888.733046122 seconds.\n</code></pre>"},{"location":"ai-testbed/cerebras/getting-started/","title":"Getting Started","text":""},{"location":"ai-testbed/cerebras/getting-started/#getting-started","title":"Getting Started","text":""},{"location":"ai-testbed/cerebras/getting-started/#connection-to-a-cs-2-node","title":"Connection to a CS-2 node","text":"<p> Connection to one of the CS-2 cluster login nodes requires an MFA passcode for authentication - either an 8-digit passcode generated by an app on your mobile device (e.g. MobilePASS+) or a CRYPTOCard-generated passcode prefixed by a 4-digit pin. This is the same passcode used to authenticate into other ALCF systems, such as Polaris. In the examples below, replace ALCFUserID with your ALCF user id. To connect to a CS-2 login:</p> <ol> <li>ssh to a desired login node:     <pre><code>ssh ALCFUserID@cer-login-01.ai.alcf.anl.gov\n</code></pre>     or     <pre><code>ssh ALCFUserID@cer-login-02.ai.alcf.anl.gov\n</code></pre>     or     <pre><code>ssh ALCFUserID@cer-login-03.ai.alcf.anl.gov\n</code></pre></li> <li>Alternatively, ssh randomly to one of the above three login nodes:     <pre><code>ssh ALCFUserID@cerebras.ai.alcf.anl.gov\n</code></pre></li> </ol>"},{"location":"ai-testbed/cerebras/job-queuing-and-submission/","title":"Job Queuing and Submission","text":"<p>The CS-2 cluster has its own Kubernetes-based system for job submission and queuing.</p> <p>Jobs are started automatically through the Python framework in modelzoo.common.pytorch.run_utils Continuous job status for a job is output to stdout/stderr; redirect the output, or consider using a persistent session started with screen, or tmux, or both.</p> <p>Jobs that have not yet completed can be listed as shown. Note: this command can take over a minute to complete.</p> <p><pre><code>(venv_cerebras_pt) $ csctl get jobs\nNAME                          AGE  DURATION  PHASE    SYSTEMS     USER     LABELS        DASHBOARD\nwsjob-thjj8zticwsylhppkbmjqe  13s  1s        RUNNING  cer-cs2-01  username name=unet_pt  https://grafana.cerebras1.lab.alcf.anl.gov/d/WebHNShVz/wsjob-dashboard?orgId=1&amp;var-wsjob=wsjob-thjj8zticwsylhppkbmjqe&amp;from=1691705374000&amp;to=now\n(venv_cerebras_pt) $\n</code></pre> To view the grafana databoard for a job, follow the instructions at Grafana WsJob Dashboard for Cerebras jobs</p> <p>Jobs can be canceled as shown:</p> <pre><code>(venv_cerebras_pt) $ csctl cancel job wsjob-eyjapwgnycahq9tus4w7id\nJob canceled successfully\n(venv_cerebras_pt) $\n</code></pre> <p>Jobs can be labeled in the command line that launches them, if they are written with Cerebras's Python framework for running appliance jobs, by adding a command line option of this form: <pre><code> --job_labels labelname=labelvalue\n</code></pre></p> <p>Jobs can also be labeled after they have been started as shown: <pre><code>(venv_cerebras_pt) $ csctl label job wsjob-ez6dyfronnsg2rz7f7fqw4 testlabel=test\njob/wsjob-ez6dyfronnsg2rz7f7fqw4 was patched\n(venv_cerebras_pt) $\n</code></pre></p> <p>Jobs with a particular label/label value can be listed as shown: <pre><code>(venv_cerebras_pt) $ csctl get jobs | grep \"testlabel=test\"\nwsjob-ez6dyfronnsg2rz7f7fqw4  19m SUCCEEDED  cer-cs2-02 username testlabel=test,user=username\n(venv_cerebras_pt) $\n</code></pre></p> <p>See <code>csctl -h</code> for more options. Add <code>-h</code> to a command for help for that command, e.g. <code>csctl get -h</code> or <code>csctl cancel -h</code>. </p> <pre><code>$ csctl -h\nCerebras cluster command line tool.\n\nUsage:\n  csctl [command]\n\nAvailable Commands:\n  cancel             Cancel job\n  clear-worker-cache Clear the worker cache\n  config             View csctl config files\n  get                Get resources\n  label              Label resources\n  log-export         Gather and download logs.\n  types              Display resource types\n\nFlags:\n  -d, --debug int          higher debug values will display more fields in output objects\n  -h, --help               help for csctl\n      --namespace string   configure csctl to talk to different user namespaces\n  -v, --version            version for csctl\n\nUse \"csctl [command] --help\" for more information about a command.\n</code></pre>"},{"location":"ai-testbed/cerebras/miscellaneous/","title":"Miscellaneous","text":""},{"location":"ai-testbed/cerebras/miscellaneous/#porting-applications-to-the-cs-2","title":"Porting applications to the CS-2","text":"<p>Cerebras documentation for porting code to run on a Cerebras CS-2 system: Ways to port your model</p>"},{"location":"ai-testbed/cerebras/miscellaneous/#grafana-wsjob-dashboard-for-cerebras-jobs","title":"Grafana WsJob Dashboard for Cerebras jobs","text":"<p>A Grafana dashboard provides support for visualizing, querying, and exploring the CS2 system\u2019s metrics and enables to access system logs and traces. See the Cerebras documentation for the Job Information Dashboard</p> <p>Here is a summary (tested to work on Ubuntu and MacOS)</p> <p>On your work machine with a web browser, e.g. your laptop, edit /etc/hosts, using your editor of choice <pre><code>sudo nano /etc/hosts\n</code></pre> Add this line <pre><code>127.0.0.1   grafana.cerebras1.lab.alcf.anl.gov\n</code></pre> Save, and exit the editor</p> <p>Download the Grafana certificate present on the Cerebras node at /opt/cerebras/certs/grafana_tls.crt to your local machine. To add this certificate to your browser keychain, </p> <ol> <li>On chrome, go to Settings-&gt;Privacy and security-&gt;Security-&gt;Manage device certificates</li> <li>Select System under \"System Keychains\" on the left hand side of your screen. Also select the \"Certificate\" tab.</li> <li>Drag and drop the downloaded certificate. Once it is added, it is visible as \"lab.alcf.anl.gov\"    </li> <li>Select the certificate, and ensure that the \"Trust\" section is set to \"Always Trust\"    </li> </ol> <p>On your work machine with a web browser, e.g. your laptop, tunnel the grafana https port on the cerebras grafana host through to localhost <pre><code>ssh -L 8443:grafana.cerebras1.lab.alcf.anl.gov:443 arnoldw@cer-login-03.ai.alcf.anl.gov\n</code></pre></p> <p>Point a browser at grafana. (Tested with Firefox and Chrome/Brave) Open browser to a job grafana url shown in csctl get jobs, adding :8443 to hostname, e.g. <pre><code>https://grafana.cerebras1.lab.alcf.anl.gov:8443/d/WebHNShVz/wsjob-dashboard?orgId=1&amp;var-wsjob=wsjob-49b7uuojdelvtrcxu3cwbw&amp;from=1684859330000&amp;to=noww\n</code></pre></p> <p>Login to the dashboard with user admin, and password prom-operator</p>"},{"location":"ai-testbed/cerebras/running-a-model-or-program/","title":"Running a Model/Program","text":""},{"location":"ai-testbed/cerebras/running-a-model-or-program/#getting-started","title":"Getting Started","text":""},{"location":"ai-testbed/cerebras/running-a-model-or-program/#job-submission-and-queuing","title":"Job submission and queuing","text":"<p>Cerebras jobs are initiated and tracked automatically within the Python framework in modelzoo.common.pytorch.run_utils. This framework interacts with the Cerebras cluster management node.</p>"},{"location":"ai-testbed/cerebras/running-a-model-or-program/#login-nodes","title":"Login nodes","text":"<p>Jobs are launched from login nodes. If you expect a loss of an internet connection for any reason, for long-running jobs we suggest logging into a specific login node and using either screen or tmux to create persistent command line sessions.  For details use:2</p> <pre><code>man screen\n# or\nman tmux\n</code></pre>"},{"location":"ai-testbed/cerebras/running-a-model-or-program/#running-jobs-on-the-wafer","title":"Running jobs on the wafer","text":"<p>Follow these instructions to compile and train a small (111m parameters) GPT3 model.</p>"},{"location":"ai-testbed/cerebras/running-a-model-or-program/#cerebras-virtual-environments","title":"Cerebras virtual environments","text":"<p>First, make a virtual environment for Cerebras for PyTorch. See Customizing Environments for the procedures for making PyTorch virtual environments for Cerebras. If an environment is made in <code>~/R_2.4.0/</code>, it would be activated as follows: <pre><code>source ~/R_2.4.0/venv_cerebras_pt/bin/activate\n</code></pre></p>"},{"location":"ai-testbed/cerebras/running-a-model-or-program/#clone-the-cerebras-modelzoo","title":"Clone the Cerebras modelzoo","text":"<pre><code>mkdir ~/R_2.4.0\ncd ~/R_2.4.0\ngit clone https://github.com/Cerebras/modelzoo.git\ncd modelzoo\ngit tag\ngit checkout Release_2.4.0\n</code></pre>"},{"location":"ai-testbed/cerebras/running-a-model-or-program/#running-a-pytorch-sample","title":"Running a Pytorch sample","text":""},{"location":"ai-testbed/cerebras/running-a-model-or-program/#activate-your-pytorch-virtual-environment-install-modelzoo-requirements-and-change-to-the-working-directory","title":"Activate your PyTorch virtual environment, install modelzoo requirements, and change to the working directory","text":"<pre><code>source ~/R_2.4.0/venv_cerebras_pt/bin/activate\npip install -r ~/R_2.4.0/modelzoo/requirements.txt\ncd ~/R_2.4.0/modelzoo/src/cerebras/modelzoo/models/nlp/gpt3\n</code></pre> <p>Next, copy a sample config file. This is for a small GPT3 model, modified to use a preprocessed dataset and to run for fewer steps. </p> <pre><code>cp /software/cerebras/dataset/OWT/Pytorch/111m_modified.yaml configs/Cerebras_GPT/111m_modified.yaml\n</code></pre>"},{"location":"ai-testbed/cerebras/running-a-model-or-program/#running-a-sample-pytorch-training-job","title":"Running a sample PyTorch training job","text":"<p>To run the sample:</p> <pre><code>export MODEL_DIR=model_dir_gpt3_111m\n# deletion of the model_dir is only needed if sample has been previously run\nif [ -d \"$MODEL_DIR\" ]; then rm -Rf $MODEL_DIR; fi\npython run.py CSX --job_labels name=gpt3_111m --params configs/Cerebras_GPT/111m_modified.yaml --num_csx=1 --mode train --model_dir $MODEL_DIR --mount_dirs /home/ /software --python_paths /home/$(whoami)/R_2.4.0/modelzoo/src --compile_dir $(whoami) |&amp; tee mytest.log\n</code></pre> <p>A successful GPT3 (111m parameters) PyTorch training run should finish with output resembling the following:</p> <pre><code>2023-11-29 18:13:13,048 INFO:   | Train Device=CSX, Step=1950, Loss=2.28834, Rate=397.31 samples/sec, GlobalRate=433.98 samples/sec\n2023-11-29 18:13:13,555 INFO:   | Train Device=CSX, Step=2000, Loss=2.34778, Rate=395.69 samples/sec, GlobalRate=431.83 samples/sec\n2023-11-29 18:13:13,555 INFO:   Saving checkpoint at step 2000\n2023-11-29 18:13:17,242 INFO:   Saved checkpoint model_dir/checkpoint_2000.mdl\n2023-11-29 18:13:55,517 INFO:   Heartbeat thread stopped for wsjob-fpwqt7maq8a5mxvblwwzbu.\n2023-11-29 18:13:55,523 INFO:   Training completed successfully!\n2023-11-29 18:13:55,523 INFO:   Processed 4000 sample(s) in 51.230697212 seconds.\n</code></pre>"},{"location":"ai-testbed/cerebras/system-overview/","title":"System Overview","text":"<p>The Cerebras CS-2 is a wafer-scale deep learning accelerator comprising 850,000 processing cores, each providing 48KB of dedicated SRAM memory for an on-chip total of 40GB and interconnected to optimize bandwidth and latency. Its software platform integrates the popular machine learning framework PyTorch.</p> <p>The ALCF CS-2 systems are configured as a Cerebras Wafer-Scale Cluster, designed to support large-scale models (up to and well beyond 1 billion parameters) and large-scale inputs. The cluster contains two CS-2 systems and can distribute jobs across one or both CS-2 systems in a data-parallel framework. The supporting CPU cluster consists of MemoryX, SwarmX, management, and input worker nodes. The Cerebras Wafer-Scale cluster is run as an appliance: a user submits a job to the appliance, and the appliance manages preprocessing and streaming of the data, IO, and device orchestration within the appliance. It provides programming via PyTorch, with data-parallel distribution when using more than one CS-2. This installation supports both Pipelined execution for models up to 1 billion parameters and Weight Streaming execution for models up to and above 1 billion parameters.</p> <p>The public Cerebras documentation is available here.</p> <p>A typical Cerebras Wafer-Scale Cluster is shown in the figure. Users connect (ssh) to one of the three login nodes. Either ssh to <code>cerebras.ai.alcf.anl.gov</code>, which randomly resolves to one of cer-login-0[1-3].ai.alcf.anl.gov, or ssh to a specific node, <code>cer-login-01.ai.alcf.anl.gov</code>, <code>cer-login-02.ai.alcf.anl.gov</code>, <code>cer-login-03.ai.alcf.anl.gov</code>. The rest of the nodes in the cluster infrastructure are not directly accessible, except by admins. The trees <code>/home</code>, <code>/projects</code>, and <code>/software</code> are shared across all three login nodes, the relevant cluster infrastructure nodes, and all ALCF AI testbed platforms.</p> <p> </p> CS-2 cluster figure <p>(Figure from https://docs.cerebras.net/en/latest/wsc/cerebras-basics/how-cerebras-works.html)</p> <p>As indicated in the figures, the CS-2 nodes on the right are responsible only for running and accelerating the computations for training and predictions with the model. The other work, including compilation, is performed by input nodes, and by MemoryX nodes, which are used for weight storage and broadcast, and SwarmX nodes, which are used for gradient accumulation. Some model verification work can be done on login nodes.</p>"},{"location":"ai-testbed/cerebras/tunneling-and-forwarding-ports/","title":"Tunneling and Forwarding Ports","text":"<p>See ALCF's Jupyter Instructions, and Tunneling and forwarding ports. The Cerebras login nodes are direct login; tunneling and port forwarding do not involve jump hosts.</p>"},{"location":"ai-testbed/data-management/data-management-overview/","title":"Data Management for the AI Testbed","text":""},{"location":"ai-testbed/data-management/data-management-overview/#home-file-system-space","title":"Home File System Space","text":"<p>Users have a shared home filesystem <code>/home</code> shared across the ALCF AI testbed systems, including the login and compute nodes. Default user quota is <code>1 TB</code> storage and <code>1,000,000 files</code>. This space is backed up. </p>"},{"location":"ai-testbed/data-management/data-management-overview/#project-file-system-space","title":"Project File System Space","text":"<p>The team project/campaign file system <code>/projects</code> is intended to facilitate project collaboration and is accessible to the team members of your project that have an ALCF account.  Default group storage quota is <code>2 TB</code> and <code>2,000,000 files</code>. Please note that this space isn't backed up.  Our policy is that data will be purged from disk 6 months after project completion.</p>"},{"location":"ai-testbed/data-management/data-management-overview/#data-transfer","title":"Data Transfer","text":"<p>Users can transfer data to and from the AI testbed using <code>Globus</code> or tools such as <code>scp</code> or <code>rsync</code>.</p>"},{"location":"ai-testbed/data-management/data-management-overview/#using-globus","title":"Using Globus","text":"<p>We have a Globus endpoint each to move data to and from the <code>/projects</code> and <code>/home</code> filesystem respectively.</p> <ul> <li>Use <code>alcf#ai_testbed_projects</code> for the <code>/projects</code> file system</li> <li>Use <code>alcf#ai_testbed_home</code> for the <code>/home</code> files system </li> </ul> <p>Relevant information regarding using globus can be found here</p>"},{"location":"ai-testbed/data-management/data-management-overview/#alcf-storage-policies","title":"ALCF Storage Policies","text":"<p>ALCF data policies is available here </p> <p>Please Note: The basic level of protection provided is UNIX file level permissions; it is the user's responsibility to ensure that file permissions and umasks are set to match their needs.</p>"},{"location":"ai-testbed/files/notes/","title":"Notes","text":"<pre><code>git submodule init; git submodule update\n</code></pre>"},{"location":"ai-testbed/files/todo/","title":"TODO","text":""},{"location":"ai-testbed/files/todo/#cosmictagger-v1x","title":"CosmicTagger v1.x","text":"<p>Note: Conversion of CT to the various machines is meant to be a tutorial as to how to convert a model.</p>"},{"location":"ai-testbed/files/todo/#cerebras-ct","title":"Cerebras CT","text":"<p>Cerebras cannot support CT and UNets in general as of 4/25/23.</p>"},{"location":"ai-testbed/files/todo/#graphcore-ct","title":"Graphcore CT","text":"<p>Alex has been very busy with conferences, etc.</p> <p>He ran CT but, it ran on the CPU.  He has stated that it may need to be completely written using, I can't remember which, Poplar or PopArt.  If that is necessary, Venkat should make the call.</p>"},{"location":"ai-testbed/files/todo/#groq-ct","title":"Groq CT","text":""},{"location":"ai-testbed/files/todo/#habana-ct","title":"Habana CT","text":"<p>Repo:    https://github.com/argonne-lcf/user-guides.git Branch:  feature/Habana002-DNP File:    docs/ai-testbed/habana/CosmicTagger-Conversion.md</p>"},{"location":"ai-testbed/files/todo/#sambanova-ct","title":"SambaNova CT","text":"<p>SN has a highly-engineered version of CT.</p> <p>They are working to support CT OOB, Out-Of-Box.</p>"},{"location":"ai-testbed/files/todo/#cerebras","title":"Cerebras","text":"<p>Repo:    https://github.com/argonne-lcf/user-guides.git Branch:  Talk to Bill.</p>"},{"location":"ai-testbed/files/todo/#graphcore","title":"Graphcore","text":"<p>Repo:  https://github.com/argonne-lcf/user-guides.git</p> <p>When you change back to 3.2, use virtual-environments.md from the commit a4ce3b5598f4d6feee7ca58accde1a6a0ea84244 \"virtual-environments.md with 3.2 edits.\"</p>"},{"location":"ai-testbed/files/todo/#groq","title":"Groq","text":"<p>Repo:   https://github.com/argonne-lcf/user-guides.git Branch: feature/Groq001-DNP</p>"},{"location":"ai-testbed/files/todo/#habana","title":"Habana","text":"<p>Repo:  https://github.com/argonne-lcf/user-guides.git Branch:  feature/Habana002-DNP</p>"},{"location":"ai-testbed/files/todo/#sambanova","title":"SambaNova","text":"<p>Repo:  https://github.com/argonne-lcf/user-guides.git</p>"},{"location":"ai-testbed/graphcore/documentation/","title":"Documentation links","text":"<p>Poplar SDK PyTorch for the IPU: User Guide Targetting the IPU from Tensorflow 2 IPU programming guide Examples Examples Github Repo POD systems POD64 specs </p>"},{"location":"ai-testbed/graphcore/example-programs/","title":"Example Programs","text":"<p>Graphcore provides examples of some well-known AI applications in their repository at https://github.com/graphcore/examples.git. Clone the examples repository to your personal directory structure: <pre><code>mkdir ~/graphcore\ncd ~/graphcore\ngit clone https://github.com/graphcore/examples.git\n</code></pre></p>"},{"location":"ai-testbed/graphcore/example-programs/#mnist-poptorch","title":"MNIST - PopTorch","text":""},{"location":"ai-testbed/graphcore/example-programs/#activate-poptorch-environment","title":"Activate PopTorch Environment","text":"<pre><code>source ~/venvs/graphcore/poptorch33_env/bin/activate\n</code></pre>"},{"location":"ai-testbed/graphcore/example-programs/#install-requirements","title":"Install Requirements","text":"<p>Change directory: <pre><code>cd ~/graphcore/examples/tutorials/simple_applications/pytorch/mnist\n</code></pre></p>"},{"location":"ai-testbed/graphcore/example-programs/#run-mnist","title":"Run MNIST","text":"<p>Execute the command: <pre><code>/opt/slurm/bin/srun --ipus=1 python mnist_poptorch.py\n</code></pre></p>"},{"location":"ai-testbed/graphcore/example-programs/#output","title":"Output","text":"<p>The expected output will resemble the following:</p> <pre><code>srun: job 10671 queued and waiting for resources\nsrun: job 10671 has been allocated resources\nTrainingModelWithLoss(\n  (model): Network(\n    (layer1): Block(\n      (conv): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (relu): ReLU()\n    )\n    (layer2): Block(\n      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (relu): ReLU()\n    )\n    (layer3): Linear(in_features=1600, out_features=128, bias=True)\n    (layer3_act): ReLU()\n    (layer3_dropout): Dropout(p=0.5, inplace=False)\n    (layer4): Linear(in_features=128, out_features=10, bias=True)\n    (softmax): Softmax(dim=1)\n  )\n  (loss): CrossEntropyLoss()\n)\nEpochs:   0%|          | 0/10 [00:00&lt;?,[23:27:06.753] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 10\nGraph compilation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00&lt;00:00]\nEpochs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:17&lt;00:00,  7.71s/it]\nGraph compilation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00&lt;00:00]                          \nAccuracy on test set: 96.85%\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00&lt;00:00]\n</code></pre>"},{"location":"ai-testbed/graphcore/example-programs/#mnist-tensorflow2","title":"MNIST - Tensorflow2","text":""},{"location":"ai-testbed/graphcore/example-programs/#activate-tensorflow2-environment","title":"Activate Tensorflow2 Environment","text":"<p>Create a TensorFlow2 environment as explained in the tensorflow-2-environment-setup and activate the same. <pre><code>source ~/venvs/graphcore/tensorflow2_33_env/bin/activate\n</code></pre></p>"},{"location":"ai-testbed/graphcore/example-programs/#install-requirements_1","title":"Install Requirements","text":"<p>Change directory: <pre><code>cd ~/graphcore/examples/tutorials/simple_applications/tensorflow2/mnist/\n</code></pre></p>"},{"location":"ai-testbed/graphcore/example-programs/#run-mnist-tensorflow","title":"Run MNIST - TensorFlow","text":"<p>Execute the command:</p> <pre><code>/opt/slurm/bin/srun --ipus=1 python mnist.py\n</code></pre>"},{"location":"ai-testbed/graphcore/example-programs/#output_1","title":"Output","text":"<p>The expected output will resemble the following:</p> <pre><code>srun: job 10672 queued and waiting for resources\nsrun: job 10672 has been allocated resources\n2023-08-22 23:35:02.925033: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 3.3.0 (de1f8de2a7) Poplar package: b67b751185\n2023-08-22 23:35:06.119772: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1619] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0\n2023-08-22 23:35:07.087287: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n2023-08-22 23:35:07.351132: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n2023-08-22T23:35:09.469066Z PL:POPOPS    3545299.3545299 W: createOutputForElementWiseOp 'while/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits/fusion.3/Op/Equal/Out' ({32,10}): No suitable input found, creating new variable with linear tile mapping\n2023-08-22 23:35:18.532415: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\nEpoch 1/4\n2000/2000 [==============================] - 13s 6ms/step - loss: 0.6220\nEpoch 2/4\n2000/2000 [==============================] - 1s 262us/step - loss: 0.3265\nEpoch 3/4\n2000/2000 [==============================] - 1s 273us/step - loss: 0.2781\nEpoch 4/4\n2000/2000 [==============================] - 1s 289us/step - loss: 0.2482\n</code></pre>"},{"location":"ai-testbed/graphcore/example-programs/#resnet50","title":"ResNet50","text":""},{"location":"ai-testbed/graphcore/example-programs/#activate-poptorch-environment_1","title":"Activate PopTorch Environment","text":"<p>Create and activate a fresh PopTorch environment <code>poptorch33_resnet50_env</code> as outlined in the virtual environment section, then activate it. <pre><code>source ~/venvs/graphcore/poptorch33_resnet50_env/bin/activate\n</code></pre></p>"},{"location":"ai-testbed/graphcore/example-programs/#install-requirements_2","title":"Install Requirements","text":"<p>Change directory <pre><code>cd ~/graphcore/examples/vision/cnns/pytorch\nmake install \nmake install-turbojpeg\n</code></pre></p>"},{"location":"ai-testbed/graphcore/example-programs/#update-configsyml","title":"Update configs.yml","text":"<p>Change directory: <pre><code>cd ~/graphcore/examples/vision/cnns/pytorch/train\n</code></pre> Open configs.yml with your favorite editor. Find in the resnet50 section <pre><code>use_bbox_info: true\n</code></pre> and change it to: <pre><code>use_bbox_info: false\n</code></pre></p>"},{"location":"ai-testbed/graphcore/example-programs/#run-resnet50","title":"Run ResNet50","text":"<p>The scripts to train a ResNet50 PyTorch model on Pod4 is located at https://github.com/graphcore/examples/tree/master/vision/cnns/pytorch/train</p> <p>Set the following environmental variables. <pre><code>mkdir -p ~/graphcore/tmp/pt_cache/\nexport PYTORCH_CACHE_DIR=~/graphcore/tmp/pt_cache/\n</code></pre> To run 4 replicas (a total for 4 IPUs) of the ResNet50 model: Make a script with the following contents, called poprun_unet.sh This script tells poprun to use the partition id of the partition created for the slurm job used to run the script. <pre><code>#!/bin/bash\npoprun -vv --vipu-partition=slurm_${SLURM_JOBID} --num-instances=1 --num-replicas=4 --executable-cache-path=$PYTORCH_CACHE_DIR python3 /home/$USER/graphcore/examples/vision/cnns/pytorch/train/train.py --config resnet50-pod4 --imagenet-data-path /mnt/localdata/datasets/imagenet-raw-dataset --epoch 2 --validation-mode none --dataloader-worker 14 --dataloader-rebatch-size 256\n</code></pre> Then <pre><code>chmod +x poprun_unet.sh\n/opt/slurm/bin/srun --ipus=4 poprun_unet.sh\n</code></pre></p> <p>This model is run with the imagenet dataset.</p>"},{"location":"ai-testbed/graphcore/example-programs/#output_2","title":"Output","text":"<p>The expected output starts with this: <pre><code>srun: job 10675 queued and waiting for resources\nsrun: job 10675 has been allocated resources\n23:48:29.160 3555537 POPRUN [I] V-IPU server address picked up from 'vipu': 10.1.3.101:8090\n23:48:29.160 3555537 POPRUN [D] Connecting to 10.1.3.101:8090\n23:48:29.162 3555537 POPRUN [D] Status for partition slurm_10673: OK (error 0)\n23:48:29.162 3555537 POPRUN [I] Partition slurm_10673 already exists and is in state: PS_ACTIVE\n23:48:29.163 3555537 POPRUN [D] The reconfigurable partition slurm_10673 is OK\n ===========================\n|      poprun topology      |\n|===========================|\n| hosts     | gc-poplar-02  |\n|-----------|---------------|\n| ILDs      |       0       |\n|-----------|---------------|\n| instances |       0       |\n|-----------|---------------|\n| replicas  | 0 | 1 | 2 | 3 |\n ---------------------------\n23:48:29.163 3555537 POPRUN [D] Target options from environment: {}\n23:48:29.163 3555537 POPRUN [D] Target options from V-IPU partition: {\"ipuLinkDomainSize\":\"4\",\"ipuLinkConfiguration\":\"default\",\"ipuLinkTopology\":\"mesh\",\"gatewayMode\":\"true\",\"instanceSize\":\"4\"}\n23:48:29.207 3555537 POPRUN [D] Found 1 devices with 4 IPUs\n23:48:29.777 3555537 POPRUN [D] Attached to device 6\n23:48:29.777 3555537 POPRUN [I] Preparing parent device 6\n23:48:29.777 3555537 POPRUN [D] Device 6 ipuLinkDomainSize=64, ipuLinkConfiguration=Default, ipuLinkTopology=Mesh, gatewayMode=true, instanceSize=4\n23:48:33.631 3555537 POPRUN [D] Target options from Poplar device: {\"ipuLinkDomainSize\":\"64\",\"ipuLinkConfiguration\":\"default\",\"ipuLinkTopology\":\"mesh\",\"gatewayMode\":\"true\",\"instanceSize\":\"4\"}\n23:48:33.631 3555537 POPRUN [D] Using target options: {\"ipuLinkDomainSize\":\"4\",\"ipuLinkConfiguration\":\"default\",\"ipuLinkTopology\":\"mesh\",\"gatewayMode\":\"true\",\"instanceSize\":\"4\"}\n</code></pre> Expected output ends with this: <pre><code>Graph compilation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04&lt;00:00][1,0]&lt;stderr&gt;:2023-08-22T23:49:40.103248Z PO:ENGINE   3556102.3556102 W: WARNING: The compile time engine option debug.branchRecordTile is set to \"5887\" when creating the Engine. (At compile time it was set to 1471)\n[1,0]&lt;stderr&gt;:\nLoss:6.7539 [1,0]&lt;stdout&gt;:[INFO] Epoch 1\u2588\u2588\u2588\u2588\u258c| 75/78 [02:42&lt;00:06,  2.05s/it][1,0]&lt;stderr&gt;:\n[1,0]&lt;stdout&gt;:[INFO] loss: 6.7462,\n[1,0]&lt;stdout&gt;:[INFO] accuracy: 0.62 %\n[1,0]&lt;stdout&gt;:[INFO] throughput: 7599.7 samples/sec\n[1,0]&lt;stdout&gt;:[INFO] Epoch 2/2\nLoss:6.7462 | Accuracy:0.62%: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78/78 [02:48&lt;00:00,  2.16s/it][1,0]&lt;stderr&gt;:\nLoss:6.2821 | Accuracy:2.42%:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 75/7[1,0]&lt;stdout&gt;:[INFO] Epoch 2,0]&lt;stderr&gt;:\n[1,0]&lt;stdout&gt;:[INFO] loss: 6.2720,\n[1,0]&lt;stdout&gt;:[INFO] accuracy: 2.48 %\n[1,0]&lt;stdout&gt;:[INFO] throughput: 8125.8 samples/sec\n[1,0]&lt;stdout&gt;:[INFO] Finished training. Time: 2023-08-22 23:54:57.853508. It took: 0:05:26.090631\nLoss:6.2720 | Accuracy:2.48%: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78/78 [02:37&lt;00:00,  2.02s/it][1,0]&lt;stderr&gt;:\n[1,0]&lt;stderr&gt;:/usr/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 14 leaked semaphore objects to clean up at shutdown\n[1,0]&lt;stderr&gt;:  warnings.warn('resource_tracker: There appear to be %d '\n23:55:02.722 3555537 POPRUN [I] mpirun (PID 3556098) terminated with exit code 0\n</code></pre></p>"},{"location":"ai-testbed/graphcore/example-programs/#gpt-2-pytorch-pod16-run","title":"GPT-2 PyTorch - POD16 run","text":"<p>The scripts to train a GPT-2 pytorch model on the POD16 are located at https://github.com/graphcore/examples/tree/master/nlp/gpt2/pytorch</p> <p>In order to run the GPT-2 Pytorch model, create a new popTorch virtual environment poptorch33_gpt2 as described in the virtual environment section and activate it.</p> <pre><code>source ~/venvs/graphcore/poptorch33_gpt2/bin/activate\n</code></pre>"},{"location":"ai-testbed/graphcore/example-programs/#install-requirements_3","title":"Install Requirements","text":"<p>Change directory: <pre><code>cd ~/graphcore/examples/nlp/gpt2/pytorch\npip3 install -r requirements.txt\n</code></pre></p>"},{"location":"ai-testbed/graphcore/example-programs/#run-gpt2-on-16-ipus","title":"Run GPT2 on 16 IPUs","text":"<p>The command for the GPT2 model is as follows is as follows. <pre><code>/opt/slurm/bin/srun --ipus=16 python /home/$USER/graphcore/examples/nlp/gpt2/pytorch/train_gpt2.py --model gpt2 --ipus-per-replica 4 --replication-factor 4 --gradient-accumulation 2048 --device-iterations 8 --batch-size 1 --layers-per-ipu 0 4 4 4 --matmul-proportion 0.15 0.15 0.15 0.15 --max-len 1024 --optimizer AdamW --learning-rate 0.00015 --lr-schedule cosine --lr-warmup 0.01 --remap-logit True --enable-sequence-serialized True --embedding-serialization-factor 4 --recompute-checkpoint-every-layer True --enable-half-partials True --replicated-tensor-sharding True --dataset 'generated' --epochs 1\n</code></pre> It runs a <code>gpt2</code> model that fits on 4 IPUS indicated by <code>--ipus-per-replica</code>. The <code>--replication-factor</code> indicates how many times the model is replicated in a data parallel manner (4 in the above example). Hence the total number of IPUs used in this example is 16.</p> <p>The effective global batch size in this example is (micro)batch-size * gradient-accumulation * replication-factor = 1 x 2048 x 4 = 8192.  The device iterations indicates the total number samples loaded in 1 training step = global batch size * device iterations = 8192*8 = 65536. To learn more about these parameters and in general batching of IPUs refer IPU batching .</p> <p>The above example is running with <code>generated</code> or <code>synthetic data</code>. To use the same example with a real world dataset, refer to data setup.</p>"},{"location":"ai-testbed/graphcore/example-programs/#output_3","title":"Output","text":"<p>Expected output starts with the following: <pre><code>srun: job 10697 queued and waiting for resources\nsrun: job 10697 has been allocated resources\nBuilding (if necessary) and loading remap_tensor_ce.\nFailed to find compiled extension; rebuilding.\nBuilding (if necessary) and loading residual_add_inplace_pattern.\nModel initializing\n-------------------- Device Allocation --------------------\nEmbedding  --&gt; IPU 0\nLayer 0  --&gt; IPU 1\nLayer 1  --&gt; IPU 1\nLayer 2  --&gt; IPU 1\nLayer 3  --&gt; IPU 1\nLayer 4  --&gt; IPU 2\nLayer 5  --&gt; IPU 2\nLayer 6  --&gt; IPU 2\nLayer 7  --&gt; IPU 2\nLayer 8  --&gt; IPU 3\nLayer 9  --&gt; IPU 3\nLayer 10 --&gt; IPU 3\nLayer 11 --&gt; IPU 3\nLM_head --&gt; IPU 0\n</code></pre> Expected output ends with the following: <pre><code>step 0 of epoch 0, loss: 10.913220405578613, acc: 2.0071864128112793e-05, lr: 0.00012803300858899104, throughput: 646.8439205981404 samples/sec\nstep 1 of epoch 0, loss: 10.836345672607422, acc: 1.9788742065429688e-05, lr: 7.5e-05, throughput: 1058.0979097185766 samples/sec\nstep 2 of epoch 0, loss: 10.831247329711914, acc: 2.0518898963928223e-05, lr: 2.1966991411008938e-05, throughput: 1058.7595523807183 samples/sec\nstep 3 of epoch 0, loss: 10.829034805297852, acc: 1.990795135498047e-05, lr: 0.0, throughput: 1059.6762623043378 samples/sec\n</code></pre></p> <p>Note: The graph compilation for a large model like GPT-2 takes about half an hour. </p>"},{"location":"ai-testbed/graphcore/getting-started/","title":"Getting Started","text":"<p>Connection to a Graphcore node is a two-step process.</p> <p>The first step is to ssh from a local machine to the login node.</p> <p>The second step is to log in to a Graphcore node from the login node.</p> <p></p>"},{"location":"ai-testbed/graphcore/getting-started/#log-in-to-login-node","title":"Log in to Login Node","text":"<p>Login to the Graphcore login node from your local machine using the below command. This uses the ALCF account ID that uses the password generated from the MobilePASS+.</p> <p>Note:  In the examples below, replace ALCFUserID with your ALCF user id.</p> <pre><code>ssh ALCFUserID@gc-login-01.ai.alcf.anl.gov\n# or\nssh ALCFUserID@gc-login-02.ai.alcf.anl.gov\n</code></pre> <p>Note: Use the ssh \"-v\" option in order to debug any ssh problems.</p>"},{"location":"ai-testbed/graphcore/getting-started/#log-in-to-a-graphcore-node","title":"Log in to a Graphcore Node","text":"<p>Once you are on the login node, ssh to one of the Graphcore nodes.</p> <pre><code>ssh gc-poplar-02.ai.alcf.anl.gov\n# or\nssh gc-poplar-03.ai.alcf.anl.gov\n# or\nssh gc-poplar-04.ai.alcf.anl.gov\n</code></pre> <p>**Note: <code>ssh gc-poplar-01.ai.alcf.anl.gov</code> is not accessible to users. However, its IPU resources are assigned by the slurm tasks.</p>"},{"location":"ai-testbed/graphcore/job-queuing-and-submission/","title":"Job Queueing and Submission","text":""},{"location":"ai-testbed/graphcore/job-queuing-and-submission/#introduction","title":"Introduction","text":"<p>ALCF's Graphcore POD64 system uses Slurm for job submission and queueing. Below are some of the important commands for using Slurm. For more information refer to Slurm Documentation.</p> <p>NOTE: Jobs that require IPUs will fail unless launched with <code>srun</code> or <code>sbatch</code>. NOTE: There is a single Slurm scheduler for the Graphcore POD64.</p>"},{"location":"ai-testbed/graphcore/job-queuing-and-submission/#srun","title":"SRun","text":"<p>The Slurm command <code>srun</code> can be used to run individual Python scripts (or other programs) in parallel with other scripts on a cluster managed by Slurm. An example of <code>srun</code> usage is shown below. Use the <code>--ipus=</code> option to specify the number of IPUs required for the run.</p> <p>Example:</p> <pre><code>srun --ipus=1 python mnist_poptorch.py\n</code></pre>"},{"location":"ai-testbed/graphcore/job-queuing-and-submission/#sbatch","title":"SBatch","text":"<p>Alternatively, these jobs can be submitted to the Slurm workload manager through a batch script by using the <code>sbatch</code> command. To do this, create a bash script (submit-mnist-poptorch-job.sh here as an example) with the commands that you want to execute.</p> <pre><code>#!/bin/sh\n\npython mnist_poptorch.py\n</code></pre> <p>Then pass the bash script as an input to the <code>sbatch</code> command as shown below, requesting the number of IPUs required:</p> <pre><code>sbatch --ipus=1 --output=mnist-poptorch-output.log submit-mnist-poptorch-job.sh\n</code></pre>"},{"location":"ai-testbed/graphcore/job-queuing-and-submission/#squeue","title":"SQueue","text":"<p>The <code>squeue</code> command provides information about jobs located in the Slurm scheduling queue.</p> <pre><code>$ squeue\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n              2572       p64 Graphcor username  R       1:12      1 gc-poplar-02\n</code></pre>"},{"location":"ai-testbed/graphcore/job-queuing-and-submission/#sinfo","title":"SInfo","text":"<p>SInfo is used to view partition and node information for a system running Slurm.</p> <pre><code>$ sinfo\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\np64*         up   infinite      3   idle gc-poplar-[02-04]\n</code></pre> <p>For more information, see SInfo.</p>"},{"location":"ai-testbed/graphcore/job-queuing-and-submission/#scancel","title":"SCancel","text":"<p>SCancel is used to signal or cancel jobs, job arrays, or job steps.</p> <pre><code>scancel job_id\n</code></pre>"},{"location":"ai-testbed/graphcore/miscellaneous/","title":"Miscellaneous","text":""},{"location":"ai-testbed/graphcore/miscellaneous/#status","title":"Status","text":""},{"location":"ai-testbed/graphcore/miscellaneous/#gc-monitor","title":"GC-Monitor","text":"<p>The command <code>gc-monitor</code> is Graphcore's device usage monitor. Run it as follows for ordinary monitoring. See <code>gc-monitor --help</code> for other options.</p> <p><pre><code>export IPUOF_VIPU_API_HOST=10.1.3.101\ngc-monitor --no-card-info --all-partitions\n# or watch gc-monitor --no-card-info --all-partitions\n</code></pre> The IPUOF_VIPU_API_HOST environment variable can conflict with the running of poptorch programs.  The graphcore nodes have a convenience script that temporarily sets this environment variable. <pre><code>wrapped_gc_monitor.sh --no-card-info --all-partitions\n</code></pre></p> <p>Note: if there are no partitions active, gc-monitor will core dump: <code>Segmentation fault (core dumped)</code></p> <p>The output will look something like:</p> <pre><code>+--------------------------------------------------------------+-----------------------+\n|      IPUs in slurm_2616 attached from other namespaces       |         Board         |\n+----+------------------------------+--------------+-----------+-----------+-----------+\n| ID |       Application host       |    Clock     |   Temp    |   Temp    |   Power   |\n+----+------------------------------+--------------+-----------+-----------+-----------+\n| 0  |         gc-poplar-02         |   1850MHz    |  24.2 C   |  21.1 C   |  92.3 W   |\n+----+------------------------------+--------------+-----------+-----------+-----------+\n</code></pre>"},{"location":"ai-testbed/graphcore/miscellaneous/#gc-info","title":"GC-Info","text":"<p>The command <code>gc-info</code> is used to display device information. See <code>gc-info --help</code> for more options.</p> <p>To list devices,  <pre><code>gc-info -l\n</code></pre></p> <p>The command <code>gc-info</code> lists the partition and different IPU Id's along with the multi-IPU configuration IDs.</p> <pre><code>-+- Id:  [0], target: [Fabric], IPU-M host:  [10.1.5.1], IPU#: [3]\n-+- Id:  [1], target: [Fabric], IPU-M host:  [10.1.5.1], IPU#: [2]\n-+- Id:  [2], target: [Fabric], IPU-M host:  [10.1.5.1], IPU#: [1]\n</code></pre> <p>One may also display detailed information for a specific device.  The devices are numbered 0-63.  For example,</p> <pre><code>gc-info --device-id 0 --device-info\n</code></pre> <p>See <code>gc-info --help</code> for more information.</p>"},{"location":"ai-testbed/graphcore/miscellaneous/#how-busy-is-the-system","title":"How busy is the system?","text":"<p>Use one of</p> <pre><code>top\nhtop\n</code></pre>"},{"location":"ai-testbed/graphcore/running-a-model-or-program/","title":"Steps to Run a Model/Program","text":"<p>Note:  Please be mindful of how you are using the system. For example, consider running larger jobs in the evening or on weekends.</p> <p>Running of any model or application includes graph compilation of the model that is then deployed on the IPUs. Below is the description of training a neural network for classification on the MNIST dataset using the PopTorch (pytorch framework optimized for IPU).</p>"},{"location":"ai-testbed/graphcore/running-a-model-or-program/#examples-repo","title":"Examples Repo","text":"<p>Graphcore provides examples of some well-known AI applications in their repository at https://github.com/graphcore/examples.git.</p> <p>Clone the examples repository to your personal directory structure, and checkout the v3.3.0 release:</p> <pre><code>mkdir ~/graphcore\ncd ~/graphcore\ngit clone https://github.com/graphcore/examples.git\ncd examples\ngit checkout v3.3.0\n</code></pre>"},{"location":"ai-testbed/graphcore/running-a-model-or-program/#mnist","title":"MNIST","text":""},{"location":"ai-testbed/graphcore/running-a-model-or-program/#activate-poptorch-environment","title":"Activate PopTorch Environment","text":"<p>Follows the steps at Poptorch environment setup to enable the Poplar SDK.</p> <pre><code>source ~/venvs/graphcore/poptorch33_env/bin/activate\n</code></pre>"},{"location":"ai-testbed/graphcore/running-a-model-or-program/#install-requirements","title":"Install Requirements","text":"<p>Change directory and install packages specific to the MNIST model:</p> <pre><code>cd ~/graphcore/examples/tutorials/simple_applications/pytorch/mnist\n</code></pre>"},{"location":"ai-testbed/graphcore/running-a-model-or-program/#run-mnist","title":"Run MNIST","text":"<p>Execute the command:</p> <pre><code>/opt/slurm/bin/srun --ipus=1 python mnist_poptorch.py\n</code></pre> <p>All models are run using Slurm, with the <code>--ipus</code> indicating how many IPUs are need to be allocated for the model being run. This example uses a batchsize of 8, and run for 10 epochs. It also set the device iteration to 50 which is the number of iterations the device should run over the data before returning to the user.  The dataset used in the example is derived from the TorchVision and the PopTorch dataloader is used to load the data required for the 50 device iterations from the host to the device in a single step.</p> <p>The model used here is a simple CNN based model with an output from a classifier (softmax layer). A simple Pytorch model is translated to a PopTorch model using <code>poptorch.Options()</code>. <code>poptorch.trainingModel</code> is the model wrapping function on the Pytorch model. The first call to <code>trainingModel</code> will compile the model for the IPU. You can observe the compilation process as part of output of the above command.</p> <pre><code>Graph compilation:   3%|\u258e         | 3/100 [00:00&lt;00:03]2023-04-26T16:53:21.225944Z PL:POPLIN    3680893.3680893 W: poplin::preplanMatMuls() is deprecated! Use poplin::preplan() instead\nGraph compilation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:20&lt;00:00]2023-04-26T16:53:38.241395Z popart:session 3680893.3680893\n</code></pre> <p>The artifacts from the graph compilations is cached in the location set by the flag <code>POPTORCH_CACHE_DIR</code>, where the <code>.popef</code> file corresponding to the model under consideration is cached.</p>"},{"location":"ai-testbed/graphcore/running-a-model-or-program/#output","title":"Output","text":"<p>The expected output will start with downloads followed by and we can observe the model used by the model, the progress bar of the compilation process, and the training progress bar.</p> <pre><code>srun: job 10671 queued and waiting for resources\nsrun: job 10671 has been allocated resources\nTrainingModelWithLoss(\n  (model): Network(\n    (layer1): Block(\n      (conv): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (relu): ReLU()\n    )\n    (layer2): Block(\n      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (relu): ReLU()\n    )\n    (layer3): Linear(in_features=1600, out_features=128, bias=True)\n    (layer3_act): ReLU()\n    (layer3_dropout): Dropout(p=0.5, inplace=False)\n    (layer4): Linear(in_features=128, out_features=10, bias=True)\n    (softmax): Softmax(dim=1)\n  )\n  (loss): CrossEntropyLoss()\n)\nEpochs:   0%|          | 0/10 [00:00&lt;?,[23:27:06.753] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 10\nGraph compilation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00&lt;00:00]\nEpochs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:17&lt;00:00,  7.71s/it]\nGraph compilation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00&lt;00:00]                          \nAccuracy on test set: 96.85%\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00&lt;00:00]\n</code></pre> <p>Refer to the script to learn more about this example.</p> <p>Example Programs lists the different example applications with corresponding commands for each of the above steps.</p>"},{"location":"ai-testbed/graphcore/system-overview/","title":"System Overview","text":"<p>The Graphcore Bow-Pod64 system is the latest-generation AI accelerator from Graphcore. This is a one-rack system consisting of 64 Bow-class Intelligence Processing Units (IPU) with a custom interconnect. The system provides for an aggregate 22 Petaflops/s of performance in half precision. It has a total of 57.6 GB In-Processor-Memory with a total of 94,208 IPU cores. The system consists of four servers for data-processing. </p> <p>For more details refer to the POD64 spec </p> <p> (Figure from https://www.graphcore.ai/products/poplar)</p> <p>The Graphcore software stack includes support for TensorFlow and PyTorch using the Poplar SDK. The Poplar\u00ae SDK is t is the toolchain specifically designed for creating graph software for ML applications.  It integrates with the traditional ML frameworks like PyTorch and TensorFlow allowing users to port their existing code to the IPU hardware-specific code. The various components of the poplar SDK stack are shown in the figure. It includes the PopTorch framework which is a wrapper over the PyTorch framework optimized to the IPU hardware. It also enlists the different PopLibs libraries supported, which enables to construct graphs, define tensor data and control how the code and data are mapped onto the IPU for execution.  </p>"},{"location":"ai-testbed/graphcore/virtual-environments/","title":"Virtual Environments","text":""},{"location":"ai-testbed/graphcore/virtual-environments/#poplar-sdk-setup","title":"Poplar SDK Setup","text":"<p>The Poplar SDK is downloaded onto the graphcore systems at the <code>/software/graphcore/poplar_sdk/</code> location. The default poplar version (3.3.0) is enabled automatically upon logging into a graphcore node.</p> <p>Check if Poplar is setup correctly:</p> <pre><code>popc --version\n</code></pre> <p>One should see:</p> <pre><code>POPLAR version 3.3.0 (de1f8de2a7)\nclang version 16.0.0 (2fce0648f3c328b23a6cbc664fc0dd0630122212)\n</code></pre> <p>If the Poplar SDK is not enabled, it can be enabled with <pre><code>source /software/graphcore/poplar_sdk/3.3.0/enable\n</code></pre></p> <p>To disable the current Poplar SDK, e.g. if one wants to use a different Poplar SDK, follow the steps below. (Otherwise, skip to section Miscellaneous Environment Variables.) This example assumes that the current installed SDK is 3.1.0 and you want to move to 3.3.0</p> <ol> <li>Check the current version    <pre><code> $ popc --version\n POPLAR version 3.1.0 (e12d5f9f01)\n clang version 15.0.0 (bab932b4fc4cdb58bb009370384b2c41579bd9d9)\n</code></pre></li> <li>Unset the current version    <pre><code>unset POPLAR_SDK_ENABLED\n</code></pre></li> <li>Enable poplar and popart    <pre><code>source /software/graphcore/poplar_sdk/3.3.0/poplar-ubuntu_20_04-3.3.0+7857-b67b751185/enable.sh \nsource /software/graphcore/poplar_sdk/3.3.0/popart-ubuntu_20_04-3.3.0+7857-b67b751185/enable.sh \n</code></pre></li> <li>Recheck for the new version.    <pre><code>$popc --version\nPOPLAR version 3.3.0 (de1f8de2a7)\nclang version 16.0.0 (2fce0648f3c328b23a6cbc664fc0dd0630122212)\n</code></pre></li> <li> <p>Set SDK env variable    <pre><code>POPLAR_SDK_ROOT=/software/graphcore/poplar_sdk/3.3.0/\nexport POPLAR_SDK_ROOT=$POPLAR_SDK_ROOT\n</code></pre></p> </li> <li> <p>Create a new virtual environment with this SDK and install popTorch and or other frameworks as needed.    <pre><code>virtualenv ~/Graphcore/workspace/poptorch33_env\nsource ~/Graphcore/workspace/poptorch33_env/bin/activate\npip install $POPLAR_SDK_ROOT/poptorch-3.3.0+113432_960e9c294b_ubuntu_20_04-cp38-cp38-linux_x86_64.whl\nexport PYTHONPATH=$POPLAR_SDK_ROOT/python:$PYTHONPATH\n</code></pre></p> </li> </ol>"},{"location":"ai-testbed/graphcore/virtual-environments/#miscellaneous-environment-variables","title":"Miscellaneous Environment Variables","text":"<pre><code>mkdir ~/tmp\nexport TF_POPLAR_FLAGS=--executable_cache_path=~/tmp\nexport POPTORCH_CACHE_DIR=~/tmp\n\nexport POPART_LOG_LEVEL=WARN\nexport POPLAR_LOG_LEVEL=WARN\nexport POPLIBS_LOG_LEVEL=WARN\n\nexport PYTHONPATH=/software/graphcore/poplar_sdk/3.3.0/poplar-ubuntu_20_04-3.3.0+7857-b67b751185/python:$PYTHONPATH\n</code></pre>"},{"location":"ai-testbed/graphcore/virtual-environments/#poptorch-environment-setup","title":"PopTorch Environment Setup","text":"<p>PopTorch is an extension of the Pytorch framework that is optimized for the IPU specific functionality. To activate the PopTorch environment, first create a virtual environment and activate it.</p> <pre><code>mkdir -p ~/venvs/graphcore\nvirtualenv ~/venvs/graphcore/poptorch33_env\nsource ~/venvs/graphcore/poptorch33_env/bin/activate\n</code></pre> <p>Use the following commands to install the PopTorch environment.</p> <pre><code>POPLAR_SDK_ROOT=/software/graphcore/poplar_sdk/3.3.0\nexport POPLAR_SDK_ROOT=$POPLAR_SDK_ROOT\npip install $POPLAR_SDK_ROOT/poptorch-3.3.0+113432_960e9c294b_ubuntu_20_04-cp38-cp38-linux_x86_64.whl\n</code></pre>"},{"location":"ai-testbed/graphcore/virtual-environments/#tensorflow-2-environment-setup","title":"TensorFlow 2 Environment Setup","text":"<p>The Poplar SDK provides TensorFlow and Keras wheels built on 2.6 that includes the IPU specific functionality and optimized for the AMD processors. It can be installed as follows.</p> <p>Create virtual environment.</p> <pre><code>virtualenv ~/venvs/graphcore/tensorflow2_33_env\nsource ~/venvs/graphcore/tensorflow2_33_env/bin/activate\n</code></pre> <p>Install the TensorFlow and Keras wheels.</p> <pre><code>POPLAR_SDK_ROOT=/software/graphcore/poplar_sdk/3.3.0\nexport POPLAR_SDK_ROOT=$POPLAR_SDK_ROOT\npip install $POPLAR_SDK_ROOT/tensorflow-2.6.3+gc3.3.0+251580+08d96978c7f+amd_znver1-cp38-cp38-linux_x86_64.whl\npip install $POPLAR_SDK_ROOT/keras-2.6.0+gc3.3.0+251582+a3785372-py2.py3-none-any.whl\n</code></pre>"},{"location":"ai-testbed/graphcore/virtual-environments/#verify-installation","title":"Verify Installation","text":"<pre><code>python -c \"from tensorflow.python import ipu\"\n</code></pre> <p>You should see:</p> <pre><code>2023-08-22 21:53:26.109934: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 3.3.0 (de1f8de2a7) Poplar package: b67b751185\n</code></pre>"},{"location":"ai-testbed/graphcore/virtual-environments/#installing-packages","title":"Installing Packages","text":"<p>Install packages in the normal manner such as:</p> <pre><code>python3 -m pip install \"some_package\"\n</code></pre> <p>For more details see Use pip for installing.</p> <p>To install a different version of a package that is already installed in one's environment, one can use:</p> <pre><code>pip install --ignore-installed  ... # or -I\n</code></pre> <p>Note: Conda is not supported on the Graphcore system.</p>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/","title":"Scaling ResNet50","text":"<p>Follow all the instructions in Getting Started to log into a Graphcore node.</p>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#examples-repo","title":"Examples Repo","text":"<p>Graphcore provides examples of some well-known AI applications in their repository at https://github.com/graphcore/examples.git.</p> <p>Clone the examples repository to your personal directory structure:</p> <pre><code>mkdir ~/graphcore\ncd ~/graphcore\ngit clone https://github.com/graphcore/examples.git\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#environment-setup","title":"Environment Setup","text":"<p>Establish a virtual environment.</p> <pre><code>mkdir -p ~/venvs/graphcore\nrm -rf ~/venvs/graphcore/poptorch31_rn50_env\nvirtualenv ~/venvs/graphcore/poptorch31_rn50_env\nsource ~/venvs/graphcore/poptorch31_rn50_env/bin/activate\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#install-poptorch","title":"Install PopTorch","text":"<p>Install PopTorch.</p> <pre><code>POPLAR_SDK_ROOT=/software/graphcore/poplar_sdk/3.1.0\nexport POPLAR_SDK_ROOT=$POPLAR_SDK_ROOT\npip install $POPLAR_SDK_ROOT/poptorch-3.1.0+98660_0a383de63f_ubuntu_20_04-cp38-cp38-linux_x86_64.whl\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#environment-variables","title":"Environment Variables","text":"<p>Establish the following environment variables.</p> <pre><code>mkdir ${HOME}/tmp\nexport TF_POPLAR_FLAGS=--executable_cache_path=${HOME}/tmp\nexport POPTORCH_CACHE_DIR=${HOME}/tmp\nexport POPART_LOG_LEVEL=WARN\nexport POPLAR_LOG_LEVEL=WARN\nexport POPLIBS_LOG_LEVEL=WARN\nexport PYTHONPATH=/software/graphcore/poplar_sdk/3.1.0/poplar-ubuntu_20_04-3.1.0+6824-9c103dc348/python:$PYTHONPATH\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#install-requirements","title":"Install Requirements","text":"<pre><code>cd ${HOME}/graphcore/examples/vision/cnns/pytorch/\nmake install\nmake install-turbojpeg\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#one-time-per-user-ssh-key-set-up","title":"One-time per user ssh key set up","text":"<p>Set up the ssh key on gc-poplar-01.</p>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#gc-poplar-01","title":"Gc-poplar-01","text":"<p>On gc-poplar-01:</p> <pre><code>mkdir ~/.ssh\ncd ~/.ssh\nssh-keygen -t rsa -b 4096\n#Accecpt default filename of id_rsa\n#Enter passphrase (empty for no passphrase):\n#Enter same passphrase again:\ncat id_rsa.pub &gt;&gt; authorized_keys\n</code></pre> <pre><code>ssh-keyscan -H gc-poplar-01 &gt;&gt; ~/.ssh/known_hosts\n</code></pre> <p>You should see:</p> <pre><code># gc-poplar-01:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-01:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-01:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-01:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-01:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n</code></pre> <pre><code>ssh-keyscan -H gc-poplar-02 &gt;&gt; ~/.ssh/known_hosts\n</code></pre> <p>You should see:</p> <pre><code># gc-poplar-02:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-02:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-02:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-02:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-02:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n</code></pre> <pre><code>ssh-keyscan -H gc-poplar-03 &gt;&gt; ~/.ssh/known_hosts\n</code></pre> <p>You should see:</p> <pre><code># gc-poplar-03:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-03:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-03:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-03:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-03:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n</code></pre> <pre><code>ssh-keyscan -H gc-poplar-04 &gt;&gt; ~/.ssh/known_hosts\n</code></pre> <p>You should see:</p> <pre><code># gc-poplar-04:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-04:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-04:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-04:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-04:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#benchmarksyml","title":"<code>benchmarks.yml</code>","text":"<p>Update ${HOME}/graphcore/examples/vision/cnns/pytorch/train/benchmarks.yml with your favorite editor to match benchmarks.yml.</p>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#configsyml","title":"<code>configs.yml</code>","text":"<p>Update ${HOME}/graphcore/examples/vision/cnns/pytorch/train/configs.yml with your favorite editor.  At about line 30, change use_bbox_info: true to use_bbox_info: false.</p>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#scale-resnet50","title":"Scale ResNet50","text":"<p>Scale and benchmark ResNet50.</p> <p>Note: The number at the end of each line indicates the number of IPUs.</p> <p>Note: Use screen because every run is long.</p> <p>\"PopRun exposes this control with the --process-placement flag and provides multiple pre-defined strategies. By default (and with --process-placement spreadnuma), PopRun is designed to be NUMA-aware. On each host, all the available NUMA nodes are divided among the instances. This means that each instance is bound to execute on and allocate memory from its assigned NUMA nodes, ensuring memory access locality. This strategy maximises memory bandwidth and is likely to yield optimal performance for most of the data loading workloads in machine learning.\" [Multi-Instance Multi-Host(https://docs.graphcore.ai/projects/poprun-user-guide/en/latest/launching.html#multi-instance-multi-host)</p>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#setup","title":"Setup","text":"<p>Move to the correct directory and establish the datasets directory.</p> <pre><code>cd ${HOME}/graphcore/examples/vision/cnns/pytorch/train\nexport DATASETS_DIR=/mnt/localdata/datasets/\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#scaling-to-16-ipus","title":"Scaling to 16 IPUs","text":"<p>One may use any of the following commands to run ResNet50 on one to sixteen IPUs.</p> <pre><code>python3 -m examples_utils benchmark --spec benchmarks.yml --benchmark pytorch_resnet50_train_real_1\npython3 -m examples_utils benchmark --spec benchmarks.yml --benchmark pytorch_resnet50_train_real_2\npython3 -m examples_utils benchmark --spec benchmarks.yml --benchmark pytorch_resnet50_train_real_4\npython3 -m examples_utils benchmark --spec benchmarks.yml --benchmark pytorch_resnet50_train_real_8\npython3 -m examples_utils benchmark --spec benchmarks.yml --benchmark pytorch_resnet50_train_real_pod16\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#scaling-to-64-ipus","title":"Scaling to 64 IPUs","text":"<p>Note: One must complete the instructions on Multi-node Setup before running this example.</p>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#establish-environment-variables","title":"Establish Environment Variables","text":"<pre><code>HOST1=`ifconfig eno1 | grep \"inet \" | grep -o '[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}' | head -1`\nOCT123=`echo \"$HOST1\" | cut -d \".\" -f 1,2,3`\nOCT4=`echo \"$HOST1\" | cut -d \".\" -f 4`\nHOST2=$OCT123.`expr $OCT4 + 1`\nHOST3=$OCT123.`expr $OCT4 + 2`\nHOST4=$OCT123.`expr $OCT4 + 3`\nexport HOSTS=$HOST1,$HOST2,$HOST3,$HOST4\nexport CLUSTER=c16\nexport IPUOF_VIPU_API_PARTITION_ID=p64\nexport TCP_IF_INCLUDE=$OCT123.0/8\nexport IPUOF_VIPU_API_HOST=$HOST1\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#64-ipu-run","title":"64 IPU Run","text":"<p>This runs to convergence.  It uses all 64 IPUs for more than 12 hours.</p> <p>Note: This should only be used if absolutely required.</p> <p>Execute:</p> <pre><code>python3 -m examples_utils benchmark --spec benchmarks.yml --benchmark pytorch_resnet50_train_real_pod64\npython3 -m examples_utils benchmark --spec benchmarks.yml --benchmark pytorch_resnet50_train_real_pod64_conv\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#benchmark-results","title":"Benchmark Results","text":""},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#one-ipu","title":"One IPU","text":"<pre><code>[INFO] 2022-12-16 17:07:32: Total runtime: 3956.836479 seconds\n[INFO] 2022-12-16 17:07:32:    throughput = '7527.626315789474'\n[INFO] 2022-12-16 17:07:32:    accuracy = '57.41'\n[INFO] 2022-12-16 17:07:32:    loss = '2.8153'\n[INFO] 2022-12-16 17:07:33:    Total compile time: 429.59 seconds\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#two-ipus","title":"Two IPUs","text":"<pre><code>[INFO] 2022-12-16 15:56:23: Total runtime: 5866.494071 seconds\n[INFO] 2022-12-16 15:56:23:    throughput = '4798.778947368421'\n[INFO] 2022-12-16 15:56:23:    accuracy = '68.23'\n[INFO] 2022-12-16 15:56:23:    loss = '2.3148'\n[INFO] 2022-12-16 15:56:24:    Total compile time: 418.75 seconds\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#four-ipus","title":"Four IPUs","text":"<pre><code>[INFO] 2022-12-16 04:05:28: Total runtime: 3070.994553 seconds\n[INFO] 2022-12-16 04:05:28:    throughput = '9959.821052631578'\n[INFO] 2022-12-16 04:05:28:    accuracy = '67.76'\n[INFO] 2022-12-16 04:05:28:    loss = '2.338'\n[INFO] 2022-12-16 04:05:29:    Total compile time: 377.4 seconds\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#eight-ipus","title":"Eight IPUs","text":"<pre><code>[INFO] 2022-12-16 02:46:45: Total runtime: 1831.437598 seconds\n[INFO] 2022-12-16 02:46:45:    throughput = '19865.263157894733'\n[INFO] 2022-12-16 02:46:45:    accuracy = '64.94'\n[INFO] 2022-12-16 02:46:45:    loss = '2.4649'\n[INFO] 2022-12-16 02:46:46:    Total compile time: 386.27 seconds\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#sixteen-ipus","title":"Sixteen IPUs","text":"<p>Epochs: 20</p> <pre><code>[INFO] 2022-12-15 22:01:14: Total runtime: 1297.274336 seconds\n[INFO] 2022-62:01:14:    throughput = '39057.447368421046'\n[INFO] 2022-12-15 22:01:14:    accuracy = '57.43'\n[INFO] 2022-12-15 22:01:14:    loss = '2.8162'\n[INFO] 2022-12-15 22:01:16:    Total compile time: 397.08 seconds\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#sixty-four-ipus","title":"Sixty-Four IPUs","text":"<pre><code>[1,0]&lt;stdout&gt;:[INFO] loss: 4.8367,\n[1,0]&lt;stdout&gt;:[INFO] accuracy: 18.83 %\n[1,0]&lt;stdout&gt;:[INFO] throughput: 51368.5 samples/sec\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/","title":"CosmicTagger Conversion","text":"<p>The intent of this page is to show conceptually how to convert a model to run on the Graphcore system. It is not necessary to convert CosmicTagger because it has already been converted and is located at CosmicTagger on the Graphcore branch. The original is located at CosmicTagger.</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#run-model-on-cpu","title":"Run Model on CPU","text":"<p>The first step to converting a model is to verify that it runs on the CPU.  This step has been verified for CosmicTagger.</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#configpy","title":"Config.py","text":"<p>CosmicTagger can run on multiple machines.  As such, it is necessary to specify the architecture that one is using.  For example, CPU or GPU.  The architecture is stored in the ComputeMode class.</p> <p>Edit src/config/config.py.  Add IPU to the ComputeMode class.</p> <pre><code>class ComputeMode(Enum):\n    CPU   = 0\n    #...\n    IPU   = 5\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#trainerpy","title":"Trainer.py","text":"<p>Edit src/utils/torch/trainer.py.</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#import-poptorch","title":"Import PopTorch","text":"<p>PopTorch is Graphcore's extension of PyTorch.</p> <p>Import poptorch at the top of the file.</p> <pre><code>import poptorch\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#wrap-model","title":"Wrap Model","text":"<p>Wrap the model using poptorch.trainingModel() so that it may be ran on IPUs for training.</p> <p>Wrap the model using poptorch.inferenceModel() when not training.</p> <p>Find the following code around line 90 in the init_network method.</p> <pre><code>        # Foregoing any fusions as to not disturb the existing ingestion pipeline\n        if self.is_training() and self.args.mode.quantization_aware:\n            self._raw_net.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n            self._net = torch.quantization.prepare_qat(self._raw_net)\n        else:\n            self._net = self._raw_net\n</code></pre> <p>After the above code, add:</p> <pre><code>        if self.args.run.compute_mode == ComputeMode.IPU:\n            if self.is_training():\n                opts = poptorch.Options()\n                self._net = poptorch.trainingModel(self._net, opts, optimizer=torch.optim.SGD(self._net.parameters(), lr=1e-3))\n            else:\n                self._net = poptorch.inferenceModel(self._net)\n</code></pre> <p>See poptorch.trainingModel() and poptorch.inferenceModel() for more information.</p> <p>There is also a Build the Model tutorial.</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#update-optimizer","title":"Update Optimizer","text":"<p>Update init_optimizer() to use the poptorch class instead of the torch class as needed.</p> <p>Change:</p> <pre><code>        if self.args.mode.optimizer.name == OptimizerKind.rmsprop:\n            self._opt = torch.optim.RMSprop(self._net.parameters(), 1.0, eps=1e-4)\n        else:\n            self._opt = torch.optim.Adam(self._net.parameters(), 1.0)\n</code></pre> <p>to:</p> <pre><code>        if self.args.mode.optimizer.name == OptimizerKind.rmsprop:\n            if self.args.run.compute_mode == ComputeMode.IPU:\n                self._opt = poptorch.optim.RMSprop(self._net.parameters(), 1.0, eps=1e-4)\n            else:\n                self._opt = torch.optim.RMSprop(self._net.parameters(), 1.0, eps=1e-4)\n        else:\n            if self.args.run.compute_mode == ComputeMode.IPU:\n                self._opt = poptorch.optim.Adam(self._net.parameters(), 1.0)\n            else:\n                self._opt = torch.optim.Adam(self._net.parameters(), 1.0)\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#update-the-forward-pass","title":"Update the Forward Pass","text":"<p>Putting the loss calculation in forward_pass() allows the loss computation to be performed on the IPUs. This will be faster because the data will not need to be transfered round-trip to the CPU.</p> <p>Change forward_pass():</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#original","title":"Original","text":"<pre><code>            if net is None:\n                logits_image = self._net(minibatch_data['image'])\n            else:\n                logits_image = net(minibatch_data['image'])\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#updated","title":"Updated","text":"<p>The following code changes are to account for the loss function, i.e., self.loss_calculator, and the image labels, i.e., labels_image, to be passed to the model's forward_pass method.  Additionally, the calculated loss is returned from the forward_pass method.</p> <pre><code>            if net is None:\n                if self.args.run.compute_mode == ComputeMode.IPU:\n                    logits_image, labels_image, loss = self._net(minibatch_data['image'], self.loss_calculator, labels_image)\n                    return logits_image, labels_image, loss\n                else:\n                    logits_image = self._net(minibatch_data['image'])\n            else:\n                if self.args.run.compute_mode == ComputeMode.IPU and self.args.mode.name != ModeKind.inference:\n                    logits_image, labels_image, loss = net(minibatch_data['image'], self.loss_calculator, labels_image)\n                    return logits_image, labels_image, loss\n                else:\n                    logits_image = net(minibatch_data['image'])\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#update-the-training-step","title":"Update the Training Step","text":"<p>Receive the extra loss variable from the forward_pass method.</p> <p>Update the train_step method.</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#original-training-step","title":"Original Training Step","text":"<pre><code>                    with self.timing_context(\"forward\"):\n                        if self.args.run.precision == Precision.mixed and self.args.run.compute_mode == ComputeMode.GPU:\n                            with torch.cuda.amp.autocast():\n                                logits_image, labels_image = self.forward_pass(minibatch_data)\n                        else:\n                            logits_image, labels_image = self.forward_pass(minibatch_data)\n\n                    verbose = False\n\n                    # Compute the loss based on the logits\n                    with self.timing_context(\"loss\"):\n                        loss = self.loss_calculator(labels_image, logits_image)\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#updated-training-step","title":"Updated Training Step","text":"<p>The forward_pass() method was changed to return the extra variable loss in the previous section.  It is now received conditionally when using an IPU(s).</p> <p>In the with self.timing_context(\"loss\"): section, only calculate loss if not using an IPU(s).</p> <pre><code>                    with self.timing_context(\"forward\"):\n                        if self.args.run.precision == Precision.mixed and self.args.run.compute_mode == ComputeMode.GPU:\n                            with torch.cuda.amp.autocast():\n                                logits_image, labels_image = self.forward_pass(minibatch_data)\n                        else:\n                            if self.args.run.compute_mode == ComputeMode.IPU:\n                                logits_image, labels_image, loss = self.forward_pass(minibatch_data)\n                            else:\n                                logits_image, labels_image = self.forward_pass(minibatch_data)\n\n                    verbose = False\n\n\n                    # Compute the loss based on the logits\n                    with self.timing_context(\"loss\"):\n                        if self.args.run.compute_mode == ComputeMode.IPU:\n                            loss = loss\n                        else:\n                            loss = self.loss_calculator(labels_image, logits_image)\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#update-validation-step","title":"Update Validation Step","text":"<p>Update the val_step method.</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#original-validation-step-code","title":"Original Validation Step Code","text":"<p>Find this code.</p> <pre><code>            if self.args.run.precision == Precision.mixed and self.args.run.compute_mode == ComputeMode.GPU:\n                with torch.cuda.amp.autocast():\n                    logits_image, labels_image = self.forward_pass(minibatch_data, net=val_net)\n            else:\n                logits_image, labels_image = self.forward_pass(minibatch_data, net=val_net)\n\n            # Compute the loss based on the logits\n            loss = self.loss_calculator(labels_image, logits_image)\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#updated-validation-step-code","title":"Updated Validation Step Code","text":"<p>Change the code to the following.</p> <pre><code>            if self.args.run.precision == Precision.mixed and self.args.run.compute_mode == ComputeMode.GPU:\n                with torch.cuda.amp.autocast():\n                    logits_image, labels_image = self.forward_pass(minibatch_data, net=val_net)\n\n                    # Compute the loss based on the logits\n                    loss = self.loss_calculator(labels_image, logits_image)\n            else:\n                if self.args.run.compute_mode == ComputeMode.IPU:\n                    logits_image, labels_image, loss = self.forward_pass(minibatch_data, net=val_net)\n                else:\n                    logits_image, labels_image = self.forward_pass(minibatch_data, net=val_net)\n\n                    # Compute the loss based on the logits\n                    loss = self.loss_calculator(labels_image, logits_image)\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#uresnet2d-model","title":"UResNet2D Model","text":""},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#update-model","title":"Update Model","text":"<p>The Graphcore system is more computationally efficient if the loss function is on the IPU.  This is accomplished by using the loss function within the model's forward method.</p> <p>Edit src/networks/torch/uresnet2D.py.</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#update-the-forward-declaration","title":"Update the Forward Declaration","text":"<p>Find the forward method.</p> <pre><code>def forward(self, input_tensor):\n</code></pre> <p>Update the argument list to include the loss function, i.e., loss_calculator and the image labels, i.e., labels_image.</p> <pre><code>def forward(self, input_tensor, loss_calculator=None, labels_image=None):\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#add-loss-calculation","title":"Add Loss Calculation","text":"<p>Add the loss calculation just before the forward method returns.</p> <pre><code>        if loss_calculator is not None:\n\n            labels_image = labels_image.long()\n            labels_image = torch.chunk(labels_image, chunks=3, dim=1)\n            shape =  labels_image[0].shape\n            labels_image = [ _label.view([shape[0], shape[-2], shape[-1]]) for _label in labels_image ]\n\n            loss = loss_calculator(labels_image, x)\n            import poptorch\n            loss = poptorch.identity_loss(loss , reduction=\"mean\")\n            return x, labels_image, loss\n\n        # This return already exists.\n        return x\n</code></pre> <p>The poptorch.identity_loss method takes a single PyTorch tensor and will backpropagate a gradient of ones through it.  You may find an example at here</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#binexecpy","title":"bin/exec.py","text":"<p>The following is included for completeness.  One will not likely find this in other code.</p> <p>Open bin/exec.py in your favorite editor.  Change:</p> <pre><code>@hydra.main(version_base=None, config_path=\"../src/config\", config_name=\"config\")\n</code></pre> <p>to</p> <pre><code>@hydra.main(config_path=\"../src/config\", config_name=\"config\")\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-ddp/","title":"CosmicTagger Conversion","text":"<p>The intent of this page is to show conceptually how to convert a Graphcore model to run on Distributed Data Parallel using PopDist. It is not necessary to convert CosmicTagger because it has already been converted and is located at CosmicTagger on the GraphcoreDDP branch. The original is located at CosmicTagger.</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-ddp/#run-model-on-cpu","title":"Run Model on CPU","text":"<p>The first step to converting a model is to verify that it runs on the CPU.  This step has been verified for CosmicTagger.</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-ddp/#starter-code","title":"Starter Code","text":"<p>You may use the code at CosmicTagger on the Graphcore branch.</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-ddp/#trainerpy","title":"Trainer.py","text":"<p>Edit src/utils/torch/trainer.py.</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-ddp/#import-poplar-packages","title":"Import Poplar Packages","text":"<p>PopTorch is Graphcore's extension of PyTorch.</p> <p>PopDist is Graphcore's distributed processing package.</p> <p>Import poptorch and popdist at the top of the file.</p> <pre><code>try:\n    import poptorch\n    import popdist\n    import popdist.poptorch\nexcept:\n    pass\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-ddp/#initialization","title":"Initialization","text":"<p>Initialize popdist for distributed computing.</p> <p>Establish a class variable name instance.  This is used to differentiate between different model instances that will be saved.</p> <p>Add the following line at the bottom of init().</p> <pre><code>        if self.args.run.compute_mode == ComputeMode.IPU and popdist.isPopdistEnvSet():\n            popdist.init()\n            self._instance = popdist.getInstanceIndex()\n        else:\n            self._instance = 0\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-ddp/#use-instance-variable","title":"Use Instance Variable","text":"<p>Use the instance variable for the model file name.</p> <p>Find def get_model_filepath.</p> <p>Change:</p> <pre><code>        name = file_path + 'model-{}.ckpt'.format(self._global_step)\n</code></pre> <p>To:</p> <pre><code>        name = file_path + f'model-{self._global_step}-{self._instance}.ckpt'\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-ddp/#establish-logging-method","title":"Establish Logging Method","text":"<p>Add a helper function to log data at the bottom of the file.</p> <pre><code>    def log_in_single_instance(self, string):\n        if self.args.run.compute_mode == ComputeMode.IPU:\n            if not popdist.isPopdistEnvSet() or popdist.getInstanceIndex() == 0:\n                logging.info(string)\n        else:\n            logging.info(string)\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-ddp/#update-init_network","title":"Update Init_network()","text":"<p>PopTorch has an Option() method which returns values that get passed to poptorch.trainingModel. The returned values are stored in opts in this example.</p> <p>Find:</p> <pre><code>        if self.args.run.compute_mode == ComputeMode.IPU:\n            if self.is_training():\n                opts = poptorch.Options()\n                self._net = poptorch.trainingModel(self._net, opts, optimizer=torch.optim.SGD(self._net.parameters(), lr=1e-3))\n            else:\n                self._net = poptorch.inferenceModel(self._net)\n</code></pre> <p>Replace it with:</p> <pre><code>        if self.args.run.compute_mode == ComputeMode.IPU:\n            if popdist.isPopdistEnvSet():\n                opts = popdist.poptorch.Options()\n                # When using the dataloader with 'auto_distributed_partitioning=True'\n                # and 'shuffle=True' we must set the random seed to ensure that tensors\n                # are in the same order in all processes.\n                opts.randomSeed(42)\n                # Replication factor is already set via PopRun so\n                # we ignore 'args.num_replicas'.\n                logging.info(f\"Num of local replicas: {popdist.getNumLocalReplicas()}\")\n            else:\n                opts = poptorch.Options()\n                opts.replicationFactor(self.args.num_replicas)\n\n            if self.is_training():\n                self._net = poptorch.trainingModel(self._net, opts, optimizer=torch.optim.SGD(self._net.parameters(), lr=1e-3))\n            else:\n                self._net = poptorch.inferenceModel(self._net)\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-ddp/#run-the-code","title":"Run The Code","text":"<p>See instructions in README_GRAPHCORE.md.</p>"},{"location":"ai-testbed/graphcore/unused/multi-node-setup/","title":"Multi-node Setup","text":"<p>These steps only need to be executed once per user.</p> <p>Running on multiple nodes is a three step process.</p> <ol> <li> <p>Create a Key</p> <pre><code>cd ~/.ssh\nssh-keygen -t rsa -b 4096\n</code></pre> </li> <li> <p>Put Key into Authorized_keys File</p> <pre><code>cat id_rsa.pub &gt;&gt; authorized_keys\n</code></pre> </li> <li> <p>Add Node IP Addresses to Known_hosts File</p> <pre><code>ssh-keyscan -H 10.1.3.101 &gt;&gt; ~/.ssh/known_hosts\nssh-keyscan -H 10.1.3.102 &gt;&gt; ~/.ssh/known_hosts\nssh-keyscan -H 10.1.3.103 &gt;&gt; ~/.ssh/known_hosts\nssh-keyscan -H 10.1.3.104 &gt;&gt; ~/.ssh/known_hosts\n</code></pre> </li> </ol>"},{"location":"ai-testbed/graphcore/unused/profiling-mnist/","title":"Profiling MNIST","text":"<p>Follow all the instructions in Getting Started to log into a Graphcore node.</p> <p>Follow the instructions in Virtual Environments up to and including PopART Environment Setup.</p> <p>Following the instructions in Example Programs up to and including MNIST, Install Requirements.</p>"},{"location":"ai-testbed/graphcore/unused/profiling-mnist/#change-directory","title":"Change Directory","text":"<pre><code>cd ~/graphcore/tutorials/simple_applications/pytorch/mnist\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/profiling-mnist/#set-poplar-options","title":"Set Poplar Options","text":"<p>Set the option to generate all reports, i.e., \"autoReport.all\":\"true\".</p> <p>Set the reports directory, i.e., \"autoReport.directory\":\"./reports\".</p> <p>Do so by running the following commands:</p> <pre><code>export POPLAR_ENGINE_OPTIONS='{\"autoReport.all\":\"true\", \"autoReport.directory\":\"./reports\"}'\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/profiling-mnist/#run-mnist","title":"Run MNIST","text":"<p>Do so by running the following command:</p> <pre><code>python mnist_poptorch.py\n</code></pre> <p>When MNIST has finished running, see Profiling to use Graph Analyser.</p>"},{"location":"ai-testbed/graphcore/unused/profiling-resnet50/","title":"Profiling ResNet50","text":"<p>Follow all the instructions in Getting Started to log into a Graphcore node.</p> <p>Follow the instructions in Virtual Environments up to and including PopART Environment Setup.</p>"},{"location":"ai-testbed/graphcore/unused/profiling-resnet50/#examples-repo","title":"Examples Repo","text":"<p>Graphcore provides examples of some well-known AI applications in their repository at https://github.com/graphcore/examples.git.</p> <p>Clone the examples repository to your personal directory structure:</p> <pre><code>mkdir ~/graphcore\ncd ~/graphcore\ngit clone https://github.com/graphcore/examples.git\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/profiling-resnet50/#install-requirements","title":"Install Requirements","text":"<p>Change directory</p> <pre><code>cd ~/graphcore/examples/vision/cnns/pytorch\npython -m pip install -r requirements.txt\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/profiling-resnet50/#export-variables","title":"Export Variables","text":"<p>Export the datasets directory.</p> <pre><code>export POPLAR_ENGINE_OPTIONS='{\"autoReport.all\":\"true\", \"autoReport.directory\":\"./reports\"}'\nexport DATASETS_DIR=/software/datasets\nHOST1=`ifconfig eno1 | grep \"inet \" | grep -o '[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}' | head -1`\nOCT123=`echo \"$HOST1\" | cut -d \".\" -f 1,2,3`\nOCT4=`echo \"$HOST1\" | cut -d \".\" -f 4`\nHOST2=$OCT123.`expr $OCT4 + 1`\nHOST3=$OCT123.`expr $OCT4 + 2`\nHOST4=$OCT123.`expr $OCT4 + 3`\nexport HOSTS=$HOST1,$HOST2,$HOST3,$HOST4\nexport CLUSTER=c16\nVIPU_SERVER=${VIPU_SERVER:=$HOST1}\nFIRST_PARTITION=`vipu-admin list partitions --api-host $VIPU_SERVER| grep ACTIVE | cut -d '|' -f 3 | cut -d ' ' -f 2 | head -1`\nPARTITON=${PARTITION:=$FIRST_PARTITION}\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/profiling-resnet50/#profile-resnet50","title":"Profile ResNet50","text":"<p>Profile ResNet50.</p> <p>Note: Use screen because every run is long.</p> <pre><code>cd train\npython3 -m examples_utils benchmark --spec benchmarks.yml --benchmark pytorch_resnet50_train_real_pod16\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/profiling-resnet50/#profile-results","title":"Profile Results","text":"<p>When ResNet50 has finished running, see Profiling to use Graph Analyser.</p>"},{"location":"ai-testbed/graphcore/unused/profiling/","title":"Profiling","text":"<p>This is an adaptation of Capturing IPU Reports.</p>"},{"location":"ai-testbed/graphcore/unused/profiling/#reports","title":"Reports","text":""},{"location":"ai-testbed/graphcore/unused/profiling/#capturing-ipu-reports","title":"Capturing IPU Reports","text":"<p>See Capturing IPU Reports for more information.</p> <p>This section describes how to generate the files that the Graph Analyser can analyze. The Graph Analyser uses report files generated during compilation and execution by the Poplar SDK.</p>"},{"location":"ai-testbed/graphcore/unused/profiling/#ipu-memory-overhead","title":"IPU Memory Overhead","text":"<p>Because of all these extra memory requirements, a model with high memory consumption may go out of memory when profiling is enabled. Depending on the model, you can adjust its parameters to leave space for the instrumentation. For example, you can try decreasing the batch size. In TensorFlow BERT you can adjust the micro batch-size.</p>"},{"location":"ai-testbed/graphcore/unused/profiling/#host-computing-overhead","title":"Host Computing Overhead","text":"<p>It is essential that you also try to reduce the iterations on each run. For instance, by reducing the number of steps or the number of batches per step you can get a lighter execution profile. This will not only reduce the host computation overhead but will also speed up visualization in the Graph Analyser.</p>"},{"location":"ai-testbed/graphcore/unused/profiling/#download-popvision","title":"Download PopVision","text":"<ol> <li> <p>Download PopVision Tools.</p> </li> <li> <p>Click Download Now button.</p> </li> <li> <p>In the Graph Analyser section, select you operating system.</p> </li> <li> <p>Install per selected operating system.</p> </li> </ol>"},{"location":"ai-testbed/graphcore/unused/profiling/#create-ssh-session","title":"Create SSH Session","text":"<p>Use ssh from your development system.</p> <p>The ssh command will use a jumphost and port forwarding.  The format is as follows:</p> <pre><code>ssh -J ALCFUserID@gc-login-dd.ai.alcf.anl.gov ALCFUserID@gc-poplar-DD -L 8090:127.0.0.1:22\nssh -J wilsonb@gc-login-01.ai.alcf.anl.gov wilsonb@gc-poplar-02.ai.alcf.anl.gov -L 8090:127.0.0.1:22\n</code></pre> <p>Where:</p> Argument Help ALCFUserID Is your ALCF user identification. dd Is the Graphcore login node to use, i.e., 01 or 02 DD Is the Graphcore node to use, i.e., 01, 02, 03, or 04. 8090 Is the port on your local machine. 127.0.0.1:22 Is the local IP address and port on the remote machine. <p>You will receive a prompt.</p>"},{"location":"ai-testbed/graphcore/unused/profiling/#launch-graph-analyser","title":"Launch Graph Analyser","text":"<p>Continue on your development machine.</p>"},{"location":"ai-testbed/graphcore/unused/profiling/#operating-system","title":"Operating System","text":""},{"location":"ai-testbed/graphcore/unused/profiling/#ubuntu","title":"Ubuntu","text":"<pre><code>cd /path/to/graph/analyser/directory\n./popvision-graph-analyser-3.11.6.AppImage\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/profiling/#user-interface","title":"User Interface","text":"<ol> <li>Click Open a report...;</li> <li>Click the remote tab;</li> <li>Enter your ALCFUserID for remote machine;</li> <li>Enter the Hostname of your local machine, i.e., 127.0.0.1;</li> <li>Enter your Port address used in the ssh command, e.g., 8090;</li> <li>Click Connect;</li> <li>Navigate to your reports directory;</li> <li>Select the training directory;</li> <li>Select archive.a file; and</li> <li>Click Open button.</li> </ol> <p>The Summary Report will be displayed.</p>"},{"location":"ai-testbed/groq/getting-started/","title":"Getting Started","text":""},{"location":"ai-testbed/groq/getting-started/#allocations","title":"Allocations","text":"<p>If you do not already have an allocation, you will need to request one here: Discretionary Allocation Request (New &amp; Renewal)</p>"},{"location":"ai-testbed/groq/getting-started/#accounts","title":"Accounts","text":"<p>If you do not have an ALCF account (but have an allocation), request one here: ALCF Account and Project Management</p>"},{"location":"ai-testbed/groq/getting-started/#setup","title":"Setup","text":"<p>Connection to a GroqRack node is a two-step process.</p> <p>The first step is to ssh from a local machine to a login node. The second, optional step is to ssh from a login node to a GroqRack node. Jobs may also be started and tracked from login nodes.</p> <p></p>"},{"location":"ai-testbed/groq/getting-started/#log-in-to-a-login-node","title":"Log in to a login node","text":"<p>Connect to a groq login node, editing this command line to use your ALCF user id. You will be prompted for a password; use the 8-digit code provided by  MobilePASS+.  <pre><code>ssh ALCFUserID@groq.ai.alcf.anl.gov\n</code></pre> This randomly selects one of the login nodes, namely <code>groq-login-01.ai.alcf.anl.gov</code> or <code>groq-login-02.ai.alcf.anl.gov</code>. You can alternatively ssh to the specific login nodes directly. </p>"},{"location":"ai-testbed/groq/getting-started/#log-in-to-a-groqrack-node","title":"Log in to a GroqRack node","text":"<p>Once you are on a login node, optionally ssh to one of the GroqRack nodes, which are numbered 1-9.</p> <pre><code>ssh groq-r01-gn-01.ai.alcf.anl.gov\n# or\nssh groq-r01-gn-09.ai.alcf.anl.gov\n# or any node with hostname of form groq-r01-gn-0[1-9].ai.alcf.anl.gov\n</code></pre>"},{"location":"ai-testbed/groq/groqview/","title":"GroqView profiler and visualizer tool","text":"<p>This section covers how to remotely use the GroqView profiler and visualizer tool.</p>"},{"location":"ai-testbed/groq/groqview/#groqview-sample","title":"GroqView sample","text":"<p>Groq compiles produce an accurate and detailed model of the performance of a model's execution on groq cards. There is no need to run a model on groqcards to use GroqView. The GroqView example adds the \"groqview=True\" parameter to the <code>groqit</code> call, then calls the <code>groqview()</code> method on the model returned by <code>groqit</code>. This is the relevant code when using GroqFlow. It tries to retrieve the compiled model from the cache, compiles the model on a cache miss, then calls <code>groqview()</code>. From <code>groqflow/examples/pytorch/groqview.py</code>:  <pre><code># Build model\ngmodel = groqit(pytorch_model, inputs, groqview=True)\n# Open GroqView\ngmodel.groqview()\n</code></pre></p>"},{"location":"ai-testbed/groq/groqview/#run-the-sample","title":"Run the sample","text":"<p>On a groq node, run the groqview.py sample (or any script that includes similar code). Note the port number chosen by GroqView. <pre><code>conda activate groqflow\ncd ~/groqflow/examples/pytorch\npython groqview.py\n# You will see something like the following.\n# The port number may be different.\n...\nOpen your web browser:\n    http://localhost:8439\n</code></pre></p>"},{"location":"ai-testbed/groq/groqview/#forward-the-port-to-your-machine-with-a-browser","title":"Forward the port to your machine with a browser","text":"<p>On your laptop/user machine with a display, set up a 2-hop ssh tunnel. Set <code>$GN_HOSTNAME</code> to the name of the host where job is running <pre><code>export GN_HOSTNAME=groq-r01-gn-09\n# Modify the port number if GroqView has chosen a different port.\n# This might happen if another user is also using GroqView.\n# Also, another user may be using the port on the login host.\n# `groq-login-01.ai.alcf.anl.gov` can be used as well.\nssh -L 8439:localhost:8439 arnoldw@groq-login-02.ai.alcf.anl.gov -t ssh -L 8439:localhost:8439 -N $GN_HOSTNAME\n# When complete, \"ctrl-c\" or equivalent in the console where the ssh tunnel\n# was started will terminate both parts of a ssh tunnel set up this way.\n</code></pre></p>"},{"location":"ai-testbed/groq/groqview/#access-the-groqview-server-for-your-application","title":"Access the GroqView server for your application:","text":"<p>Point a Google Chrome-family web browser at this url, adjusting the port number if necessary. (Chrome, Brave, Vivaldi, Opera tested.) <pre><code>http://localhost:8439\n</code></pre></p>"},{"location":"ai-testbed/groq/job-queuing-and-submission/","title":"Job Queueing and Submission","text":"<p>Groq jobs in the AI Testbed's groqrack are managed by the PBS job scheduler. Overview: PBS For additional information, see  https://docs.alcf.anl.gov/running-jobs/job-and-queue-scheduling/ Man pages are available. These are the key commands: <pre><code># qsub - to submit a batch job using a script\nman qsub\n# qstat - to display queue information\nman qstat\n# qdel - to delete (cancel) a job:\nman qdel\n# qhold - to hold a job\nman qhold\n</code></pre></p>"},{"location":"ai-testbed/groq/running-a-model-or-program/","title":"Running a Model/Program","text":"<p>Jobs are launched from any GroqRack node, or from login nodes.  If you expect a loss of an internet connection for any reason, for long-running jobs we suggest logging into a specific node and using either screen or tmux to create persistent command line sessions.  For details use:</p> <p><pre><code>man screen\n# or\nman tmux\n</code></pre> or online man pages: screen, tmux</p>"},{"location":"ai-testbed/groq/running-a-model-or-program/#running-jobs-on-groq-nodes","title":"Running jobs on Groq nodes","text":""},{"location":"ai-testbed/groq/running-a-model-or-program/#groqflow","title":"GroqFlow","text":"<p>GroqFlow is the simplest way to port applications running inference to groq. The groqflow github repo includes many sample applications. See GroqFlow.</p>"},{"location":"ai-testbed/groq/running-a-model-or-program/#clone-the-groqflow-github-repo","title":"Clone the GroqFlow github repo","text":"<p>Clone the groqflow github repo and change current directory to the clone: <pre><code>cd ~/\ngit clone https://github.com/groq/groqflow.git\ncd groqflow\n</code></pre></p>"},{"location":"ai-testbed/groq/running-a-model-or-program/#groqflow-conda-environments","title":"GroqFlow conda environments","text":"<p>Create a groqflow conda environment, and activate it. Follow the instructions in the Virtual Environments  section. Note: Similar install instructions are in <code>~/groqflow/docs/install.md</code> or GroqFlow\u2122 Installation Guide The conda enviroment should be reinstalled whenever new groqflow code is pulled from the groqflow github; with a groqflow conda environment activated, redo just the pip install steps.</p>"},{"location":"ai-testbed/groq/running-a-model-or-program/#running-a-groqflow-sample","title":"Running a groqflow sample","text":"<p>Each groqflow sample directory in the <code>~/groqflow/proof_points</code> tree has a README.md describing the sample and how to run it.</p>"},{"location":"ai-testbed/groq/running-a-model-or-program/#optionally-activate-your-groqflow-conda-environment","title":"Optionally activate your GroqFlow conda environment","text":"<pre><code>conda activate groqflow\n</code></pre>"},{"location":"ai-testbed/groq/running-a-model-or-program/#run-a-sample-using-pbs-in-batch-mode","title":"Run a sample using PBS in batch mode","text":"<p>See Job Queueing and Submission for more information about the PBS job scheduler.</p> <p>Create a script <code>run_minilmv2.sh</code> with the following contents. It assumes that conda was installed in the default location. The conda initialize section can also be copied from your .bashrc if the conda installer was allowed to add it. <pre><code>#!/bin/bash\n# &gt;&gt;&gt; conda initialize &gt;&gt;&gt;\n# !! Contents within this block are managed by 'conda init' !!\n__conda_setup=\"$(${HOME}'/miniconda3/bin/conda' 'shell.bash' 'hook' 2&gt; /dev/null)\"\nif [ $? -eq 0 ]; then\n    eval \"$__conda_setup\"\nelse\n    if [ -f \"${HOME}/miniconda3/etc/profile.d/conda.sh\" ]; then\n        . \"${HOME}/miniconda3/etc/profile.d/conda.sh\"\n    else\n        export PATH=\"${HOME}/miniconda3/bin:$PATH\"\n    fi\nfi\nunset __conda_setup\n# &lt;&lt;&lt; conda initialize &lt;&lt;&lt;\nconda activate groqflow\ncd ~/groqflow/proof_points/natural_language_processing/minilm\npip install -r requirements.txt\npython minilmv2.py\n</code></pre></p> <p>Then run the script as a batch job with PBS. This will reserve a full eight-card(chip) node. <pre><code>qsub -l  select=1,place=excl run_minilmv2.sh\n</code></pre></p> <p>Note: the number of chips used by a model can be found in the compile cache dir for the model after it is compiled. E.g. <pre><code>$ grep num_chips_used ~/.cache/groqflow/minilmv2/minilmv2_state.yaml\nnum_chips_used: 1\n</code></pre> The groqflow proofpoints models use 1, 2 or 4 chips. </p> <p>If your <code>~/.bashrc</code> initializes conda, an alternative to copying the conda initilization script into your execution scripts is to comment out this section in your \"~/.bashrc\": <pre><code># If not running interactively, don't do anything\ncase $- in\n    *i*) ;;\n      *) return;;\nesac\n</code></pre> to <pre><code>## If not running interactively, don't do anything\n#case $- in\n#    *i*) ;;\n#      *) return;;\n#esac\n</code></pre> Then the execution script becomes: <pre><code>#!/bin/bash\nconda activate groqflow\ncd ~/groqflow/proof_points/natural_language_processing/minilm\npip install -r requirements.txt\npython minilmv2.py\n</code></pre> Job status can  be tracked with qstat: <pre><code>$ qstat\nJob id            Name             User              Time Use S Queue\n----------------  ---------------- ----------------  -------- - -----\n3084.groq-r01-co* run_minilmv2     user              0 R workq           \n$ \n</code></pre></p> <p>Output will by default go to two files with names like the following, where the suffix is the job id. One standard output for the job. The other is the standard error for the job. <pre><code>$ ls -la run_minilmv2.sh.*\n-rw------- 1 user users   448 Oct 16 18:40 run_minilmv2.sh.e3082\n-rw------- 1 user users 50473 Oct 16 18:42 run_minilmv2.sh.o3082\n</code></pre></p>"},{"location":"ai-testbed/groq/running-a-model-or-program/#run-a-sample-using-pbs-in-interactive-mode","title":"Run a sample using PBS in interactive mode","text":"<p>An alternative is to use an interactive PBS job. This may be useful when debugging new or changed code. Here is an example that starts a 24 hour interactive job. It reserves a full eight-card(chip) node.  <pre><code>qsub -IV -l walltime=24:00:00 -l select=1,place=excl\n</code></pre> Then activate your groqflow environment, and run python scripts with <pre><code>conda activate groqflow\npython scriptname.py\n</code></pre></p>"},{"location":"ai-testbed/groq/system-overview/","title":"System Overview","text":"<p>ALCF's Groq system consists of a single <code>GroqRackTM compute cluster</code> that provides an extensible accelerator network consisting of 9 <code>GroqNodeTM</code> [ groq-r01-gn-01 through groq-r01-gn-09 ] nodes with a rotational multi-node network topology. Each of these GroqNodes consists of 8 GroqCardTM accelerators in them with integrated chip-to-chip connections with a dragonfly multi-chip topology.</p> <p><code>GroqCardTM accelerator</code> is a dual-width, full-height, three-quarter length PCI-Express Gen4 x16 adapter that includes a single <code>GroqChipTM processor</code> with 230 MB of on-chip memory. Based on the proprietary Tensor Streaming Processor (TSP) architecture, the GroqChip processor is a low latency and high throughput single core SIMD compute engine capable of 750 TOPS (INT8) and 188 TFLOPS (FP16) @ 900 MHz that includes advanced vector and matrix mathematical acceleration units.  The GroqChip processor is deterministic, providing predictable and repeatable performance. </p> <p>The <code>GroqWare suite SDK</code> uses a API based programming model and enables users to develop, compile, and run models on the GroqCard accelerator in a host server system. The SDK uses a ONNX/MLIR enabled DAG compiler and it consists of Groq Compiler, Groq API, and utility tools like GroqView\u2122 profiler and groq-runtime. </p> <pre><code>\n</code></pre> <p>For more information refer to the following links:</p> <p>GroqRack spec sheet GroqNode spec sheet GroqCard spec sheet GroqChip spec sheet (via)</p>"},{"location":"ai-testbed/groq/virtual-environments/","title":"Virtual Environments","text":""},{"location":"ai-testbed/groq/virtual-environments/#install-conda","title":"Install conda","text":"<p>If conda is not already installed: <pre><code>rm Miniconda3-latest-Linux-x86_64.sh*\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh\n# answer y/yes to all prompts\n# exit ssh session, then start a new ssh session\nexit\n</code></pre></p>"},{"location":"ai-testbed/groq/virtual-environments/#groqflow-conda-environment-setup","title":"GroqFlow conda environment setup","text":""},{"location":"ai-testbed/groq/virtual-environments/#create-and-activate-a-groqflow-conda-environment","title":"Create and activate a groqflow conda environment","text":"<p>Create a groqflow conda environment and activate it <pre><code>export PYTHON_VERSION=3.10.12\nconda create -n groqflow python=$PYTHON_VERSION -y\nconda activate groqflow\n</code></pre></p>"},{"location":"ai-testbed/groq/virtual-environments/#install-groqflow-into-the-groqflow-conda-environment","title":"Install groqflow into the groqflow conda environment","text":"<p>Execute the following commands to install groqflow into the activated groqflow conda environment</p> <pre><code># Alter this if you have cloned groqflow to some other location.\ncd ~/groqflow\nif [ -d \"groqflow.egg-info\" ]; then rm -r groqflow.egg-info; fi\npip install --upgrade pip\npip list --format=freeze &gt; frozen.txt\npip install -r frozen.txt -e .\npushd . \ncd demo_helpers\nif [ -d \"groqflow_demo_helpers.egg-info\" ]; then rm -r groqflow_demo_helpers.egg-info; fi\npip install -e .\npopd\npip install soundfile\npip install datasets==2.21.0\n</code></pre> <p>Note: if you encounter problems trying to update an existing groqflow conda environment, consider removing the existing environment with the following command, and recreating it. Make sure you deactivate the environment before removing it.      <pre><code>  conda remove --name groqflow --all -y\n</code></pre></p>"},{"location":"ai-testbed/groq/virtual-environments/#use-groqflow","title":"Use Groqflow","text":"<p>To use groqfloq, <pre><code>conda activate groqflow\n</code></pre> Note: Always use a personal conda environment when installing packages on groq nodes; otherwise they can get installed into <code>~/.local</code> and can cause problems when your shared home directory is used on other systems. If you encounter mysterious package dependency/version issues, check your <code>~/.local/lib</code> and <code>~/.local/bin</code> for mistakenly installed packages.</p> <p>Note: The conda enviroment should be reinstalled whenever new groqflow code is pulled from the groqflow github; with a groqflow conda environment activated, redo just the pip install steps, including the removal of the egg-info directories.</p>"},{"location":"ai-testbed/sambanova/TODO/","title":"TODO","text":"<ul> <li> docs/ai-testbed/sambanova_gen2/example-multi-node-programs.md</li> <li> docs/ai-testbed/sambanova_gen2/ GPT2 example</li> </ul> <p>Using /data/ANL/results/sn30-r1-h1/wilsonb/032223.18/GPT1.5B.out for output Using /data/ANL/results/sn30-r2-h1/wilsonb/032223.19/GPT1.5B.out for output</p> <p>Using /data/ANL/results/sn30-r2-h1/wilsonb/032223.19/BertLarge.out for output</p>"},{"location":"ai-testbed/sambanova/documentation/","title":"Documentation","text":"<p>The SambaNova documentation is now available online SambaNova Documentation.</p> <p>The documentation for the SambaTune (a profiling and performance tuning tool for SambaNova systems) is now available at SambaTune Documentation.</p>"},{"location":"ai-testbed/sambanova/example-modelzoo-programs/","title":"SambaNova Model Zoo samples","text":"<p>The SambaNova Model Zoo is SambaNova's new github repository for delivering RDU-compatible source code, including example applications for compiling and running models on SambaNova hardware.</p> <p>In the ALCF SN30 cluster, the Model Zoo samples run inside of Singularity containers. The Singularity image includes support for compiling and running models.</p> <p>The procedures in this section are drawn from Walkthrough\u2014\u200bInference and Fine-tuning with Llama2 7B for Chat.  The Model Zoo inference sample used as an example in this section is described in more detail here About the Generation Example Apps. This readme (on GitHub) also describes the changes made to a CPU mode sample to run on an RDU. The original python scripts and scripts converted to run on an RDU are also supplied in the modelzoo. cpu_generate_text.py rdu_generate_text.py and cpu_train_llm.py rdu_train_llm.py rdu_train_llm_dp.py </p>"},{"location":"ai-testbed/sambanova/example-modelzoo-programs/#setup","title":"Setup","text":""},{"location":"ai-testbed/sambanova/example-modelzoo-programs/#cloning-the-model-zoo-repository","title":"Cloning the Model Zoo Repository","text":"<p>Clone the repo in your usual location.  <pre><code>mkdir ~/sambanova\ncd ~/sambanova\ngit clone https://github.com/sambanova/modelzoo.git\n</code></pre> Note: your home directory is mounted by default in the singularity containers.</p>"},{"location":"ai-testbed/sambanova/example-modelzoo-programs/#starting-a-container","title":"Starting a container:","text":"<p>Change directory to your Model Zoo clone, and set an environment variable to be host SambaNova runts version, then start the container. This example binds a directory containing an OpenWebText dataset.  <pre><code>cd ~/sambanova/modelzoo\nexport TARGET_SAMBAFLOW_VERSION=$((rpm -q sambanova-runtime 2&gt;/dev/null || dpkg -s sambanova-runtime 2&gt;/dev/null) | egrep -m 1 -o \"[0-9]+\\.[0-9]+\\.[0-9]+\")\necho $TARGET_SAMBAFLOW_VERSION\n# should be of the form 1.19.1\n./start_container.sh -b /data/ANL/openwebtext/hdf5/hdf5:/opt/datasets/openweb_hdf54096/ -b  /software:/software / /software/sambanova/singularity/images/llm-modelzoo/Modelzoo/ModelzooDevbox_0.2.0.sif \n</code></pre> Container startup output should look like: <pre><code>APP_ROOT: /home/arnoldw/sambanova/modelzoo\nUsing singularity with image /software/sambanova/singularity/images/llm-modelzoo/Modelzoo/ModelzooDevbox_1.sif\n\nRunning singularity instance with name: devbox_arnoldw_1724873417\nSingularity start command: singularity instance start --writable-tmpfs --bind /home/arnoldw/github.com/sambanova/modelzoo:/opt/modelzoo --bind /tmp:/tmp --bind /data/ANL/openwebtext/hdf5/hdf5:/opt/datasets/openweb_hdf54096/ --bind /software/models/:/opt/ckpts/ --bind /dev/hugepages:/dev/hugepages --bind /opt/sambaflow/pef/:/opt/sambaflow/pef/ --bind /opt/sambaflow/runtime/:/opt/sambaflow/runtime/ --bind /var/lib/sambaflow/ccl/ccl_config.db:/var/lib/sambaflow/ccl/ccl_config.db --bind /var/snml.sock:/var/snml.sock --bind /opt/sambanova/lib/python3.8/site-packages/pysnml:/opt/sambanova/lib/python3.8/site-packages/pysnml --bind /opt/sambanova/lib/python3.8/site-packages/pysnrdureset:/opt/sambanova/lib/python3.8/site-packages/pysnrdureset --bind /opt/sambanova/lib/python3.8/site-packages/pysnrdutools:/opt/sambanova/lib/python3.8/site-packages/pysnrdutools --bind /opt/sambanova/lib/python3.8/site-packages/sambaruntime:/opt/sambanova/lib/python3.8/site-packages/sambaruntime /software/sambanova/singularity/images/llm-modelzoo/Modelzoo/ModelzooDevbox_1.sif devbox_arnoldw_1724873417\nINFO:    instance started successfully\nSingularity instance devbox_arnoldw_1724873417 started\nRun command: singularity exec instance://devbox_arnoldw_1724873417 /bin/bash\nSingularity&gt; \n</code></pre></p> <p>To list all running containers (while outside a container, e.g. a different SSH session): <pre><code>$ singularity instance list\nINSTANCE NAME                PID        IP    IMAGE\ndevbox_arnoldw_1724873417    1649294          /software/sambanova/singularity/images/llm-modelzoo/Modelzoo/ModelzooDevbox_1.sif\n</code></pre> To re-enter an exited but still-running container (while outside a container): <pre><code>$ singularity exec instance://devbox_arnoldw_1724873417 /bin/bash\nSingularity&gt; \n</code></pre></p> <p>To stop all your running containers (while outside a container): <pre><code>$ singularity instance stop devbox_&lt;youruserid&gt;_*\n</code></pre></p>"},{"location":"ai-testbed/sambanova/example-modelzoo-programs/#set-up-the-python-environment-in-the-container","title":"Set up the Python environment in the container","text":"<pre><code>cd ~/sambanova/modelzoo/\npip install -r requirements/requirements.txt \npip install --upgrade pip\npip install -e . \n</code></pre>"},{"location":"ai-testbed/sambanova/example-modelzoo-programs/#optionally-download-the-hugging-face-model-for-llama-2-7b","title":"Optionally, download the Hugging Face model for Llama-2-7b","text":"<p>This model is also avaiable in <code>/software/models/Llama-2-7b-hf/</code> First, create a Hugging Face account at https://huggingface.co/join if you do not already have one. Go to meta-llama/Llama-2-7b-hf and accept the terms of use for Llama2 7B. You will need to wait (minutes at least) until the request is proccessed. In your Hugging Face account settings, generate a user access token. A read-only token works. Record the token such that it can easily be copy-pasted in the future. <pre><code># if working in an environment (e.g. laptop) where git-lfs is not installed, \n# sudo apt install git-lfs \ngit lfs install # Only needs to be done once \ncd ~/sambanova\ngit clone https://huggingface.co/meta-llama/Llama-2-7b-hf\n# Enter your HF user name and user access token (copy;paste) when prompted.\n</code></pre></p>"},{"location":"ai-testbed/sambanova/example-modelzoo-programs/#text-generation-sample","title":"Text generation sample","text":""},{"location":"ai-testbed/sambanova/example-modelzoo-programs/#compile-a-text-generation-sample-that-uses-the-hf-model","title":"Compile a text generation sample that uses the HF model","text":"<p>Compile a LLaMA-7b text generation sample (using the Hugging Face model). This will take 20 minutes</p> <pre><code>cd ~/sambanova\n# or ./Llama-2-7b-hf if downloaded\npython ./modelzoo/examples/nlp/text_generation/rdu_generate_text.py \\\ncommand=compile \\\ncheckpoint.model_name_or_path=/software/models/Llama-2-7b-hf/ \\\nsamba_compile.output_folder=/home/$(whoami)/sambanova/out_generation \\\n+samba_compile.target_sambaflow_version=LATEST\n</code></pre> <p>Note: each compile will add a new subdirectory to the ouput folder (<code>/home/$(whoami)/sambanova/out_generation</code>), containing compile artifacts. The folder can be deleted when testing is complete;</p>"},{"location":"ai-testbed/sambanova/example-modelzoo-programs/#run-the-text-generation-sample","title":"Run the text generation sample","text":"<p>Run the sample, using the <code>.pef</code> binary created by the compile. Note: The expression in the command line finds the most recent pef file.</p> <pre><code>cd ~/sambanova\nexport PEF=$(find /home/$(whoami)/sambanova/out_generation -type f -name \"*.pef\" -printf \"%T@ %p\\n\" | sort -n | tail -n1 | awk '{print $2}')\n# or ./Llama-2-7b-hf if downloaded\npython ./modelzoo/examples/nlp/text_generation/rdu_generate_text.py \\\n  command=run \\\n  checkpoint.model_name_or_path=/software/models/Llama-2-7b-hf/ \\\n  samba_run.pef=${PEF}\n</code></pre> <p>The end of the console output should resemble the following: <pre><code>Generating 32 tokens ...\nDecoding ...\nCompletion:\n[', there was a little boy who lived in a small town.\\nHe was a good boy, but sometimes he had a hard time following the rules.\\n']\n\nlatencies\n    time to first token 1.1981s\n    tokens,  excluding first token 0.3330s\n    tokens,  overall 0.3600s\n    Total Latency 1.5310s\nthroughputs\n    tokens/second excluding first token 3.0032\n    tokens/second overall 2.7777\nSingularity&gt; \n</code></pre></p>"},{"location":"ai-testbed/sambanova/example-modelzoo-programs/#model-finetuning-sample","title":"Model Finetuning Sample","text":"<p>Fine-tune the Llama2 7B model using a chat dataset.</p>"},{"location":"ai-testbed/sambanova/example-modelzoo-programs/#data-preparation","title":"Data preparation","text":"<p>NOTE: These data preparation steps should be performed on a SambaNova node, and not in a singularity container.</p>"},{"location":"ai-testbed/sambanova/example-modelzoo-programs/#install-the-generative-data-prep-package-in-a-virtualenv","title":"Install the Generative Data Prep package in a virtualenv","text":"<pre><code>cd ~/sambanova\ngit clone https://github.com/sambanova/generative_data_prep.git\ncd generative_data_prep\npython -m venv gdp_venv\nsource gdp_venv/bin/activate\npip install .\ncd ~/sambanova\n</code></pre>"},{"location":"ai-testbed/sambanova/example-modelzoo-programs/#download-ultrachat-from-its-hugging-face-page","title":"Download UltraChat from its Hugging Face page","text":"<p>Make sure that you have git lfs installed, with <code>git lfs install</code> <pre><code>cd ~/sambanova\ngit clone https://huggingface.co/datasets/stingning/ultrachat\n</code></pre></p>"},{"location":"ai-testbed/sambanova/example-modelzoo-programs/#convert-the-dataset-to-the-jsonl-format","title":"Convert the dataset to the <code>.jsonl</code> format","text":"<pre><code>cd ~/sambanova\nsource generative_data_prep/gdp_venv/bin/activate\n# This step makes a single jsonl file\npython ./modelzoo/examples/nlp/training/utils/convert_ultrachat.py -src ultrachat/ -dest ultrachat_processed.jsonl\n# get a small subset to keep the 1 epoch runtime down.\nmv ~/sambanova/ultrachat_processed.jsonl ~/sambanova/ultrachat_processed_full.jsonl\nhead -1000 ~/sambanova/ultrachat_processed_full.jsonl &gt; ~/sambanova/ultrachat_processed.jsonl\n# This step makes a directory of hdf5 files from the single jsonl file\nexport TOKENIZER=\"meta-llama/Llama-2-7b-hf\"\nexport MAX_SEQ_LENGTH=4096\npython -m generative_data_prep pipeline --input_file_path=./ultrachat_processed.jsonl --output_path=./ultrachat_dialogue --pretrained_tokenizer=${TOKENIZER} --max_seq_length=${MAX_SEQ_LENGTH}\ndeactivate\n</code></pre>"},{"location":"ai-testbed/sambanova/example-modelzoo-programs/#compile-a-sample-that-finetunes-the-hf-model","title":"Compile a sample that finetunes the HF model","text":""},{"location":"ai-testbed/sambanova/example-modelzoo-programs/#start-container","title":"Start container","text":"<p>If you are not already in a Singularity container (with the pre-reqs installed), start a new Model Zoo Singularity container with <pre><code>cd ~/sambanova/modelzoo\nexport TARGET_SAMBAFLOW_VERSION=$((rpm -q sambanova-runtime 2&gt;/dev/null || dpkg -s sambanova-runtime 2&gt;/dev/null) | egrep -m 1 -o \"[0-9]+\\.[0-9]+\\.[0-9]+\")\necho $TARGET_SAMBAFLOW_VERSION\n# should be of the form 1.19.1\n./start_container.sh -b /data/ANL/openwebtext/hdf5/hdf5:/opt/datasets/openweb_hdf54096/ -b  /software:/software /software/sambanova/singularity/images/llm-modelzoo/Modelzoo/ModelzooDevbox_1.sif\n</code></pre> or use an existing container with instructions at starting a container</p>"},{"location":"ai-testbed/sambanova/example-modelzoo-programs/#install-pre-reqs","title":"Install pre-reqs","text":"<p>Then install the pre-reqs into the container with <pre><code>cd ~/sambanova/modelzoo/\npip install -r requirements/requirements.txt \npip install --upgrade pip\npip install -e . \n</code></pre></p>"},{"location":"ai-testbed/sambanova/example-modelzoo-programs/#compile-the-sample-for-fine-tuning","title":"Compile the sample for fine tuning","text":"<pre><code>cd ~/sambanova\nexport CHECKPOINT=/software/models/Llama-2-7b-hf/ # or ./Llama-2-7b-hf\nexport MAX_SEQ_LENGTH=4096\nexport BATCH_SIZE=8\nexport ARCH=sn30\npython modelzoo/examples/nlp/training/rdu_train_llm.py \\\n    command=compile \\\n    checkpoint.config_name=${CHECKPOINT} \\\n    model.max_seq_length=${MAX_SEQ_LENGTH} \\\n    training.batch_size=${BATCH_SIZE} \\\n    samba_compile.arch=${ARCH} \\\n    samba_compile.output_folder=/home/$(whoami)/sambanova/out_train \\\n    +samba_compile.target_sambaflow_version=LATEST\n</code></pre> <p>Note: each compile will add a new subdirectory to the ouput folder (<code>/home/$(whoami)/sambanova/out_train</code>), containing compile artifacts. The folder can be deleted when testing is complete;</p>"},{"location":"ai-testbed/sambanova/example-modelzoo-programs/#run-finetuning-using-generated-pef-file","title":"Run finetuning using generated pef file","text":"<p>This will run for 1 full epoch and takes 1 hour to execute, using a single RDU. It uses the config file <code>modelzoo/examples/nlp/training/config/base_config_rdu.yaml</code></p> <pre><code>cd ~/sambanova\nexport CHECKPOINT=/software/models/Llama-2-7b-hf/ # or ./Llama-2-7b-hf\nexport MAX_SEQ_LENGTH=4096\nexport DATASET=./ultrachat_dialogue;  # or container path to dataset\n# Finds most recent pef file in tree\nexport PEF=$(find /home/$(whoami)/sambanova/out_train -type f -name \"*.pef\" -printf \"%T@ %p\\n\" | sort -n | tail -n1 | awk '{print $2}')\npython -u modelzoo/examples/nlp/training/rdu_train_llm.py \\\n    command=run \\\n    checkpoint.model_name_or_path=${CHECKPOINT} \\\n    model.max_seq_length=${MAX_SEQ_LENGTH} \\\n    samba_run.pef=${PEF} \\\n    training.dataset=${DATASET}\n</code></pre> <p>The end of the console output should resemble the following if run for a full epoch: <pre><code>Targeting samba-runtime v4.2.5. Samba is running with --target-runtime-version=1.3.10 on a system with installed runtime None.\n\nLog ID initialized to: [arnoldw][python][1003] at /var/log/sambaflow/runtime/sn.log\nLoading dataset for epoch 1...\n\nNumber of epochs: 1\nBatch size: 8\nNumber of batches (steps): 1,143\n\nStarting training for epoch 1...\nEpoch [1/1], Step [1/1143], Loss: 0.8184\nEpoch [1/1], Step [2/1143], Loss: 0.2452\nEpoch [1/1], Step [3/1143], Loss: 0.3727\nEpoch [1/1], Step [4/1143], Loss: 0.2945\n...\nEpoch [1/1], Step [1134/1143], Loss: 0.2529\nEpoch [1/1], Step [1135/1143], Loss: 0.2713\nEpoch [1/1], Step [1136/1143], Loss: 0.2669\nEpoch [1/1], Step [1137/1143], Loss: 0.2144\nEpoch [1/1], Step [1138/1143], Loss: 0.2129\nEpoch [1/1], Step [1139/1143], Loss: 0.2229\nEpoch [1/1], Step [1140/1143], Loss: 0.2263\nEpoch [1/1], Step [1141/1143], Loss: 0.2434\nEpoch [1/1], Step [1142/1143], Loss: 0.2131\nEpoch [1/1], Step [1143/1143], Loss: 0.1626\nFinished training.\nSaving checkpoint...\nCheckpoint saved at finetuned_model/\nSaving summary...\nSummary saved at finetuned_model/summary.txt\nSingularity&gt; \n</code></pre></p>"},{"location":"ai-testbed/sambanova/example-multi-node-programs/","title":"Example Multi-Node Programs","text":"<p>In this section we will learn how to extend the UNet2d and Gpt1.5B applications scripts that we introduced in the Example Programs to compile and run multiple instances of the model in a data parallel fashion across multiple tiles or across multiple nodes.</p>"},{"location":"ai-testbed/sambanova/example-multi-node-programs/#unet2d","title":"UNet2d","text":""},{"location":"ai-testbed/sambanova/example-multi-node-programs/#set-up","title":"Set Up","text":"<p>Create the following directory and change to it if you have not already done so.</p> <pre><code>mkdir -p ~/apps/image/unet\ncd ~/apps/image/unet\n</code></pre>"},{"location":"ai-testbed/sambanova/example-multi-node-programs/#create-unet2dsh-and-unet_batchsh","title":"Create Unet2d.sh and unet_batch.sh","text":"<p>Create the file Unet2d.sh and unet_batch.sh in the current directory using your favorite editor. Copy and paste the contents of Unet2d.sh and unet_batch.sh to files with the same name into the current directory using your favorite editor.</p> <pre><code>chmod +x Unet2d.sh\nchmod +x unet_batch.sh\n</code></pre>"},{"location":"ai-testbed/sambanova/example-multi-node-programs/#compile-and-run","title":"Compile and run","text":"<p>Run these commands for training (compile + train): The compile and run scripts have the following input arguments.</p> <ol> <li> <p>image size:  The images are square.  Valid sizes include 256, 512, and 1024.</p> </li> <li> <p>Batch size: local batch size.  The global batch size is local batch size * Num of instances.</p> </li> <li> <p>num of instances: Total number of instances of Unet2d run in data parallel framework.</p> </li> <li> <p>RunID: A unique Id for the compile or run process.</p> </li> </ol> <p>The script uses the arguments <code>pcompile</code> and <code>prun</code> for the data parallel compile and run.</p> <pre><code>./Unet2d.sh pcompile &lt;image size&gt; &lt;batch_size&gt; &lt;num of instances&gt; &lt;RunID&gt;\n./Unet2d.sh prun &lt;image size&gt; &lt;batch_size&gt; &lt;num of instances&gt; &lt;RunID&gt;\n</code></pre> <p>For a image size of 256x256 and local batch size of 256 when running 8 instance, the commands are provided as follows.</p> <pre><code>./Unet2d.sh pcompile 256 256 8 unet2d_8inst_pcompile\n./Unet2d.sh prun 256 256 8 unet2d_8inst_prun\n</code></pre> <p>The above commands displays the file that contains the output for the execution of the above scripts, usually <code>/data/ANL/results/&lt;hostname&gt;/&lt;userId&gt;/&lt;RunID&gt;/Unet2d.out</code></p> <p>You can inspect the compile command that contains <code>--data-parallel -ws 2</code> arguments to ensure that the <code>pef</code> file is compatible for data parallel runs. The pef generated from the compilation process for the above compile command is placed under out/Unet2d/unet_train_256_256_NP_4 inside the current working directory.</p> <pre><code>python /opt/sambaflow/apps/image/segmentation/compile.py compile --mac-v2 --in-channels=3 --in-width=${2} --in-height=${2} --batch-size=${BS} --enable-conv-tiling --num-tiles=4 --pef-name=unet_train_${BS}_${2}_NP_${NUM_TILES}  --data-parallel -ws 2 --output-folder=${OUTDIR}\n</code></pre> <p>Once the model is compiled, sbatch is used to launch the multiple instances. The below example shows that a total of 8 tasks or instances are launched over the host on which the script is launched.</p> <pre><code>sbatch --gres=rdu:1 --tasks-per-node ${NP} --nodes 1 --nodelist $(hostname) --cpus-per-task=${cpus} $(pwd)/unet_batch.sh ${NP} ${NUM_WORKERS} ${BS} ${2} ${5}\n</code></pre> <p>The <code>run</code> command has <code>--data-parallel --reduce-on-rdu</code> arguments that is compatible with data parallel run.</p> <pre><code>srun --mpi=pmi2 python /opt/sambaflow/apps/image/segmentation//hook.py run --data-cache=${CACHE_DIR}  --data-in-memory --num-workers=${NUM_WORKERS} --enable-tiling  --min-throughput 395 --in-channels=3 --in-width=${IM} --in-height=${IM} --init-features 32 --batch-size=${BS} --epochs 10 --data-dir ${DS} --log-dir log_dir_unet_${IM}_${BS}_${NP} --data-parallel --reduce-on-rdu --pef=${OUTDIR}/unet_train_${BS}_${IM}_NP_4/unet_train_${BS}_${IM}_NP_4.pef\n</code></pre> <p>The throughput is calculated by averaging the <code>e2e samples_per_sec</code> over the different instances.</p> <pre><code>inner train loop time : 36.314290046691895 for 10 epochs, number of global steps: 10, e2e samples_per_sec: 563.9653143065\ninner train loop time : 33.36756229400635 for 10 epochs, number of global steps: 10, e2e samples_per_sec: 613.7697389922524\ninner train loop time : 33.94625234603882 for 10 epochs, number of global steps: 10, e2e samples_per_sec: 603.3066563941279\ninner train loop time : 32.309499979019165 for 10 epochs, number of global steps: 10, e2e samples_per_sec: 633.8692958200872\ninner train loop time : 31.418426036834717 for 10 epochs, number of global steps: 10, e2e samples_per_sec: 651.8467849404489\ninner train loop time : 28.164129495620728 for 10 epochs, number of global steps: 10, e2e samples_per_sec: 727.1660927132315\ninner train loop time : 30.29698896408081 for 10 epochs, number of global steps: 10, e2e samples_per_sec: 675.9747651583616\ninner train loop time : 25.332663536071777 for 10 epochs, number of global steps: 10, e2e samples_per_sec: 808.442427336472\n</code></pre>"},{"location":"ai-testbed/sambanova/example-multi-node-programs/#gpt-15b","title":"Gpt 1.5B","text":""},{"location":"ai-testbed/sambanova/example-multi-node-programs/#set-up_1","title":"Set up","text":"<pre><code>mkdir ~/nlp-multiNodetest\ncd ~/nlp-multiNodetest\n</code></pre>"},{"location":"ai-testbed/sambanova/example-multi-node-programs/#create-and-run-gpt15b_compilesh-and-gpt15b_runsh","title":"Create and run Gpt1.5B_compile.sh and Gpt1.5B_run.sh","text":"<p>Create the files Gpt1.5B_compile.sh and Gpt1.5B_run.sh in the current directory. Copy the contents of Gpt1.5B_compile.sh and Gpt1.5B_run.sh. Alternatively, the files can be accessed at <code>/data/ANL/scripts/1.23.5-46/legacy_models/Gpt1.5B_compile.sh</code> and <code>/data/ANL/scripts/1.23.5-46/legacy_models/Gpt1.5B_run.sh</code> on any of the compute node and can be copied over to the working directory.</p>"},{"location":"ai-testbed/sambanova/example-multi-node-programs/#compile-and-run_1","title":"Compile and Run","text":"<p>This script consists of commands to <code>compile</code> and <code>run</code> multiple instances of Gpt1.5B model across multiple nodes. Run the Gpt1.5B_compile.sh to first compile and generate the <code>pef</code> file for the model and it in turn launches the <code>Gpt1.5B_run.sh</code> script to run multiple instances of the model over the different nodes.</p> <pre><code>chmod +x Gpt1.5B_compile.sh\nchmod +x Gpt1.5B_run.sh\n./Gpt1.5B_compile.sh\n</code></pre> <p>You can see the log file path displayed on the screen as seen in the example below. You can use the <code>tail</code> command to check the progress of the run.</p> <pre><code>vsastry@sn30-r1-h1:~/nlp-multiNodetest$ ./Gpt1.5B_compile.sh\nUsing /data/ANL/results/sn30-r1-h1/vsastry/041823.19/GPT1.5B.out for output\n</code></pre> <p>The artifacts of the compile process is produced in the path : <code>/data/scratch/&lt;userId&gt;</code>.</p> <p>Inspect the <code>compile</code> command in the script to see that it includes additional arguments <code>--data-parallel</code> and <code>-ws 2</code> to generate a <code>pef</code> that is compatible for data parallel runs.</p> <pre><code>python /opt/sambaflow/apps/nlp/transformers_on_rdu/transformers_hook.py compile --module_name gpt2_pretrain --task_name clm --max_seq_length 1024 -b 16 --output_dir=${OUTDIR}/hf_output --overwrite_output_dir --do_train  --per_device_train_batch_size 16 --cache ${OUTDIR}/cache/ --tokenizer_name gpt2 --model_name gpt2 --mac-v2 --non_split_head --mac-human-decision /opt/sambaflow/apps/nlp/transformers_on_rdu/human_decisions_gm/mac_v2_overrides/gpt2_48_enc_full_recompute_training_spatialmapping_tiling16_clmerge_gm_nonpardp_lnsd.json --compiler-configs-file /opt/sambaflow/apps/nlp/transformers_on_rdu/human_decisions_gm/compiler_configs/compiler_configs_gpt2_sc_recompute_spatialmapping_tiling16_clsmerge_withcls_nonpardp_norc_e2e.json --skip_broadcast_patch --config_name /opt/sambaflow/apps/nlp/transformers_on_rdu/customer_specific/mv/configs/gpt2_config_xl_50260.json --no_index_select_patch --data-parallel -ws 2 --weight_decay 0.1  --max_grad_norm_clip 1.0 --num-tiles 4 --pef-name=gpt15 --output-folder=${OUTDIR}\n</code></pre> <p>Once the model is compiled, <code>sbatch</code> is used to launch the multiple instances across the nodes. The below example shows that a total of <code>32 tasks</code> or instances are launched over <code>2 nodes</code> with each node having a maximum of <code>16 tasks</code>. Slurm allocates any 2 of the available nodes in this example.</p> <pre><code>/usr/local/bin/sbatch --output=${HOME}/slurm-%A.out --ntasks 32 --gres=rdu:1 --ntasks-per-node 16  --nodes 2 --cpus-per-task=8  Gpt1.5B_run.sh ${1} &gt;&gt; ${OUTPUT_PATH} 2&gt;&amp;1\n</code></pre> <p>The <code>run</code> command for each of this instance is present in the <code>Gpt1.5B_run.sh</code> script. You can inspect the command in the script to see that <code>--data-parallel --reduce-on-rdu</code> arguments are present to ensure that the model is run in a data parallel fashion and that the gradient accumulation takes place on the RDU.</p> <pre><code>/usr/local/bin/srun --mpi=pmi2 python /opt/sambaflow/apps/nlp/transformers_on_rdu/transformers_hook.py run  -b 16  --module_name gpt2_pretrain --task_name clm --max_seq_length 1024  --overwrite_output_dir --do_train  --per_device_train_batch_size 16 --cache ${OUTDIR}/cache/  --tokenizer_name gpt2 --model_name gpt2 --non_split_head --skip_broadcast_patch --no_index_select_patch --output_dir=${OUTDIR}/hf_output --config_name /opt/sambaflow/apps/nlp/transformers_on_rdu/customer_specific/mv/configs/gpt2_config_xl_50260.json --max_grad_norm_clip 1.0 --skip_checkpoint --data-parallel --reduce-on-rdu --data_dir /data/ANL/ss1024 --data_dir /data/ANL/ss1024  --logging_steps 1 --max_steps 900000 --learning_rate 0.00025 --steps_this_run 800 --min_throughput 299000 --max_throughput 600000 --pef=${OUTDIR}/gpt15/gpt15.pef &gt;&gt; ${OUTPUT_PATH} 2&gt;&amp;1\n</code></pre> <p><code>squeue</code> shows that the model is run on 2 nodes <code>sn30-r1-h1</code> and <code>sn30-r2-h2</code>.</p> <pre><code>JOBID PARTITION                      NAME     USER ST       TIME  NODES NODELIST(REASON)\n10191 sambanova            Gpt1.5B_run.sh  vsastry  R      23:18      2 sn30-r1-h1,sn30-r2-h2\n</code></pre> <p><code>sntilestat</code> can also be used to check the total numbers of tiles used for the runs.</p> <pre><code>TILE                 %idle %exec %pload %aload %chkpt %quiesce    PID     USER COMMAND\n/XRDU_0/RDU_0/TILE_0   8.0  91.6    0.3    0.1    0.0      0.0 2750333  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_0/TILE_1   8.0  91.6    0.3    0.1    0.0      0.0 2750333  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_0/TILE_2   7.9  91.6    0.3    0.3    0.0      0.0 2750333  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_0/TILE_3   7.7  91.8    0.3    0.3    0.0      0.0 2750333  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_0/TILE_4   7.6  91.9    0.4    0.1    0.0      0.0 2750339  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_0/TILE_5   7.5  91.9    0.5    0.1    0.0      0.0 2750339  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_0/TILE_6   7.5  91.8    0.5    0.3    0.0      0.0 2750339  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_0/TILE_7   7.3  92.0    0.6    0.0    0.0      0.0 2750339  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_1/TILE_0   8.9  89.9    1.0    0.1    0.0      0.0 2750338  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_1/TILE_1   9.0  89.9    0.9    0.1    0.0      0.0 2750338  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_1/TILE_2   8.6  89.8    1.4    0.1    0.0      0.0 2750338  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_1/TILE_3   8.5  89.9    1.4    0.1    0.0      0.0 2750338  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_1/TILE_4   7.9  90.9    0.9    0.4    0.0      0.0 2750343  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_1/TILE_5   7.7  90.9    0.9    0.5    0.0      0.0 2750343  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_1/TILE_6   7.7  91.0    0.9    0.4    0.0      0.0 2750343  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_1/TILE_7   8.0  91.0    0.6    0.4    0.0      0.0 2750343  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_0/TILE_0   7.6  92.0    0.3    0.1    0.0      0.0 2750345  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_0/TILE_1   7.6  92.0    0.3    0.1    0.0      0.0 2750345  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_0/TILE_2   7.5  92.1    0.3    0.1    0.0      0.0 2750345  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_0/TILE_3   7.5  92.1    0.3    0.1    0.0      0.0 2750345  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_0/TILE_4   7.5  92.1    0.3    0.1    0.0      0.0 2750335  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_0/TILE_5   7.5  92.1    0.3    0.1    0.0      0.0 2750335  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_0/TILE_6   7.5  92.1    0.3    0.1    0.0      0.0 2750335  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_0/TILE_7   7.5  92.1    0.3    0.1    0.0      0.0 2750335  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_1/TILE_0   7.7  91.5    0.4    0.4    0.0      0.0 2750330  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_1/TILE_1   7.9  91.5    0.3    0.4    0.0      0.0 2750330  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_1/TILE_2   7.9  91.5    0.3    0.4    0.0      0.0 2750330  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_1/TILE_3   7.6  91.8    0.4    0.3    0.0      0.0 2750330  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_1/TILE_4   7.7  91.9    0.4    0.0    0.0      0.0 2750334  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_1/TILE_5   7.7  91.9    0.4    0.0    0.0      0.0 2750334  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_1/TILE_6   7.9  91.9    0.3    0.0    0.0      0.0 2750334  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_1/TILE_7   7.9  91.9    0.3    0.0    0.0      0.0 2750334  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_0/TILE_0   8.0  91.8    0.1    0.1    0.0      0.0 2750346  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_0/TILE_1   8.0  91.8    0.1    0.1    0.0      0.0 2750346  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_0/TILE_2   8.0  91.8    0.1    0.1    0.0      0.0 2750346  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_0/TILE_3   7.7  91.9    0.1    0.3    0.0      0.0 2750346  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_0/TILE_4   7.5  92.0    0.5    0.0    0.0      0.0 2750336  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_0/TILE_5   7.6  91.9    0.5    0.0    0.0      0.0 2750336  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_0/TILE_6   7.6  91.9    0.4    0.1    0.0      0.0 2750336  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_0/TILE_7   7.5  91.9    0.4    0.3    0.0      0.0 2750336  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_1/TILE_0   7.5  91.8    0.6    0.1    0.0      0.0 2750331  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_1/TILE_1   7.5  91.8    0.6    0.1    0.0      0.0 2750331  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_1/TILE_2   7.7  91.6    0.5    0.1    0.0      0.0 2750331  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_1/TILE_3   7.7  91.6    0.5    0.1    0.0      0.0 2750331  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_1/TILE_4   7.9  91.4    0.8    0.0    0.0      0.0 2750329  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_1/TILE_5   7.9  91.4    0.8    0.0    0.0      0.0 2750329  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_1/TILE_6   8.1  91.4    0.5    0.0    0.0      0.0 2750329  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_1/TILE_7   8.2  91.4    0.4    0.0    0.0      0.0 2750329  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_0/TILE_0   7.5  91.8    0.4    0.4    0.0      0.0 2750344  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_0/TILE_1   7.5  91.8    0.4    0.4    0.0      0.0 2750344  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_0/TILE_2   7.5  91.8    0.4    0.4    0.0      0.0 2750344  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_0/TILE_3   7.5  91.8    0.4    0.4    0.0      0.0 2750344  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_0/TILE_4   7.6  91.8    0.3    0.4    0.0      0.0 2750337  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_0/TILE_5   7.7  91.8    0.1    0.4    0.0      0.0 2750337  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_0/TILE_6   7.7  91.8    0.3    0.3    0.0      0.0 2750337  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_0/TILE_7   7.7  91.9    0.3    0.1    0.0      0.0 2750337  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_1/TILE_0   7.7  92.0    0.1    0.1    0.0      0.0 2750347  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_1/TILE_1   7.7  92.0    0.1    0.1    0.0      0.0 2750347  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_1/TILE_2   7.7  92.1    0.1    0.0    0.0      0.0 2750347  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_1/TILE_3   7.7  92.1    0.1    0.0    0.0      0.0 2750347  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_1/TILE_4   7.3  91.9    0.5    0.3    0.0      0.0 2750332  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_1/TILE_5   7.3  91.9    0.5    0.3    0.0      0.0 2750332  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_1/TILE_6   7.3  91.9    0.5    0.3    0.0      0.0 2750332  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_1/TILE_7   7.3  92.0    0.5    0.1    0.0      0.0 2750332  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n</code></pre> <p>The Slurm log associated with the JOBID (10191 in the above example) is located in the home directory. You can use the <code>tail</code> command to check the progress of the training.</p> <pre><code>vsastry@sn30-r1-h1:~$ tail -f ~/slurm-10191.out\nUsing /data/ANL/results/sn30-r1-h1/vsastry/041823.03/Gpt1.5B.out for output\n</code></pre> <pre><code>vsastry@sn30-r1-h1:~$ tail -f /data/ANL/results/sn30-r1-h1/vsastry/041823.03/Gpt1.5B.out\n</code></pre> <p>Once the run is completed, check the log file for the performance results.</p> <pre><code>{'e2e_train_time': 2179.2292835712433, 'training_sequences_per_second': 192467.31088004305, 'final_loss': 4.781678199768066}\n247/3247 [01:03&lt;00:00, 50.76it/s]\n</code></pre>"},{"location":"ai-testbed/sambanova/example-programs/","title":"Example Programs","text":"<p>You can use the link to the tutorials on the SambaNova GitHub site or the examples on the compute node (as explained below).</p> <ul> <li>Find the tutorials on the SambaNova GitHub site. If you use those instructions, ensure that you still use the steps for accessing the SN compute node, setting the required environment and compiling and running the applications as described in this documentation. </li> <li>Use the examples of well-known simple AI applications under the path: <code>/opt/sambaflow/apps/starters</code>, on all SambaNova compute nodes, as discussed on this page.  </li> </ul> <p>Make a copy of this to your home directory:</p> <pre><code>cd ~/\nmkdir apps\ncp -r /opt/sambaflow/apps/starters apps/starters\n</code></pre> <p>Deactivate any active conda environment. If you have conda installed and a conda environment is active, you will see something like <code>(base)</code> at the beginning of the command prompt. If so, you will need to deactivate it with <code>conda deactivate</code>. Conda is not used on the SambaNova SN30 cluster. </p>"},{"location":"ai-testbed/sambanova/example-programs/#lenet","title":"LeNet","text":"<p>Change directory</p> <pre><code>cd ~/apps/starters/lenet\n</code></pre>"},{"location":"ai-testbed/sambanova/example-programs/#common-arguments","title":"Common Arguments","text":"<p>Below are some of the common arguments used across most of the models in the example code.</p> Argument Default Help -b 1 Batch size for training -n, 100 Number of iterations to run --num-iterations the pef for -e, 1 Number epochs for training --num-epochs --log-path 'check Log path points' --num-workers 0 Number of workers --measure-train- None Measure training performance performance"},{"location":"ai-testbed/sambanova/example-programs/#lenet-arguments","title":"LeNet Arguments","text":"Argument Default Help --lr 0.01 Learning rate for training --momentum 0.0 Momentum value for training --weight-decay 0.01 Weight decay for training --data-path './data' Data path --data-folder 'mnist_ Folder containing mnist data data' <p>Note:  If you receive an \\\"HTTP error\\\" message on any of the following commands, run the command again. Such errors (e.g 503) are commonly an intermittent failure to download a dataset.</p> <p>Run these commands to compile and train the LeNet model:</p> <pre><code>srun python lenet.py compile -b=1 --pef-name=\"lenet\" --output-folder=\"pef\"\nsrun python lenet.py run --pef=\"pef/lenet/lenet.pef\"\n</code></pre> <p>Alternatively to use Slurm sbatch, create submit-lenet-job.sh with the following contents:</p> <pre><code>#!/bin/sh\n\npython lenet.py compile -b=1 --pef-name=\"lenet\" --output-folder=\"pef\"\npython lenet.py run --pef=\"pef/lenet/lenet.pef\"\n</code></pre> <p>Then</p> <pre><code>mkdir -p pef/lenet\nsbatch --output=pef/lenet/output.log submit-lenet-job.sh\n</code></pre> <p>Squeue will give you the queue status.</p> <pre><code>squeue\n# One may also...\nwatch squeue\n</code></pre> <p>One may see the run log using:</p> <pre><code>cat pef/lenet/output.log\n</code></pre>"},{"location":"ai-testbed/sambanova/example-programs/#mnist-feed-forward-network","title":"MNIST - Feed Forward Network","text":"<p>Change directory</p> <pre><code>cd ~/apps/starters/ffn_mnist/\n</code></pre> <p>Commands to run MNIST example:</p> <pre><code>srun python ffn_mnist.py  compile -b 1 --pef-name=\"ffn_mnist\" --mac-v2\nsrun python ffn_mnist.py  run -b 1 -p out/ffn_mnist/ffn_mnist.pef\n</code></pre> <p>To run the same using Slurm sbatch, create and run the submit-ffn_mnist-job.sh with the following contents.</p> <pre><code>#!/bin/sh\npython ffn_mnist.py  compile -b 1 --pef-name=\"ffn_mnist\" --mac-v2\npython ffn_mnist.py  run -b 1 -p out/ffn_mnist/ffn_mnist.pef\n</code></pre> <pre><code>mkdir -p pef/ffn_mnist\nsbatch --output=pef/ffn_mnist/output.log submit-ffn_mnist-job.sh\n</code></pre>"},{"location":"ai-testbed/sambanova/example-programs/#logistic-regression","title":"Logistic Regression","text":"<p>Change directory</p> <pre><code>cd ~/apps/starters/logreg\n</code></pre>"},{"location":"ai-testbed/sambanova/example-programs/#logistic-regression-arguments","title":"Logistic Regression Arguments","text":"<p>This is not an exhaustive list of arguments.</p> <p>Arguments</p> Argument Default Help Step --lr 0.001 Learning rate for training Compile --momentum 0.0 Momentum value for training Compile --weight-decay 1e-4 Weight decay for training Compile --num-features 784 Number features for training Compile --num-classes 10 Number classes for training Compile --weight-norm na Enable weight normalization Compile <p>Run these commands:</p> <pre><code>srun python logreg.py compile --pef-name=\"logreg\" --output-folder=\"pef\"\nsrun python logreg.py run --pef=\"pef/logreg/logreg.pef\"\n</code></pre> <p>To use Slurm, create submit-logreg-job.sh with the following contents:</p> <pre><code>#!/bin/sh\npython logreg.py compile --pef-name=\"logreg\" --output-folder=\"pef\"\npython logreg.py run --pef=\"pef/logreg/logreg.pef\"\n</code></pre> <p>Then</p> <pre><code>mkdir -p pef/logreg\nsbatch --output=pef/logreg/output.log submit-logreg-job.sh\n</code></pre> <p>The output, pef/logreg/output.log, will look something like this:</p> <pre><code>2023-03-08 21:18:25.168190: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-03-08 21:18:25.334389: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2023-03-08 21:18:25.334430: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2023-03-08 21:18:26.422458: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2023-03-08 21:18:26.422701: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2023-03-08 21:18:26.422709: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n[Info][SAMBA]# Placing log files in /home/wilsonb/apps/starters/logreg/pef/logreg/logreg.samba.log\n[Info][MAC]# Placing log files in /home/wilsonb/apps/starters/logreg/pef/logreg/logreg.mac.log\n...\n\nEpoch [1/1], Step [10000/60000], Loss: 0.4642\nEpoch [1/1], Step [20000/60000], Loss: 0.4090\nEpoch [1/1], Step [30000/60000], Loss: 0.3863\nEpoch [1/1], Step [40000/60000], Loss: 0.3703\nEpoch [1/1], Step [50000/60000], Loss: 0.3633\nEpoch [1/1], Step [60000/60000], Loss: 0.3553\nTest Accuracy: 91.40  Loss: 0.3014\n2023-03-08T21:19:08 : [INFO][LIB][2688517]: sn_create_session: PEF File: pef/logreg/logreg.pef\n</code></pre>"},{"location":"ai-testbed/sambanova/example-programs/#unet2d","title":"UNet2D","text":"<p>The UNet application example is provided in the the path : <code>/opt/sambaflow/apps/image/segmentation/</code>. As any other application, we first compile and then train the model using compile and run arguments respectively. The scripts containing the compile and run commands for UNet2D model can be accessed at Unet2d.sh or at <code>/data/ANL/scripts/Unet2d.sh</code> on any SN30 compute node.</p> <p>Change directory and copy files.</p> <pre><code>mkdir -p ~/apps/image/unet\ncd ~/apps/image/unet\n</code></pre> <p>Copy and paste the contents of Unet2d.sh to a file with the same name into the current directory using your favorite editor.</p> <pre><code>chmod +x Unet2d.sh\n</code></pre> <p>Run these commands for training (compile + train):</p> <pre><code>./Unet2d.sh compile &lt;image size&gt; &lt;batch_size&gt; &lt;num of instances&gt; &lt;RunID&gt;\n./Unet2d.sh run &lt;image size&gt; &lt;batch_size&gt; &lt;num of instances&gt; &lt;RunID&gt;\n</code></pre> <p>The <code>compile</code> and <code>run</code> arguments of the script can only be run with number of instances equal to 1, indicating that this is a simple 4 tile run without data parallel framework. For a image size of 256x256 and batch size 256 when running just 1 instance, the commands are provided as follows.</p> <p>Note</p> <p>The compilation runs for over 30 minutes.</p> <pre><code>./Unet2d.sh compile 256 256 1 unet2d_single_compile\n./Unet2d.sh run 256 256 1 unet2d_single_run\n</code></pre> <p>The above commands displays the file that contains the output for the execution of the above scripts, usually <code>/data/ANL/results/&lt;hostname&gt;/&lt;userid&gt;/&lt;RunID&gt;/Unet2d.out</code></p> <p>If we inspect the compile and run commands for the UNet application provided in the script, we see that the application is compiled with <code>--num-tiles 4</code>, which means that the entire application fits on 4 tiles or half of a RDU. The pef generated from the compilation process of the above command is placed under <code>out/Unet2d/unet_train_256_256_single_4</code> inside the current working directory.</p> <pre><code>python ${UNET}/compile.py compile --mac-v2 --in-channels=3 --in-width=${2} --in-height=${2} --batch-size=${BS} --enable-conv-tiling --num-tiles=4 --pef-name=unet_train_${BS}_${2}_single_${NUM_TILES} --output-folder=${OUTDIR}\n</code></pre> <pre><code>srun --nodelist $(hostname) python /opt/sambaflow/apps/image/segmentation//hook.py run --data-cache=${CACHE_DIR}  --data-in-memory --num-workers=${NUM_WORKERS} --enable-tiling  --min-throughput 395 --in-channels=3 --in-width=${2} --in-height=${2} --init-features 32 --batch-size=${BS} --epochs 10 --data-dir ${DS} --log-dir log_dir_unet_${2}_${BS}_single_${NUM_TILES} --pef=${OUTDIR}/unet_train_${BS}_${2}_single_${NUM_TILES}/unet_train_${BS}_${2}_single_${NUM_TILES}.pef\n</code></pre> <p>The performance data is located at the bottom of log file.</p> <pre><code>inner train loop time : 374.6789753437042 for 10 epochs, number of global steps: 130, e2e samples_per_sec: 88.82270474202953\n</code></pre>"},{"location":"ai-testbed/sambanova/example-programs/#gpt-15b","title":"GPT 1.5B","text":"<p>The GPT 1.5B application example is provided in the the path : <code>/opt/sambaflow/apps/nlp/transformers_on_rdu/</code>. The scripts containing the <code>compile</code> and <code>run</code> commands for the GPT 1.5B model can be accessed at the path <code>/data/ANL/scripts/1.23.5-46/legacy_models/Gpt1.5B_base_single_compile.sh</code> and <code>/data/ANL/scripts/1.23.5-46/legacy_models/Gpt1.5B_base_single_run.sh</code> on any SN30 compute node. This script is compiled and run for only 1 instance and the model fits on 4 tiles or half of a RDU. The scripts are provided for reference. </p> <p>Change directory and copy files.</p> <pre><code>mkdir -p ~/apps/nlp/Gpt1.5B_single\ncd ~/apps/nlp/Gpt1.5B_single\n</code></pre> <p>Copy and paste the contents of Gpt1.5B_base_single_compile.sh and Gpt1.5B_base_single_run.sh  to a file with the same names into the current directory using your favorite editor.</p> <p>or copy the contents from <code>/data/ANL/scripts/Gpt1.5B_base_single_compile.sh</code> and <code>/data/ANL/scripts/Gpt1.5B_base_single_run.sh</code>.</p> <pre><code>cp /data/ANL/scripts/1.23.5-46/legacy_models/Gpt1.5B_base_single_compile.sh ~/apps/nlp/Gpt1.5B_single/\ncp /data/ANL/scripts/1.23.5-46/legacy_models/Gpt1.5B_base_single_run.sh ~/apps/nlp/Gpt1.5B_single/\n</code></pre> <p>Run the script with batch size as an argument(shown below with an example of 32).</p> <pre><code>chmod +x Gpt1.5B_base_single_compile.sh \n./Gpt1.5B_base_single_compile.sh 32\n</code></pre> <p>The Gpt1.5B_base_single_compile.sh  script will internally call the Gpt1.5B_base_single_run.sh to perform the training. You can inspect the <code>compile</code> and <code>run</code> commands in the scripts to learn that this model trains with a batch size of 32 for 1 instance over 4 tiles. The human decision file and the compiler config file helps to optimize the compute and memory resources specific to this Gpt 1.5B model run.</p> <pre><code>python /opt/sambaflow/apps/nlp/transformers_on_rdu/transformers_hook.py compile --pef-name=GPT1.5B_base_single_32 --output-folder=/data/scratch/user/GPT1.5B_base_single_32 --module_name gpt2_pretrain --task_name clm --max_seq_length 1024 -b 32  --output_dir=/data/scratch/user/GPT1.5B_base_single_32/hf_gpt1dot5b_ss1k_gas_1_bs32  --overwrite_output_dir --do_train  --per_device_train_batch_size 32   --tokenizer_name gpt2 --model_name gpt2 --mac-v2 --non_split_head --mac-human-decision /opt/sambaflow/apps/nlp/transformers_on_rdu/human_decisions_gm/mac_v2_overrides/gpt2_48_enc_full_recompute_training_spatialmapping_tiling16_clmerge_gm_pardp2_lnsd.json --compiler-configs-file /opt/sambaflow/apps/nlp/transformers_on_rdu/human_decisions_gm/compiler_configs/compiler_configs_gpt1dot5b_perf.json --skip_broadcast_patch --config_name /opt/sambaflow/apps/nlp/transformers_on_rdu/customer_specific/mv/configs/gpt2_config_xl_50260.json --no_index_select_patch --weight_decay 0.1  --max_grad_norm_clip 1.0 --num-tiles 4 --enable-stochastic-rounding\n</code></pre> <pre><code>COMMAND= /usr/local/bin/srun --mpi=pmi2 python /opt/sambaflow/apps/nlp/transformers_on_rdu/transformers_hook.py run  -b 32  --data_dir /data/ANL/ss1024 --pef=/data/scratch/user/GPT1.5B_base_single_32/GPT1.5B_base_single_32/GPT1.5B_base_single_32.pef --output_dir=/data/scratch/user/GPT1.5B_base_single_32/hf_gpt1dot5b_ss1k_gas_1_bs16 --module_name gpt2_pretrain --task_name clm --max_seq_length 1024  --overwrite_output_dir --do_train  --per_device_train_batch_size 32 --tokenizer_name gpt2 --model_name gpt2 --non_split_head --skip_broadcast_patch --no_index_select_patch --config_name /opt/sambaflow/apps/nlp/transformers_on_rdu/customer_specific/mv/configs/gpt2_config_xl_50260.json --max_grad_norm_clip 1.0 --skip_checkpoint --logging_steps 1 --max_steps 75000 --learning_rate 0.00025 --steps_this_run 100\n</code></pre> <p>The <code>sntilestat</code> command shows that the application runs on 4 tiles as shown below.</p> <pre><code>/XRDU_0/RDU_0/TILE_0   2.1  96.9    0.8    0.1    0.0      0.0 796481  user python /opt/sambaflow/apps/nlp/transformers_on_rdu/\n/XRDU_0/RDU_0/TILE_1   2.1  96.9    0.8    0.1    0.0      0.0 796481  user python /opt/sambaflow/apps/nlp/transformers_on_rdu/\n/XRDU_0/RDU_0/TILE_2   2.5  96.9    0.4    0.1    0.0      0.0 796481  user python /opt/sambaflow/apps/nlp/transformers_on_rdu/\n/XRDU_0/RDU_0/TILE_3   2.5  96.9    0.4    0.1    0.0      0.0 796481  user python /opt/sambaflow/apps/nlp/transformers_on_rdu/\n/XRDU_0/RDU_0/TILE_4 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_0/TILE_5 100.0   0.0    0.0    0.0    0.0      0.0\n...\n</code></pre>"},{"location":"ai-testbed/sambanova/getting-started/","title":"Getting Started","text":""},{"location":"ai-testbed/sambanova/getting-started/#on-boarding","title":"On-Boarding","text":"<p>SambaNova SN30 can be accessed using your ALCF account. See Get Started to request an account and for additional information.</p>"},{"location":"ai-testbed/sambanova/getting-started/#setup","title":"Setup","text":""},{"location":"ai-testbed/sambanova/getting-started/#system-view","title":"System View","text":"<p>Connection to a SambaNova node is a two-step process. The first step is to ssh to the login node. This step requires an MFA passcode for authentication - an eight-digit passcode generated by an app on your mobile device, e.g., MobilePASS+. The second step is to log in to a SambaNova node from the login node.</p> <p></p>"},{"location":"ai-testbed/sambanova/getting-started/#log-in-to-login-node","title":"Log in to Login Node","text":"<p>Log in to the SambaNova login node from your local machine using the below command. This uses the MobilePASS+ token generated every time you log in to the system. This is the same passcode used to authenticate into other ALCF systems, such as Polaris.</p> <p>In the examples below, replace ALCFUserID with your ALCF user id.</p> <pre><code>ssh ALCFUserID@sambanova.alcf.anl.gov\nPassword: &lt; MobilePASS+ code &gt;\n</code></pre> <p>Note: Use the ssh \"-v\" option in order to debug any ssh problems.</p>"},{"location":"ai-testbed/sambanova/getting-started/#log-in-to-a-sambanova-node","title":"Log in to a SambaNova Node","text":"<p>Once you are on the login node, a SambaNova node can be accessed using an alias, sn30-r[1-4]-h[1-2] where 'r' stands for the rack number, and 'h' stands for host. sn30-r1-h1 is the first host of the first rack.</p> <p>The 8 nodes are aliased as : sn30-r1-h1 , sn30-r1-h2, sn30-r2-h1, sn30-r2-h2, sn30-r3-h1, sn30-r3-h2, sn30-r4-h1, sn30-r4-h2.</p> <p>sn30-r1-h1 can be accessed as below.</p> <pre><code>ssh sn30-r1-h1\n</code></pre>"},{"location":"ai-testbed/sambanova/getting-started/#sdk-setup","title":"SDK setup","text":"<p>The required software environment (SambaFlow software stack and the associated environmental variables) for a SN30 node is set up automatically at login. This is unlike the SN10 where the environment had to be set up by each user.</p>"},{"location":"ai-testbed/sambanova/job-queuing-and-submission/","title":"Job Queueing and Submission","text":""},{"location":"ai-testbed/sambanova/job-queuing-and-submission/#introduction","title":"Introduction","text":"<p>SambaNova uses Slurm for job submission and queueing. Below are some of the important commands for using Slurm. For more information refer to Slurm Documentation.</p> <p>Note: Run the Python scripts using 'srun' or 'sbatch', to ensure that concurrent jobs do not interfere with each other.</p> <p>Note: There is just one scheduler for all of the SambaNova nodes.</p>"},{"location":"ai-testbed/sambanova/job-queuing-and-submission/#srun","title":"SRun","text":"<p>The Slurm command <code>srun</code> can be used to run individual Python scripts in parallel with other scripts on a cluster managed by Slurm. Examples of <code>srun</code> usage are shown below.</p> <p>Slurm will assign a nodelist/host to run a job if a host is not specified.</p> <p>Example:</p> <pre><code>srun python lenet.py compile -b=1 --pef-name=\"lenet\" --output-folder=\"pef\"\nsrun python lenet.py run --pef=\"pef/lenet/lenet.pef\"\n</code></pre> <p>You may specify which node/host on which to run a job.</p> <p>Reasons to specify a node list:</p> <ul> <li>One wants to test a specific node to verify the function of the HW and SW  (daily smoke tests do this)</li> <li>The nodes are at different software levels and one wants to use a node that has the needed software level for one's application.</li> </ul> <p>Example:</p> <pre><code>srun --nodelist=sn30-r1-h1 python lenet.py compile -b=1 --pef-name=\"lenet\" --output-folder=\"pef\"\n</code></pre>"},{"location":"ai-testbed/sambanova/job-queuing-and-submission/#sbatch","title":"SBatch","text":"<p>Alternatively, these jobs can be submitted to the Slurm workload manager through a batch script by using the <code>sbatch</code> command. To do this, create a bash script (submit-lenet-job.sh here as an example) with the commands that you want to execute.</p> <pre><code>#!/bin/sh\n\npython lenet.py compile -b=1 --pef-name=\"lenet\" --output-folder=\"pef\"\npython lenet.py run --pef=\"pef/lenet/lenet.pef\"\n</code></pre> <p>Then pass the bash script as an input to the <code>sbatch</code> command as shown below.</p> <pre><code>sbatch --output=pef/lenet/output.log submit-lenet-job.sh\n</code></pre> <p>In case of the need to use multiple RDUs (2 in the example shown below), the <code>sbatch</code> command would be altered as:</p> <pre><code>sbatch --gres=rdu:2 &lt;your_script.sh&gt;\n</code></pre>"},{"location":"ai-testbed/sambanova/job-queuing-and-submission/#squeue","title":"SQueue","text":"<p>The <code>squeue</code> command provides information about jobs located in the Slurm scheduling queue.</p> <pre><code>squeue\n</code></pre>"},{"location":"ai-testbed/sambanova/job-queuing-and-submission/#sinfo","title":"SInfo","text":"<p>SInfo is used to view partition and node information for a system running Slurm.</p> <p>Here is a suggested command:</p> <pre><code>sinfo -O AllocNodes, GresUsed, Gres, NodeList\n</code></pre> <p>For more information, see SInfo.</p>"},{"location":"ai-testbed/sambanova/job-queuing-and-submission/#scancel","title":"SCancel","text":"<p>SCancel is used to signal or cancel jobs, job arrays, or job steps.</p> <pre><code>scancel job_id\n</code></pre>"},{"location":"ai-testbed/sambanova/miscellaneous/","title":"Miscellaneous","text":""},{"location":"ai-testbed/sambanova/miscellaneous/#sdk-version","title":"SDK Version","text":"<p>To find the SDK version, run the following commands</p> <pre><code># TODO\n(venv) ALCFUserID@sn30-r1-h1:~$ python\nPython 3.7.6 (default, Feb 18 2020, 21:28:31)\n[GCC 9.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; import sambaflow\n&gt;&gt;&gt; sambaflow.__version__\n'1.11.5'\n&gt;&gt;&gt;\n</code></pre>"},{"location":"ai-testbed/sambanova/miscellaneous/#omp_num_threads","title":"OMP_NUM_THREADS","text":"<p>The OMP_NUM_THREADS environment variable sets the number of threads to use for parallel regions.</p> <p>The value of this environment variable must be a list of positive integer values. The values of the list set the number of threads to use for parallel regions at the corresponding nested levels.</p> <p>For the SambaNova system it, is usually set to one.</p> <pre><code>export OMP_NUM_THREADS=16\n</code></pre>"},{"location":"ai-testbed/sambanova/miscellaneous/#where-is-the-model","title":"Where is the Model?","text":"<p>Two copies of the model are maintained.  One in host CPU memory and one in RDU memory. They do not interfere with each other unless you explicitly sync the model/parameter in between using:</p> <pre><code>SambaTensor.rdu() # Moves the CPU model to the RDU\nSambaTensor.cpu() # Moves the RDU model to the CPU\n</code></pre> <p>In order to run the model on the CPU, you can simply use the PyTorch model as if there is no RDU. In order to run the model on RDU, you would need to use session.run().</p>"},{"location":"ai-testbed/sambanova/miscellaneous/#useful-commands","title":"Useful Commands","text":""},{"location":"ai-testbed/sambanova/miscellaneous/#sn-configuration","title":"SN Configuration","text":"<pre><code>snconfig show Node static\n</code></pre> <p>The snconfig utility shows the static configuration of the system. The configuration for the first node is as follows:</p> <pre><code>======================================================\n=======                NODE Info               =======\n======================================================\n=======                Static Info             =======\nTimestamp: 2023-03-16 17:00:04\nPlatform Name: DataScale SN30-8\nNode Name: NODE\n    Number of XRDUS: 4\n    XRDU Name: XRDU_0\n        Number of RDUS: 2\n        RDU name: RDU_0\n            Serial Number     : 205057B469B35895\n            Number of TILES: 8\n            TILE Name: TILE_0\n                Serial Number     : N/A\n            TILE Name: TILE_1\n                Serial Number     : N/A\n\n\n...\n\n\n                    Size              : 128.0 GB\n                    Serial Number     : 1F5BC22\n            DDR CH Name: DDRCH_6\n                Number of DIMMS: 1\n                DIMM Name: DIMM_L0\n                    Size              : 128.0 GB\n                    Serial Number     : 1F5BC99\n            DDR CH Name: DDRCH_7\n                Number of DIMMS: 1\n                DIMM Name: DIMM_M0\n                    Size              : 128.0 GB\n                    Serial Number     : 1F5BB68\n        Total XRDU_3 memory size (GB): 2048.0\n</code></pre>"},{"location":"ai-testbed/sambanova/miscellaneous/#sambanova-daemon-service","title":"SambaNova Daemon Service","text":"<p>The following command checks if the SambaNova daemon service is running.</p> <pre><code>systemctl status snd\n</code></pre> <p>The output should look something like this:</p> <pre><code>\u25cf snd.service - SN Devices Service\n     Loaded: loaded (/lib/systemd/system/snd.service; enabled; vendor preset: enabled)\n    Drop-In: /etc/systemd/system/snd.service.d\n             \u2514\u2500override.conf\n     Active: active (running) since Fri 2023-01-27 04:03:14 UTC; 1 months 18 days ago\n   Main PID: 5635 (snd)\n      Tasks: 9 (limit: 629145)\n     Memory: 156.8M\n     CGroup: /system.slice/snd.service\n             \u2514\u25005635 /opt/sambaflow/bin/snd\n\nWarning: some journal files were not opened due to insufficient permissions.\n</code></pre>"},{"location":"ai-testbed/sambanova/miscellaneous/#tile-status","title":"Tile status","text":"<pre><code>sntilestat\nwatch sntilestat\n</code></pre> <p>The output shown below is when the system is completely idle.</p> <pre><code>TILE                 %idle %exec %pload %aload %chkpt %quiesce    PID     USER COMMAND\n/XRDU_0/RDU_0/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_0/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_0/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_0/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_0/TILE_4 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_0/TILE_5 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_0/TILE_6 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_0/TILE_7 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_1/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_1/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_1/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_1/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_1/TILE_4 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_1/TILE_5 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_1/TILE_6 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_1/TILE_7 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_0/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_0/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_0/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_0/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_0/TILE_4 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_0/TILE_5 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_0/TILE_6 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_0/TILE_7 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_1/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_1/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_1/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_1/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_1/TILE_4 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_1/TILE_5 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_1/TILE_6 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_1/TILE_7 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_0/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_0/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_0/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_0/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_0/TILE_4 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_0/TILE_5 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_0/TILE_6 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_0/TILE_7 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_1/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_1/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_1/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_1/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_1/TILE_4 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_1/TILE_5 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_1/TILE_6 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_1/TILE_7 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_0/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_0/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_0/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_0/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_0/TILE_4 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_0/TILE_5 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_0/TILE_6 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_0/TILE_7 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_1/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_1/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_1/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_1/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_1/TILE_4 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_1/TILE_5 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_1/TILE_6 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_1/TILE_7 100.0   0.0    0.0    0.0    0.0      0.0\n</code></pre>"},{"location":"ai-testbed/sambanova/miscellaneous/#finding-hung-tiles","title":"Finding Hung Tiles","text":"<pre><code>snconfig show Node dynamic | grep perfect\n</code></pre>"},{"location":"ai-testbed/sambanova/miscellaneous/#how-busy-is-the-system","title":"How busy is the system?","text":"<p>Use one of</p> <pre><code>top\nhtop\n</code></pre>"},{"location":"ai-testbed/sambanova/running-a-model-or-program/","title":"Running a Model/Program","text":"<p>Note:  Please be mindful of how you are using the system. For example, consider running larger jobs in the evening or on weekends</p> <p>Note: Please use only Slurm commands, i.e., srun and sbatch, to run your code. If you run your code directly using the 'python' command, it may cause conflicts on the system.</p> <p>Note: If you have conda installed and a conda environment is active, you will see something like <code>(base)</code> at the beginning of the command prompt. If so, you will need to deactivate it with <code>conda deactivate</code>. Conda is not used on the SambaNova SN30 cluster.</p>"},{"location":"ai-testbed/sambanova/running-a-model-or-program/#introduction","title":"Introduction","text":"<p>The SambaNova workflow includes the following main steps to run a model.</p> <ol> <li>Compile</li> <li>Run</li> <li>Test (optional)</li> </ol> <p>The system uses the Slurm job scheduler to schedule the jobs and manage the workload on the system. For more information on Slurm, see Job Queueing and Submission.</p> <p>Example Programs lists the different example applications with corresponding commands for each of the above steps.</p>"},{"location":"ai-testbed/sambanova/running-a-model-or-program/#compile","title":"Compile","text":"<p>Compiles the model and generates a .pef file. This file contains information on how to reconfigure the hardware, and map the compute and memory resources required to run an application on RDUs. The pef files are by default saved in the 'out' directory; the SambaNova documentation advises saving pef files in separate directories with the '--output-folder' option.</p> <p>It is necessary to re-compile only when the model changes, or parameters specific to the model graph change, including the batch size.</p> <p>Compile times can be significant. Compiling the UNet sample, for example, when using images of size 32x32 pixels, takes 358(s), and 1844(s) for images of size 256x256.</p> <p>The entire compile process is executed on the host and no RDUs are involved in the compile step.</p> <p>Example of compiling the LeNet application:</p> <pre><code>srun python lenet.py compile -b=1 --pef-name=\"lenet\" --output-folder=\"pef\"\n</code></pre> <p>where</p> Argument Default Help -b 1 Batch size for training"},{"location":"ai-testbed/sambanova/running-a-model-or-program/#run","title":"Run","text":"<p>As part of this step, the model is trained on the RDUs by passing in the PEF file and the training dataset. The location of the pef file generated in the compile step is passed as an argument to the run command. Below is the example of the <code>run</code> command that trains a LeNet model.</p> <pre><code>srun python lenet.py run --pef=\"pef/lenet/lenet.pef\"\n</code></pre> <p>The location of the pef file generated in the compile step is passed as an argument to the run command.</p>"},{"location":"ai-testbed/sambanova/running-a-model-or-program/#test-optional","title":"Test (Optional)","text":"<p>This command is used to run the model on both the host CPU and a SambaNova RDU.  It compares the results from the CPU and RDU and will report if any discrepancies are found. Pass the pef file generated as part of the compile step as the input to this command.</p> <pre><code>srun python lenet.py test --pef=\"pef/lenet/lenet.pef\"\n</code></pre>"},{"location":"ai-testbed/sambanova/sambatune/","title":"Profiling and performance tuning with SambaTune","text":"<p>This section covers how to use the SambaTune profiling performance tuning tool, and the SambaTune UI for viewing the results.</p>"},{"location":"ai-testbed/sambanova/sambatune/#_1","title":"SambaTune for profiling and performance tuning","text":"<p>SambaTune uses a yaml file that describes how to profile an application. There are samples in <code>/opt/sambaflow/sambatune/configs</code>.  This section shows how to run the simplest sample, a linear net.</p> <p>First, ssh into one of the nodes in the SN30 cluster.  Next, start a slurm interative job reserving a full node (8 RDUs), for 8 hours (480 minutes): <pre><code>$ /usr/local/bin/srun --time=480 --gres=rdu:8 --pty bash\n</code></pre> Record the hostname: <pre><code>$ hostname\nsn30-r1-h1\n</code></pre></p> <p>Next, set an environment variable indicating where the profiling information should be stored: <pre><code>export DUMP_ROOT=~/Sambatune\n</code></pre></p> <p>If running a large model, the profiling information can be hundreds of gigabytes or more, and the DUMP_ROOT should be set to some location with more storage than your home directory (which has a quota). E.g. somewhere that you have write access to in <code>/projects</code></p> <p>Optionally, examine the sample yaml file. You will see that it has 5 top-level sections: <code>app:</code>, <code>model-args:</code>, <code>compile-args:</code>, <code>run-args:</code>, <code>env:</code></p> <p>Next, run sambatune using a sample sambatune yaml configuration file. This sample command line requests profiling with the <code>benchmark</code>, <code>instrument</code>, and <code>run</code> modes. <pre><code>$ sambatune --modes benchmark instrument run -- /opt/sambaflow/sambatune/configs/linear_net.yaml\n</code></pre></p> <p>This will take a while to run, particularly if the yaml for a larger model is used.</p> <p>Then, run <code>sambatune_ui</code>: <pre><code>$ export ST_PORT=8576\n$ sambatune_ui --directory $DUMP_ROOT/artifact_root/sambatune_gen --port $ST_PORT\n</code></pre></p> <p>Copy the password shown (e.g. to your clipboard). The userid is always admin. The password is different for every sambatune_ui run. </p> <p>In a fresh console on your working machine where you will run the browser, set up a two-hop ssh tunnel to the target node. Replace the <code>ALCFUserID</code> in the ssh command line with your ALCF userid. <pre><code>$ export ST_PORT=8576\n$ ssh -L $ST_PORT:localhost:$ST_PORT ALCFUserID@sambanova.alcf.anl.gov  -t ssh -L $ST_PORT:localhost:$ST_PORT -N sn30-r1-h1\n</code></pre></p> <p>Put localhost:8576 in the url bar of a Chrome-family browser. (Chrome, Brave, Vivaldi, Opera tested.) A login prompt for the sambatune ui should show. Enter admin and the password copied previously. You should now see the SambaTune UI. </p> <p>If the browser does not show a login prompt, or if any previous step complains about a port conflict, try another value for ST_PORT on both the target node and for the ssh tunnel command, e.g. 8577.</p> <p>See SambaNova's SambaTune documentation for more information about using SambaTune and the SambaTune UI. This section is a good starting point: Workflow overview</p> <p>When finished: - Break the ssh tunnel with Ctrl+C (SIGINT) or equivalent. - Stop the sambatune_ui server on the target node with Ctrl+C or equivalent. - Exit the interactive slurm job to release the reserved resources.</p> <p>A disconnected job can be canceled by determining its job id with <code>squeue -a</code> and canceling the job with <code>scancel &lt;jobid&gt;</code></p>"},{"location":"ai-testbed/sambanova/system-overview/","title":"System Overview","text":""},{"location":"ai-testbed/sambanova/system-overview/#introduction","title":"Introduction","text":"<p>The SambaNova DataScale SN30 system is architected around the next-generation Reconfigurable Dataflow Unit (RDU) processor for optimal dataflow processing and acceleration. The AI Testbed's SambaNova SN30 system consists of eight nodes in 4 full racks, each node featuring eight RDUs interconnected to enable model and data parallelism. SambaFlow, Sambanova's software stack, extracts, optimizes, and maps the dataflow graphs to the RDUs from standard machine learning frameworks like PyTorch.</p> <p>Below are some of the links to SambaNova documentation.</p> <p>SambaNova white paper: Accelerated Computing with a Reconfigurable Dataflow Architecture</p> <p>SN30 documentation: SambaNova Documentation</p>"},{"location":"ai-testbed/sambanova/tunneling-and-forwarding-ports/","title":"Tunneling and Forwarding Ports","text":"<p>Port forwarding is covered here.  This is specifically for TensorBoard.</p>"},{"location":"ai-testbed/sambanova/tunneling-and-forwarding-ports/#tensorboard-port-forwarding","title":"TensorBoard Port Forwarding","text":"<p>This section describes the steps to be followed to set up port forwarding for applications, like TensorBoard, which runs on the SambaNova system and binds to one or more ports. This example uses 6006 and 16006 as port numbers. Using port numbers other than these may avoid collisions with other users.</p>"},{"location":"ai-testbed/sambanova/tunneling-and-forwarding-ports/#from-your-local-machine","title":"From Your Local Machine","text":"<p>Replace ALCFUserID with your ALCF User ID.</p> <p>Run</p> <pre><code># Forward a port number from sambanova.alcf.anl.gov to your local machine.\nssh -v -N -f -L localhost:16006:localhost:16006 ALCFUserID@sambanova.alcf.anl.gov\n...\nPassword: &lt; MobilePass+ code &gt;\n\n# Connect to sambanova.alcf.anl.gov\nssh ALCFUserID@sambanova.alcf.anl.gov\n...\nPassword: &lt; MobilePass+ code &gt;\n</code></pre>"},{"location":"ai-testbed/sambanova/tunneling-and-forwarding-ports/#from-sambanovaalcfanlgov","title":"From sambanova.alcf.anl.gov","text":"<p>Below are the commands specific to sn30-r1-h1. You may replace sn30-r1-h1 with any other node when using the appropriate system.</p> <p>Run</p> <p>Note:  The full name is sn30-r1-h1.ai.alcf.anl.gov and it may also be used.</p> <pre><code># Forward the port.\nssh -N -f -L localhost:16006:localhost:6006 ALCFUserID@sn30-r1-h1\n# Connect to the system.\nssh ALCFUserID@sn30-r1-h1\n</code></pre>"},{"location":"ai-testbed/sambanova/tunneling-and-forwarding-ports/#on-sn30-r1-h1","title":"On sn30-r1-h1","text":"<p>Activate the venv appropriate to your project.</p> <p>Navigate to the appropriate directory for your model. Launch your model using srun or sbatch.</p> <pre><code>cd /path/to/your/project\nsbatch --output=pef/my_model/output.log submit-my_model-job.sh\n</code></pre>"},{"location":"ai-testbed/sambanova/tunneling-and-forwarding-ports/#on-another-sn30-r1-h1-terminal-window","title":"On Another sn30-r1-h1 Terminal Window","text":"<p>The SambaNova system has a bash shell script to setup the required software environment. This sets up the SambaFlow software stack, the associated environmental variables and activates a pre-configured virtual environment.</p> <p>Use the command appropriate for your environment.</p> <p>For example, if you are using LogReg:</p> <pre><code>ALCFUserID@sn30-r1-h1:~$ source /opt/sambaflow/apps/starters/logreg/venv/bin/activate\n(venv) ALCFUserID@sn30-r1-h1:~$\n</code></pre> <p>Navigate to the appropriate directory for your model.</p> <pre><code>cd /path/to/your/project\ntensorboard --logdir /logs --port 6006\n</code></pre>"},{"location":"ai-testbed/sambanova/tunneling-and-forwarding-ports/#browser-on-local-machine","title":"Browser on Local Machine","text":"<p>Then, navigate in your browser to, in this example, http://localhost:16006 on your local machine.</p>"},{"location":"ai-testbed/sambanova/tunneling-and-forwarding-ports/#notes","title":"Notes","text":"<p>Explanation of ssh command:</p> <pre><code>-N : no remote commands\n\n-f : put ssh in the background\n\n-L &lt;machine1&gt;:&lt;portA&gt;:&lt;machine2&gt;:&lt;portB&gt; :\n\nThe full command line will forward &lt;machine2&gt;:&lt;portB&gt; (remote scope) to &lt;machine1&gt;:&lt;portA&gt; (local scope)\n</code></pre> <p>Adapted from:  How can I run Tensorboard on a remote server?</p>"},{"location":"ai-testbed/sambanova/virtual-environment/","title":"Virtual Environments","text":""},{"location":"ai-testbed/sambanova/virtual-environment/#using-a-venv","title":"Using a Venv","text":"<p>To create a virtual environment, one can use the --system-site-packages flag:</p> <pre><code>python -m venv --system-site-packages my_env\nsource my_env/bin/activate\n</code></pre>"},{"location":"ai-testbed/sambanova/virtual-environment/#installing-packages","title":"Installing Packages","text":"<p>Install packages in the normal manner such as:</p> <pre><code>python3 -m pip install &lt;package&gt;\n</code></pre> <p>For more details see Use pip for installing.</p> <p>To install a different version of a package that is already installed in one's environment, one can use:</p> <pre><code>pip install --ignore-installed  ... # or -I\n</code></pre>"},{"location":"ai-testbed/sambanova/virtual-environment/#pre-built-sample-venv","title":"Pre-Built Sample Venv","text":"<p>Each of the samples or application examples provided by SambaNova has its own pre-built virtual environment which can be readily used. They are present in the <code>/opt/sambaflow/apps/</code> directory tree within each of the applications.</p> <p>Note: Conda is not supported on the SambaNova system.</p>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/","title":"CosmicTagger Conversion","text":"<p>The intent of this page is to show conceptually how to convert a model to run on the SambaNova system. It is not necessary to convert CosmicTagger because it has already been converted and is located at CosmicTagger on the SambaNova branch. The original is located at CosmicTagger.</p>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#run-model-on-cpu","title":"Run Model on CPU","text":"<p>The first step to converting a model is to verify that it runs on the CPU.  This step has been verified for CosmicTagger.</p>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#configpy","title":"Config.py","text":"<p>CosmicTagger can run on multiple machines.  As such, it is necessary to specify the architecture that one is using.  For example, CPU or GPU.  The architecture is stored in the ComputeMode class.</p> <p>Edit src/config/config.py.  Add RDU to the ComputeMode class.</p> <pre><code>class ComputeMode(Enum):\n    CPU   = 0\n    #...\n    RDU   = 6\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#trainerpy","title":"Trainer.py","text":"<p>Edit src/utils/torch/trainer.py.</p>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#import-sambanova-packages","title":"Import SambaNova Packages","text":"<p>Insert the imports at the top of the file.</p> <p>SambaFlow is a complete software stack designed to take input from standard machine learning frameworks such as PyTorch and TensorFlow. SambaFlow automatically extracts, optimizes, and maps dataflow graphs onto RDUs.</p> <pre><code>try:\n    from sambaflow import samba\n\n    import sambaflow.samba.utils as utils\n    from sambaflow.samba.utils.argparser import parse_app_args\n    from sambaflow.samba.utils.common import common_app_driver\nexcept:\n    pass\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#wrap-model","title":"Wrap Model","text":"<p>Wrap the model using poptorch.trainingModel() so that it may be ran on IPUs for training.</p> <p>Wrap the model using poptorch.inferenceModel() when not training.</p> <p>Find the following code around line 90 in the init_network method.</p> <pre><code>        # Foregoing any fusions as to not disturb the existing ingestion pipeline\n        if self.is_training() and self.args.mode.quantization_aware:\n            self._raw_net.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n            self._net = torch.quantization.prepare_qat(self._raw_net)\n        else:\n            self._net = self._raw_net\n</code></pre> <p>After the above code, add:</p> <pre><code>        if self.args.run.compute_mode == ComputeMode.IPU:\n            if self.is_training():\n                opts = poptorch.Options()\n                self._net = poptorch.trainingModel(self._net, opts, optimizer=torch.optim.SGD(self._net.parameters(), lr=1e-3))\n            else:\n                self._net = poptorch.inferenceModel(self._net)\n</code></pre> <p>See poptorch.trainingModel() and poptorch.inferenceModel() for more information.</p> <p>There is also a Build the Model tutorial.</p>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#update-optimizer","title":"Update Optimizer","text":"<p>Update init_optimizer() to use the poptorch class instead of the torch class as needed.</p> <p>Change:</p> <pre><code>        if self.args.mode.optimizer.name == OptimizerKind.rmsprop:\n            self._opt = torch.optim.RMSprop(self._net.parameters(), 1.0, eps=1e-4)\n        else:\n            self._opt = torch.optim.Adam(self._net.parameters(), 1.0)\n</code></pre> <p>to:</p> <pre><code>        if self.args.mode.optimizer.name == OptimizerKind.rmsprop:\n            if self.args.run.compute_mode == ComputeMode.IPU:\n                self._opt = poptorch.optim.RMSprop(self._net.parameters(), 1.0, eps=1e-4)\n            else:\n                self._opt = torch.optim.RMSprop(self._net.parameters(), 1.0, eps=1e-4)\n        else:\n            if self.args.run.compute_mode == ComputeMode.IPU:\n                self._opt = poptorch.optim.Adam(self._net.parameters(), 1.0)\n            else:\n                self._opt = torch.optim.Adam(self._net.parameters(), 1.0)\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#update-the-forward-pass","title":"Update the Forward Pass","text":"<p>Putting the loss calculation in forward_pass() allows the loss computation to be performed on the IPUs. This will be faster because the data will not need to be transfered round-trip to the CPU.</p> <p>Change forward_pass():</p>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#original","title":"Original","text":"<pre><code>            if net is None:\n                logits_image = self._net(minibatch_data['image'])\n            else:\n                logits_image = net(minibatch_data['image'])\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#updated","title":"Updated","text":"<p>The following code changes are to account for the loss function, i.e., self.loss_calculator, and the image labels, i.e., labels_image, to be passed to the model's forward_pass method.  Additionally, the calculated loss is returned from the forward_pass method.</p> <pre><code>            if net is None:\n                if self.args.run.compute_mode == ComputeMode.IPU:\n                    logits_image, labels_image, loss = self._net(minibatch_data['image'], self.loss_calculator, labels_image)\n                    return logits_image, labels_image, loss\n                else:\n                    logits_image = self._net(minibatch_data['image'])\n            else:\n                if self.args.run.compute_mode == ComputeMode.IPU and self.args.mode.name != ModeKind.inference:\n                    logits_image, labels_image, loss = net(minibatch_data['image'], self.loss_calculator, labels_image)\n                    return logits_image, labels_image, loss\n                else:\n                    logits_image = net(minibatch_data['image'])\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#update-the-training-step","title":"Update the Training Step","text":"<p>Receive the extra loss variable from the forward_pass method.</p> <p>Update the train_step method.</p>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#original-training-step","title":"Original Training Step","text":"<pre><code>                    with self.timing_context(\"forward\"):\n                        if self.args.run.precision == Precision.mixed and self.args.run.compute_mode == ComputeMode.GPU:\n                            with torch.cuda.amp.autocast():\n                                logits_image, labels_image = self.forward_pass(minibatch_data)\n                        else:\n                            logits_image, labels_image = self.forward_pass(minibatch_data)\n\n                    verbose = False\n\n                    # Compute the loss based on the logits\n                    with self.timing_context(\"loss\"):\n                        loss = self.loss_calculator(labels_image, logits_image)\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#updated-training-step","title":"Updated Training Step","text":"<p>The forward_pass() method was changed to return the extra variable loss in the previous section.  It is now received conditionally when using an IPU(s).</p> <p>In the with self.timing_context(\"loss\"): section, only calculate loss if not using an IPU(s).</p> <pre><code>                    with self.timing_context(\"forward\"):\n                        if self.args.run.precision == Precision.mixed and self.args.run.compute_mode == ComputeMode.GPU:\n                            with torch.cuda.amp.autocast():\n                                logits_image, labels_image = self.forward_pass(minibatch_data)\n                        else:\n                            if self.args.run.compute_mode == ComputeMode.IPU:\n                                logits_image, labels_image, loss = self.forward_pass(minibatch_data)\n                            else:\n                                logits_image, labels_image = self.forward_pass(minibatch_data)\n\n                    verbose = False\n\n\n                    # Compute the loss based on the logits\n                    with self.timing_context(\"loss\"):\n                        if self.args.run.compute_mode == ComputeMode.IPU:\n                            loss = loss\n                        else:\n                            loss = self.loss_calculator(labels_image, logits_image)\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#update-validation-step","title":"Update Validation Step","text":"<p>Update the val_step method.</p>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#original-validation-step-code","title":"Original Validation Step Code","text":"<p>Find this code.</p> <pre><code>            if self.args.run.precision == Precision.mixed and self.args.run.compute_mode == ComputeMode.GPU:\n                with torch.cuda.amp.autocast():\n                    logits_image, labels_image = self.forward_pass(minibatch_data, net=val_net)\n            else:\n                logits_image, labels_image = self.forward_pass(minibatch_data, net=val_net)\n\n            # Compute the loss based on the logits\n            loss = self.loss_calculator(labels_image, logits_image)\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#updated-validation-step-code","title":"Updated Validation Step Code","text":"<p>Change the code to the following.</p> <pre><code>            if self.args.run.precision == Precision.mixed and self.args.run.compute_mode == ComputeMode.GPU:\n                with torch.cuda.amp.autocast():\n                    logits_image, labels_image = self.forward_pass(minibatch_data, net=val_net)\n\n                    # Compute the loss based on the logits\n                    loss = self.loss_calculator(labels_image, logits_image)\n            else:\n                if self.args.run.compute_mode == ComputeMode.IPU:\n                    logits_image, labels_image, loss = self.forward_pass(minibatch_data, net=val_net)\n                else:\n                    logits_image, labels_image = self.forward_pass(minibatch_data, net=val_net)\n\n                    # Compute the loss based on the logits\n                    loss = self.loss_calculator(labels_image, logits_image)\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#uresnet2d-model","title":"UResNet2D Model","text":""},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#update-model","title":"Update Model","text":"<p>The Graphcore system is more computationally efficient if the loss function is on the IPU.  This is accomplished by using the loss function within the model's forward method.</p> <p>Edit src/networks/torch/uresnet2D.py.</p>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#update-the-forward-declaration","title":"Update the Forward Declaration","text":"<p>Find the forward method.</p> <pre><code>def forward(self, input_tensor):\n</code></pre> <p>Update the argument list to include the loss function, i.e., loss_calculator and the image labels, i.e., labels_image.</p> <pre><code>def forward(self, input_tensor, loss_calculator=None, labels_image=None):\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#add-loss-calculation","title":"Add Loss Calculation","text":"<p>Add the loss calculation just before the forward method returns.</p> <pre><code>        if loss_calculator is not None:\n\n            labels_image = labels_image.long()\n            labels_image = torch.chunk(labels_image, chunks=3, dim=1)\n            shape =  labels_image[0].shape\n            labels_image = [ _label.view([shape[0], shape[-2], shape[-1]]) for _label in labels_image ]\n\n            loss = loss_calculator(labels_image, x)\n            import poptorch\n            loss = poptorch.identity_loss(loss , reduction=\"mean\")\n            return x, labels_image, loss\n\n        # This return already exists.\n        return x\n</code></pre> <p>The poptorch.identity_loss method takes a single PyTorch tensor and will backpropagate a gradient of ones through it.  You may find an example at here</p>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#binexecpy","title":"bin/exec.py","text":"<p>The following is included for completeness.  One will not likely find this in other code.</p> <p>Open bin/exec.py in your favorite editor.  Change:</p> <pre><code>@hydra.main(version_base=None, config_path=\"../src/config\", config_name=\"config\")\n</code></pre> <p>to</p> <pre><code>@hydra.main(config_path=\"../src/config\", config_name=\"config\")\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/performance-tools/","title":"Performance Tools","text":""},{"location":"ai-testbed/sambanova/unused/performance-tools/#tile-status","title":"Tile Status","text":"<pre><code>sntilestat\nwatch sntilestat\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/performance-tools/#measure-tflops","title":"Measure TFLOPs","text":"<p>This is an example for measuring TFLOPs for Conv2D forward pass.</p> <pre><code>elif args.command == 'run':\n    samba.session.run(inputs, section_types=['fwd'])\n    #samba.session.run(inputs, section_types=['bckwd'])\n    n_iters = 100\n    forward_pass_time = []\n    print(\"run starts\")\n    start_time_forward = time.time()\n    for loop in range(n_iters):\n        samba.session.run(inputs, section_types=['fwd'])\n        #samba.session.run(inputs, section_types=['bckwd'])\n        #samba.session.run(inputs, section_types=['fwd', 'bckwd'])\n    end_time_forward = time.time()\n    forward_pass_time.append(end_time_forward - start_time_forward)\n    print(\"run ends\")\n\n    w_0 = (args.w + 2*args.pad_w - args.s)/args.wstride + 1\n    h_0 = (args.h + 2*args.pad_h - args.r)/args.hstride + 1\n    tflops = 2 * (w_0*h_0) * args.s * args.r * args.c * args.k * args.n\n    tflops_forw = tflops/(sum(forward_pass_time)/n_iters/5)/(10**12) #tflops\n    print(tflops)\n    print(sum(forward_pass_time))\n    print(\"tflops: %f\"%tflops_forw)\n    print(\"SN,Training,%s,Conv2d_fwd,%d,100,1,%d,%d,%d,%d,%d,%d,%d,0.0,%f,None,%f,%f,%f\" % (\"dtype\", args.n, args.w, args.h, args.c, args.k, args.s, args.pad_w, args.wstride, (sum(forward_pass_time)/n_iters)/args.n, args.n/(sum(forward_pass_time)/n_iters), tflops_forw, (sum(forward_pass_time)/n_iters)/args.n))\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/running-GPT2-multi-node/","title":"Running GPT-2 on Multiple Nodes","text":"<p>This GPT-2 example is for 1.5B parameters on two (2) nodes. Each node has eight (8) RDUs for a total of sixteen (16) RDUs.</p>"},{"location":"ai-testbed/sambanova/unused/running-GPT2-multi-node/#create-a-directory","title":"Create a Directory","text":"<pre><code>cd &lt;path to desired directory&gt;\nmkdir GPT1.5B\ncd GPT1.5B\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/running-GPT2-multi-node/#establish-script","title":"Establish Script","text":"<p>Using your favorite editor, create the file 'Gpt1.5B.sh'.</p> <p>Copy the contents of Gpt1.5B.sh.</p> <p>Make the script executable:</p> <pre><code>chmod +x Gpt1.5B.sh\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/running-GPT2-multi-node/#multiple-nodes","title":"Multiple Nodes","text":"<p>Gpt1.5B.sh contains the sbatch command:</p> <pre><code>/usr/local/bin/sbatch --output=${HOME}/slurm-%A.out --ntasks 32 --gres=rdu:1 --ntasks-per-node 16  --nodes 2 --cpus-per-task=8  /data/ANL/scripts/Gpt1.5B_run.sh ${1} &gt;&gt; ${OUTPUT_PATH} 2&gt;&amp;1\n</code></pre> <p>The sbatch nodes argument specifies the number of nodes to use.</p> <p>nodes 2 Nodes to use.</p> <p>Additionally, here are the other sbatch arguments.</p> <p>--ntasks 32: This option specifies the number of tasks to be used in the job.</p> <p>ntasks-per-node 16: This option specifies the number of tasks per node.</p> <p>gres=rdu:1 Indicates the model fits on a single RDU.</p> <p>cpus-per-task=8 CPUs per task.</p>"},{"location":"ai-testbed/sambanova/unused/running-GPT2-multi-node/#run","title":"Run","text":"<p>The script accepts an optional first parameter to specify the log directory.</p> <p>Run the script:</p> <pre><code>./Gpt1.5B.sh &lt;optional log directory&gt;\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/running-GPT2-multi-node/#output","title":"Output","text":"<p>The output can be found at /data/ANL/results/$(hostname)/${USER}/${LOGDIR}/${MODEL_NAME}.out. The actual path will be displayed on the screen.</p>"},{"location":"ai-testbed/sambanova/unused/running-GPT2/","title":"Running GPT2","text":"<p>The Pile and OWT data are located in:</p> <pre><code>/data/ANL/pile\n/data/ANL/openwebtext_ss2048\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/running-bert-large-on-sn30/","title":"Running BERT-Large on SambaNova DataScale SN30-8","text":""},{"location":"ai-testbed/sambanova/unused/running-bert-large-on-sn30/#set-up","title":"Set Up","text":"<p>Establish a test directory from which to work.</p> <pre><code>mkdir $HOME/app-test\ncd $HOME/app-test\n</code></pre> <p>Copy BertLarge.sh into your current directory.</p> <pre><code>cp /data/ANL/scripts/BertLarge.sh .\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/running-bert-large-on-sn30/#running-bert-large-options","title":"Running Bert Large Options","text":"<p>Let's cover several options for executing the script.</p> <ol> <li>Basic</li> </ol> <pre><code>sbatch --output=${HOME}/app-test/slurm-%A.out --cpus-per-task=128 --gres=rdu:16 BertLarge.sh\n</code></pre> <ol> <li>Specify a Log File</li> </ol> <p>This is helpful if doing multiple runs and one wishes to specify a run ID.    This bash script argument is optional.  Place it at the very end of the command.</p> <p>Example:</p> <pre><code>sbatch --output=${HOME}/app-test/slurm-%A.out --cpus-per-task=128 --gres=rdu:16 BertLarge.sh my_runID\n</code></pre> <ol> <li>Specify Nodelist</li> </ol> <p>One may optionally specify a nodelist for sbatch. An example is to use hostname.</p> <pre><code>sbatch --nodelist $(hostname) --output=${HOME}/app-test/slurm-%A.out --cpus-per-task=128 --gres=rdu:16 BertLarge.sh\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/running-bert-large-on-sn30/#running-bert-large","title":"Running Bert Large","text":"<p>Let's specify the log file and the nodelist.</p> <p>Run</p> <pre><code>sbatch --nodelist $(hostname) --output=${HOME}/app-test/slurm-%A.out --cpus-per-task=128 --gres=rdu:16 BertLarge.sh\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/running-bert-large-on-sn30/#output","title":"Output","text":"<p>Display the slurm output.  For example:</p> <pre><code>cat slurm-9637.out\n</code></pre> <p>The output will look something like:</p> <pre><code>Using /data/ANL/results/sn30-r3-h1/userid/040423.19/BertLarge.out for output\n</code></pre> <p>You may display that file.  You may want to use less to do so because it is quite long.</p> <pre><code>less /data/ANL/results/sn30-r3-h1/userid/040423.19/BertLarge.out\n</code></pre> <p>The organization of the file is:</p> <ol> <li>System Status</li> <li>Compile (very long)</li> <li>Run</li> <li>System Status</li> <li>Run Duration</li> </ol>"},{"location":"ai-testbed/sambanova/unused/sambatune-user-guide/","title":"SambaTune","text":""},{"location":"ai-testbed/sambanova/unused/sambatune-user-guide/#notes","title":"Notes","text":"<p>Rick  4/16/2023 [10:16 AM] /home/rweisner/sambatune_ui_dir contains  the 1.15.3 version which is the latest released version. It should work on your experimental. You will need browser access to wherever you install it.</p> <pre><code>cd /home/rweisner/tmp/uno_test\n</code></pre> <pre><code>#TODOBRW\nssh wilsonb@homes.cels.anl.gov\nssh sm-02\nMobilePass+ password\nOn sm-02\nsource /opt/sambaflow/venv/bin/activate\nexport PATH=/opt/sambaflow/bin:$PATH\nsambatune linear_net.yaml --artifact-root $(pwd)/artifact_root --modes benchmark instrument run\nsambatune_ui --directory /home/wilsonb/tmp/sambatune_gen --port 8580\n#There will be a username and password displayed that you will use in your browser on your laptop.\nCommand used on laptop for port forward\nssh -XL 8580:127.0.0.1:8580 wilsonb@sm-02.cels.anl.gov\nMobilePass+ password\n# You will be logged into sm-02 but, you do not need to do anything.\naddress used in browser on laptop localhost:8580\n#Use username and password from sambatune_ui.\nUsername\nPassword\n\n#TODOBRW\n/home/wilsonb/DL/Sambanova/apps_1.12/private/anl/2022-09-21T19-21-05.html\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/sambatune-user-guide/#about-sambatune","title":"About SambaTune","text":"<p>SambaTune is a tool for profiling, debugging, and tuning the performance of applications running on SN hardware.</p> <p>The tool automates the collection of hardware performance counters, metrics aggregation, report generation, and visualization. It also automates benchmarking of the application to compute average throughput over a sufficient number of runs. The tool is designed to aid the user with performance bottleneck analysis and tuning.</p> <p>SambaTune is currently used by SN engineers involved in performance tuning efforts. SambaTune is also planned for release to external customers to aid with performance bottleneck analysis and resolution.</p>"},{"location":"ai-testbed/sambanova/unused/sambatune-user-guide/#run-sambatune","title":"Run SambaTune","text":"<pre><code>ssh ALCFUserID@sambanova.alcf.anl.gov\n# Enter MobilePass+ pass code\nssh sm-01\n</code></pre> <pre><code>#TODOBRW\nssh wilsonb@sambanova.alcf.anl.gov\n# Enter MobilePass+ pass code\nssh sm-01\n</code></pre> <p>First, enter the virtual environment on sm-01 or sm-02:</p> <pre><code>source /opt/sambaflow/venv/bin/activate\n</code></pre> <p>Update path:</p> <pre><code>export PATH=/opt/sambaflow/bin:$PATH\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/sambatune-user-guide/#usage","title":"Usage","text":"<pre><code>usage: sambatune [-h] [--artifact-root ARTIFACT_ROOT] [--disable-override]\n                 [--compile-only | -m MODES [MODES ...]] [--version]\n                 config\n\npositional arguments:\n  config                YAML file with model, compile, run configuration.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --artifact-root ARTIFACT_ROOT\n                        Custom location to save compile/run artifacts;\n                        defaults to '$DUMP_ROOT/artifact_root' (default: None)\n  --disable-override    Reuse the placement from the baseline compilation\n                        (default: False)\n  --compile-only        Run compilation of PEFs for selected modes only\n                        (default: False)\n  -m MODES [MODES ...], --modes MODES [MODES ...]\n                        Select modes to execute from ['benchmark',\n                        'instrument', 'run'] (default: ['benchmark'])\n  --version             version of sambatune and sambaflow.\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/sambatune-user-guide/#command-overview","title":"Command Overview","text":"<p>By default, it will run with the benchmarking mode enabled. Use the --modes flag to run modes individually or in any combination. Benchmark-Only:</p> <pre><code>sambatune example_net.yaml --artifact-root $(pwd)/artifact_root --modes benchmark\n</code></pre> <p>Instrument-Only:</p> <pre><code>sambatune example_net.yaml --artifact-root $(pwd)/artifact_root --modes instrument\n</code></pre> <p>All modes:</p> <pre><code>sambatune example_net.yaml --artifact-root $(pwd)/artifact_root --modes instrument\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/sambatune-user-guide/#command-example","title":"Command Example","text":"<pre><code># From Bill\npython /opt/sambaflow/apps/private/anl/uno_full.py compile --weight-sharing -b 16 -mb 4 --num-spatial-batches 500 --mapping spatial --mac-human-decision /opt/sambaflow/apps/private/anl/samba_uno/human_decisions_spatial.json --pef-name=uno_16_4_500_ws --output-folder=/home/arnoldw//models_dir/1520847 --mac-v1\n\npython /opt/sambaflow/apps/private/anl/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches 500 --mapping spatial --pef=/home/arnoldw//models_dir/1520847/uno_16_4_500_ws/uno_16_4_500_ws.pef --in_dir /var/tmp/raw/ --mac-v1\n</code></pre> <pre><code># From Bill --&gt; Bruce\npython /opt/sambaflow/apps/private/anl/uno_full.py compile --weight-sharing -b 16 -mb 4 --num-spatial-batches 500 --mapping spatial --mac-human-decision /opt/sambaflow/apps/private/anl/samba_uno/human_decisions_spatial.json --pef-name=uno_16_4_500_ws --output-folder='.' --mac-v1\n\nexport OMP_NUM_THREADS=1\npython /opt/sambaflow/apps/private/anl/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches 500 --mapping spatial --pef=./uno_16_4_500_ws/uno_16_4_500_ws.pef --in_dir /var/tmp/raw/ --mac-v1\n</code></pre> <pre><code>#TODOBRW  This works.  9/19/22\nsm-01/home/wilsonb/tmp/uno_test/uno_ccle.yaml\napp: /opt/sambaflow/apps/private/anl/uno_full.py\n\nmodel-args: --weight-sharing -b 16 -mb 4 --num-spatial-batches 500 --mapping spatial\n\ncompile-args: compile --plot --mac-human-decision /opt/sambaflow/apps/private/anl/samba_uno/human_decisions_spatial.json --mac-v1\n\nrun-args: --multiprocess-pickle --use-pickle-train  --measure-spatial --train-samba-spatial --mac-v1 --train_source CCLE --lr 0.001 --data-dir /software/sambanova/dataset/CCLE_16_500 --converted-pickle\n\nenv:\n     OMP_NUM_THREADS: 16,\n     SF_RNT_NUMA_BIND: 2\n</code></pre> <p>Run the following example:</p> <pre><code>sambatune uno_ccle.yaml --artifact-root $(pwd)/artifact_root --modes benchmark instrument run\n</code></pre> <pre><code>#TODOBRW\n# Stand-alone\nexport UNO=.\nexport NS=500\nsrun python /opt/sambaflow/apps/private/anl/uno_full.py compile --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --mac-human-decision /opt/sambaflow/apps/private/anl/samba_uno/human_decisions_spatial.json --pef-name=uno_16_4_${NS}_ws --output-folder='.' --mac-v1\n\nexport OMP_NUM_THREADS=1\nsrun python /opt/sambaflow/apps/private/anl/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=./uno_16_4_${NS}_ws/uno_16_4_${NS}_ws.pef --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE --data-dir /software/sambanova/dataset/CCLE_16_${NS}\n\nexport UNO=.\nexport NS=500\nexport OMP_NUM_THREADS=1\nsrun pyinstrument /opt/sambaflow/apps/private/anl/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=./uno_16_4_${NS}_ws/uno_16_4_${NS}_ws.pef --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE --data-dir /software/sambanova/dataset/CCLE_16_${NS} &gt; pyinstrument_1.13.log 2&gt;&amp;1\n\n\n\nRicks run python ${UNO}/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=\u201cout/uno_16_4_${NS}/uno_16_4_${NS}.pef\u201d --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE\n</code></pre> <pre><code>#TODOBRW\nsm-01/home/wilsonb/DL/Sambanova/apps_1.12/private/anl/uno_brw_CCLE_1_12.yaml\nexport OMP_NUM_THREADS=16\napp: /home/wilsonb/DL/Sambanova/apps_1.12/private/anl/uno_full.py\n\nmodel-args: --weight-sharing -b 16 -mb 4 --num-spatial-batches 500 --mapping spatial\n\ncompile-args: compile --plot --mac-human-decision /opt/sambaflow/apps/private/anl/samba_uno/human_decisions_spatial.json --mac-v1\n\nrun-args: --measure-spatial --train-samba-spatial --mac-v1 --train_source CCLE --lr 0.001 --data-dir /software/sambanova/dataset/CCLE_16_500\n\nenv:\n     OMP_NUM_THREADS: 16,\n     SF_RNT_NUMA_BIND: 2\n</code></pre> <p>Run the following example:</p> <pre><code>sambatune uno_brw_CCLE_1_12.yaml --artifact-root $(pwd)/artifact_root --modes benchmark instrument run\n\nexport UNO=.\nexport NS=50\nexport OMP_NUM_THREADS=1\n\nsrun python /opt/sambaflow/apps/private/anl/uno_full.py compile --mac-human-decision /opt/sambaflow/apps/private/anl/samba_uno/human_decisions_spatial.json --mac-v1\n\nxsrun pyinstrument /opt/sambaflow/apps/private/anl/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=./uno_16_4_${NS}_ws/uno_16_4_${NS}_ws.pef --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE --data-dir /software/sambanova/dataset/CCLE_16_${NS} --epochs 1 &gt; my.log 2&gt;&amp;1\n\nsrun python /opt/sambaflow/apps/private/anl/uno_full.py run --multiprocess-pickle  --measure-spatial --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=./out/uno_full_16_47_${NS}/uno_full_16_47_${NS}.pef --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE --lr 0.001 --data-dir /software/sambanova/dataset/CCLE_16_${NS} &gt; pyinstrument_1.13.log 2&gt;&amp;1\n\ncat my.log # Has pyinstrument run name.\npyinstrument --load-prev 2022-09-21T19-21-05 -r html\n\n\n1.13\n\nsource /opt/sambaflow/venv/bin/activate\ncd ~/tmp/uno_test/\nexport UNO=.\nexport NS=500\nexport OMP_NUM_THREADS=1\nexport PATH=/opt/sambaflow/bin:$PATH\nsntilestat\n\n\n\n./uno_pickl.sh compile 500\n./uno_pickl.sh run 500\n</code></pre> <pre><code>sambatune uno_brw_CCLE_1_12.yaml --artifact-root $(pwd)/artifact_root --modes benchmark instrument run\n\nexport UNO=.\nexport NS=50\nexport OMP_NUM_THREADS=1\n\nsrun python /opt/sambaflow/apps/private/anl/uno_full.py compile --mac-human-decision /opt/sambaflow/apps/private/anl/samba_uno/human_decisions_spatial.json --mac-v1\n\nxsrun pyinstrument /opt/sambaflow/apps/private/anl/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=./uno_16_4_${NS}_ws/uno_16_4_${NS}_ws.pef --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE --data-dir /software/sambanova/dataset/CCLE_16_${NS} --epochs 1 &gt; my.log 2&gt;&amp;1\n\nsrun python /opt/sambaflow/apps/private/anl/uno_full.py run --multiprocess-pickle  --measure-spatial --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=./out/uno_full_16_47_${NS}/uno_full_16_47_${NS}.pef --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE --lr 0.001 --data-dir /software/sambanova/dataset/CCLE_16_${NS} &gt; pyinstrument_1.13.log 2&gt;&amp;1\n\ncat my.log # Has pyinstrument run name.\npyinstrument --load-prev 2022-09-21T19-21-05 -r html\n\n\n1.13\n\nsource /opt/sambaflow/venv/bin/activate\ncd ~/tmp/uno_test/\nexport UNO=.\nexport NS=500\nexport OMP_NUM_THREADS=1\nexport PATH=/opt/sambaflow/bin:$PATH\nsntilestat\n</code></pre> <p>uno_pickl.sh</p> <pre><code>#! /bin/bash -x\n#set -e\nsource /opt/sambaflow/venv/bin/activate\nSECONDS=0\nNS=${2}\nUNO=/opt/sambaflow/apps/private/anl/\nDS=\"ALL\"\nDS=\"CCLE\"\n\nBS=$((NS*16))\nexport OMP_NUM_THREADS=16\n\necho \"Model: UNO_SPA_TRN\"\necho \"Date: \" $(date +%m/%d/%y)\necho \"Time: \" $(date +%H:%M)\nif [ \"${1}\" == \"convert\" ] ; then\npython3 ${UNO}/uno/uno_data_loaders_converted.py   --in_dir /var/tmp/raw/ --out_dir /software/sambanova/dataset/${DS}_16_${NS}  --batch-size ${BS} --train_sources ${DS} --file-write-frequency 10\n\n\nelif [ \"${1}\" == \"compile\" ] ; then\n  echo \"COMPILE\"\n  python ${UNO}/uno_full.py compile --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --mac-human-decision ${UNO}/samba_uno/human_decisions_spatial.json --pef-name=\"uno_16_4_${NS}\" --mac-v1\n\n\nelif [ \"${1}\" == \"run\" ] ; then\n  echo \"RUN ${DS}\"\n  SF_RNT_NUMA_BIND=2\n  #python ${UNO}/uno_full.py run --acc-test --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\" --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE\n  python ${UNO}/uno_full.py run --mac-v1 --multiprocess-pickle --use-pickle-train --train-samba-spatial -b 16 -mb 4 --num-spatial-batches ${NS} --lr 0.001 --mapping spatial --data-dir /software/sambanova/dataset/${DS}_16_${NS} --converted-pickle --train_sources ${DS} --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\" --epochs 1\n  #python ${UNO}/uno_full.py run --mac-v1 --multiprocess-pickle --use-pickle-train --train-samba-spatial -b 16 -mb 4 --num-spatial-batches ${NS} --lr 0.001 --mapping spatial --data-dir /software/sambanova/dataset/${DS}_16_${NS} --converted-pickle --train_sources ${DS} --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\"\n\nelif [ \"${1}\" == \"pyinstrument\" ] ; then\n  echo \"RUN ${DS}\"\n  SF_RNT_NUMA_BIND=2\n  #python ${UNO}/uno_full.py run --acc-test --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\" --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE\n  pyinstrument ${UNO}/uno_full.py run --mac-v1 --multiprocess-pickle --use-pickle-train --train-samba-spatial -b 16 -mb 4 --num-spatial-batches ${NS} --lr 0.001 --mapping spatial --data-dir /software/sambanova/dataset/${DS}_16_${NS} --converted-pickle --train_sources ${DS} --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\" --epochs 1\n  #python ${UNO}/uno_full.py run --mac-v1 --multiprocess-pickle --use-pickle-train --train-samba-spatial -b 16 -mb 4 --num-spatial-batches ${NS} --lr 0.001 --mapping spatial --data-dir /software/sambanova/dataset/${DS}_16_${NS} --converted-pickle --train_sources ${DS} --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\"\n\nelif [ \"${1}\" == \"no_pickle\" ] ; then\n  echo \"no_pickle ${DS}\"\n  SF_RNT_NUMA_BIND=2\n  python ${UNO}/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\" --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE\n\nelif [ \"${1}\" == \"mp\" ] ; then\necho \"Duration: \" $SECONDS\n\nelif [ \"${1}\" == \"mp\" ] ; then\necho \"Duration: \" $SECONDS\necho \"PERF\"\npython uno_full.py measure-performance --measure-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\" --num-iterations 20 --mac-v1\nfi\n\necho \"Duration: \" $SECONDS\n</code></pre> <pre><code>./uno_pickl.sh compile 500\n./uno_pickl.sh run 500\n./uno_pickl.sh pyinstrument 500\npyinstrument --load-prev 2022-09-22T18-31-24 -r html\nstdout is a terminal, so saved profile output to /tmp/tmpeo5ehksn.html\ncp /tmp/tmpeo5ehksn.html .\n</code></pre> <p>On dev terminal</p> <pre><code>scp wilsonb@sambanova.alcf.anl.gov:tmp/uno_test/tmpeo5ehksn.html .\n</code></pre> <p>View in local browser.</p>"},{"location":"ai-testbed/sambanova/unused/sambatune-user-guide/#running","title":"Running","text":"<p>Create a directory for your work.</p> <pre><code>mkdir ~/sambatune\ncd ~/sambatune\n</code></pre> <p>Create small_vae.yaml with the following content using your favorite editor.</p> <pre><code>app: /opt/sambaflow/apps/private/anl/moleculevae.py\n\nmodel-args: -b 128 --in-width 512 --in-height 512\n\ncompile-args: compile --plot --enable-conv-tiling --compiler-configs-file /opt/sambaflow/apps/private/anl/moleculevae/compiler_configs_conv.json --mac-v2 --mac-human-decision /opt/sambaflow/apps/private/anl/moleculevae/symmetric_human_decisions_tiled_v2.json\n\nrun-args: --input-path /var/tmp/dataset/moleculevae/ras1_prot-pops.h5 --out-path ${HOME}/moleculevae_out --model-id 0 --epochs 10\n\nenv:\n     OMP_NUM_THREADS: 16\n     SF_RNT_FSM_POLL_BUSY_WAIT: 1\n     SF_RNT_DMA_POLL_BUSY_WAIT: 1\n     CONVFUNC_DEBUG_RUN: 0\n</code></pre> <p>Run the following example:</p> <pre><code>sambatune small_vae.yaml --artifact-root $(pwd)/artifact_root --modes benchmark instrument run\n</code></pre> <p>Create linear_net.yaml with the following content using your favorite editor.</p> <pre><code>app: /opt/sambaflow/apps/micros/linear_net.py\n\nmodel-args: &gt;\n  -b 1024\n  -mb 64\n  --in-features 8192\n  --out-features 4096\n  --repeat 128\n  --inference\n\ncompile-args: &gt;\n  --n-chips 2\n  --plot\n\nenv:\n  SF_RNT_FSM_POLL_BUSY_WAIT: 1\n  SF_RNT_DMA_POLL_BUSY_WAIT: 1\n  CONVFUNC_DEBUG_RUN\": 0\n</code></pre> <p>NOTE: The following takes 45 minutes to run.</p> <p>Run the following example:</p> <pre><code>sambatune linear_net.yaml --artifact-root $(pwd)/artifact_root --modes benchmark instrument run\n</code></pre> <pre><code>#TODOBRW\ncd ~/tmp/uno_test\nscreen\nsambatune uno.yaml --artifact-root $(pwd)/artifact_root --modes benchmark instrument run\n</code></pre> <p>where linear_net.yaml is a user-specified configuration file you created above.</p>"},{"location":"ai-testbed/sambanova/unused/sambatune-user-guide/#sambatune-ui","title":"SambaTune UI","text":""},{"location":"ai-testbed/sambanova/unused/sambatune-user-guide/#port-availability","title":"Port Availability","text":"<p>It is recommended that you check if the port you want to use is available. You may check by:</p> <pre><code>ps -elf | grep desired_port\n</code></pre> <p>Example:</p> <pre><code>ps -elf | grep 8576\n</code></pre> <p>Alternatively, you may check for all ports in use by sambatune_ui:</p> <pre><code>ps -elf | grep sambatune_ui\n</code></pre> <p>If you need to free a port that you are finished with, you may use the kill command.</p>"},{"location":"ai-testbed/sambanova/unused/sambatune-user-guide/#start-sambatune-ui","title":"Start SambaTune UI","text":"<p>If you followed the above directions, your artifact_root will be at ~/sambatune/artifact_root.</p> <p>Start the UI:</p> <p>It will tell you the username and password.</p> <p>NOTE: It is recommended to use a port other than 8576 in case someone else is using it.  Select another port close to 8576.</p> <p>Next</p> <pre><code>sambatune_ui --directory ~/sambatune/artifact_root/sambatune_gen/ --port 8576\n</code></pre> <pre><code>#TODOBRW\nsambatune_ui --directory ~/sambatune/artifact_root/sambatune_gen/ --port 8580\nsambatune_ui --directory /home/wilsonb/tmp/uno_test/artifact_root/sambatune_gen --port 8580\nusername: \"admin\", password: \"4f7cac2c-351e-11ed-93a3-f7ef9c6e5d46\"\nusername: \"admin\", password: \"aaf1fc88-35c8-11ed-93a3-f7ef9c6e5d46\"\nusername: \"admin\", password: \"bf64e4f8-3831-11ed-93a3-f7ef9c6e5d46\"\nusername: \"admin\", password: \"8feca89e-384c-11ed-93a3-f7ef9c6e5d46\"\nusername: \"admin\", password: \"355222d6-3a88-11ed-93a3-f7ef9c6e5d46\"\n</code></pre> <p>You will see something like:</p> <pre><code>with the,\n    username: \"admin\", password: \"05c63938-2941-11ed-93a3-f7ef9c6e5d46\"\n[2022-08-31 15:24:36 +0000] [1344959] [Info] Starting gunicorn 20.1.0\n[2022-08-31 15:24:36 +0000] [1344959] [Info] Listening at: http://0.0.0.0:8576 (1344959)\n[2022-08-31 15:24:36 +0000] [1344959] [Info] Using worker: sync\n[2022-08-31 15:24:36 +0000] [1345092] [Info] Booting worker with pid: 1345092\n[2022-08-31 15:24:36 +0000] [1345093] [Info] Booting worker with pid: 1345093\n</code></pre> <p>NOTE: Write down the username and password.</p> <p>NOTE: The password only works with this one instance of sambatune_ui.  If you stop this instance of sambatune_ui and start another instance, it will have a new password.</p> <p>NOTE: You will need to &gt; or use the kill command to stop sambatune_ui when you have finished. Not doing so will tie up the port. You can ps -elf | grep the_port_you_used to find the running processes. If you are not comfortable doing this, please ask for help."},{"location":"ai-testbed/sambanova/unused/sambatune-user-guide/#use-port-forwarding","title":"Use Port-Forwarding","text":"<p>This describes the steps to set up port-forwarding for applications, like SambaTune UI, which runs on the SambaNova system and binds to one or more ports. This example uses 8576 and 18576 as port numbers. Using port numbers other than these may avoid collisions with other users.</p>"},{"location":"ai-testbed/sambanova/unused/sambatune-user-guide/#from-your-local-machine","title":"From your local machine","text":"<p>This command sets up a port forward SambaNova login node to your local machine.</p> <p>Run</p> <pre><code>ssh -N -f -L localhost:18576:localhost:18576 ALCFUserID@sambanova.alcf.anl.gov\n...\nPassword: &lt; MobilePass+ code &gt;\n\nssh ALCFUserID@sambanova.alcf.anl.gov\n</code></pre> <pre><code>#TODOBRW\nssh -v -N -f -L localhost:8580:localhost:8580 wilsonb@sambanova.alcf.anl.gov\nssh -N -f -L localhost:8580:localhost:8580 wilsonb@sambanova.alcf.anl.gov\n...\nPassword: &lt; MobilePass+ code &gt;\n\nssh wilsonb@sambanova.alcf.anl.gov\n</code></pre> <p>replacing ALCFUserID with your ALCF User ID.</p>"},{"location":"ai-testbed/sambanova/unused/sambatune-user-guide/#from-sambanovaalcfanlgov","title":"From sambanova.alcf.anl.gov","text":"<p>This command sets up a port forward from a SambaNova node to the sambanova login machine.</p> <p>Below are the commands specific to sm-01. You may replace sm-01 with sm-02 when using that system.</p> <p>Run</p> <p>NOTE:  The full name is sm-01.ai.alcf.anl.gov and it may also be used.</p> <pre><code>ssh -N -f -L localhost:18576:localhost:8576 ALCFUserID@sm-01\n</code></pre> <pre><code>#TODOBRW\nssh -N -f -L localhost:8580:localhost:8580 wilsonb@sm-01\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/sambatune-user-guide/#browser-on-local-machine","title":"Browser on Local Machine","text":"<p>Then, navigate in your browser to, in this example, http://localhost:18576 on your local machine.</p> <p>Use the username and password from sm-01 to log in.</p>"},{"location":"ai-testbed/sambanova/unused/sambatune-user-guide/#ssh-notes","title":"SSH Notes","text":"<p>Explanation of ssh command:</p> <pre><code>-N : no remote commands\n\n-f : put ssh in the background\n\n-L &lt;machine1&gt;:&lt;portA&gt;:&lt;machine2&gt;:&lt;portB&gt; :\n\nThe full command line will forward &lt;machine1&gt;:&lt;portA&gt; (local scope) to &lt;machine2&gt;:&lt;portB&gt; (remote scope)\n</code></pre> <p>Adapted from:  How can I run Tensorboard on a remote server?</p>"},{"location":"aurora/aurora-pe/","title":"Aurora Programming Environment","text":""},{"location":"aurora/aurora-pe/#overview","title":"Overview","text":"<p>The Aurora Programming Environment (Aurora PE) is Aurora's default software environment and consists of the OneAPI SDK, MPICH, and the Spack PE. The Aurora PE is loaded in the user environment through a default module set. Alternative versions of the Aurora PE, as well as Aurora PE components not loaded by default, are available through the module interface. Besides the latter three modules, this set of modules is loaded from the Aurora PE: <code>oneapi</code>, <code>intel_compute_runtime</code>, and <code>mpich</code>. Each version of the PE may have multiple compilers, runtimes, and/or MPICH installations; these are interchangeable within a particular PE version. In addition to the OneAPI and MPICH installations, the Aurora PE contains the Spack PE, which includes a myriad of general and scientific computing software. See the Spack PE page for more details.</p>"},{"location":"aurora/aurora-pe/#switching-pe-or-sdk-versions","title":"Switching PE or SDK versions","text":"<p>Aurora PE and OneAPI SDK versions can be viewed with <code>module avail</code> and switched with <code>module load</code> commands. The Aurora PE modules utilize Lmod's hierarchical module system to allow seamless switching between versions of the Aurora PE and its components. For example, if the user loads a different version of the <code>oneapi</code> SDK, the other modules in the Aurora PE, such as <code>intel_compute_runtime</code>, <code>mpich</code>, and <code>spack-pe-gcc</code>, will be reloaded to guarantee compatibility. In short, the hierarchical modulefile system ensures that the module environment is self-consistent with minimal user input.</p>"},{"location":"aurora/aurora-pe/#aurora-pe-and-soft","title":"Aurora PE and /soft","text":"<p>The Aurora PE is installed in <code>/opt/aurora</code> and is mounted as a read-only squashfs. <code>/soft</code> is also available to provide software not present in the Aurora PE. Modules in <code>/soft/modulefiles</code> are not part of the default environment in order to avoid filesystem metadata overhead, which can have significant performance impacts. Users wishing to use software in <code>/soft</code> will need to first run <code>module use /soft/modulefiles</code> to access the modules.</p> <p>The Aurora PE has a longer-term upgrade cadence (on the order of months), so ad-hoc software requests will be fulfilled through software installations in <code>/soft</code>. Pre-release previews of Aurora PE components may also be made available in <code>/soft</code> for testing. <code>/soft</code> installations will be considered for incorporation into the Aurora PE during upgrade cycles.</p> <p>Modules in <code>/soft</code> may conflict with modules in the Aurora PE. Lmod has some limitations in automatically handling module conflicts, so the user may need to manually resolve conflicts arising from modules outside of the Aurora PE. Users are also advised to sanitize module paths that they add from other non-standard locations, such as <code>/home</code>, to avoid conflicts. Additionally, packages targeted for future Aurora PE updates are staged for testing under a separate module path <code>/soft/preview-modules/&lt;PE_VERSION&gt;</code> to prevent accidental module clashes.</p>"},{"location":"aurora/getting-started-on-aurora/","title":"Getting Started on Aurora","text":""},{"location":"aurora/getting-started-on-aurora/#logging-into-aurora","title":"Logging Into Aurora:","text":"<p>To log into Aurora: <pre><code>ssh &lt;username&gt;@aurora.alcf.anl.gov\n</code></pre> Then, type in the password from your CRYPTOCard/MobilePASS+ token.</p>"},{"location":"aurora/getting-started-on-aurora/#hardware-overview","title":"Hardware Overview","text":"<p>An overview of the Aurora system, including details on the compute node architecture, is available on the Machine Overview page.</p>"},{"location":"aurora/getting-started-on-aurora/#compiling-applications","title":"Compiling Applications","text":"<p>Users are encouraged to read through the Compiling and Linking Overview page and corresponding pages depending on the target compiler and programming model.</p> <p>Autotools and CMake are available in the default Aurora Programming Environment (PE) and can be loaded via Lmod modules:</p> <pre><code>module load autoconf cmake\n</code></pre>"},{"location":"aurora/getting-started-on-aurora/#submitting-and-running-jobs","title":"Submitting and Running Jobs","text":"<p>Users are encouraged to read through the Running Jobs with PBS at the ALCF page for information on using the PBS scheduler and preparing job submission scripts. For Aurora-specific job documentation, refer to Running Jobs on Aurora.</p>"},{"location":"aurora/getting-started-on-aurora/#early-user-notes-and-known-issues","title":"Early User Notes and Known Issues","text":"<ul> <li>Hardware instabilities - possible frequent downtime</li> <li>Software instabilities - non-optimized compilers, libraries, and tools; frequent software updates</li> <li>Non-final configurations (storage, OS versions, etc.)</li> <li>Short notice for downtimes (scheduled downtimes will be with 4-hour notice, but sometimes downtimes may occur with just an email notice). Notices go to the aurora-notify@alcf.anl.gov email list. All users with access are added to the list initially.</li> </ul> <p>See Early User Notes and Known Issues for details.</p>"},{"location":"aurora/getting-started-on-aurora/#python-on-aurora","title":"Python on Aurora","text":"<p>Frameworks on Aurora can be loaded into a user's environment by loading the <code>frameworks</code> module as follows. The conda environment loaded with this module makes available TensorFlow, Horovod, and PyTorch with Intel extensions and optimizations.</p> <pre><code>module load frameworks\n</code></pre> <p>Note that there is a separate Python installation in <code>spack-pe-gcc</code> which is used as a dependency of a number of Spack PE packages. Users will need to exercise caution when loading both <code>frameworks</code> and <code>python</code> from the Spack PE. For more details about Python on Aurora, please review Python on Aurora.</p>"},{"location":"aurora/getting-started-on-aurora/#software-environment","title":"Software Environment","text":"<p>The Aurora Programming Environment (Aurora PE) provides the OneAPI SDK, MPICH, runtime libraries, and a suite of additional tools and libraries. The Aurora PE is available in the default environment and is accessible through modules. For example, tools and libraries like <code>cmake</code>, <code>boost</code>, and <code>hdf5</code> are available in the default environment. <pre><code>module load cmake\n</code></pre></p> <p>More details are on the Aurora PE page.</p> <p>Additional software is installed in <code>/soft</code> and can be accessed by adding <code>/soft/modulefiles</code> to the module search path. <pre><code>module use /soft/modulefiles\n</code></pre></p> <p>This will make available a handful of additional software modules, such as <code>kokkos</code>.</p>"},{"location":"aurora/getting-started-on-aurora/#proxy","title":"Proxy","text":"<p>If the node you are on doesn\u2019t have outbound network connectivity, add the following to your <code>~/.bash_profile</code> file to access the proxy host:</p> <pre><code># proxy settings\nexport HTTP_PROXY=\"http://proxy.alcf.anl.gov:3128\"\nexport HTTPS_PROXY=\"http://proxy.alcf.anl.gov:3128\"\nexport http_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport https_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport ftp_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport no_proxy=\"admin,polaris-adminvm-01,localhost,*.cm.polaris.alcf.anl.gov,polaris-*,*.polaris.alcf.anl.gov,*.alcf.anl.gov\"\n</code></pre>"},{"location":"aurora/getting-started-on-aurora/#file-systems-and-daos","title":"File Systems and DAOS","text":""},{"location":"aurora/getting-started-on-aurora/#home-and-project-directories","title":"Home and Project Directories","text":"<p>Home directories on Aurora are <code>/home/username</code>, available on login and compute nodes. This is provided from <code>/lus/gecko/home</code>. The default quota is 50 GB. </p> <p>Lustre project directories are under <code>/lus/flare/projects</code>. Default quota is 1 TB. The project PI should email support@alcf.anl.gov if their project requires additional storage.</p>"},{"location":"aurora/getting-started-on-aurora/#daos","title":"DAOS","text":"<p>The primary storage system on Aurora is not a file system, but rather an object store called the Distributed Asynchronous Object Store. This is a key-array based system embedded directly in the Slingshot fabric, which provides much faster I/O than conventional block-based parallel file systems such as Lustre (even those using non-spinning disk and/or burst buffers). Project PIs will have requested a storage pool on DAOS via INCITE/ALCC/DD allocation proposals.</p> <p>Aurora project PIs should email support@alcf.anl.gov to request DAOS storage with the following information:</p> <ul> <li>Project name </li> <li>Storage capacity (if this is different than in the current project proposal, please give brief justification)</li> <li>Who should be the designated owner of the pool (please provide their ALCF username). The owner can be the project PI or any other team member, but they need to have an active ALCF account.</li> <li>Have you used DAOS in the past? Is yes, please provide details.</li> </ul> <p>See DAOS Overview for more on using DAOS for I/O.</p>"},{"location":"aurora/getting-started-on-aurora/#lustre-file-striping","title":"Lustre File Striping","text":"<p>In addition to the content above, here is a document on Lustre File Striping Basics:</p> <ul> <li>Lustre File Striping Basics</li> </ul>"},{"location":"aurora/getting-started-on-aurora/#getting-assistance","title":"Getting Assistance","text":"<p>Please direct all questions, requests, and feedback to support@alcf.anl.gov.</p>"},{"location":"aurora/known-issues/","title":"Early User Notes and Known Issues","text":""},{"location":"aurora/known-issues/#last-updated-2025-01-21-1400-cst","title":"Last Updated: 2025-01-21 14:00 CST","text":""},{"location":"aurora/known-issues/#early-user-notes","title":"Early User Notes","text":"<p>Please check back here often for updates (indicated by the \"Last Updated\" timestamp above). As early production users encounter new issues and find solutions or workarounds, we will update these notes. As Aurora matures and becomes more stable over the first year of production, this section should become obsolete.</p>"},{"location":"aurora/known-issues/#outages-and-downtime-expectations","title":"Outages and Downtime \u2013 Expectations","text":"<p>Expect unplanned outages and downtime. The stability of the Aurora system has improved significantly over the months leading up to production availability, but stability issues remain. Early users need to be proactive about verifying correctness, watching for hangs, and otherwise adopting work methods that are mindful of and resilient to instability.</p>"},{"location":"aurora/known-issues/#scheduling","title":"Scheduling","text":"<p>The current queue policy for Aurora is set up based on experiences to date to help maximize productive use of the machine by projects.</p> <ul> <li> <p>The initial goal for teams is to start testing at small scales, ensure correct results (and performance), and ramp up to generating scientific results in production campaigns.</p> </li> <li> <p>Focus initially on making good use of the system with &lt;=2048 nodes per job; the key is to validate code+runs and start generating science results. Initially, the prod queue will only allow 2048 nodes max. Except for ESP projects, project teams will be required to email support to request running on more than 2048 nodes (with evidence that they are likely to succeed at larger scales).</p> </li> </ul>"},{"location":"aurora/known-issues/#storage","title":"Storage","text":""},{"location":"aurora/known-issues/#flare-lustre-file-system","title":"Flare (Lustre File System)","text":"<p>This is the primary and most stable storage filesystem for now. It is still possible that heavy use may trigger significant lags and performance degradations, and possibly lead to compute nodes crashing. We will continue to monitor filesystem stability as production use ramps up. We encourage teams to start out easy on I/O (both amount and job size), if possible, and report issues.</p>"},{"location":"aurora/known-issues/#daos-object-store","title":"DAOS (Object Store)","text":"<p>DAOS is a scratch file system. Please note that data may be removed or unavailable at any time.</p> <p>The initial configuration of DAOS has a smaller number of nodes, resulting in smaller project allocations. We expect DAOS to grow over the year, and when that happens, changes will be announced/posted in user docs. Please email support@alcf.anl.gov if you are hitting limits and need the allocation size to be increased.</p> <p>The performance of DAOS has been impressive, but we continue to experience crashes with large jobs, including loss of data. Projects may use it, but should not consider it stable or safe for long-term storage.</p>"},{"location":"aurora/known-issues/#grandeagle","title":"Grand/Eagle","text":"<p>These won\u2019t be mounted on Aurora initially, but they might be mounted around May 2025, depending on feasibility. Similarly, Flare will not initially be mounted on Polaris. DTNs and Globus are the best means to transfer data between Polaris and Aurora.</p>"},{"location":"aurora/known-issues/#checkpointing","title":"Checkpointing","text":"<p>Checkpointing is absolutely essential. The mean time between application interrupts caused by system instability may be as short as an hour for larger jobs. The frequency of checkpointing is something that needs to be decided for each individual application based on the scale of runs.</p> <ul> <li> <p>If checkpointing has minimal overhead, consider checkpointing once every 15 minutes.</p> </li> <li> <p>If checkpointing has substantial overhead, then consider checkpointing every 30-60 minutes.</p> </li> <li> <p>It may be the case that the highest throughput initially will be with creating job dependency chains where scripts are able to 1) automatically restart from the latest available checkpoint file and 2) confirm that the prior run generated reasonable/correct results.</p> </li> </ul>"},{"location":"aurora/known-issues/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":"<p>As always, INCITE and ALCC projects should report all issues to the Catalyst point of contact.</p>"},{"location":"aurora/known-issues/#ping-failures","title":"Ping Failures","text":"<p>Network instabilities may lead to some nodes or MPI ranks losing communication with others. If your job output shows an error message like these:</p> <pre><code>ping failed on x4707c6s4b0n0: Couldn't forward RPC ping(24c93b8c-3434-4fb5-a8f0-53cff4cbbe42) to child x4707c7s6b0n0.hostmgmt2707.cm.aurora.alcf.anl.gov: Resource temporarily unavailable\n</code></pre> <pre><code>ping RPC timeout from x4212c7s0b0n0.hostmgmt2212.cm.aurora.alcf.anl.gov after 120s\n</code></pre> <p>then most likely your application will crash or hang. If you see these, the best action is to kill the job and re-run it (from the last checkpoint, if there has been one).</p>"},{"location":"aurora/known-issues/#hangs","title":"Hangs","text":"<p>There are multiple failure modes that can lead to jobs hanging. For known hardware or low-level software issues such as ping failures as discussed above, just restart the job.</p> <p>To avoid a hung job running out all the requested wallclock time on all its nodes, we suggest devising ways to monitor job progress. For example, if your application regularly writes small output to a logfile, then you could launch a \u201cwatcher\u201d script that looks for that expected output and collects a stack trace and kills the job if it's been too long since progress was made. Please engage your Catalyst POC if you are interested in evaluating this for your application.</p>"},{"location":"aurora/known-issues/#gpu-segfaults-aka-page-faults","title":"GPU Segfaults (a.k.a. \"Page Faults\")","text":"<p>Memory errors on the GPUs are caught when illegal accesses exceed a page boundary. When you see an error message indicating <code>Unexpected page fault from GPU at &lt;address&gt;</code></p> <p>The best tools for debugging these are <code>gdb-oneapi</code> and <code>DDT</code>, both of which allow debugging into GPU kernel threads and looking at GPU data structures. You may also dump and step through the PVC assembly code using the debuggers if helpful. It is possible that there remain bugs in the IGC compiler that produces invalid assembly code, though as always the most likely cause of segfaults is memory errors in application code. To use the debuggers effectively in GPU kernels, you should compile and link your application with <code>-g -O0</code>. Keep in mind that the IGC compilation of GPU kernels takes place during the link phase if you're using AoT compilation.</p>"},{"location":"aurora/known-issues/#known-issues","title":"Known Issues","text":"<p>This is a collection of known issues that have been encountered during Aurora's early user phase. Documentation will be updated as issues are resolved. Users are encouraged to email support@alcf.anl.gov to report issues.</p> <p>A known issues page can be found in the CELS Wiki space used for NDA content. Note that this page requires a JLSE Aurora early hw/sw resource account for access.</p>"},{"location":"aurora/known-issues/#runtime-errors","title":"Runtime Errors","text":""},{"location":"aurora/known-issues/#1-cassini-event-queue-overflow-detected","title":"1. <code>Cassini Event Queue overflow detected</code>","text":"<p><code>Cassini Event Queue overflow detected</code> errors may occur for certain MPI communications and may happen for a variety of reasons - software and hardware, job placement, job routing, and the state of the machine. Simply speaking, it means one of the network interfaces is getting messages too fast and cannot keep up with processing them.</p> <pre><code>libfabric:16642:1701636928::cxi:core:cxip_cq_eq_progress():531&lt;warn&gt; x4204c1s3b0n0: Cassini Event Queue overflow detected.\n</code></pre> <p>As a workaround, the following environment variables can be set to try alleviating the problem.</p> <pre><code>export FI_CXI_DEFAULT_CQ_SIZE=131072\nexport FI_CXI_OFLOW_BUF_SIZE=8388608\nexport FI_CXI_CQ_FILL_PERCENT=20\n</code></pre> <p>The value of <code>FI_CXI_DEFAULT_CQ_SIZE</code> can be set to something larger if issues persist. This is directly impacted by the number of unexpected messages sent and so may need to be increased as the scale of the job increases.</p> <p>It may be useful to use other libfabric environment settings. In particular, the setting below may be useful to try. These are what Cray MPI sets by default Cray MPI libfabric Settings.</p>"},{"location":"aurora/known-issues/#2-failed-to-convert-gotpcrel-relocation","title":"2. <code>failed to convert GOTPCREL relocation</code>","text":"<p>If you see</p> <pre><code>_libm_template.c:(.text+0x7): failed to convert GOTPCREL relocation against '__libm_acos_chosen_core_func_x'; relink with --no-relax\n</code></pre> <p>try linking with <code>-flink-huge-device-code</code></p>"},{"location":"aurora/known-issues/#3-sycl-device-free-memory-query-error","title":"3. SYCL Device Free Memory Query Error","text":"<p>Note that if you are querying the free memory on a device with the Intel SYCL extension \"get_info();\", you will need to set <code>export ZES_ENABLE_SYSMAN=1</code>. Otherwise, you may see an error like: <pre><code>x1921c1s4b0n0.hostmgmt2000.cm.americas.sgi.com 0: The device does not have the ext_intel_free_memory aspect -33 (PI_ERROR_INVALID_DEVICE)\nx1921c1s4b0n0.hostmgmt2000.cm.americas.sgi.com 0: terminate called after throwing an instance of 'sycl::_V1::invalid_object_error'\n  what():  The device does not have the ext_intel_free_memory aspect -33 (PI_ERROR_INVALID_DEVICE)\n</code></pre>"},{"location":"aurora/known-issues/#4-no-vnis-available-in-internal-allocator","title":"4. <code>No VNIs available in internal allocator.</code>","text":"<p>If you see an error like <code>start failed on x4102c5s2b0n0: No VNIs available in internal allocator</code>, pass <code>--no-vni</code> to <code>mpiexec</code></p>"},{"location":"aurora/known-issues/#5-pmix-error-pmix_err_not_found-and-pmix-error-pmix_error","title":"5. <code>PMIX ERROR: PMIX_ERR_NOT_FOUND</code> and <code>PMIX ERROR: PMIX_ERROR</code>","text":"<p>When running on a single node, you may observe this error message:</p> <pre><code>PMIX ERROR: PMIX_ERR_NOT_FOUND in file dstore_base.c at line 1567 \nPMIX ERROR: PMIX_ERROR in file dstore_base.c at line 2334\n</code></pre> <p>These errors can be safely ignored.</p>"},{"location":"aurora/known-issues/#submitting-jobs","title":"Submitting Jobs","text":"<p>Jobs may fail to successfully start at times (particularly at higher node counts). If no error message is apparent, then one thing to check is the <code>comment</code> field in the full job information for the job using the command <code>qstat -xfw &lt;JOBID&gt; | grep comment</code>. Some example comments follow.</p> <pre><code>comment = Job held by &lt;USER&gt; on Tue Feb 6 05:20:00 2024 and terminated\n</code></pre> <p>The user has placed the job on hold; the user can <code>qrls</code> the job when ready for it to be queued again.</p> <pre><code>comment = Not Running: Queue not started. and terminated\n</code></pre> <p>The user has submitted to a queue that is not currently running; the user should <code>qmove</code> the job to an appropriate queue.</p> <pre><code>comment = job held, too many failed attempts to run\n</code></pre> <p>The job tried and failed to start. In this scenario, the user should find that their job was placed on hold. This does not indicate a problem with the user's job script, but indicates PBS made several attempts to find a set of nodes to run the job and was not able to. Users can <code>qdel</code> the job and resubmit or <code>qrls</code> the job to try running it again.</p> <pre><code>comment = Not Running: Node is in an ineligible state: down and terminated\n</code></pre> <p>There are an insufficient number of nodes online and free for the job to start.</p> <p>In the event of a node going down during a job, users may encounter messages such as <code>ping failed on x4616c0s4b0n0: Application 047a3c9f-fb41-4595-a2ad-4a4d0ec1b6c1 not found</code>. The node will likely have started a reboot and won't be included in jobs again until checks pass.</p> <p>Use of the <code>qsub -V</code> flag (note: upper-case) is discouraged, as it can lead to startup failures. The following message (found via <code>pbsnodes -l</code>):</p> <pre><code>failed to acquire job resources; job startup aborted (jobid: &lt;YOUR JOBID&gt;)\n</code></pre> <p>indicates such a failure. It is recommended to instead use <code>-v</code> (note: lower-case) and explicitly export any environment variables that your job may require.</p> <p>To increase the chances that a large job does not terminate due to a node failure, you may choose to interactively route your MPI job around nodes that fail during your run. See this page on Working Around Node Failures for more information.</p>"},{"location":"aurora/known-issues/#other-issues","title":"Other Issues","text":"<ul> <li>A large number of Machine Check Events from the PVC, which causes nodes to panic and reboot.</li> <li>HBM mode is not automatically validated. Jobs requiring flat memory mode should test by looking at <code>numactl -H</code> for 4 NUMA memory nodes instead of 16 on the nodes.</li> <li>Application failures at single-node are tracked in the JLSE wiki/confluence page</li> </ul>"},{"location":"aurora/running-jobs-aurora/","title":"Running Jobs on Aurora","text":""},{"location":"aurora/running-jobs-aurora/#queues","title":"Queues","text":"<p>There are four production queues you can target in your qsub (<code>-q &lt;queue name&gt;</code>):</p> Queue Name Node Min Node Max Time Min Time Max Notes debug 1 2 5 min 1 hr 32 exclusive nodes with growth up to 64 nodes;   Max 1 job running/accruing/queued per-user debug-scaling 2 31 5 min 1 hr Max 1 job running/accruing/queued per-user prod 32 2048 5 min 18 hrs Routing queue for tiny, small, and medium queues;  See table below prod-large 2048 10624 5 min 24 hrs By request only  Routing queue for large jobs; See table below visualization 1 32 5 min 8 hrs By request only <p>Note</p> <p>The debug queue has 32 exclusively dedicated nodes. If there are free nodes in production, then debug jobs can take another 32 nodes for a total of 64.</p> <p><code>prod</code> and <code>prod-large</code> are routing queues and routes your job to one of the following eight execution queues:</p> Queue Name Node Min Node Max Time Min Time Max Notes tiny 32 512 5 min 6 hrs small 513 1024 5 min 12 hrs medium 1025 2048 5 min 18 hrs large 2048 10624 5 min 24 hrs Only accessible with access to prod-large queue backfill-tiny 32 512 5 min 6 hrs Low priority, negative project balance backfill-small 513 1024 5 min 12 hrs Low priority, negative project balance backfill-medium 1025 2048 5 min 18 hrs Low priority, negative project balance backfill-large 2049 10624 5 min 24 hrs Only accessible with access to prod-large queue  Low priority, negative project balance <p>Warning</p> <p>You cannot submit to these queues directly; you can only submit to the routing queue <code>prod</code> or <code>prod-large</code>.</p> <p>Note</p> <p>All of these queues have a limit of ten (10) jobs running/accruing per-project.   All of these queues have a limit of one hundred (100) jobs queued (not accruing score) per-project.</p>"},{"location":"aurora/running-jobs-aurora/#submitting-a-job","title":"Submitting a job","text":"<p>Note: Jobs should be submitted only from your allocated project directory and not from your home directory or from <code>/soft/modulefiles</code>. Submitting an interactive job from <code>/soft/modulefiles</code> will result in your job ending abruptly.</p> <p>For example, a one-node interactive job requiring access to the <code>/flare</code> filesystem can be requested for 30 minutes with the following command, where <code>&lt;your_ProjectName&gt;</code> is replaced with an appropriate project name.</p> <pre><code>qsub -l select=1 -l walltime=30:00 -l filesystems=flare -A &lt;your_ProjectName&gt; -q debug -I\n</code></pre> <p>For DAOS access, users will need to include either <code>daos_user</code> or <code>daos_perf</code> (only for select teams approved by ALCF) as a filesystem option. More information can be found on the DAOS page.</p> <p>Tip</p> <p>To view the available filesystem options, execute the <code>qstat -Bf</code> command and view the <code>resources_available.valid_filesystems</code> entry.</p> <p>Recommended PBSPro options follow.</p> <pre><code>#!/bin/bash -l\n#PBS -A &lt;your_ProjectName&gt;\n#PBS -N &lt;your_JobName&gt;\n#PBS -l walltime=&lt;requested_walltime_value&gt;\n#PBS -l filesystems=&lt;requested_fs1:requested_fs2&gt;\n#PBS -k doe\n#PBS -l place=scatter\n#PBS -q &lt;requested_Queue&gt;\n</code></pre> <p>More information on the PBS options above, as well as other PBS options, can be found here.</p>"},{"location":"aurora/running-jobs-aurora/#working-around-node-failures","title":"Working Around Node Failures","text":"<p>As Aurora is still a pre-production supercomputer, node failures are a fact of life. If you would like to increase the chances that a large job does not terminate due to a node failure, you may choose to interactively route your MPI job around nodes that fail during your run. To do this, you must run interactively and use must manually adjust your run on the fly to remove nodes that have been marked as failed.</p> <p>We recommend against using <code>-W tolerate_node_failures=all</code> in your qsub command, but we acknowledge its use can be helpful. However, you MUST MANUALLY VERIFY your job and remove faulted nodes from your mpiexec command YOURSELF!</p> <ol> <li>Start your interactive job</li> <li>When the job transitions to Running state, run <code>pbsnodes -l | grep &lt;jobid&gt;</code></li> <li> <p>Manually REMOVE all nodes identified in that output from inclusion in your mpiexec</p> <pre><code>cat $PBS_NODEFILE &gt; local.hostfile\n# edit local.hostfile to remove problem nodes\nmpiexec --hostfile local.hostfile &lt;other mpiexec arguments&gt;\n</code></pre> </li> <li> <p>Continue to execute</p> </li> <li>If other nodes go down during your job, it will not be killed, and you can further exclude those nodes from your mpiexec as needed</li> </ol> <p>It is important to note that all nodes marked as faulty by PBS will not be used in subsequent jobs. This mechanism only provides you with a means to execute additional mpiexec commands under the same interactive job after manually removing nodes identified as faulty. Once your PBS job has exited, those faulty nodes will remain offline until further intervention by Aurora staff.</p>"},{"location":"aurora/running-jobs-aurora/#aurora-mpich","title":"Aurora MPICH","text":"<p>The standard version of the MPI (Message Passing Interface) library on Aurora is Aurora MPICH. This resulted from a collaboration between Intel and the Argonne MPICH developer team. The <code>mpiexec</code> and <code>mpirun</code> commands used to launch multi-rank jobs come from the Cray PALS (Parallel Application Launch Service) system.</p> <p>There are many, many configuration and tuning parameters for Aurora MPICH. Simple ASCII text documentation of the environment variables usable to control behavior is in</p> <pre><code>$MPI_ROOT/share/doc/mpich/README.envvar\n</code></pre> <p>This includes, for example, settings to select different optional sub-algorithms used in MPI collective operations.</p>"},{"location":"aurora/running-jobs-aurora/#running-mpiopenmp-applications","title":"Running MPI+OpenMP Applications","text":"<p>Once a submitted job is running calculations can be launched on the compute nodes using <code>mpiexec</code> to start an MPI application. Documentation is accessible via <code>man mpiexec</code> and some helpful options follow.</p> <ul> <li><code>-n</code> total number of MPI ranks</li> <li><code>-ppn</code> number of MPI ranks per node</li> <li><code>--cpu-bind</code> CPU binding for application</li> <li><code>--depth</code> number of cpus per rank (useful with <code>--cpu-bind</code>)</li> <li><code>--env</code> set environment variables (<code>--env OMP_NUM_THREADS=2</code>)</li> <li><code>--hostfile</code> indicate file with hostnames (the default is <code>--hostfile $PBS_NODEFILE</code>)</li> </ul> <p>A sample submission script with directives is below for a 4-node job with 28 MPI ranks on each node and 4 OpenMP threads per rank (1 per CPU core).</p> <pre><code>#!/bin/bash -l\n#PBS -N AFFINITY\n#PBS -l select=4\n#PBS -l place=scatter\n#PBS -l walltime=0:10:00\n#PBS -l filesystems=&lt;fs1:fs2&gt;\n#PBS -q debug-scaling\n#PBS -A &lt;MYPROJECT&gt;\n\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS=28 # Number of MPI ranks to spawn per node\nNDEPTH=4 # Number of hardware threads per rank (i.e. spacing between MPI ranks)\nNTHREADS=4 # Number of software threads per rank to launch (i.e. OMP_NUM_THREADS)\n\nNTOTRANKS=$(( NNODES * NRANKS ))\n\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS} THREADS_PER_RANK= ${NTHREADS}\"\n\ncd /home/knight/affinity\nmpiexec -n ${NTOTRANKS} -ppn ${NRANKS} --depth=${NDEPTH} --cpu-bind depth -env OMP_NUM_THREADS=${NTHREADS} --env OMP_PLACES=cores ./hello_affinity\n</code></pre>"},{"location":"aurora/running-jobs-aurora/#running-gpu-enabled-applications","title":"Running GPU-enabled Applications","text":"<p>GPU-enabled applications will similarly run on the compute nodes using the above example script. - The environment variable <code>MPICH_GPU_SUPPORT_ENABLED=1</code> needs to be set if your application requires MPI-GPU support whereby the MPI library sends and receives data directly from GPU buffers. - If running on a specific GPU or subset of GPUs and/or tiles is desired, then the <code>ZE_AFFINITY_MASK</code> environment variable can be used. For example, if one only wanted an application to access the first two GPUs on a node, then setting <code>ZE_AFFINITY_MASK=0,1</code> could be used.</p>"},{"location":"aurora/running-jobs-aurora/#mpi-rank-and-thread-binding-to-cores-and-gpus","title":"MPI rank and thread binding to cores and GPUs","text":"<p>Each node on Aurora has 2 sockets, each with 1 CPU and 3 PVC GPUs. Each CPU has 52 physical cores, with 2 logical processors (provided by Intel hyper threading) per physical core, for a total of 104 physical cores and 208 logical processors on the CPUs per Aurora node. Each GPU has two tiles on it, for a total of 6 GPUs and 12 GPU tiles on the GPUs per Aurora node. When a parallel job is run, the job must have some way of mapping MPI ranks or threads to each of the 208 logical processors and 6 GPUs or 12 GPU tiles. Mapping is typically done by an affinity mask, which assigns hardware resources to each MPI rank or thread to use.</p> <p>A visual representation of node in Aurora is shown below. Each socket is represented by a large blue bubble. Inside, each CPU is represented by a red bubble. Inside of CPU, the white boxes represent the physical cores, and the two grey squares in each tile represent the two logical processors. Each GPU is represented by a large white box, with two grey boxes inside to represent the two tiles.</p> <p> </p> Simplified representation of Aurora node  <p>For the two CPUs, the numbers inside the boxes identify the specific logical processors in the core. That is, logical processor 0 and 104 are the 2 logical processors on the first physical core. Logical processors 1 and 105 are the 2 logical processors that share the second physical core. Since there are 208 logical processors, the numbers run from 0 to 207. For i from 0 to 51, logical processors i and i+104 share a physical core. </p> <p>For the six GPUs, the GPU number identifies the GPU, and the tile numbers identify the tile in the GPU, with tiles from 0 to 5 with each GPU have two tiles each $gpu.0 and $gpu.1.</p>"},{"location":"aurora/running-jobs-aurora/#binding-mpi-ranks-and-threads-to-cores","title":"Binding MPI ranks and threads to cores","text":"<p>Using the <code>\u2013cpu-bind</code> argument to mpiexec, MPI ranks and threads can be assigned to run on specific logical processors on the CPUs. For more information about the flags to <code>mpiexec</code>, see Running MPI+OpenMP Applications. Four examples of using <code>mpiexec</code> are given below to show how the <code>cpu-bind=depth</code>, <code>cpu-bind=list</code>, <code>--depth</code> arguments affect where MPI ranks and OpenMP threads are mapped.</p>"},{"location":"aurora/running-jobs-aurora/#example-1-2-nodes-4-ranksnode-1-threadrank","title":"Example 1: 2 nodes, 4 ranks/node, 1 thread/rank","text":"<pre><code>mpiexec -n 8 -ppn 4 --depth 1 --cpu-bind=depth &lt;app&gt; &lt;app_args&gt;\n</code></pre> <ul> <li>The <code>-n 8</code> argument says to use 8 MPI ranks in total and <code>-ppn 4</code> places 4 ranks per node.</li> <li>The <code>--depth 1</code> argument says to use 1 logical processor for each MPI rank.</li> <li>The <code>--cpu-bind depth</code> argument says to spread out the ranks in a round robin manner across the logical processors, first putting one rank on the first logical processor of one physical core, and then looping back to put a second one on the second logical processor. This is done such that there's N logical processors for each MPI rank, where N is the value from the --depth argument (so it's 1 in this case).</li> </ul> <p>This is the same as</p> <pre><code>mpiexec -n 8 -ppn 4 --cpu-bind=list:0:1:2:3 &lt;app&gt; &lt;app_args&gt;\n</code></pre> <ul> <li>The <code>--cpu-bind list</code> argument explicitly lists which logical processor to bind to per node. Each MPI rank is bound to the logical processors that are listed between <code>:</code>. So here, rank 0 to logical processor 0, rank 1 to logical processor 1, etc.</li> </ul>"},{"location":"aurora/running-jobs-aurora/#resulting-mapping","title":"Resulting mapping","text":"<p>MPI ranks 0,1,2,3,4,5,6,7 map to logical processors 0,1,2,3 on each of the two nodes. Assuming the job was allocated on node 0 and node 1:</p> <ul> <li>MPI rank 0 \u2192 node 0, logical processor 0</li> <li>MPI rank 1 \u2192 node 0, logical processor 1</li> <li>MPI rank 2 \u2192 node 0, logical processor 2</li> <li>MPI rank 3 \u2192 node 0, logical processor 3</li> <li>MPI rank 4 \u2192 node 1, logical processor 0</li> <li>MPI rank 5 \u2192 node 1, logical processor 1</li> <li>MPI rank 6 \u2192 node 1, logical processor 2</li> <li>MPI rank 7 \u2192 node 1, logical processor 3</li> </ul> <p>The figure below shows the mapping, where the different colors are different MPI ranks.</p> <p> </p> Example 1 Mapping"},{"location":"aurora/running-jobs-aurora/#example-2-2-nodes-2-ranksnode-2-threadrank","title":"Example 2: 2 nodes, 2 ranks/node, 2 thread/rank","text":"<pre><code>OMP_PLACES=threads OMP_NUM_THREADS=2 mpiexec -n 4 -ppn 2 --depth 2 --cpu-bind=depth &lt;app&gt; &lt;app_args&gt;\n</code></pre> <ul> <li>The <code>-n 4</code> argument says to use 4 MPI ranks in total and <code>-ppn 2</code> places 2 ranks per node.</li> <li>The <code>--depth 2</code> argument says to use 2 logical processor for each MPI rank.</li> <li>The <code>--cpu-bind depth</code> argument says to spread out the ranks in a round robin manner across the logical processors, first putting one rank on the first logical processor of one physical core, and then looping back to put a second one on the second logical processor. This is done such that there's N logical processors for each MPI rank, where N is the value from the --depth argument (so it's 2 in this case).</li> <li>OMP_NUM_THREADS=2 launches two threads per MPI rank</li> <li>OMP_PLACES=threads says to bind the OpenMP threads to logical processors</li> </ul> <p>This is the same as:</p> <pre><code>OMP_PLACES=threads OMP_NUM_THREADS=2 mpiexec -n 4 -ppn 2 --cpu-bind=list:0,1:2,3 &lt;app&gt; &lt;app_args&gt;\n</code></pre> <ul> <li>The <code>--cpu-bind list</code> argument explicitly lists which logical processor to bind to. Each MPI rank is bound to the logical processors that are listed between <code>:</code>. Between <code>:</code>, the logical processors to bind to are listed in a comma-separated manner. So here, rank 0 is bound to logical processors 0 and 1, rank 2 to logical processors 2 and 3. OMP_PLACES=threads then binds the specific threads to the logical processors in the list.</li> </ul>"},{"location":"aurora/running-jobs-aurora/#resulting-mapping_1","title":"Resulting mapping","text":"<p>Assuming the job was allocated on node 0 and node 1:</p> <ul> <li>MPI rank 0, OpenMP thread 0 \u2192 node 0, logical processor 0</li> <li>MPI rank 0, OpenMP thread 1 \u2192 node 0, logical processor 1</li> <li>MPI rank 1, OpenMP thread 0 \u2192 node 0, logical processor 2</li> <li>MPI rank 1, OpenMP thread 1 \u2192 node 0, logical processor 3</li> <li>MPI rank 2, OpenMP thread 0 \u2192 node 1, logical processor 0</li> <li>MPI rank 2, OpenMP thread 1 \u2192 node 1, logical processor 1</li> <li>MPI rank 3, OpenMP thread 0 \u2192 node 1, logical processor 2</li> <li>MPI rank 3, OpenMP thread 1 \u2192 node 1, logical processor 3</li> </ul> <p>The figure below shows the mapping, where the different colors are different MPI ranks.</p> <p> </p> Example 2 Mapping"},{"location":"aurora/running-jobs-aurora/#example-3-2-nodes-2-ranksnode-1-threadrank-compact-fashion","title":"Example 3: 2 nodes, 2 ranks/node, 1 thread/rank, compact fashion","text":"<pre><code>mpiexec -n 4 -ppn 2 --cpu-bind=list:0:104 &lt;app&gt; &lt;app_args&gt;\n</code></pre> <ul> <li>The <code>--cpu-bind list</code> argument explicitly lists which logical processor to bind to per node. Each MPI rank is bound to the logical processors that are listed between <code>:</code>. So here, rank 0 to logical processor 0, rank 1 to logical processor 104, which share the same physical core.</li> </ul>"},{"location":"aurora/running-jobs-aurora/#resulting-mapping_2","title":"Resulting mapping","text":"<p>Assuming the job was allocated on node 0 and node 1:</p> <ul> <li>MPI rank 0 \u2192 node 0, logical processor 0</li> <li>MPI rank 1 \u2192 node 0, logical processor 104</li> <li>MPI rank 2 \u2192 node 1, logical processor 0</li> <li>MPI rank 3 \u2192 node 1, logical processor 104</li> </ul> <p>The figure below shows the mapping, where the different colors are different MPI ranks.</p> <p> </p> Example 3 Mapping"},{"location":"aurora/running-jobs-aurora/#example-4-1-node-12-ranksnode","title":"Example 4: 1 node, 12 ranks/node","text":"<p>This setup is a common case for applications: 12 ranks/node, where each rank will offload to one of the 12 GPU tiles. Note that explicit list binding is needed here to avoid binding a MPI rank to a logical processor on different socket than the GPU it might be targetting (as would happen if cpu_bind=depth was used). </p> <pre><code>mpiexec -n 12 -ppn 12 --cpu-bind=list:0-7:8-15:16-23:24-31:32-39:40-47:52-59:60-67:68-75:76-83:84-91:92-99 &lt;app&gt; &lt;app_args&gt;\n</code></pre> <ul> <li>The <code>--cpu-bind list</code> argument explicitly lists which logical processor to bind to per node. Each MPI rank is bound to the logical processors that are listed between <code>:</code>. So here, rank 0 to logical processors 0-7, rank 1 to logical processors 8-15, etc.</li> </ul>"},{"location":"aurora/running-jobs-aurora/#resulting-mapping_3","title":"Resulting mapping","text":"<p>Assuming the job was allocated on node 0 and node 1, the mapping looks like:</p> <ul> <li>MPI rank 0 \u2192 node 0, socket 0, logical processors 0-7</li> <li>MPI rank 1 \u2192 node 0, socket 0, logical processor 8-15</li> <li>MPI rank 2 \u2192 node 0, socket 0, logical processor 16-23</li> <li>MPI rank 3 \u2192 node 0, socket 0, logical processor 24-31</li> <li>MPI rank 4 \u2192 node 0, socket 0, logical processor 32-39</li> <li>MPI rank 5 \u2192 node 0, socket 0, logical processor 40-47</li> <li>MPI rank 6 \u2192 node 0, socket 1, logical processor 52-59</li> <li>MPI rank 7 \u2192 node 0, socket 1, logical processor 60-67</li> <li>MPI rank 8 \u2192 node 0, socket 1, logical processor 68-75</li> <li>MPI rank 9 \u2192 node 0, socket 1, logical processor 76-83</li> <li>MPI rank 10 \u2192 node 0, socket 1, logical processor 84-91</li> <li>MPI rank 11 \u2192 node 0, socket 1, logical processor 92-99</li> </ul> <p>The important point here is that with explicit binding, we were able to ensure socket 0 had 6 ranks and socket 1 has 6 ranks. Note how MPI rank 5 ends at logical processor 47, but MPI rank 6 begins with logical processor 52, so this involves leaving several cores empty. However, it allows the cores to be spread evenly across the two sockets.   </p> <p>The figure below shows the mapping, where the different colors are different MPI ranks.</p> <p> </p> Example 4 Mapping  <p>If instead we used <code>--depth</code> as so: <pre><code>mpiexec -n 12 -ppn 12 --depth 8 --cpu-bind=depth &lt;app&gt; &lt;app_args&gt;\n</code></pre> then the mapping is:</p> <ul> <li>MPI rank 0 \u2192 node 0, socket 0, logical processors 0-7</li> <li>MPI rank 1 \u2192 node 0, socket 0, logical processor 8-15</li> <li>MPI rank 2 \u2192 node 0, socket 0, logical processor 16-23</li> <li>MPI rank 3 \u2192 node 0, socket 0, logical processor 24-31</li> <li>MPI rank 4 \u2192 node 0, socket 0, logical processor 32-39</li> <li>MPI rank 5 \u2192 node 0, socket 0, logical processor 40-47</li> <li>MPI rank 6 \u2192 node 0, socket 0 and socket 1, logical processor 48-55</li> <li>MPI rank 7 \u2192 node 0, socket 1, logical processor 56-63</li> <li>MPI rank 8 \u2192 node 0, socket 1, logical processor 64-71</li> <li>MPI rank 9 \u2192 node 0, socket 1, logical processor 72-79</li> <li>MPI rank 10 \u2192 node 0, socket 1, logical processor 80-87</li> <li>MPI rank 11 \u2192 node 0, socket 1, logical processor 88-95</li> </ul> <p>Note that the threads MPI rank 6 are bound to cross both socket 0 and socket 1, which potentially will lead to worse performance than using cpu-bind=list to explicitly spread out the ranks and avoid splitting one over two sockets. This is shown in the image below. Note that the pink MPI rank (rank 6) is split between socket 0 and socket 1.</p> <p> </p> Example 4 Mapping Which Splits a MPI Rank Across Sockets  <p>Info</p> <p>For a script to help provide cpu-bindings, you can use get_cpu_bind_aurora. Please see User Guide for Aurora CPU Binding Script for documentation. </p>"},{"location":"aurora/running-jobs-aurora/#binding-mpi-ranks-to-gpus","title":"Binding MPI ranks to GPUs","text":"<p>Support in MPICH on Aurora to bind MPI ranks to GPUs is currently work-in-progress. For applications that need this support, this instead can be handled by use of a small helper script that will appropriately set <code>ZE_AFFINITY_MASK</code> for each MPI rank. Users are encouraged to use the <code>/soft/tools/mpi_wrapper_utils/gpu_tile_compact.sh</code> script for instances where each MPI rank is to be bound to a single GPU tile with a round-robin assignment.</p> <p>This script can be placed just before the executable in an <code>mpiexec</code> command like so.</p> <pre><code>mpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth /soft/tools/mpi_wrapper_utils/gpu_tile_compact.sh ./hello_affinity\n</code></pre> <p>A simple version of this script is below to illustrate how <code>ZE_AFFINITY_MASK</code> is uniquely set for each MPI rank.</p> <pre><code>#!/bin/bash -l\nnum_gpu=6\nnum_tile=2\ngpu_id=$(( (PALS_LOCAL_RANKID / num_tile ) % num_gpu ))\ntile_id=$((PALS_LOCAL_RANKID % num_tile))\nunset EnableWalkerPartition\nexport ZE_ENABLE_PCI_ID_DEVICE_ORDER=1\nexport ZE_AFFINITY_MASK=$gpu_id.$tile_id\n#echo \u201cRANK= ${PMI_RANK} LOCAL_RANK= ${PMI_LOCAL_RANK} gpu= ${gpu}\u201d\nexec \"$@\"\n</code></pre> <p>Users with different MPI-GPU affinity needs, such as assigning multiple GPUs/tiles per MPI rank, are encouraged to modify a local copy of <code>/soft/tools/mpi_wrapper_utils/gpu_tile_compact.sh</code> to suit their needs.</p> <p>One example below shows a common mapping of MPI ranks to cores and GPUs.</p>"},{"location":"aurora/running-jobs-aurora/#example-1-1-node-12-ranksnode-1-threadrank-1-rankgpu","title":"Example 1: 1 node, 12 ranks/node, 1 thread/rank, 1 rank/GPU","text":"<pre><code>mpiexec -n 12 -ppn 12 --cpu-bind=list:0-7:8-15:16-23:24-31:32-39:40-47:52-59:60-67:68-75:76-83:84-91:92-99 /soft/tools/mpi_wrapper_utils/gpu_tile_compact.sh &lt;app&gt; &lt;app_args&gt;\n</code></pre> <ul> <li>The <code>-n 12</code> argument says to use 12 MPI ranks in total and <code>-ppn 12</code> places 12 ranks per node.</li> <li>The <code>--cpu-bind list</code> argument gives the mapping of MPI ranks to cores, as described in Binding MPI ranks and threads to cores.</li> <li>The /soft/tools/mpi_wrapper_utils/gpu_tile_compact.sh wrapper sets ZE_AFFINITY_MASK for each of the 12 ranks such that rank 0 maps to GPU 0, Tile 0, rank 1 maps to GPU 0, Tile 1, rank 2 naps to GPU 1, Tile 0 etc. in a round-robin compact fashion.  </li> </ul>"},{"location":"aurora/running-jobs-aurora/#resulting-mapping_4","title":"Resulting mapping","text":"<p>This is one of the most common cases, with 1 MPI rank targeting each GPU tile. A figure representing this is below. The different MPI ranks are represented by different colors. Assuming the job was allocated on node 0 and node 1, the mapping looks like:</p> <ul> <li>MPI rank 0 \u2192 node 0, socket 0, logical processors 0-7, GPU 0, Tile 0</li> <li>MPI rank 1 \u2192 node 0, socket 0, logical processor 8-15, GPU 0, Tile 1</li> <li>MPI rank 2 \u2192 node 0, socket 0, logical processor 16-23, GPU 1, Tile 0</li> <li>MPI rank 3 \u2192 node 0, socket 0, logical processor 24-31, GPU 1, Tile 1</li> <li>MPI rank 4 \u2192 node 0, socket 0, logical processor 32-39, GPU 2, Tile 0</li> <li>MPI rank 5 \u2192 node 0, socket 0, logical processor 40-47, GPU 2, Tile 1</li> <li>MPI rank 6 \u2192 node 0, socket 1, logical processor 52-59, GPU 3, Tile 0</li> <li>MPI rank 7 \u2192 node 0, socket 1, logical processor 60-67, GPU 3, Tile 1</li> <li>MPI rank 8 \u2192 node 0, socket 1, logical processor 68-75, GPU 4, Tile 0</li> <li>MPI rank 9 \u2192 node 0, socket 1, logical processor 76-83, GPU 4, Tile 1</li> <li>MPI rank 10 \u2192 node 0, socket 1, logical processor 84-91, GPU 5, Tile 0</li> <li>MPI rank 11 \u2192 node 0, socket 1, logical processor 92-99, GPU 5, Tile 1</li> </ul> <p> </p> Example 1 GPU Tile Mapping"},{"location":"aurora/running-jobs-aurora/#interactive-jobs-on-compute-nodes","title":"Interactive Jobs on Compute Nodes","text":"<p>Here is how to submit an interactive job to, for example, edit/build/test an application on Aurora compute nodes:</p> <pre><code>qsub -I -l select=1,walltime=1:00:00,place=scatter -l filesystems=&lt;fs1:fs2&gt; -A &lt;MYPROJECT&gt; -q debug\n</code></pre> <p>This command requests 1 node for a period of 1 hour in the <code>workq</code> queue. After waiting in the queue for a node to become available, a shell prompt on a compute node will appear. You may then start building applications and testing gpu affinity scripts on the compute node.</p> <p>Warning</p> <p>If you want to <code>ssh</code> or <code>scp</code> to one of your assigned compute nodes you will need to make sure your <code>$HOME</code> directory and your <code>$HOME/.ssh</code> directory permissions are both set to <code>700</code>.</p>"},{"location":"aurora/running-jobs-aurora/#running-with-multiple-compute-command-streamers-ccss","title":"Running with Multiple Compute Command Streamers (CCSs)","text":"<p>The Intel PVC GPUs contain 4 Compute Command Streamers (CCSs) on each tile, which can be used to group Execution Units (EUs) into common pools.  These pools can then be accessed by separate processes thereby allowing users to bind multiple processes to a single tile and enabling applications to run up to 48 MPI processes per node on the 6 PVC available. Enabling multiple CCSs on Aurora is similar to the MPS capabilities on NVIDIA GPUs. By default, all EUs are assigned to a single CCS, but EUs can be distributed equally into 2 or 4 groups by exposing 2 or 4 CCSs, respectively.  This feature is enabled with the <code>ZEX_NUMBER_OF_CCS</code> environment variable, which takes a comma-separated list of device-mode pairs. For example, to enable 4 CCSs on all 6 PVC, execute <pre><code>export ZEX_NUMBER_OF_CCS=0:4,1:4,2:4,3:4,4:4,5:4\n</code></pre></p> <p>Additional notes when running with multiple CCSs</p> <ul> <li>Please be mindful of the device hierarchy selected. When running with <code>ZE_FLAT_DEVICE_HIERARCHY=COMPOSITE</code>, 6 PVC are exposed to the applications and the above command should be used, noting that <code>export ZEX_NUMBER_OF_CCS=0:4</code> exposes 4 CCSs on both tiles of GPU 0. When running with <code>ZE_FLAT_DEVICE_HIERARCHY=FLAT</code>, the 12 PVC tiles are exposed to the applications (tile-as-device), thus <code>export ZEX_NUMBER_OF_CCS=0:4</code> only refers to tile 0 of GPU 0. To expose multiple CCSs on all tiles, users should use <code>export ZEX_NUMBER_OF_CCS=0:4,1:4,2:4,3:4,4:4,5:4,6:4,7:4,8:4,9:4,10:4,11:4</code>.</li> <li>Users should also be mindful of the CPU binding affinity guidelines described above, ensuring that MPI processes are bound to the correct socket and GPU pairs.</li> <li><code>ZE_AFFINITY_MASK</code> is read by the Level Zero driver prior to <code>ZEX_NUMBER_OF_CCS</code>, thus <code>ZEX_NUMBER_OF_CCS</code> should refer to the GPU IDs of the masked devices.</li> <li>Users can expose different number of CCSs on the different GPU and tiles, the desired CCS mode does not need to be uniform across the GPUs on a node. </li> </ul> <p>More information can be found on Intel's documentation and GitHub pages.</p>"},{"location":"aurora/running-jobs-aurora/#running-multiple-mpi-applications-on-a-node","title":"Running Multiple MPI Applications on a node","text":"<p>Multiple applications can be run simultaneously on a node by launching several <code>mpiexec</code> commands and backgrounding them. For performance, it will likely be necessary to ensure that each application runs on a distinct set of CPU resources and/or targets specific GPUs and tiles. One can provide a list of CPUs using the <code>--cpu-bind</code> option, which when combined with <code>ZE_AFFINITY_MASK</code> provides a user with specifying exactly which CPU and GPU resources to run each application on. In the simple example below, twelve instances of the application are simultaneously running on a single node. In the first instance, the application is spawning MPI ranks 0-3 on CPU cores 0-3 and using GPU 0 tile 0.</p> <pre><code>export OMP_NUM_THREADS=1\nexport ZE_ENABLE_PCI_ID_DEVICE_ORDER=1\n\nexport ZE_AFFINITY_MASK=0.0\nmpiexec --np 4 --ppn 4 --cpu-bind list:0:1:2:3 ./hello_affinity &amp;\n\nexport ZE_AFFINITY_MASK=0.1\nmpiexec -n 4 --ppn 4 --cpu-bind list:4:5:6:7 ./hello_affinity &amp;\n\nexport ZE_AFFINITY_MASK=1.0\nmpiexec -n 4 --ppn 4 --cpu-bind list:8:9:10:11 ./hello_affinity &amp;\n\n\nexport ZE_AFFINITY_MASK=5.1\nmpiexec -n 4 --ppn 4 --cpu-bind list:40:41:42:43 ./hello_affinity &amp;    \n\nwait\n</code></pre> <p>Users will likely find it beneficial to launch processes across CPU cores in both sockets of a node.</p>"},{"location":"aurora/running-jobs-aurora/#compute-node-access-to-the-internet","title":"Compute Node Access to the Internet","text":"<p>Currently, the only access to the internet is via a proxy.  Here are the proxy environment variables for Aurora:</p> <pre><code>export http_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport https_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport ftp_proxy=\"http://proxy.alcf.anl.gov:3128\"\n</code></pre> <p>In the future, though we don't have a timeline on this because it depends on future features in slingshot and internal software development, we intend to have public IP addresses be a schedulable resource.  For instance, if only your head node needed public access your select statement might looks something like: <code>-l select=1:pubnet=True+63</code>.</p>"},{"location":"aurora/running-jobs-aurora/#controlling-where-your-job-runs","title":"Controlling Where Your Job Runs","text":"<p>If you wish to have your job run on specific nodes form your select like this: <code>-l select=1:vnode=&lt;node name1&gt;+1:vnode=&lt;node name2&gt;...</code> . Obviously, that gets tedious for large jobs.</p> <p>If you want to control the location of a few nodes, for example 2 out of 64, but the rest don't matter, you can do something like this: <code>-l select=1:vnode=&lt;node name1&gt;+1:vnode=&lt;node name2&gt;+62:system=foo</code>.</p>"},{"location":"aurora/applications-and-libraries/libraries/cabana-aurora/","title":"Cabana","text":""},{"location":"aurora/applications-and-libraries/libraries/cabana-aurora/#cabana_1","title":"Cabana","text":"<p>Cabana is built atop Kokkos. It provides class templates useful for implementing particle codes.</p>"},{"location":"aurora/applications-and-libraries/libraries/cabana-aurora/#cabana-documentation","title":"Cabana Documentation","text":"<ul> <li>Cabana Wiki</li> <li>Cabana GitHub</li> </ul>"},{"location":"aurora/applications-and-libraries/libraries/cabana-aurora/#cabana-on-aurora","title":"Cabana on Aurora","text":"<p>Built against the prebuilt Kokkos on Aurora, the prebuilt Cabana includes three backends: Serial and OpenMP for CPU execution and SYCL for GPU execution. To use it, run:</p> <pre><code>module load cabana\n</code></pre> <p>Currently, Cabana is a headers-only installation; there are no libraries per se.</p>"},{"location":"aurora/applications-and-libraries/libraries/math-libraries/","title":"Math Libraries","text":""},{"location":"aurora/applications-and-libraries/libraries/mkl/","title":"MKL","text":""},{"location":"aurora/applications-and-libraries/libraries/mpi/","title":"Aurora MPICH","text":"<p>Placeholder</p>"},{"location":"aurora/applications-and-libraries/libraries/onedal/","title":"oneDAL","text":"<p>oneAPI Data Analytics Library (oneDAL) is a library for big data analysis, which includes support for CPUs and GPUs. It includes machine learning algorithms such as k-means clustering and random forests, and it provides support for batch, online, and distributed processing modes. This page is about using the C++ API on Aurora. However, oneDAL can also be used in Python via the Intel Extension for scikit-learn, which we document in the Data Science section. </p> <p>For more information, see the oneDAL Github page, the documentation, or Intel's website. To run on the Intel GPUs, use the oneAPI DPC++ interface. (DPC++, or Data Parallel C++, is oneAPI's implementation of SYCL, so you also see references to options with and without SYCL support in the oneDAL documentation.) </p>"},{"location":"aurora/applications-and-libraries/libraries/onedal/#environment-setup","title":"Environment Setup","text":"<p>oneAPI, including oneDAL, is included in the default environment on Aurora. </p>"},{"location":"aurora/applications-and-libraries/libraries/onedal/#usage","title":"Usage","text":"<p>For a full GPU example, see the oneDAL documentation Quick Start page. However, note that you do not need to set up the environment if you are using the oneDAL installation in the default Aurora environment. </p> <p>For distributed mode, oneDAL offers two communication options: CCL and MPI. We recommend using the MPI option, which is more tested on Aurora. For examples of distributed oneDAL, see the samples/oneapi/dpc/mpi folder in oneDAL's Github repo. </p>"},{"location":"aurora/applications-and-libraries/libraries/spack-pe/","title":"Spack PE","text":"<p>The Spack PE is a software stack that provides various build tools, utilities, and libraries. The Spack PE consists of two parts: <code>spack-pe-gcc</code> and <code>spack-pe-oneapi</code>. <code>spack-pe-gcc</code> contains commonly used software packages compiled for CPU. <code>spack-pe-oneapi</code> is based on the E4S Project and provides performant HPC libraries built with the OneAPI SDK. <code>spack-pe-oneapi</code> depends on both <code>spack-pe-gcc</code> and the OneAPI SDK; in combination, the Spack PE with the OneAPI SDK and MPICH constitute the Aurora PE.</p>"},{"location":"aurora/applications-and-libraries/libraries/spack-pe/#using-software-from-the-spack-pe","title":"Using software from the Spack PE","text":"<p>The Spack PE is loaded into the environment by default as part of the Aurora PE. To view the available modules, run <code>module avail</code>. A full listing of software, including hidden dependencies, can be viewed with <code>module --show-hidden avail</code>. The Spack PE modules will be in paths under <code>/opt/aurora/&lt;AURORA_PE_VERSION&gt;/spack</code>. These can be loaded like any other module, for example, with <code>module load cmake</code>.</p>"},{"location":"aurora/applications-and-libraries/libraries/spack-pe/#inspecting-packages","title":"Inspecting packages","text":"<p>When a module within the Spack PE is loaded, several environment variables are updated to integrate the package into the user's environment. Additionally, the <code>PACKAGE_ROOT</code> variable is set to contain the path to the installation prefix of the package. For example, after loading <code>cmake</code>:</p> <pre><code>$ echo $CMAKE_ROOT\n/opt/aurora/23.275.2/spack/gcc/0.6.1/install/linux-sles15-x86_64/gcc-12.2.0/cmake-3.27.7-mbl7dvgbiblpavhu53h5cheyrmpaikdz\n$ ls -a $CMAKE_ROOT\n.  ..  bin  doc  share  .spack\n</code></pre> <p>This variable can be used to inspect software installations. Additionally, Spack packages have a <code>.spack</code> directory in the installation prefix, which contains build logs and information on configure and build options.</p>"},{"location":"aurora/applications-and-libraries/libraries/spack-pe/#rpath-linking","title":"RPATH linking","text":"<p>Spack PE packages are built with <code>RPATH</code> linking. <code>RPATH</code> hardcodes a default search path for dynamic runtime linking of binaries. By setting <code>RPATH</code>, the loader only needs to search a single path for each library, reducing the number of filesystem calls performed when loading libraries. However, this means <code>LD_LIBRARY_PATH</code> will be ignored when loading binaries installed in the Spack PE. This has been set both to provide a performance benefit and to guarantee proper compatibility of linked libraries.</p> <p>Software installed outside of the Spack PE tree, such as in <code>/soft</code>, will typically be installed with <code>RUNPATH</code> linking or with no runtime search path, both of which respect <code>LD_LIBRARY_PATH</code>. <code>RUNPATH</code> linking, like <code>RPATH</code>, hardcodes a default path, but it does not have precedence over <code>LD_LIBRARY_PATH</code>. Spack PE preview deployments in <code>/soft</code> are installed with <code>RUNPATH</code> linking.</p>"},{"location":"aurora/applications-and-libraries/libraries/spack-pe/#building-software-with-spack","title":"Building software with Spack","text":"<p>Spack is a powerful package manager designed for HPC. The Spack PE is installed and managed with Spack; users can also install Spack in their own home or project directory to manage their software builds. Spack has a steep learning curve, but it may benefit workflows involving frequent builds with complex dependencies.</p> <p>For users who wish to use Spack to install their own software, we provide configuration files corresponding to the Spack PE deployments. These configuration files can be found in <code>/opt/aurora/&lt;AURORA_PE_VERSION&gt;/spack</code> in <code>config</code> directories organized by Spack PE version. Not all of these settings will be useful for all builds, and it is not recommended to adopt these wholesale as global settings. The recommended method is to include these settings ad hoc in a Spack environment to control what information Spack uses for its builds. However, we do recommend using the provided configurations for the compilers, OneAPI SDK components, and MPICH, as these can be difficult to configure properly.</p> <p>Support requests and feedback for ALCF-specific issues should be directed to support@alcf.anl.gov. For general Spack questions, users are encouraged to consult the following resources:</p> <ul> <li>Spack development website</li> <li>Spack documentation</li> <li>Spack tutorial</li> <li>Spack Slack channel</li> </ul>"},{"location":"aurora/build-tools/cmake-aurora/","title":"CMake","text":""},{"location":"aurora/build-tools/cmake-aurora/#cmake_1","title":"CMake","text":"<p>CMake is a build configuration system that uses higher-level description files to automatically generate Makefiles.</p>"},{"location":"aurora/build-tools/cmake-aurora/#cmake-documentation","title":"CMake Documentation","text":"<ul> <li>CMake website</li> </ul>"},{"location":"aurora/build-tools/cmake-aurora/#cmake-on-aurora","title":"CMake on Aurora","text":"<p>To use CMake on Aurora, run:</p> <pre><code>module load cmake\n</code></pre>"},{"location":"aurora/compiling-and-linking/aurora-example-program-makefile/","title":"Aurora Example Program Makefile","text":"<p>Several simple examples of building CPU and GPU-enabled codes on Aurora are available in the ALCF GettingStarted repo for supported programming models. If building your application on the login node is problematic for some reason (e.g., absence of a GPU), then users are encouraged to build and test applications directly on one of the Aurora compute nodes via an interactive job. The discussion below makes use of the <code>oneAPI</code> compilers in the default environment as illustrative examples.</p>"},{"location":"aurora/compiling-and-linking/aurora-example-program-makefile/#cpu-mpiopenmp-example","title":"CPU MPI+OpenMP Example","text":"<p>One of the first useful tasks with any new machine, scheduler, and job launcher is to ensure one is binding MPI ranks and OpenMP threads to the host CPU as intended. A simple HelloWorld MPI+OpenMP example is available here to get started with.</p> <p>The Aurora compute nodes are dual-socket with 52 physical cores in each socket for a total of 104 cores. As hyperthreading is enabled, each core will show up as two CPUs for a total of 208. In many of the examples below, only a single process is spawned on each physical core.</p> <p>The application can be straightforwardly compiled using the MPICH compiler wrappers in the default environment.</p> <pre><code>mpicxx -g -fopenmp -O3 main.cpp\n</code></pre> <p>The executable <code>hello_affinity</code> can then be launched in a job script (or directly in the shell of an interactive job) using <code>mpiexec</code> as discussed here.</p> <pre><code>#!/bin/bash -l\n#PBS -l select=1\n#PBS -l place=scatter\n#PBS -l walltime=0:15:00\n#PBS -q &lt;queue&gt;\n#PBS -A &lt;ProjectName&gt;\n#PBS -l filesystems=&lt;fs1:fs2&gt;\n\n#cd ${PBS_O_WORKDIR}\n\n# MPI example w/ MPI ranks and OpenMP threads spread evenly across cores (one process per physical core)\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS_PER_NODE=26\nNDEPTH=4\nNTHREADS=4\n\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}\"\n\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth --env OMP_NUM_THREADS=${NTHREADS} --env OMP_PLACES=cores ./hello_affinity\n</code></pre>"},{"location":"aurora/compiling-and-linking/aurora-example-program-makefile/#gpu-openmp-example","title":"GPU OpenMP Example","text":"<p>A simple OpenMP offload example is available here. Compilation proceeds similarly to the above CPU-only example except for the use of compiler flags to enable GPU offload.</p> <pre><code>mpicxx -fiopenmp -fopenmp-targets=spir64 main.cpp\n</code></pre> <p>Running the example with 12 MPI ranks and no other settings will generate output like the following.</p> <pre><code>$ make\n\n$ mpiexec -n 12 --ppn 12 --depth=1 --cpu-bind depth ./hello_affinity\nNUM_OF_NODES= 1 TOTAL_NUM_RANKS= 12 RANKS_PER_NODE= 12 THREADS_PER_RANK= 1\n\n  Using OPENMP v5.0\n  num_devices=     6\n  Default device=  0\n  Host=            6\n  num_teams=       896\n  num_threads=     1\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 0  list_cores= (0)  num_devices= 6  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 1  list_cores= (1)  num_devices= 6  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 2  list_cores= (2)  num_devices= 6  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 3  list_cores= (3)  num_devices= 6  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 4  list_cores= (4)  num_devices= 6  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 5  list_cores= (5)  num_devices= 6  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 6  list_cores= (6)  num_devices= 6  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 7  list_cores= (7)  num_devices= 6  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 8  list_cores= (8)  num_devices= 6  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 9  list_cores= (9)  num_devices= 6  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 10  list_cores= (10)  num_devices= 6  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 11  list_cores= (11)  num_devices= 6  gpu_id= 0\n</code></pre> <p>This simple application does not handle binding of MPI ranks to GPUs, so each of the 12 MPI ranks detects all six GPUs on the node and by default all will select the first GPU listed. The binding of MPI ranks to GPUs can be handled by <code>mpiexec</code> in the near future, but for the time being a simple helper script is available for those that need it. There is a centrally installed general <code>gpu_tile_compact.sh</code> script available for use, but the examples include the following example script for convenience in case one would like to explore different CPU-GPU bindings (e.g., bind first N MPI ranks to the first GPU).</p> <pre><code>$ cat set_affinity_gpu_sunspot.sh \n#!/usr/bin/env bash\n\nnum_gpu=6\nnum_tile=2\n\ngpu_id=$(( (PALS_LOCAL_RANKID / num_tile ) % num_gpu ))\ntile_id=$((PALS_LOCAL_RANKID % num_tile))\n\nunset EnableWalkerPartition\nexport ZE_ENABLE_PCI_ID_DEVICE_ORDER=1\nexport ZE_AFFINITY_MASK=$gpu_id.$tile_id\n\necho \"RANK= ${PALS_RANKID} LOCAL_RANK= ${PALS_LOCAL_RANKID} gpu= ${gpu_id}  tile= ${tile_id}\"\n\n#https://stackoverflow.com/a/28099707/7674852\n\"$@\"\n</code></pre> <p>The <code>ZE_AFFINITY_MASK</code> environment variable sets the devices that will be available to the CPU process and can be a comma-separated list of GPUs and/or GPU tiles. Each Aurora GPU consists of two tiles that can be separately bound to CPU processes. This simple script will set <code>ZE_AFFINITY_MASK</code> for each MPI rank such that GPU tiles on a node are round-robin assigned.</p> <pre><code>$ mpiexec -n 12 --ppn 12 --depth=1 --cpu-bind depth ./set_affinity_gpu_sunspot.sh ./hello_affinity\nNUM_OF_NODES= 1 TOTAL_NUM_RANKS= 12 RANKS_PER_NODE= 12 THREADS_PER_RANK= 1\n\n  Using OPENMP v5.0\n  num_devices=     1\n  Default device=  0\n  Host=            1\n  num_teams=       448\n  num_threads=     1\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 0  list_cores= (0)  num_devices= 1  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 1  list_cores= (1)  num_devices= 1  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 2  list_cores= (2)  num_devices= 1  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 3  list_cores= (3)  num_devices= 1  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 4  list_cores= (4)  num_devices= 1  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 5  list_cores= (5)  num_devices= 1  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 6  list_cores= (6)  num_devices= 1  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 7  list_cores= (7)  num_devices= 1  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 8  list_cores= (8)  num_devices= 1  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 9  list_cores= (9)  num_devices= 1  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 10  list_cores= (10)  num_devices= 1  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 11  list_cores= (11)  num_devices= 1  gpu_id= 0\n</code></pre>"},{"location":"aurora/compiling-and-linking/aurora-example-program-makefile/#gpu-sycl-example","title":"GPU SYCL Example","text":"<p>A simple SYCL offload example is available here. Compilation proceeds similarly to the above examples except for the compiler flags enabling GPU offload.</p> <pre><code>mpicxx -std=c++17 -fsycl -fsycl-targets=spir64 main.cpp\n</code></pre> <p>Note, this particular example makes use of the Level-Zero API and requires linking with <code>-lze_loader</code>, which is not something required of a typical SYCL application. Running the SYCL example using the affinity script binding MPI ranks to individual GPU tiles results in output like the following.</p> <pre><code>$ make\n\n$ mpiexec -n 12 --ppn 12 --depth=1 --cpu-bind depth ./set_affinity_gpu_sunspot.sh ./hello_affinity\n\nNUM_OF_NODES= 1 TOTAL_NUM_RANKS= 12 RANKS_PER_NODE= 12 THREADS_PER_RANK= 1\nCOMMAND= mpiexec -n 12 --ppn 12 --depth=1 --cpu-bind depth ./set_affinity_gpu_sunspot.sh ./hello_affinity\n\n\"RANK= 0 LOCAL_RANK= 0 gpu= 0 tile= 0\"\n\"RANK= 1 LOCAL_RANK= 1 gpu= 0 tile= 1\"\n\"RANK= 2 LOCAL_RANK= 2 gpu= 1 tile= 0\"\n\"RANK= 3 LOCAL_RANK= 3 gpu= 1 tile= 1\"\n\"RANK= 4 LOCAL_RANK= 4 gpu= 2 tile= 0\"\n\"RANK= 5 LOCAL_RANK= 5 gpu= 2 tile= 1\"\n\"RANK= 6 LOCAL_RANK= 6 gpu= 3 tile= 0\"\n\"RANK= 7 LOCAL_RANK= 7 gpu= 3 tile= 1\"\n\"RANK= 8 LOCAL_RANK= 8 gpu= 4 tile= 0\"\n\"RANK= 9 LOCAL_RANK= 9 gpu= 4 tile= 1\"\n\"RANK= 10 LOCAL_RANK= 10 gpu= 5 tile= 0\"\n\"RANK= 11 LOCAL_RANK= 11 gpu= 5 tile= 1\"\n\nTo affinity and beyond!! nname= x1922c2s6b0n0  rnk= 0  list_cores= (0)  num_devices= 1  gpu_uuid=  01000000-0000-0000-dbb1-2f985946b0dd\nTo affinity and beyond!! nname= x1922c2s6b0n0  rnk= 1  list_cores= (1)  num_devices= 1  gpu_uuid=  02000000-0000-0000-dbb1-2f985946b0dd\n\nTo affinity and beyond!! nname= x1922c2s6b0n0  rnk= 2  list_cores= (2)  num_devices= 1  gpu_uuid=  01000000-0000-0000-9d4c-a3a038130bd2\nTo affinity and beyond!! nname= x1922c2s6b0n0  rnk= 3  list_cores= (3)  num_devices= 1  gpu_uuid=  02000000-0000-0000-9d4c-a3a038130bd2\n\nTo affinity and beyond!! nname= x1922c2s6b0n0  rnk= 4  list_cores= (4)  num_devices= 1  gpu_uuid=  01000000-0000-0000-f684-455a4554b231\nTo affinity and beyond!! nname= x1922c2s6b0n0  rnk= 5  list_cores= (5)  num_devices= 1  gpu_uuid=  02000000-0000-0000-f684-455a4554b231\n\nTo affinity and beyond!! nname= x1922c2s6b0n0  rnk= 6  list_cores= (6)  num_devices= 1  gpu_uuid=  01000000-0000-0000-d04a-9a289a53274e\nTo affinity and beyond!! nname= x1922c2s6b0n0  rnk= 7  list_cores= (7)  num_devices= 1  gpu_uuid=  02000000-0000-0000-d04a-9a289a53274e\n\nTo affinity and beyond!! nname= x1922c2s6b0n0  rnk= 8  list_cores= (8)  num_devices= 1  gpu_uuid=  01000000-0000-0000-a178-e2f3a2a0df2b\nTo affinity and beyond!! nname= x1922c2s6b0n0  rnk= 9  list_cores= (9)  num_devices= 1  gpu_uuid=  02000000-0000-0000-a178-e2f3a2a0df2b\n\nTo affinity and beyond!! nname= x1922c2s6b0n0  rnk= 10  list_cores= (10)  num_devices= 1  gpu_uuid=  01000000-0000-0000-1b72-105049dfed26\nTo affinity and beyond!! nname= x1922c2s6b0n0  rnk= 11  list_cores= (11)  num_devices= 1  gpu_uuid=  02000000-0000-0000-1b72-105049dfed26\n</code></pre> <p>Upon carefully comparing the UUIDs from each rank, one can see the first field distinguishing the 1st or 2nd tile on a GPU and the last two fields distinguishing the 6 GPUs on a compute node. If the affinity script was not used for binding MPI ranks to GPUs, then each MPI rank would report UUIDs for all GPUs like in the following.</p> <pre><code>To affinity and beyond!! nname= x1922c2s6b0n0  rnk= 0  list_cores= (0)  num_devices= 6  gpu_uuid=  00000000-0000-0000-dbb1-2f985946b0dd 00000000-0000-0000-9d4c-a3a038130bd2 00000000-0000-0000-f684-455a4554b231 00000000-0000-0000-d04a-9a289a53274e 00000000-0000-0000-a178-e2f3a2a0df2b 00000000-0000-0000-1b72-105049dfed26\n</code></pre>"},{"location":"aurora/compiling-and-linking/aurora-example-program-makefile/#gpu-opencl-example","title":"GPU OpenCL Example","text":"<p>A simple OpenCL example is available here. The <code>include</code> and <code>lib</code> directories for the OpenCL headers and libraries are in the default environment. One simply needs to link the application against <code>-lOpenCL</code>.</p> <pre><code>mpicxx main.cpp -lOpenCL\n</code></pre> <p>This simple example can be run on a single tile of an Aurora GPU as follows.</p> <pre><code>$ export XE_AFFINITY_MASK=0.0\n$ ./vecadd\nRunning on GPU!\nUsing double-precision\n\n    CL_DEVICE_NAME: Intel(R) Data Center GPU Max 1550\n    CL_DEVICE_VERSION: OpenCL 3.0 NEO \n    CL_DEVICE_OPENCL_C_VERSION: OpenCL C 1.2 \n    CL_DEVICE_MAX_COMPUTE_UNITS: 896\n    CL_DEVICE_MAX_CLOCK_FREQUENCY: 1600\n    CL_DEVICE_MAX_WORK_GROUP_SIZE: 1024\n\nResult is CORRECT!! :)\n</code></pre>"},{"location":"aurora/compiling-and-linking/aurora-programming-models/","title":"Aurora Programming Models","text":"<p>The software environment on Aurora supports several parallel programming models targeting the CPUs and GPUs.</p>"},{"location":"aurora/compiling-and-linking/aurora-programming-models/#cpu-parallel-programming-models","title":"CPU Parallel Programming Models","text":"<p>The Aurora MPICH compiler wrappers <code>mpicc</code>, <code>mpicxx</code>, and <code>mpifort</code> are recommended for MPI applications to be built using the oneAPI compilers. A summary of available CPU parallel programming models and relevant compiler flags is shown below. Users are encouraged to review the corresponding man pages and documentation.</p> Programming Model oneAPI OpenMP -fiopenmp or -qopenmp <p>Note: <code>-fopenmp</code> uses OpenMP as it is implemented by the LLVM community.</p> <p>Higher-level programming models such as Kokkos and Raja may also be used for CPU programming on Aurora.</p>"},{"location":"aurora/compiling-and-linking/aurora-programming-models/#gpu-programming-models","title":"GPU Programming Models","text":"<p>A summary of available GPU programming models and relevant compiler flags is shown below for compilers that generate offloadable code. Two modes of compilation are currently available with the oneAPI compilers: Just-in-Time (JIT) and Ahead-of-Time (AoT). With AoT compilation, flags for specifying the backend are only needed when linking the application. Users are encouraged to review the corresponding man pages and documentation.</p> Programming Model oneAPI (JIT) oneAPI (AoT) OpenCL -- N/A OpenMP -fiopenmp -fopenmp-targets=spir64 -fiopenmp -fopenmp-targets=spir64_gen -Xopenmp-target-backend=spir64_gen \"-device pvc\" SYCL --intel -fsycl -fsycl-targets=spir64 --intel -fsycl -fsycl-targets=spir64_gen -Xsycl-target-backend \"-device pvc\" <p>For some build systems (e.g., <code>cmake</code>), it may be necessary to use the backslash character to escape the double quotes when specifying the device in AoT builds.</p> <pre><code>-Xopenmp-target-backend=spir64_gen \\\"-device pvc\\\"\n</code></pre> <p>OpenCL is supported, but does not require specific compiler flags per-se as the offloaded kernels are JIT-compiled. One does need to link against the OpenCL library <code>-lOpenCL</code>. Abstraction programming models, such as Kokkos, can be built on top of these programming models.</p>"},{"location":"aurora/compiling-and-linking/cce-compilers-aurora/","title":"CCE Compilers on Polaris","text":""},{"location":"aurora/compiling-and-linking/compiling-and-linking-overview/","title":"Compiling and Linking Overview","text":""},{"location":"aurora/compiling-and-linking/compiling-and-linking-overview/#compiling-on-aurora-login-and-compute-nodes","title":"Compiling on Aurora Login and Compute Nodes","text":"<p>If your build system does not require GPUs for the build process, the compilation of GPU-accelerated codes is generally expected to work well on the Aurora login nodes. If your build system does require GPUs, then currently that must be done on the compute nodes either via an interactive or batch job submission. Doing this interactively in a single-node job may be the preferred route as it also provides an opportunity to quickly test the executable.</p>"},{"location":"aurora/compiling-and-linking/compiling-and-linking-overview/#filesystem","title":"Filesystem","text":"<p>It is helpful to realize that the home filesystem is <code>gecko</code> and the <code>project</code> spaces reside on the Lustre filesystem called <code>Flare</code>. The filesystems are not backed up, and users should take care to retain copies of important files (e.g., local resources or ALCF's <code>eagle</code> filesystem).</p>"},{"location":"aurora/compiling-and-linking/compiling-and-linking-overview/#oneapi-programming-environment","title":"OneAPI Programming Environment","text":"<p>The oneAPI programming environment is currently the single environment for building and running software to maximally use the available hardware resources. The oneAPI environment is loaded by default for users and is principally defined by the following set of modules and related variants.</p> <ul> <li>oneapi: Intel oneAPI HPC toolkit.</li> <li>mpich: MPI libraries.</li> </ul> <p>Additional modules loading <code>GNU</code> CPU compilers and parallel application launch support (e.g., libfabric and cray-pals) are also provided in the default environment. The oneAPI environment provides C, C++, and Fortran compilers and associated MPICH MPI wrappers for building applications targeting CPUs and GPUs based on the OpenMP, SYCL, and OpenCL programming models.</p> <ul> <li><code>mpicc</code> - C Compiler</li> <li><code>mpicxx</code> - C++ Compiler (a.k.a <code>mpic++</code>)</li> <li><code>mpifort</code> - Fortran Compiler (a.k.a <code>mpif70</code> &amp; <code>mpif90</code>)</li> </ul>"},{"location":"aurora/compiling-and-linking/compiling-and-linking-overview/#compilers-provided-by-cray-programming-environments","title":"Compilers provided by Cray Programming Environments","text":"<p>The <code>PrgEnv-gnu</code> and <code>PrgEnv-cray</code> Cray programming environments are currently available as modules for users that need them. The compilers provided by these environments do not currently support Intel GPUs and should only be used, if at all, for CPU-only code.</p>"},{"location":"aurora/compiling-and-linking/compiling-and-linking-overview/#mixed-cc-fortran-applications","title":"Mixed C/C++ &amp; Fortran Applications","text":"<p>For applications consisting of a mix of programming languages that use MPI, it is important to use the same Fortran compiler for building the application as was used to build MPI because of mpi.mod (and similar) incompatibilities.</p>"},{"location":"aurora/compiling-and-linking/compiling-and-linking-overview/#additional-software-and-build-tools","title":"Additional Software and Build Tools","text":"<p>A suite of build tools and libraries are available in the default Aurora PE environment. Users can look at the list of available modules with <code>module avail</code> to find build tools such as <code>cmake</code>.</p> <pre><code>$ module load cmake\n$ cmake --version\ncmake version 3.27.7\n</code></pre>"},{"location":"aurora/compiling-and-linking/continuous-integration-aurora/","title":"Continuous Integration Aurora","text":""},{"location":"aurora/compiling-and-linking/gnu-compilers-aurora/","title":"GNU Compilers Polaris","text":""},{"location":"aurora/compiling-and-linking/llvm-compilers-aurora/","title":"LLVM Compilers Aurora","text":""},{"location":"aurora/containers/containers/","title":"Containers on Aurora","text":"<p>Users of Aurora can benefit from container-based workloads for seamless compatibility across Intel systems. This guide details the use of containers on Aurora, including custom container creation, large-scale execution, and common pitfalls.</p> <p>We support Apptainer and Podman (documentation coming soon) on Aurora.</p>"},{"location":"aurora/containers/containers/#apptainer","title":"Apptainer","text":"<p>Aurora employs Apptainer (formerly known as Singularity) for container management.</p>"},{"location":"aurora/containers/containers/#login-and-queue-a-job","title":"Login and queue a job","text":"<p><pre><code>ssh &lt;username&gt;@aurora.alcf.anl.gov\n</code></pre> Refer to Getting Started on Aurora for additional information. In particular, you need to set the environment variables that provide access to the proxy host.</p> <p>Note</p> <p>The instructions below should be run directly from a compute node.</p> <p>Explicitly, to request an interactive job (from <code>aurora-uan</code>): <pre><code>qsub -I -q &lt;your_Queue&gt; -l select=1,walltime=60:00 -A &lt;your_ProjectName&gt; -l filesystems=&lt;fs1:fs2&gt;\n</code></pre></p> <p>Refer to job scheduling and execution for additional information.</p>"},{"location":"aurora/containers/containers/#load-modules-on-compute-node","title":"Load Modules on Compute Node","text":"<pre><code># load apptainer\nmodule load spack-pe-gcc\nmodule load apptainer\nmodule load fuse-overlayfs\n</code></pre>"},{"location":"aurora/containers/containers/#building-from-docker-or-argonne-github-container-registry","title":"Building from Docker or Argonne GitHub Container Registry","text":"<p>Containers on Aurora can be built by writing Dockerfiles on a local machine and then publishing the container to DockerHub, or by directly building them on an ALCF compute node by writing an Apptainer recipe file. If you prefer to use existing containers, you can pull them from various registries like DockerHub and run them on Aurora.</p> <p>Since Docker requires root privileges, which users do not have on Aurora, existing Docker containers must be converted to Apptainer. To build a Docker-based container on Aurora, use the following as an example:</p> <pre><code>## Build an image\napptainer build intel-optimized-pytorch.sing docker://intel/intel-optimized-pytorch\n</code></pre>"},{"location":"aurora/containers/containers/#example-to-run-hello-world-using-apptainer","title":"Example to run Hello World using Apptainer","text":"<pre><code>apptainer exec docker://ghcr.io/apptainer/lolcow cowsay 'Fresh from the internet'\n</code></pre>"},{"location":"aurora/containers/containers/#example-to-run-postgres-database-using-apptainer","title":"Example to run Postgres Database using Apptainer","text":"<p>To run Postgres on an Aurora compute node, below is a full example.</p> apptainer_aurora_example.sh<pre><code># qsub from a UAN/login node\nqsub -l select=1 -l walltime=60:00 -A &lt;Projectname&gt; -q &lt;Queue&gt; -l filesystems=&lt;fs1:fs2&gt; -I\n\n# Set proxy on compute node\nexport HTTP_PROXY=\"http://proxy.alcf.anl.gov:3128\"\nexport HTTPS_PROXY=\"http://proxy.alcf.anl.gov:3128\"\nexport http_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport https_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport ftp_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport no_proxy=\"admin,*.hostmgmt.cm.aurora.alcf.anl.gov,*.alcf.anl.gov,localhost\"\n\n# Load apptainer\nmodule load spack-pe-gcc\nmodule load apptainer\nmodule load fuse-overlayfs\n\n# Build postgres image\napptainer build postgres.sing docker://postgres # do this once\n\n# Create an environment file\ncat &gt;&gt; pg.env &lt;&lt;EOF\nexport POSTGRES_USER=pguser\nexport POSTGRES_PASSWORD=mypguser123\nexport POSTGRES_DB=mydb\nexport POSTGRES_INITDB_ARGS=\"--encoding=UTF-8\"\nEOF\n\n# Create a data and run directory to bind to the running container\nmkdir -p pgdata pgrun\n\n# Start an instance of the container using nohup\nnohup apptainer run -B pgrun:/var/run/postgresql -B pgdata:/var/lib/postgresql/data --env-file pg.env postgres.sing postgres &amp;\n\n# Save PID\necho $! &gt; pg_pid.txt\necho \"Postgres started with PID $(cat pg_pid.txt)\"\n\n# Interact with running container using psql client\napptainer exec \\\n  -B pgrun:/var/run/postgresql \\\n  -B pgdata:/var/lib/postgresql/data \\\n  --env-file pg.env \\\n  postgres.sing psql -h 127.0.0.1 -p 5432 -U pguser -d mydb -c \"SELECT version();\"\n\n# Stop the container and kill the process\nkill \"$(cat pg_pid.txt)\"\nrm pg_pid.txt\n</code></pre>"},{"location":"aurora/data-management/copper/copper/","title":"Copper","text":"<p>Copper is a co-operative caching layer for scalable parallel data movement in Exascale Supercomputers developed at Argonne Leadership Computing Facility.</p>"},{"location":"aurora/data-management/copper/copper/#introduction","title":"Introduction","text":"<p>Copper is a read-only cooperative caching layer aimed at enabling scalable data loading on massive amounts of compute nodes. This aims to avoid the I/O bottleneck in the storage network and effectively use the compute network for data movement.</p> <p>The current intended use of Copper is to improve the performance of Python imports - dynamic shared library loading on Aurora. However, Copper can be used to improve the performance of any type of redundant data loading on a supercomputer.</p> <p>It is recommended to use Copper for any applications [preferably Python and I/O &lt;500 MB] in order to scale beyond 2k nodes.</p> <p></p>"},{"location":"aurora/data-management/copper/copper/#how-to-use-copper-on-aurora","title":"How to use Copper on Aurora","text":"<p>In your job script or from an interactive session:</p> <pre><code>module load copper\nlaunch_copper.sh\n</code></pre> <p>Then run your <code>mpiexec</code> as you would normally run.</p> <p>If you want your I/O to go through Copper, add <code>/tmp/${USER}/copper/</code> to the beginning of your PATHS. Here, only the root compute node will do the I/O directly with the Lustre file system. If <code>/tmp/${USER}/copper/</code> is not added to the beginning of your paths, then all compute nodes would do I/O directly to the Lustre file system.</p> <p>For example, if you have a local conda environment located in a path at <code>/lus/flare/projects/Aurora_deployment/kaushik/copper/oct24/copper/run/copper_conda_env</code>, you need to prepath the Copper path as <code>/tmp/${USER}/copper/lus/flare/projects/Aurora_deployment/kaushik/copper/oct24/copper/run/copper_conda_env</code>. The same should be done for any type of PATHS, like PYTHONPATH, CONDAPATH, and your input file path.</p> <p>Python Example:</p> <pre><code>time mpirun --np ${NRANKS} --ppn ${RANKS_PER_NODE} --cpu-bind=list:4:9:14:19:20:25:56:61:66:71:74:79 --genvall \\\n            --genv=PYTHONPATH=/tmp/${USER}/copper/lus/flare/projects/Aurora_deployment/kaushik/copper/oct24/copper/run/copper_conda_env \\\n            python3 -c \"import numpy; print(numpy.__file__)\"\n</code></pre> <p>Non-Python Example:</p> <pre><code>time mpiexec -np $ranks -ppn 12 --cpu-bind list:4:9:14:19:20:25:56:61:66:71:74:79 --no-vni -genvall \\\n        /lus/flare/projects/CSC250STDM10_CNDA/kaushik/thunder/svm_mpi/run/aurora/wrapper.sh \\\n        /lus/flare/projects/CSC250STDM10_CNDA/kaushik/thunder/svm_mpi/build_ws1024/bin/thundersvm-train \\\n            -s 0 -t 2 -g 1 -c 10 -o 1 /tmp/${USER}/copper/lus/flare/projects/CSC250STDM10_CNDA/kaushik/thunder/svm_mpi/data/sc-40-data/real-sim_M100000_K25000_S0.836\n</code></pre> <p>Finally, you can add an optional <code>stop_copper.sh</code>.</p>"},{"location":"aurora/data-management/copper/copper/#copper-options","title":"Copper Options","text":"<pre><code>-l log_level                [Allowed values: 6[no logging], 5[less logging], 4, 3, 2, 1[more logging]] [Default: 6]\n-t log_type                 [Allowed values: file or file_and_stdout] [Default: file]\n-T trees                    [Allowed values: any number] [Default: 1]\n-M max_cacheable_byte_size  [Allowed values: any number in bytes] [Default: 10MB]\n-s sleeptime                [Allowed values: Any number] [Default: 20 seconds] Recommended to use 60 seconds for 4k nodes\n-b physcpubind              [Allowed values: \"CORE NUMBER-CORE NUMBER\"] [Default: \"48-51\"]\n</code></pre> <p>For example, you can change the default values to:</p> <pre><code>launch_copper.sh -l 2 -t stdout_and_file -T 2 -s 40\n</code></pre>"},{"location":"aurora/data-management/copper/copper/#notes","title":"Notes","text":"<ul> <li>Copper currently does not support write operations.</li> <li>Only the following file system operations are supported: init, open, read, readdir, readlink, getattr, ioctl, destroy.</li> <li>Copper works only from the compute nodes, and you need a minimum of 2 nodes up to a max of any number of nodes (Aurora max 10624 nodes).</li> <li>Recommended trees are 1 or 2.</li> <li>Recommended size for max cacheable byte size is 10MB to 100MB.</li> <li>To be used only from the compute node.</li> <li>More examples at GitHub Copper Examples and Copper Documentation.</li> </ul>"},{"location":"aurora/data-management/daos/daos-overview/","title":"DAOS Architecture","text":"<p>Warning</p> <p>DAOS is a scratch file system. Please note that data may be removed or unavailable at any time. </p> <p>DAOS is a major file system in Aurora with 230 PB delivering upto &gt;30 TB/s with 1024 DAOS server storage Nodes. DAOS is an open-source software-defined object store designed for massively distributed Non Volatile Memory (NVM) and NVMe SSD.  DAOS presents a unified storage model with a native Key-array Value storage interface supporitng POSIX, MPIO, DFS and HDF5. Users can use DAOS for their I/O and checkpointing on Aurora.  DAOS is fully integrated with the wider Aurora compute fabric as can be seen in the overall storage architecture below.  </p>"},{"location":"aurora/data-management/daos/daos-overview/#daos-overview","title":"DAOS Overview","text":"<p>The first step in using DAOS is to get DAOS POOL space allocated for your project.  Users should submit a request as noted below to have a DAOS pool created for your project.</p>"},{"location":"aurora/data-management/daos/daos-overview/#daos-pool-allocation","title":"DAOS Pool Allocation","text":"<p>DAOS pool is a physically allocated dedicated storage space for your project. </p> <p>Email support@alcf.anl.gov to request a DAOS pool with the following information.</p> <ul> <li>Project Name</li> <li>ALCF User Names</li> <li>Total Space requested (typically 100 TBs++)</li> <li>Justification</li> <li>Preferred pool name </li> </ul>"},{"location":"aurora/data-management/daos/daos-overview/#note","title":"Note","text":"<p>This is an initial test DAOS configuration and as such, any data on the DAOS system will eventually be deleted when the configuration is changed into a larger system. Warning will be given before the system is wiped to allow time for users to move any important data off.</p>"},{"location":"aurora/data-management/daos/daos-overview/#modules","title":"Modules","text":"<p>Please load the <code>daos</code> module when using DAOS. This should be done on the login node (UAN) or in the compute node (jobscript):</p> <pre><code>module use /soft/modulefiles\nmodule load daos/base\n</code></pre>"},{"location":"aurora/data-management/daos/daos-overview/#pool","title":"Pool","text":"<p>Pool is a dedicated space allocated to your project. Once your pool is allocated for your project space. </p> <p>Confirm you are able to query the pool via: <pre><code>daos pool query &lt;pool_name&gt;\n</code></pre></p> Example output:<pre><code>daos pool query hacc\nPool 050b20a3-3fcc-499b-a6cf-07d4b80b04fd, ntarget=640, disabled=0, leader=2, version=131\nPool space info:\n- Target(VOS) count:640\n- Storage tier 0 (SCM):\nTotal size: 6.0 TB\n  Free: 4.4 TB, min:6.5 GB, max:7.0 GB, mean:6.9 GB\n- Storage tier 1 (NVMe):\n  Total size: 200 TB\n  Free: 194 TB, min:244 GB, max:308 GB, mean:303 GB\nRebuild done, 4 objs, 0 recs\n</code></pre>"},{"location":"aurora/data-management/daos/daos-overview/#daos-container","title":"DAOS Container","text":"<p>The container is the basic unit of storage. A POSIX container can contain hundreds of millions of files, you can use it to store all of your data. You only need a small set of containers; perhaps just one per major unit of project work is sufficient.</p> <p>There are 3 modes with which we can operate with the DAOS containers 1. POSIX container POSIX Mode 2. POSIX Container MPI-IO Mode 3. DFS container through DAOS APIs.</p>"},{"location":"aurora/data-management/daos/daos-overview/#create-a-posix-container","title":"Create a POSIX container","text":"<pre><code>$ DAOS_POOL=datascience\n$ DAOS_CONT=LLM-GPT-1T\n$ daos container create --type POSIX ${DAOS_POOL}  ${DAOS_CONT} --properties rd_fac:1 \n  Container UUID : 59747044-016b-41be-bb2b-22693333a380\n  Container Label: LLM-GPT-1T                           \n  Container Type : POSIX                               \n\nSuccessfully created container 59747044-016b-41be-bb2b-22693333a380\n</code></pre> <p>If you prefer a higher data protection and recovery you can <code>--properties rd_fac:2</code> and if you don't need data protection and recovery, you can remove <code>--properties rd_fac:1</code>. We recommend to have at least <code>--properties rd_fac:1</code>.</p> <p></p>"},{"location":"aurora/data-management/daos/daos-overview/#daos-sanity-checks","title":"DAOS sanity checks","text":"<p>If any of the following command results in an error, then you can confirm DAOS is currently down.  'Out of group or member list' error is an exception and can be safely ignored. This error message will be fixed in the next daos release.</p> <pre><code>module use /soft/modulefiles\nmodule load daos/base\nenv | grep DRPC       \nps \u2013ef | grep daos                                   \nclush --hostfile ${PBS_NODEFILE} ps \u2013ef | grep agent | grep -v grep'  | dshbak -c     #to check on all compute nodes\nexport DAOS_POOL=Your_allocated_pool_name\ndaos pool query ${DAOS_POOL}\ndaos cont list ${DAOS_POOL}\ndaos container get-prop  $DAOS_POOL_NAME  $DAOS_CONT_NAME \n</code></pre> <ul> <li>Look for messages like <code>Rebuild busy and state degraded in the daos pool query.</code> </li> <li>Look for messages like <code>Health (status) : UNCLEAN in the get prop</code></li> </ul> <pre><code>daos pool      autotest  $DAOS_POOL_NAME \ndaos container check --pool=$DAOS_POOL_NAME --cont=$DAOS_CONT_NAME \n</code></pre>"},{"location":"aurora/data-management/daos/daos-overview/#mount-a-posix-container","title":"Mount a POSIX container","text":"<p>Currently, you must manually mount your container prior to use on any node you are working on. In the future, we hope to automate some of this via additional <code>qsub</code> options.</p>"},{"location":"aurora/data-management/daos/daos-overview/#to-mount-a-posix-container-on-a-login-node","title":"To mount a POSIX container on a login node","text":"<pre><code>mkdir \u2013p /tmp/${DAOS_POOL}/${DAOS_CONT}\nstart-dfuse.sh -m /tmp/${DAOS_POOL}/${DAOS_CONT} --pool ${DAOS_POOL} --cont ${DAOS_CONT} # To mount\nmount | grep dfuse # To confirm if its mounted\n\n# Mode 1\nls /tmp/${DAOS_POOL}/${DAOS_CONT} \ncd /tmp/${DAOS_POOL}/${DAOS_CONT} \ncp ~/temp.txt ~ /tmp/${DAOS_POOL}/${DAOS_CONT}/ \ncat /tmp/${DAOS_POOL}/${DAOS_CONT}/temp.txt\n\nfusermount3 -u /tmp/${DAOS_POOL}/${DAOS_CONT} # To unmount\n</code></pre>"},{"location":"aurora/data-management/daos/daos-overview/#to-mount-a-posix-container-on-compute-nodes","title":"To mount a POSIX container on Compute Nodes","text":"<p>You need to mount the container on all compute nodes.</p> <p><pre><code>launch-dfuse.sh ${DAOS_POOL_NAME}:${DAOS_CONT_NAME} # launched using pdsh on all compute nodes mounted at: /tmp/&lt;pool&gt;/&lt;container&gt;\nmount | grep dfuse # To confirm if its mounted\n\nls /tmp/${DAOS_POOL}/${DAOS_CONT}/\n\nclean-dfuse.sh  ${DAOS_POOL_NAME}:${DAOS_CONT_NAME} # To unmount on all nodes \n</code></pre> DAOS Data mover instruction is provided at here.</p>"},{"location":"aurora/data-management/daos/daos-overview/#job-submission","title":"Job Submission","text":"<p>The <code>-l filesystems=daos_user</code> and <code>-l daos=daos_user</code> switch will ensure that DAOS is accessible on the compute nodes.</p> <p>Job submission without requesting DAOS: <pre><code>qsub -l select=1 -l walltime=01:00:00 -A &lt;ProjectName&gt; -k doe -l filesystems=flare -q debug ./pbs_script1.sh  or - I \n</code></pre></p> <p>Job submission with DAOS:  <pre><code>qsub -l select=1 -l walltime=01:00:00 -A &lt;ProjectName&gt; -k doe -l filesystems=flare:daos_user -l daos=daos_user -q debug ./pbs_script1.sh  or - I \n</code></pre></p>"},{"location":"aurora/data-management/daos/daos-overview/#nic-and-core-binding","title":"NIC and Core Binding","text":"<p>Each Aurora compute node has 8 NICs and each DAOS server node has 2 NICs.  Each NIC is capable of driving 20-25 GB/s unidirection for data transfer.  Every read and write goes over the NIC and hence NIC binding is the key to achieve good performance. </p> <p>For 12 PPN, the following binding is recommended: <pre><code>CPU_BINDING1=list:4:9:14:19:20:25:56:61:66:71:74:79\n</code></pre> </p>"},{"location":"aurora/data-management/daos/daos-overview/#interception-library-for-posix-containers","title":"Interception library for POSIX containers","text":"<p>The interception library (IL) is a next step in improving DAOS performance. This provides kernel-bypass for I/O data, leading to improved performance. The libioil IL will intercept basic read and write POSIX calls while all metadata calls still go through dFuse. The libpil4dfs IL should be used for both data and metadata calls to go through dFuse. The IL can provide a large performance improvement for bulk I/O as it bypasses the kernel and commuNICates with DAOS directly in userspace. It will also take advantage of the multiple NICs on the node based on how many MPI processes are running on the node and which CPU socket they are on.</p> <p></p> <pre><code>Interception library for POSIX mode \n\nmpiexec                                            # no interception\nmpiexec --env LD_PRELOAD=/usr/lib64/libioil.so     # only data is intercepted \nmpiexec --env LD_PRELOAD=/usr/lib64/libpil4dfs.so  # preferred - both metadata and data is intercepted. This provides close to DFS mode performance.\n</code></pre>"},{"location":"aurora/data-management/daos/daos-overview/#sample-job-script","title":"Sample job script","text":"<p>Currently, <code>--no-vni</code> is required in the <code>mpiexec</code> command to use DAOS. </p> <pre><code>#!/bin/bash -x\n#PBS -l select=512\n#PBS -l walltime=01:00:00\n#PBS -A &lt;ProjectName&gt;\n#PBS -q prod\n#PBS -k doe\n#PBS -l filesystems=flare:daos_user\n#PBS -l daos=daos_user\n\n# qsub -l select=512:ncpus=208 -l walltime=01:00:00 -A &lt;ProjectName&gt; -l filesystems=flare:daos_user -l daos=daos_user -q prod ./pbs_script.sh or - I \n\n# please do not miss -l filesystems=daos_user and -l daos=daos_user in your qsub :'(\n\nexport TZ='/usr/share/zoneinfo/US/Central'\ndate\nmodule use /soft/modulefiles\nmodule load daos\nenv | grep DRPC                                     #optional\nps -ef|grep daos                                    #optional\nclush --hostfile ${PBS_NODEFILE}  'ps -ef|grep agent|grep -v grep'  | dshbak -c  #optional\nDAOS_POOL=datascience\nDAOS_CONT=thundersvm_exp1\ndaos pool query ${DAOS_POOL}                        #optional\ndaos cont list ${DAOS_POOL}                         #optional\ndaos container destroy   ${DAOS_POOL}  ${DAOS_CONT} #optional\ndaos container create --type POSIX ${DAOS_POOL}  ${DAOS_CONT} --properties rd_fac:1 \ndaos container query     ${DAOS_POOL}  ${DAOS_CONT} #optional\ndaos container get-prop  ${DAOS_POOL}  ${DAOS_CONT} #optional\ndaos container list      ${DAOS_POOL}               #optional\nlaunch-dfuse.sh ${DAOS_POOL}:${DAOS_CONT}           # To mount on a compute node \n\n# mkdir -p /tmp/${DAOS_POOL}/${DAOS_CONT}           # To mount on a login node\n# start-dfuse.sh -m /tmp/${DAOS_POOL}/${DAOS_CONT}     --pool ${DAOS_POOL} --cont ${DAOS_CONT}  # To mount on a login node\n\nmount|grep dfuse                                    #optional\nls /tmp/${DAOS_POOL}/${DAOS_CONT}                   #optional\n\n# cp /lus/flare/projects/CSC250STDM10_CNDA/kaushik/thundersvm/input_data/real-sim_M100000_K25000_S0.836 /tmp/${DAOS_POOL}/${DAOS_CONT} #one time\n# daos filesystem copy --src /lus/flare/projects/CSC250STDM10_CNDA/kaushik/thundersvm/input_data/real-sim_M100000_K25000_S0.836 --dst daos://tmp/${DAOS_POOL}/${DAOS_CONT}  # check https://docs.daos.io/v2.4/testing/datamover/ \n\n\ncd $PBS_O_WORKDIR\necho Jobid: $PBS_JOBID\necho Running on nodes `cat $PBS_NODEFILE`\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nRANKS_PER_NODE=12          # Number of MPI ranks per node\nNRANKS=$(( NNODES * RANKS_PER_NODE ))\necho \"NUM_OF_NODES=${NNODES}  TOTAL_NUM_RANKS=${NRANKS}  RANKS_PER_NODE=${RANKS_PER_NODE}\"\nCPU_BINDING1=list:4:9:14:19:20:25:56:61:66:71:74:79\n\nexport THUN_WS_PROB_SIZE=1024\nexport ZE_FLAT_DEVICE_HIERARCHY=COMPOSITE\nexport AFFINITY_ORDERING=compact\nexport RANKS_PER_TILE=1\nexport PLATFORM_NUM_GPU=6\nexport PLATFORM_NUM_GPU_TILES=2\n\n\ndate \nLD_PRELOAD=/usr/lib64/libpil4dfs.so mpiexec -np ${NRANKS} -ppn ${RANKS_PER_NODE} --cpu-bind ${CPU_BINDING1}  \\\n                                            --no-vni -genvall  thunder/svm_mpi/run/aurora/wrapper.sh thunder/svm_mpi/build_ws1024/bin/thundersvm-train \\\n                                            -s 0 -t 2 -g 1 -c 10 -o 1  /tmp/datascience/thunder_1/real-sim_M100000_K25000_S0.836 \ndate\n\nclean-dfuse.sh ${DAOS_POOL}:${DAOS_CONT} #to unmount on compute node\n# fusermount3 -u /tmp/${DAOS_POOL}/${DAOS_CONT} #to unmount on login node\n</code></pre>"},{"location":"aurora/data-management/daos/daos-overview/#mpi-io-mode","title":"MPI-IO Mode","text":"<p>Mode 2 </p> <p>The ROMIO MPI-IO layer provides multiple I/O backends including a custom DAOS backend. MPI-IO can be used with dFuse and the interception library when using the <code>ufs</code> backend but the <code>daos</code> backend will provide optimal performance. In order to use this, one can prefix the file names with <code>daos:</code> which will tell MPI-IO to use the DAOS backend.</p> <pre><code>export ROMIO_PRINT_HINTS=1\n\necho \"cb_nodes 128\" &gt;&gt; ${PBS_O_WORKDIR}/romio_hints\n\nmpiexec  --env ROMIO_HINTS = romio_hints_file program daos:/mpi_io_file.data\n\nor\n\nmpiexec  --env MPICH_MPIIO_HINTS = path_to_your_file*:cb_config_list=#*:2#\n       :romio_cb_read=enable\n       :romio_cb_write=enable\n       :cb_nodes=32 \n       program daos:/mpi_io_file.data\n</code></pre>"},{"location":"aurora/data-management/daos/daos-overview/#dfs-mode","title":"DFS Mode","text":"<p>Mode 3</p> <p>DFS is the user level API for DAOS. This API is very similar to POSIX but still has many differences that would require code changes to utilize DFS directly. The DFS API can provide the best overall performance for any scenario other than workloads which benefit from caching.</p> <p>Reference code for using DAOS through DFS mode and DAOS APIs Full code at <code>/soft/daos/examples/src</code></p> <pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;mpi.h&gt;\n#include &lt;daos.h&gt;\n#include &lt;daos_fs.h&gt;\nint main(int argc, char **argv)\n{\n    dfs_t *dfs;\n    d_iov_t global;\n    ret = MPI_Init(&amp;argc, &amp;argv);   \n    ret = MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);\n\n    ret = dfs_init();\n\n    ret = dfs_connect(getenv(\"DAOS_POOL\"), NULL, getenv(\"DAOS_CONT\"), O_RDWR, NULL, &amp;dfs);    \n    ret = dfs_open(dfs, NULL, filename, S_IFREG|S_IRUSR|S_IWUSR,  O_CREAT|O_WRONLY,  obj_class, chunk_size, NULL, &amp;obj);\n    ret = dfs_write(dfs, obj, &amp;sgl, off, NULL);\n    ret = dfs_read(dfs, obj, &amp;sgl, off, &amp;read, NULL);\n    ret = dfs_disconnect(dfs);\n    ret = daos_fini();\n    ret = MPI_Finalize(); \n}\n</code></pre>"},{"location":"aurora/data-management/daos/daos-overview/#daos-hardware","title":"DAOS Hardware","text":"<p>Each DAOS server nodes is based on the Intel Coyote Pass platform. * (2) Xeon 5320 CPU (Ice Lake) * (16) 32GB DDR4 DIMMs * (16) 512GB Intel Optane Persistent Memory 200 * (16) 15.3TB Samsung PM1733 NVMe * (2) HPE Slingshot NIC</p> <p></p>"},{"location":"aurora/data-management/daos/daos-overview/#darshan-profiler-for-daos","title":"Darshan profiler for DAOS","text":"<p>Currently, you need to install your own local darshan-daos profiler You need to use DFS mode (3) or POSIX with interception library to profile</p> <pre><code>module use /soft/modulefiles\nmodule load daos\nmodule list\ngit clone https://github.com/darshan-hpc/darshan.git\ngit checkout snyder/dev-daos-module-3.4\n./prepare.sh \nmkdir /home/kaushikvelusamy/soft/profilers/darshan-daos/darshan-install\n\n./configure --prefix=/home/kaushikvelusamy/soft/profilers/darshan-daos/darshan-install  \\\n            --with-log-path=/home/kaushikvelusamy/soft/profilers/darshan-daos/darshan-logs \\\n            --with-jobid-env=PBS_JOBID \\\n            CC=mpicc --enable-daos-mod\n\nmake &amp;&amp; make install \n\nchmod 755 ~/soft/profilers/darshan-daos/darshan/darshan-install/darshan-mk-log-dirs.pl\nmkdir /home/kaushikvelusamy/soft/profilers/darshan-daos/darshan-logs\ncd /home/kaushikvelusamy/soft/profilers/darshan-daos/darshan-logs\n~/soft/profilers/darshan-daos/darshan/darshan-install/darshan-mk-log-dirs.pl\n~/soft/profilers/darshan-daos/darshan-install/bin/darshan-config  --log-path\n</code></pre> <p>Preload darshan first then DAOS interception library:</p> <pre><code>mpiexec --env LD_PRELOAD=~/soft/profilers/darshan-daos/darshan-install/lib/libdarshan.so:/usr/lib64/libpil4dfs.so   \n        -np 32 -ppn 16  --no-vni -genvall \\\n        ior -a DFS  --dfs.pool=datascience_ops --dfs.cont=ior_test1   \\\n            -i 5 -t 16M -b 2048M  -w  -r -C -e    -c  -v -o /ior_2.dat \n</code></pre> <p>Install <code>darshan-util</code> from laptop:</p> <pre><code>conda info \u2013envs\nconda activate env-non-mac-darshan-temp\n/Users/kvelusamy/Desktop/tools/spack/share/spack/setup-env.sh \n\nspack install darshan darshan-util\n\nexport DYLD_FALLBACK_LIBRARY_PATH=/Users/kvelusamy/Desktop/tools/spack/opt/spack/darwin-ventura-m1/apple-clang-14.0.3/darshan-util-3.4.4-od752jyfljrrey3d4gjeypdcppho42k2/lib/:$DYLD_FALLBACK_LIBRARY_PATH\n\ndarshan-parser ~/Downloads/kaushikv_ior_id917110-44437_10-23-55830-632270104473632905_1.darshan \npython3 -m darshan summary ~/Downloads/kaushikv_ior_id917110-44437_10-23-55830-632270104473632905_1.darshan #coming soon\n</code></pre>"},{"location":"aurora/data-management/daos/daos-overview/#cluster-size","title":"Cluster Size","text":"<p>DAOS cluster size is the number of available DAOS servers. While we are working towards bringing up the entire 1024 DAOS server available users, currently different number of DAOS nodes could be up. Please check with support or run an IOR test to get an estimate on the current number of DAOS servers available. The bandwidth listed here in the last column is a theoretical peak bandwidth.</p> <p></p>"},{"location":"aurora/data-management/daos/daos-overview/#best-practices","title":"Best practices","text":"<pre><code>Check that you requested DAOS\n    qsub \u2013l filesystems=daos_user -l daos=daos_user\nDid you load DAOS module?\n    module load daos\nDo you have your DAOS pool allocated?\n    daos pool query datascience\nIs DAOS client running on all your nodes? \n    ps \u2013ef | grep daos   \nIs your container mounted on all nodes?\n    mount | grep dfuse  \nCan you ls in your container?                 \n    ls /tmp/${DAOS_POOL}/${DAOS_CONT}  \nDid your I/O actually fail?\nWhat is the health property in your container?  \n    daos container get-prop $DAOS_POOL $CONT    \nIs your space full? Min and max\n    daos pool query datascience\nDoes your query show failed targets or rebuild in process?\n    daos pool query datascience\n    daos pool autotest\n    daos container check \n</code></pre>"},{"location":"aurora/data-management/lustre/flare/","title":"Flare Filesystem","text":"<p>Flare is a 91 PB Lustre Filesystem with 160 OSTs, 40 MDTs, and 48 Gateway nodes mounted at <code>/lus/flare/projects/</code> with a peak theoretical performance of 650 GB/s. You should launch jobs only from this Flare space.</p> <p>Home is a 12 PB Gecko Lustre Filesystem with 32 OSTs and 12 MDTs.</p> <p>Follow this link for more basic information on I/O optimization for the Lustre Filesystem I/O</p>"},{"location":"aurora/data-management/moving_data_to_aurora/daos_datamover/","title":"To move data to your DAOS POSIX container","text":""},{"location":"aurora/data-management/moving_data_to_aurora/daos_datamover/#using-cp","title":"Using <code>cp</code>","text":"<pre><code>cp /lus/flare/projects/CSC250STDM10_CNDA/kaushik/thundersvm/input_data/real-sim_M100000_K25000_S0.836 /tmp/${DAOS_POOL}/${DAOS_CONT}\n</code></pre>"},{"location":"aurora/data-management/moving_data_to_aurora/daos_datamover/#using-daos-filesystem-copy","title":"Using DAOS filesystem copy","text":"<pre><code>daos filesystem copy --src /lus/flare/projects/CSC250STDM10_CNDA/kaushik/thundersvm/input_data/real-sim_M100000_K25000_S0.836 --dst daos://tmp/${DAOS_POOL}/${DAOS_CONT}\n</code></pre> <p>You may have to replace the <code>DAOS_POOL</code> and <code>DAOS_CONT</code> labels with their UUIDs. UUIDs can be copied from:</p> <pre><code>daos pool query ${DAOS_POOL}\ndaos container query $DAOS_POOL_NAME $DAOS_CONT_NAME\n</code></pre>"},{"location":"aurora/data-management/moving_data_to_aurora/daos_datamover/#using-mpifileutils-distributed-cp-dcp","title":"Using mpifileutils distributed <code>cp</code> (DCP)","text":"<p>You can also use other mpifileutils binaries.</p> <pre><code>mpifileutils/bin&gt; ls\ndbcast  dbz2  dchmod  dcmp  dcp  dcp1  ddup  dfilemaker1  dfind  dreln  drm  dstripe  dsync  dtar  dwalk\n</code></pre> <p>Ref: DAOS Data Mover Documentation</p>"},{"location":"aurora/data-management/moving_data_to_aurora/globus/","title":"Transferring Files through Globus","text":""},{"location":"aurora/data-management/moving_data_to_aurora/globus/#during-acceptance-testing-period","title":"During Acceptance Testing Period","text":"<p>We have set up a temporary Globus endpoint for Flare that you can use to transfer data out. The endpoint is called \"alcf#dtn_flare_at\" and is set up for read-only access. The endpoint will be available throughout the acceptance testing (AT) period and will be shut down after AT concludes.</p>"},{"location":"aurora/data-management/moving_data_to_aurora/globus/#before-acceptance-testing","title":"Before Acceptance Testing","text":"<p>Currently, only Globus Personal is supported on Aurora. Perform the following steps to transfer data to/from the Aurora login nodes.</p> <ol> <li> <p>On a fresh connection to the login nodes, ensure no proxies are being set (which may require commenting out the proxy settings in the <code>~/.bashrc</code> or <code>~/.bash_profile</code> files), and execute:</p> <pre><code>/soft/tools/proxychains/bin/proxychains4 -f /soft/tools/proxychains/etc/proxychains.conf /soft/tools/globusconnect/globusconnect -setup --no-gui\n</code></pre> </li> <li> <p>Paste the link provided by the above command into a browser and follow the instructions to set up a personal endpoint:</p> <ul> <li>When requested, input your ALCF username and one-time password from your CRYPTOCard/MobilePASS+ token.</li> <li>Select the Allow button.</li> <li>Enter the authentication code generated back into the terminal.</li> <li>Enter a name for the endpoint (e.g., <code>aurora_login_uan11</code>).</li> </ul> </li> <li> <p>On the same terminal, execute:</p> <pre><code>/soft/tools/proxychains/bin/proxychains4 -f /soft/tools/proxychains/etc/proxychains.conf /soft/tools/globusconnect/globusconnect -start &amp;\n</code></pre> <ul> <li>By default, the command only gives access to your home directory.</li> <li>You can add <code>-restrict-paths /lus/flare/projects/YOURPROJECT</code> to access your project directory.</li> </ul> </li> <li> <p>Open the Globus web app and search for the endpoint name defined above. You will now see your home directory (and project directory, if requested) on Aurora and can initiate transfers with other endpoints (e.g., the Eagle file system on Polaris at <code>alcf#dtn_eagle</code>).</p> </li> </ol>"},{"location":"aurora/data-management/moving_data_to_aurora/scp/","title":"Data Transfer Using <code>scp</code>","text":"<p>Use <code>scp</code> and <code>SFTP</code> to transfer data to/from Aurora.</p>"},{"location":"aurora/data-management/moving_data_to_aurora/scp/#transferring-files-from-aurora-flare-to-sunspot-gila","title":"Transferring files from Aurora (Flare) to Sunspot (Gila)","text":"<p>From an Aurora login node, you can transfer files to Sunspot's <code>gila</code> file system using the <code>scp</code> command.</p> <p>First, you need to create an SSH key pair on Aurora and copy the public key (<code>*.pub</code>) to the <code>~/.ssh/authorized_keys</code> file on Sunspot.</p> <ol> <li> <p>Create SSH keys on the laptop/desktop/remote machine.    See Creating SSH Keys.</p> </li> <li> <p>Copy the public key (<code>*.pub</code>) from the <code>~/.ssh</code> folder on Aurora to the <code>~/.ssh/authorized_keys</code> file on Sunspot (<code>sunspot.alcf.anl.gov</code>).</p> </li> <li> <p>Run the <code>scp</code> command on Aurora to transfer files to Sunspot.</p> </li> <li> <p>Copy the contents of the public key file (<code>*.pub</code>) located in the <code>~/.ssh/</code> folder on Aurora, and append it to the <code>~/.ssh/authorized_keys</code> file on Sunspot (<code>sunspot.alcf.anl.gov</code>).</p> </li> <li> <p>Run the <code>scp</code> command on Aurora to transfer files to Sunspot:</p> <pre><code>src=\"test_file\"\ndst=\"/lus/gila/projects/Aurora_deployment/username\"\nscp test_file username@sunspot.alcf.anl.gov:$dst\n</code></pre> <pre><code># then, from Sunspot:\n$ cat test_file\nthis is a test file\n</code></pre> </li> </ol>"},{"location":"aurora/data-management/moving_data_to_aurora/scp/#transferring-files-to-aurora-flare","title":"Transferring files to Aurora (Flare)","text":"<p>With the bastion pass-through nodes currently used to access both Sunspot and Aurora, users will find it helpful to modify their <code>.ssh/config</code> files on Aurora appropriately to facilitate transfers to Aurora from other ALCF systems.</p> <p>These changes are similar to what Sunspot users may have already implemented.</p> <p>From an Aurora login node, this readily enables one to transfer files from Sunspot's <code>gila</code> filesystem or one of the production filesystems at ALCF (<code>home</code> and <code>eagle</code>) mounted on an ALCF system's login node.</p> <p>With the use of <code>ProxyJump</code> below, entering the MobilePass+ or Cryptocard passcode twice will be needed (once for bastion and once for the other resource).</p> <pre><code>username@aurora-uan-0009:~&gt; cat .ssh/config\nHost bastion.alcf.anl.gov\n    User username\n\nHost polaris.alcf.anl.gov\n    ProxyJump bastion.alcf.anl.gov\n    DynamicForward 3142\n    User username\n</code></pre> <pre><code>username@aurora-uan-0009:~&gt; scp username@polaris.alcf.anl.gov:/eagle/catalyst/proj-shared/username/test.txt ./\n---------------------------------------------------------------------------\n                            Notice to Users\n...\n[Password:\n---------------------------------------------------------------------------\n                            Notice to Users\n... \n[Password:\nusername@aurora-uan-0009:~&gt; cat test.txt \nfrom_polaris eagle\n</code></pre>"},{"location":"aurora/data-management/moving_data_to_aurora/scp/#scp-examples","title":"<code>scp</code>: Examples<sup>1</sup>","text":"<ul> <li> <p>Copy a local file to a remote host:</p> <pre><code>scp path/to/local_file remote_host:path/to/remote_file\n</code></pre> </li> <li> <p>Use a specific port when connecting to the remote host:</p> <pre><code>scp -P port path/to/local_file remote_host:path/to/remote_file\n</code></pre> </li> <li> <p>Copy a file from a remote host to a local directory:</p> <pre><code>scp remote_host:path/to/remote_file path/to/local_directory\n</code></pre> </li> <li> <p>Recursively copy the contents of a directory from a remote host to a local directory:</p> <pre><code>scp -r remote_host:path/to/remote_directory path/to/local_directory\n</code></pre> </li> <li> <p>Copy a file between two remote hosts transferring through the local host:</p> <pre><code>scp -3 host1:path/to/remote_file host2:path/to/remote_directory\n</code></pre> </li> <li> <p>Use a specific username when connecting to the remote host:</p> <pre><code>scp path/to/local_file remote_username@remote_host:path/to/remote_directory\n</code></pre> </li> <li> <p>Use a specific SSH private key for authentication with the remote host:</p> <pre><code>scp -i ~/.ssh/private_key path/to/local_file remote_host:path/to/remote_file\n</code></pre> </li> <li> <p>Use a specific proxy when connecting to the remote host:</p> <pre><code>scp -J proxy_username@proxy_host path/to/local_file remote_host:path/to/remote_file\n</code></pre> </li> </ul> <ol> <li> <p>Examples copied from: scp \u21a9</p> </li> </ol>"},{"location":"aurora/data-science/julia/","title":"Julia on Aurora","text":""},{"location":"aurora/data-science/jupyter/","title":"Running JupyterLab and Jupyter Notebook on Aurora","text":"<p>ALCF provides a JupyterHub for running JupyterLab and Jupyter Notebook on Polaris and Sophia with minimal setup required from the user. While this service is not yet available on Aurora, users can still run JupyterLab and Notebook using SSH tunneling and port forwarding.</p>"},{"location":"aurora/data-science/jupyter/#prerequisites","title":"Prerequisites","text":"<p>Please note that you only need a terminal (to SSH into Aurora) and a browser on your local machine (laptop or desktop). All the required packages need to be installed on Aurora only.</p>"},{"location":"aurora/data-science/jupyter/#1-initial-setup","title":"1. Initial Setup","text":"<ol> <li> <p>SSH into Aurora:    The first step is to connect to Aurora through SSH. Note that tunneling or port forwarding is not required for this step.    If you have a problem with this step, please check Getting Started on Aurora.    <pre><code>ssh &lt;your-username&gt;@aurora.alcf.anl.gov\n</code></pre></p> </li> <li> <p>Create a Virtual Environment:    If you already have a Python environment, you can skip this step.    <pre><code>python3 -m venv myenv\n</code></pre>    Note that this command will create a directory called <code>myenv</code> in the current working directory, and <code>myenv</code> will also be the name of the virtual environment. You can change the name as you see fit.    You can find more information about creating a virtual environment on Aurora here.</p> </li> <li> <p>Install Required Packages:    The third step is to install <code>jupyterlab</code> and/or <code>notebook</code> as well as <code>ipykernel</code>.    Note that you need to activate the virtual environment before installing the packages. Here we assume that the virtual environment is named <code>myenv</code> and is located in the current working directory.    <pre><code>source myenv/bin/activate\npip install jupyterlab notebook ipykernel\n</code></pre></p> </li> <li> <p>Install IPython Kernel:    The fourth step is to install the IPython kernel for the current virtual environment. IPython kernels enable easily switching between different Python environments on JupyterLab and Notebook.    <pre><code>python -m ipykernel install --user --name myenv\n</code></pre>    Steps 2, 3, and 4 are only required once for each virtual environment.</p> </li> </ol>"},{"location":"aurora/data-science/jupyter/#2-run-jupyterlab-on-a-login-node","title":"2. Run JupyterLab on a Login Node","text":"<p>Warning</p> <p>This is not recommended for compute-intensive or memory-intensive workloads. Run the JupyterLab server on a compute node (see below section) if the workload is heavy.</p> <ol> <li> <p>Start JupyterLab:    <pre><code>source myenv/bin/activate\njupyter lab --no-browser --port=9999\n</code></pre></p> </li> <li> <p>Copy the Address:    The address will be displayed as the output of the previous command.    <pre><code>http://127.0.0.1:9999/?token=&lt;provided-token&gt;\n</code></pre></p> </li> <li> <p>Set Up SSH Tunneling:    Open a new terminal tab or window on your local machine and run the following command:    <pre><code>ssh -L 9999:127.0.0.1:9999 &lt;your-username&gt;@aurora.alcf.anl.gov\n</code></pre></p> </li> <li> <p>Replace <code>9999</code> with another port if it is unavailable.</p> </li> <li> <p>Access JupyterLab:    Open your browser and navigate to the address copied above:    <pre><code>http://localhost:9999/?token=&lt;provided-token&gt;\n</code></pre></p> </li> </ol>"},{"location":"aurora/data-science/jupyter/#3-run-jupyterlab-on-a-compute-node","title":"3. Run JupyterLab on a Compute Node","text":""},{"location":"aurora/data-science/jupyter/#step-1-request-a-compute-node","title":"Step 1: Request a Compute Node","text":"<p>You need a job running on Aurora to launch JupyterLab on a compute node. Below is an example of how to submit an interactive job to request a compute node. Note that you can also connect to one of the compute nodes of any of your non-interactive jobs that is running on Aurora. See Running Jobs on Aurora for more information.</p> <ol> <li> <p>Submit a Job:    Submit an interactive job to request a compute node:    <pre><code>qsub -l select=1 -l walltime=60:00 -A &lt;project_name&gt; -q &lt;queue_name&gt; -I\n</code></pre>    You need to modify the <code>-A</code> and <code>-q</code> options to match your project name and queue name as well as the resources you need.</p> </li> <li> <p>Find Compute Node Hostname:    Once the interactive job starts, you will be connected to the compute node. You can check the hostname with:    <pre><code>hostname\n</code></pre>    If you are not running an interactive job, you can find the hostname of the compute node by checking the <code>qstat</code> output.    First, you need to find the job ID of the job you are interested in. The following command will list all the jobs you have submitted. You need the ID of any one of the running jobs you are interested in.    <pre><code>qstat -u &lt;your_username&gt;\n</code></pre>    Then, we can find the hostname of the compute node by running:    <pre><code>qstat -f &lt;job_id&gt;\n</code></pre>    The hostname of the compute node will be displayed in the <code>exec_host</code> field. You can extract the hostname from the output with the following command:    <pre><code>qstat -f &lt;job_id&gt; | awk -F '=' '/exec_host/ {print $2}' | tr '+' '\\n' | cut -d '/' -f 1\n</code></pre>    This should give you a hostname like <code>x4603c0s0b0n0</code>. Note that you can SSH into the compute nodes only when your job is running.</p> </li> </ol>"},{"location":"aurora/data-science/jupyter/#step-2-start-jupyterlab-on-the-compute-node","title":"Step 2: Start JupyterLab on the Compute Node","text":"<ol> <li> <p>SSH to the Compute Node:    From the login node:    <pre><code>ssh &lt;compute_node_hostname&gt;\n</code></pre></p> </li> <li> <p>Activate the Environment:    <pre><code>source &lt;path_to_your_virtual_environment&gt;/bin/activate\n</code></pre></p> </li> <li> <p>Start JupyterLab:    <pre><code>jupyter lab --no-browser --port=9999\n</code></pre></p> </li> </ol>"},{"location":"aurora/data-science/jupyter/#step-3-set-up-ssh-tunneling","title":"Step 3: Set Up SSH Tunneling","text":"<ol> <li> <p>Tunnel from Compute Node to Local Machine:    On your local machine, run:    <pre><code>ssh -L 9999:127.0.0.1:9999 -J &lt;your-username&gt;@aurora.alcf.anl.gov &lt;your-username&gt;@&lt;compute_node_hostname&gt;\n</code></pre>    Please note that the <code>-J</code> option is used to specify the jump host, which is the Aurora login node.    Replace <code>9999</code> with another port if it is unavailable.</p> </li> <li> <p>Access JupyterLab:    Open your browser and navigate to:    <pre><code>http://localhost:9999/?token=&lt;your-token&gt;\n</code></pre></p> </li> </ol> <p>Tip</p> <p>You can use <code>tmux</code> or <code>screen</code> to keep JupyterLab running if the SSH connection drops.</p>"},{"location":"aurora/data-science/profiling_dl/","title":"Profiling Deep Learning Applications","text":"<p>On Aurora, we can use the <code>unitrace</code> profiler from Intel to profile deep learning applications. Refer to the <code>unitrace</code> documentation page for details.</p>"},{"location":"aurora/data-science/profiling_dl/#example-usage","title":"Example Usage","text":"<p>We can use <code>unitrace</code> to trace an application running on multiple ranks and multiple nodes. A simple example, where we use a wrapper script to trace rank 0 on each node of a 4-node job running a PyTorch application, is below.</p> <p>There are several important shell variables in the wrapper, which may require modification:</p> <pre><code>#!/bin/bash\n## This wrapper should be used with unitrace to trace in any number of nodes.\n## The script for this example is set up to trace rank 0 of the first 4 nodes in the case of\n## profiling a job running on more than 4 nodes.\nFNAME_EXT=$(basename \"$2\")\nFNAME=\"${FNAME_EXT%%.*}\"\n\nNNODES=`wc -l &lt; $PBS_NODEFILE`\n\nWORK_DIR=/path/to/the/Python/program\nUNITRACE_DIR=/opt/aurora/24.180.1/support/tools/pti-gpu/063214e # (1)!\nUNITRACE_LIB=${UNITRACE_DIR}/lib64\nUNITRACE_BIN=${UNITRACE_DIR}/bin\nUNITRACE_EXE=${UNITRACE_BIN}/unitrace\nDTAG=$(date +%F_%H%M%S)\nUNITRACE_OUTDIR=${WORK_DIR}/logs/unitrace_profiles/name_of_choice_json_n${NNODES}_${DTAG}/${FNAME}_n${NNODES}_${DTAG}\nmkdir -p ${UNITRACE_OUTDIR}\nUNITRACE_OPTS=\" --ccl-summary-report --chrome-mpi-logging --chrome-sycl-logging \\\n--chrome-device-logging \\\n--chrome-ccl-logging --chrome-call-logging --chrome-dnn-logging --device-timing --host-timing \\\n--output-dir-path ${UNITRACE_OUTDIR} --output ${UNITRACE_OUTDIR}/UNITRACE_${FNAME}_n${NNODES}_${DTAG}.txt \"  # (2)!\n\nexport LD_LIBRARY_PATH=${UNITRACE_LIB}:${UNITRACE_BIN}:$LD_LIBRARY_PATH\n\n# Use $PMIX_RANK for MPICH and $SLURM_PROCID with srun.\nPROFRANK=0 # (3)!\nRANKCUTOFF=48 # (4)!\n\nif [[ $PALS_LOCAL_RANKID -eq $PROFRANK ]] &amp;&amp; [[ $PMIX_RANK -lt $RANKCUTOFF ]]; then\n  echo \"On rank $PMIX_RANK, collecting traces \"\n  $UNITRACE_EXE $UNITRACE_OPTS \"$@\"\nelse\n  \"$@\"\nfi\n</code></pre> <ol> <li><code>UNITRACE_DIR</code>: This is the main <code>unitrace</code> directory, which may change after an update to the programming environment.</li> <li><code>UNITRACE_OPTS</code>: These are the options that <code>unitrace</code> uses to trace data at different levels. Based on the number of options, the sizes of the output profiles will vary. Usually, enabling more options leads to a larger profile (in terms of storage in MB).</li> <li><code>PROFRANK</code>: As implemented, this variable is set by the user to trace the rank of choice. For example, this wrapper will trace rank 0 on each node.</li> <li><code>RANKCUTOFF</code>: This variable is Aurora-specific. As we can run as many as 12 ranks per node (without using CCS), the first 4 nodes of a job will have 48 ranks running. This provides the upper cutoff of the label (in number) of ranks, beyond which <code>unitrace</code> will not trace any rank. A user can change the number according to the number of maximum ranks running per node to set up how many ranks to be traced. <code>unitrace</code> will produce a profile (<code>json</code> file, by default) per traced rank. This profile can be viewed using the Perfetto trace viewer.</li> </ol>"},{"location":"aurora/data-science/profiling_dl/#deployment","title":"Deployment","text":"<p>The wrapper above can be deployed using the following PBS job script:</p> job_script.sh<pre><code>#!/bin/bash -x\n#PBS -l select=4\n#PBS -l place=scatter\n#PBS -l walltime=00:10:00\n#PBS -q debug-scaling\n#PBS -l filesystems=&lt;fs1:fs2&gt;\n#PBS -A &lt;ProjectName&gt;\n\nWORK_DIR=/path/to/the/Python/program\nUNITRACE_WRAPPER=${WORK_DIR}/unitrace_wrapper.sh\n\n# MPI and OpenMP settings\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS_PER_NODE=12\n\nlet NRANKS=${NNODES}*${NRANKS_PER_NODE}\n\nmodule load frameworks/2024.2.1_u1\n\nmpiexec --pmi=pmix -n ${NRANKS} -ppn ${NRANKS_PER_NODE} -l --line-buffer \\\n${UNITRACE_WRAPPER} python ${WORK_DIR}/application.py \n</code></pre>"},{"location":"aurora/data-science/python/","title":"Python on Aurora","text":"<p>Importing Python modules at scale</p> <p>We have system-installed frameworks modules described in this page, which contain common AI/ML packages such as PyTorch and TensorFlow. If a custom package or virtual environment is installed in your own home or project directory, it is highly recommended to use the Copper package to help reduce I/O overhead when importing Python modules at large node counts. We have seen that beyond 1000 nodes, importing Python modules from a home or Lustre project directory might be significantly slower, or it may even crash the Lustre file system. Please refer to Copper for detailed instructions on loading custom-installed Python packages using Copper.</p> <p>Info</p> <p>If you only use packages from the system installed framework module, Copper is not needed. </p>"},{"location":"aurora/data-science/python/#aiml-framework-module","title":"AI/ML Framework Module","text":"<p>For most Python users on Aurora, a good starting point is the AI/ML framework module.  The Anaconda environment loaded with this module makes available TensorFlow, Horovod, and PyTorch with Intel extensions and optimizations, among other popular Python and ML packages. </p> <p>The following command can be used both from an interactive session on a terminal and within a batch job script to load the latest module <pre><code>module load frameworks\n</code></pre></p> <p>Please note that:</p> <ul> <li>The <code>frameworks</code> module automatically activates a pre-built <code>conda</code> environment which comes with GPU-supported builds of PyTorch and TensorFlow. Both of these frameworks have <code>Horovod</code> support for multi-node calculations, as well as PyTorch DDP with oneCCL.</li> <li>The <code>frameworks</code> module may load a different oneAPI compiler SDK than the default module</li> <li>The <code>frameworks</code> module is updated approximately every quarter</li> </ul> <p>For more information on PyTorch and TensorFlow on Aurora, please see the respective pages: </p> <ul> <li>PyTorch</li> <li>TensorFlow</li> </ul>"},{"location":"aurora/data-science/python/#virtual-environments-via-venv","title":"Virtual environments via <code>venv</code>","text":"<p>While the Anaconda environment automatically loaded with the <code>frameworks</code> module contains many  of the most commonly used Python packages for our users, you may still  encounter a scenario in which you need to extend the functionality of the  environment (i.e. install additional packages). In this case, we suggest the use of Python virtual environments. </p> <p>Warning</p> <p>There are several alternative approaches for extending or modifying the base Anaconda environments that are generally not recommended on ALCF machines. On Aurora, there are additional performance and functionality pitfalls with those approaches. More detailed information on the alternatives can be seen on the Polaris Python documentation.</p> <p>Creating and activating a new virtual environment (<code>venv</code>) is straightforward:</p> <pre><code>python3 -m venv /path/to/new/venv --system-site-packages\nsource /path/to/new/venv/bin/activate\n</code></pre> <p>The <code>--system-site-packages</code> flag will make sure that all the packages included in the <code>frameworks</code> module are available after sourcing the <code>venv</code>. If, however, you would like to create an empty <code>venv</code>, simply remove this flag. You can always retroactively change the <code>--system-site-packages</code> flag state for  this virtual environment by editing <code>venv/pyvenv.cfg</code> and changing the value  of <code>include-system-site-packages</code> to <code>true</code>.</p> <p>To install a different version of a package that is already installed in the  base environment, you can use: <pre><code>pip install --ignore-installed ... # or -I\n</code></pre> The base environment is not writable, so it is not possible to remove or  uninstall packages from it. The packages installed with the above <code>pip</code> command  should shadow those installed in the base environment.</p> <p>An alternative, although not recommended, approach to creating a <code>venv</code> is to install packages with <pre><code>pip install --user ...\n</code></pre> which will install packages in <code>$PYTHONUSERBASE/lib/pythonX.Y/site-packages</code>. Note that this approach may require the <code>PATH</code> environment variable to be modified with <code>export PATH=$PYTHONUSERBASE/bin:$PATH</code>. Cloning the Anaconda environment provided with the <code>frameworks</code> module, or using <code>venv</code> are both more flexible and transparent methods compared to <code>--user</code> installs.</p>"},{"location":"aurora/data-science/python/#intels-data-parallel-extensions-for-python-dpep","title":"Intel's Data Parallel Extensions for Python (DPEP)","text":"<p>On Aurora, users can access Intel's Python stack comprising of compilers and libraries for programming heterogenous devices, namely the Data Parallel Extensions for Python (DPEP). DPEP is composed of three main packages for programming on CPUs and GPUs:</p> <ul> <li>dpnp - Data Parallel Extensions for Numpy is a library that implements a subset of Numpy that can be executed on any data parallel device. The subset is a drop-in replacement of core Numpy functions and numerical data types, similar to CuPy for CUDA devices.</li> <li>dpctl - Data Parallel Control library provides utilities for device selection, allocation of data on devices, tensor data structure along with Python Array API Standard implementation, and support for creation of user-defined data-parallel extensions.</li> <li>numba_dpex - Data Parallel Extensions for Numba is an extension to Numba compiler for programming data-parallel devices similar to developing programs with Numba for CPU or CUDA devices.</li> </ul> <p>The DPEP packages follow the compute-follows-data programming model,  meaning that the offload target for a Python library call, or a hand-written kernel using numba-dpex,  does not need to be specified directly when making the call. Instead, the offload target is inferred from the input arguments to the library call. With this programming model, the user only needs to specify the offload target when creating the tensor/ndarray objects. Note that operating on arrays created on different devices will raise an exception.</p> <p>For example, <pre><code>import dpctl.tensor as dpt\n\nx_gpu = dpt.arange(100, device=\u201dgpu\u201d)\nsqx_gpu = dpt.square(x_gpu) # (1)!\nprint(sqx_gpu.device) # (2)!\n</code></pre></p> <ol> <li><code>dpct.square()</code> offloads to the \"gpu\" device</li> <li><code>sqx_gpu</code> is created on the \"gpu\" device</li> </ol> Output <pre><code>Device(level_zero:gpu:0)\n</code></pre>"},{"location":"aurora/data-science/python/#accessing-the-dpep-packages","title":"Accessing the DPEP Packages","text":"<p>Integration of DPEP packaged with AI/ML frameworks</p> <p>The current <code>frameworks</code> module does not come with the DPEP packages installed. Users need to install them separately as shown below. This will be addressed in the next <code>frameworks</code> module.</p> <p>Users can obtain the DPEP packages by executing</p> <pre><code>module load frameworks\nconda create -y --prefix /path/to/dpep_env python=3.11 pip\nconda activate /path/to/dpep_env\nconda install -y -c https://software.repos.intel.com/python/conda/ -c conda-forge dpctl dpnp numba-dpex\n</code></pre>"},{"location":"aurora/data-science/python/#dpnp","title":"dpnp","text":"<p>The dpnp library implements the Numpy API using DPC++ and is meant as a drop-in replacement for <code>numpy</code>. Following the minimal example below</p> <pre><code>import dpnp as np\n\nx = np.asarray([1, 2, 3]) # (1)!\nprint(\"Array x allocated on the device:\", x.device)\ny = np.sum(x) # (2)!\nprint(\"Result y is located on the device:\", y.device)\n</code></pre> <ol> <li><code>np.asarray()</code> creates an array on the default SYCL device, which is the Intal Max 1550 GPU on Aurora. The queue associated with this array is now carried with <code>x</code>, and the pre-compiled kernel for <code>np.sum(x)</code> is submitted to that queue. </li> <li>The result <code>y</code> is allocated on the device and is associated with the queue of <code>x</code>.</li> </ol> Output <pre><code>Array x allocated on the device: Device(level_zero:gpu:0)\nResult y is located on the device: Device(level_zero:gpu:0)\n</code></pre> <p>All dpnp array creation routines and random number generators have additional optional keyword arguments (device, queue, and usm_type) which users can leverage to explicitly specify on which device or queue they want the data to be created along with the USM memory type to be used.</p> <p>Changes after version 0.15.0</p> <p>For dpnp version &lt;= 0.15.0, all dpnp kernels are hard-coded to sync with the CPU after completion (i.e., <code>event.wait()</code> is inserted before returning).  From dpnp version &gt; 0.15.0, all kernels are run asynchronously, with linear ordering of groups of tasks (similar to CuPy).  This results in faster runtime and better GPU utilization.  To time kernels with dpnp &gt; 0.15.0, insert <code>.sycl_queue.wait()</code> before measuring the end time, for example <pre><code>import dpnp as np\nfrom time import perf_counter\n\nx = np.random.randn(1000,1000)\ntic = perf_counter()\ny = np.matmul(x,x)\ny.sycl_queue.wait()\nprint(f\"Execution time: {perf_counter() - tic} sec\")\n</code></pre></p>"},{"location":"aurora/data-science/python/#dpctl","title":"dpctl","text":"<p>The dpctl package lets users access devices supported by the DPC++ SYCL runtime.  The package exposes features such as device instrospection, execution queue creation, memory allocation, and kernel submission.  Below are some of the basic device management functions, but more functionality is available on the dpctl documentation.</p> <pre><code>import dpctl\n\ndpctl.lsplatform()\nnum_devices = dpctl.get_num_devices(device_type=\"gpu\") # (1)!\ndevice_list =  dpctl.get_devices(device_type=\"gpu\") # (2)!\nprint(f\"Found {num_devices} GPU devices\")\nfor device in device_list:\n    print(f\"\\t{device}\")\nprint(\"\\nFound CPU devices: \", dpctl.has_cpu_devices()) # (3)!\n</code></pre> <ol> <li>Get the number of GPU devices on the node</li> <li>Get the list of GPU devices on the node</li> <li>Check if CPU devices are available on the node</li> </ol> Output <pre><code>Intel(R) Level-Zero 1.5\nFound 12 GPU devices\n    &lt;dpctl.SyclDevice [backend_type.level_zero, device_type.gpu,  Intel(R) Data Center GPU Max 1550] at 0x154e8da03430&gt;\n    &lt;dpctl.SyclDevice [backend_type.level_zero, device_type.gpu,  Intel(R) Data Center GPU Max 1550] at 0x154e8da034f0&gt;\n    &lt;dpctl.SyclDevice [backend_type.level_zero, device_type.gpu,  Intel(R) Data Center GPU Max 1550] at 0x154e8da035b0&gt;\n    &lt;dpctl.SyclDevice [backend_type.level_zero, device_type.gpu,  Intel(R) Data Center GPU Max 1550] at 0x154e8da03530&gt;\n    &lt;dpctl.SyclDevice [backend_type.level_zero, device_type.gpu,  Intel(R) Data Center GPU Max 1550] at 0x154e8da03f70&gt;\n    &lt;dpctl.SyclDevice [backend_type.level_zero, device_type.gpu,  Intel(R) Data Center GPU Max 1550] at 0x154e8d005770&gt;\n    &lt;dpctl.SyclDevice [backend_type.level_zero, device_type.gpu,  Intel(R) Data Center GPU Max 1550] at 0x154e8b5a3ab0&gt;\n    &lt;dpctl.SyclDevice [backend_type.level_zero, device_type.gpu,  Intel(R) Data Center GPU Max 1550] at 0x154e8d2467b0&gt;\n    &lt;dpctl.SyclDevice [backend_type.level_zero, device_type.gpu,  Intel(R) Data Center GPU Max 1550] at 0x154e8da24ef0&gt;\n    &lt;dpctl.SyclDevice [backend_type.level_zero, device_type.gpu,  Intel(R) Data Center GPU Max 1550] at 0x154e8da03fb0&gt;\n    &lt;dpctl.SyclDevice [backend_type.level_zero, device_type.gpu,  Intel(R) Data Center GPU Max 1550] at 0x154e8da240b0&gt;\n    &lt;dpctl.SyclDevice [backend_type.level_zero, device_type.gpu,  Intel(R) Data Center GPU Max 1550] at 0x154e8da24130&gt;\n\nFound CPU devices:  False\n</code></pre> <p>Managing GPU and CPU devices on Aurora</p> <p>On Aurora, <code>ONEAPI_DEVICE_SELECTOR=level_zero:gpu</code> is set by default, meaning that the GPU are the only devices visible to dpctl.  For this reason, <code>dpctl.has_cpu_devices()</code> returns <code>False</code>.  This setting allows dpnp and dpctl to use the GPU as the default SYCL device without needing to explicitly specify it. To access the CPU as a SYCL device, set <code>ONEAPI_DEVICE_SELECTOR=opencl:cpu</code>.</p> <p>In addition, the number of GPU devices visible on each node depends on the <code>ZE_FLAT_DEVICE_HIERARCHY</code> environment variable. With <code>ZE_FLAT_DEVICE_HIERARCHY=FLAT</code> 12 devices are visible (tile as device mode),  whereas with <code>ZE_FLAT_DEVICE_HIERARCHY=COMPOSITE</code> 6 devices are visible (GPU as device).</p> <p>The dpctl library contains <code>dpctl.tensor</code>, which is a tensor library implemented using DPC++ that follows the Python Array API standard. We refer the user to the dpctl.tensor documentation for details on all the array creation, manipulation, and linear algebra functions.  </p> <p>Changes after version 0.17.0</p> <p>Similarly to dpnp, dpctl version &gt; 0.17.0 runs all kernels asynchronously, therefore <code>.sycl_queue.wait()</code> must be used to measure execution time on GPU. </p>"},{"location":"aurora/data-science/python/#numba-dpex","title":"numba-dpex","text":"<p>Numba-dpex is Intel's Data Parallel Extension for Numba which allows users to apply Numba's JIT compiler and generate performant, parallel code on Intel's GPU. Its LLVM-based code generator implements a new kernel programming API (kapi) in pure Python that is modeled after the SYCL API. The example below implements and launches simple vector addition as a range kernel. Range kernels implement a basic parallel-for calculation that is ideally suited for embarrassingly parallel operations, such as element-wise computations over n-dimensional arrays.</p> <pre><code>import dpnp\nimport numba_dpex as dpex\nfrom numba_dpex import kernel_api as kapi\n\n# Data parallel kernel implementation of vector sum\n@dpex.kernel # (1)!\ndef vecadd(item: kapi.Item, a, b, c):\n    i = item.get_id(0) # (2)!\n    c[i] = a[i] + b[i]\n\nN = 1024 # (3)!\na = dpnp.ones(N)\nb = dpnp.ones_like(a)\nc = dpnp.zeros_like(a)\ndpex.call_kernel(vecadd, dpex.Range(N), a, b, c)\nassert dpnp.allclose(c,a+b)\nprint(\"Sum completed successfully\")\n</code></pre> <ol> <li>Decorate the <code>vecadd</code> function as a dpex kernel</li> <li>Get the work item</li> <li>Define the number of work items</li> </ol> <p>The <code>vecadd</code> function, when decorated as a dpex kernel, is compiled with numba-dpex into a data-parallel function to be executed individually by a set of work items (<code>item.get_id(0)</code>).  Numba-dpex follows the SPMD programming model, wherein each work item runs the function for a subset of the elements of the input arrays. The set of work items is defined by the <code>dpex.Range()</code> object and the <code>dpex.call_kernel()</code> call instructs every work item in the range to execute the <code>vecadd</code> kernel for a specific subset of the data. Numba-dpex also follows the compute-follows-data programming model, meaning that the kernel is run on the same device as the dpnp and dpctl arrays passed as inputs.</p> <p>Note that the numba-dpex kapi allows for more complex data parallel kernels (e.g., nd-range kernels) and the ability to create device callable functions.  For these and more features, we refer the users to the numba-dpex documentation.</p>"},{"location":"aurora/data-science/python/#dlpack","title":"DLPack","text":"<p>Thanks to dpctl supporting the Python Array API standard, both dpnp and dpctl provide interoperability with other Python libraries that follow the same standards, such as Numpy and PyTorch, through DLPack. This allows for zero-copy data access across the Python ecosystem.</p> <p>An example of using DLPack to pass arrays between dpnp and PyTorch is shown below  <pre><code>import dpnp as dp\nimport torch\nimport intel_extension_for_pytorch as ipex\n\nt_ary = torch.arange(4).to('xpu') # array [0, 1, 2, 3] on GPU\ndp_ary = dp.from_dlpack(t_ary)\nt_ary[0] = -2.0 # modify the PyTorch array\nprint(f'Original PyTorch array: {t_ary}')\nprint(f'dpnp view of PyTorch array: {dp_ary} on device {dp_ary.device}\\n')\ndel t_ary, dp_ary\n\ndp_ary = dp.arange(4) # array [0, 1, 2, 3] on GPU\nt_ary = torch.from_dlpack(dp_ary)\ndp_ary[0] = -3.0 # modify the dpnp array\nprint(f'Original dpnp array: {dp_ary} on device {dp_ary.device}')\nprint(f'PyTorch view of dpnp array: {t_ary}')\n</code></pre></p> Output <pre><code>Original PyTorch array: tensor([-2,  1,  2,  3], device='xpu:0')\ndpnp view of PyTorch array: [-2  1  2  3] on device Device(level_zero:gpu:0)\n\nOriginal dpnp array: [-3  1  2  3] on device Device(level_zero:gpu:0)\nPyTorch view of dpnp array: tensor([-3,  1,  2,  3], device='xpu:0')\n</code></pre> <p>DLPack notes on Aurora</p> <ul> <li><code>ZE_FLAT_DEVICE_HIERARCHY</code> must be set to <code>FLAT</code></li> <li>Zero-copy interoperability is supported between dpnp, dpctl, and PyTorch on CPU and GPU, and between Numpy as well on CPU only</li> <li>Interoperability between TensorFlow and the other packages is limited on the GPU due to TensorFlow not being compatible with the latest DLPack rules and still requiring the use of <code>dlcapsules</code></li> <li>Numba-dpex does not directly support DLPack, however numba-dpex kernels take as inputs dpnp and dpctl arrays, thus inperoperability between PyTorch and numba-dpex is available through those packages </li> </ul>"},{"location":"aurora/data-science/applications/gpt-neox/","title":"Instruction for gpt-neox on Aurora","text":""},{"location":"aurora/data-science/frameworks/dask/","title":"Dask","text":"<p>Dask is a Python library for parallel and distributed computing.  A Dask cluster is composed of one scheduler that coordinates the job of many workers, which can have access to CPU or GPU resources.  Here we show how to install Dask in a conda environment on Aurora and how to start a cluster with GPU workers and run a simple example script. </p>"},{"location":"aurora/data-science/frameworks/dask/#install-dask-on-aurora","title":"Install Dask on Aurora","text":"<p>From one of Aurora's login nodes, use the following commands to create a conda environment and install Dask.  This will also install other libraries needed to run an example script and create a Jupyter kernel that allows you to work interactively from a notebook. </p> <pre><code>module load frameworks\nconda create -y -n dask -c conda-forge python=3.11 pip dask ipykernel jupyterlab\nconda activate dask\n# install additional libraries\nconda install -y -c https://software.repos.intel.com/python/conda/ -c conda-forge dpnp\n# create the jupyter kernel\npython -m ipykernel install --prefix=${CONDA_PREFIX} --name dask\n</code></pre>"},{"location":"aurora/data-science/frameworks/dask/#start-a-dask-cluster","title":"Start a Dask cluster","text":"<p>Copy the following script into a file called <code>start_dask_aurora.sh</code> and make it executable with:</p> <pre><code>chmod a+x ./start_dask_aurora.sh\n</code></pre> start_dask_aurora.sh<pre><code>#!/bin/bash\n\n# start_dask_aurora.sh\n# Usage: \n# mpiexec -n NNODES * NUM_WORKERS_PER_NODE --ppn NUM_WORKERS_PER_NODE ./start_dask_aurora.sh WORKER_TYPE NUM_WORKERS_PER_NODE\n# Examples on two nodes:\n# mpiexec -n 12 --ppn 6 ./start_dask_aurora.sh gpu 6\n# mpiexec -n 208 --ppn 104 ./start_dask_aurora.sh cpu 104\n\nWORKER_TYPE=$1\nNUM_WORKERS_PER_NODE=$2\n# if using 12 GPU workers, assign one worker per tile, otherwise use one worker per GPU (2 tiles)\nif [ $NUM_WORKERS_PER_NODE = 12 ] &amp;&amp; [ $WORKER_TYPE = 'gpu' ]; then\n    export ZE_FLAT_DEVICE_HIERARCHY=FLAT\n    export ZE_ENABLE_PCI_ID_DEVICE_ORDER=1\nelse\n    export ZE_FLAT_DEVICE_HIERARCHY=COMPOSITE\nfi\n\n# Number of threads per worker (208 CPU threads per node divided by num workers)\nNTHREADS=$(( 208 / NUM_WORKERS_PER_NODE ))  # 208 / 12 \u2248 17\n# Memory limit per worker (1100GB RAM per node divided by num workers)\nMEMORY_PER_WORKER=$(( 1100 / NUM_WORKERS_PER_NODE ))GB  # 1100GB / 12 \u2248 91GB\nLOCAL_DIRECTORY=~/dask-local-directory\nDASK_DASHBOARD_PORT=${DASK_DASHBOARD_PORT:-8787}\nDASK_SCHEDULER_PORT=${DASK_SCHEDULER_PORT:-8786}\n\n# Start Dask scheduler on rank 0\nif [ $PALS_RANKID = 0 ]; then\n    # Purge Dask worker, log directories and config directories\n    rm -rf ${LOCAL_DIRECTORY}/* /tmp/dask-workers/*  ~/.config/dask\n    mkdir -p ${LOCAL_DIRECTORY}/logs /tmp/dask-workers\n    # Setup scheduler\n    nohup dask scheduler --port ${DASK_SCHEDULER_PORT} --dashboard-address $DASK_DASHBOARD_PORT \\\n        --scheduler-file ${LOCAL_DIRECTORY}/scheduler.json &gt; ${LOCAL_DIRECTORY}/logs/$HOSTNAME-scheduler.log 2&gt;&amp;1 &amp;\nfi\nsleep 10\n# Setup workers\nif [ $WORKER_TYPE = 'gpu' ]; then\n    ZE_AFFINITY_MASK=$PALS_LOCAL_RANKID dask worker \\\n        --resources \"GPU=1\" --memory-limit ${MEMORY_PER_WORKER} \\\n        --nthreads ${NTHREADS}  --local-directory /tmp/dask-workers \\\n        --scheduler-file ${LOCAL_DIRECTORY}/scheduler.json &gt;&gt; ${LOCAL_DIRECTORY}/logs/$HOSTNAME-worker.log 2&gt;&amp;1\nelse\n    dask worker \\\n        --nthreads ${NTHREADS} --local-directory /tmp/dask-workers \\\n        --scheduler-file ${LOCAL_DIRECTORY}/scheduler.json &gt;&gt; ${LOCAL_DIRECTORY}/logs/$HOSTNAME-worker.log 2&gt;&amp;1\nfi\n</code></pre>"},{"location":"aurora/data-science/frameworks/dask/#start-a-cluster-with-cpu-workers","title":"Start a cluster with CPU workers","text":"<p>Run the following commands from a compute node on Aurora to start a Dask cluster with 104 CPU workers per node: <pre><code>module load frameworks\nconda activate dask\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nmpiexec -n $(( $NNODES * 104 )) --ppn 104 ./start_dask_aurora.sh cpu 104\n</code></pre></p>"},{"location":"aurora/data-science/frameworks/dask/#start-a-cluster-with-gpu-workers","title":"Start a cluster with GPU workers","text":"<p>Run the following commands from a compute node on Aurora to start a Dask cluster with 6 GPU workers per node: <pre><code>module load frameworks\nconda activate dask\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nmpiexec -n $(( $NNODES * 6 )) --ppn 6 ./start_dask_aurora.sh gpu 6\n</code></pre></p>"},{"location":"aurora/data-science/frameworks/dask/#example","title":"Example","text":"<p>In this example, we will estimate Pi using a Monte Carlo method. </p> <p>Paste the following Python script into a file called <code>pi_dask_gpu.py</code>. Here is a breakdown of what the script does:</p> <ol> <li>It connects to the Dask cluster (that you should start beforehand) and prints some information including the number of workers and available memory.</li> <li>It divides the total number of points to sample between the workers, and each worker uses its GPU to</li> <li>generate random points uniformly inside the unit square</li> <li>return the number of points that are inside the unit circle</li> <li>When the results from the workers are ready, they are aggregated to compute Pi.</li> <li>A total of 5 Pi calculations are performed and timed (the very first iterations will incur initialization and warmup costs).</li> <li>At the end, the Dask cluster is shut down.</li> </ol> pi_dask_gpu.py<pre><code>import json\nimport pathlib\nfrom dask.distributed import Client\n\n\nfname = f'{pathlib.Path.home().as_posix()}/dask-local-directory/scheduler.json'\nwith open(fname, 'r') as f:\n    scheduler = json.load(f)\nclient = Client(scheduler['address'])\nprint(client)\n\n\nimport time\nimport dpnp as np\n\n\ndef count_points_inside_circle(N):\n    x = np.random.uniform(low=-1.0, high=1.0, size=(N, 2))\n    inside_circle = ((x * x).sum(axis=1) &lt; 1.).sum()\n    return int(inside_circle)\n\n\ndef compute_pi(inside_circle, N):\n    return 4 * inside_circle / N\n\n\ndef run():\n    start = time.time()\n    num_workers = len(client.scheduler_info()['workers'])\n    N = 10_400_000_004\n\n    # number of points per worker\n    Neach_section, extras = divmod(N, num_workers)\n    points_per_worker = [Neach_section for _ in range(num_workers)]\n    points_per_worker[-1] += extras\n\n    futures = client.map(count_points_inside_circle, points_per_worker)\n    inside_circle = client.submit(sum, futures).result()\n    pi = compute_pi(inside_circle, N)\n    end = time.time()\n    return f\"Num samples: {N:.2E}\\t\\tEstimate: {pi:.9f}\\t\\tTime taken: {end - start:.3f} s\"\n\n\ndef main(runs=5):\n    for i in range(runs):\n        print(f\"Run {i}\\t\\t{run()}\")\n\n\nmain()\nclient.shutdown()\n</code></pre>"},{"location":"aurora/data-science/frameworks/dask/#run-the-pi_dask_gpupy-example","title":"Run the <code>pi_dask_gpu.py</code> example","text":"<ul> <li>First, request an interactive job on 1 node.</li> <li>Then, start a Dask cluster with 6 GPU workers and wait about 10 seconds for the cluster to start.</li> <li>Press Ctrl+Z (SIGTSTP) and then execute <code>bg</code> to continue running the process in the background, or open a new shell and SSH onto the compute node. </li> <li>Run the example script:   <pre><code>module load frameworks\nconda activate dask\npython pi_dask_gpu.py\n</code></pre></li> </ul> Output <pre><code>&lt;Client: 'tcp://10.168.0.10:8786' processes=6 threads=204, memory=1.00 TiB&gt;\nRun 0           Num samples: 1.04E+10           Estimate: 3.141653798           Time taken: 1.596 s\nRun 1           Num samples: 1.04E+10           Estimate: 3.141570887           Time taken: 1.354 s\nRun 2           Num samples: 1.04E+10           Estimate: 3.141651954           Time taken: 1.451 s\nRun 3           Num samples: 1.04E+10           Estimate: 3.141636617           Time taken: 0.518 s\nRun 4           Num samples: 1.04E+10           Estimate: 3.141650108           Time taken: 0.511 s\n</code></pre>"},{"location":"aurora/data-science/frameworks/dask/#connect-to-a-dask-cluster-from-jupyterlab","title":"Connect to a Dask cluster from JupyterLab","text":"<p>Here are the steps to start a Dask cluster and connect to it interactively from a Jupyter notebook:</p> <ul> <li>First, request an interactive job on 1 node. Print the compute node's hostname (that you get with the command <code>hostname</code>), which will be used later.</li> <li>Then, start a Dask cluster and wait about 10 seconds for the cluster to start.</li> <li>On your local machine, open an SSH tunnel to the compute node (<code>COMPUTE_NODE</code> is the compute node's hostname and <code>YOUR_ALCF_USERNAME</code> is your ALCF username):   <pre><code>ssh -t -L 23456:localhost:23456 -L 8787:localhost:8787 YOUR_ALCF_USERNAME@bastion.alcf.anl.gov ssh -t -L 23456:localhost:23456 -L 8787:localhost:8787 login.aurora.alcf.anl.gov ssh -t -L 23456:localhost:23456 -L 8787:localhost:8787 COMPUTE_NODE\n</code></pre></li> </ul> <p>Failure</p> <p>If you have issues with the above sequence of <code>ssh</code> commands, check this page for troubleshooting.</p> <ul> <li>On the compute node where you land with the above ssh command, start JupyterLab:   <pre><code>module load frameworks\nconda activate dask\njupyter lab --no-browser --port=23456\n</code></pre></li> <li>Copy the line starting with <code>http://localhost:23456/lab?token=&lt;TOKEN&gt;</code> at the end of the Jupyter command's output.</li> <li>On your local machine, open a browser window and go to that URL.</li> <li>On the JupyterLab page, select the <code>dask</code> kernel and use this script to connect to the Dask cluster:   <pre><code>import json\nimport pathlib\nfrom dask.distributed import Client\n\nfname = f'{pathlib.Path.home().as_posix()}/dask-local-directory/scheduler.json'\nwith open(fname, 'r') as f:\n    scheduler = json.load(f)\nclient = Client(scheduler['address'])\nclient\n</code></pre></li> <li>The Dask dashboard will be available at http://localhost:8787</li> </ul>"},{"location":"aurora/data-science/frameworks/deepspeed/","title":"DeepSpeed","text":"<p>The base <code>frameworks</code> environment on Aurora does not come with Microsoft's DeepSpeed pre-installed, and it needs to be installed by the user. Further instructions for working with the base environment can be found here.</p> <p>We describe below the steps needed to get started with DeepSpeed on Aurora.</p> <p>We focus on the <code>cifar</code> example provided in the DeepSpeedExamples repository, though this approach should be generally applicable for running any model with DeepSpeed support.</p>"},{"location":"aurora/data-science/frameworks/deepspeed/#running-deepspeed-on-aurora","title":"Running DeepSpeed on Aurora","text":"<p>Note</p> <p>The instructions below should be run directly from a compute node.</p> <p>Explicitly, to request an interactive job (from <code>uan-00xx</code>): <pre><code>qsub -A &lt;project&gt; -q debug -l filesystems=&lt;fs1:fs2&gt; -l select=1 -l walltime=01:00:00 -I\n</code></pre></p> <p>Refer to job scheduling and execution for additional information.</p> <ol> <li> <p>Load <code>frameworks</code> module:</p> <pre><code>module load frameworks\n</code></pre> </li> <li> <p>Create a (new) virtual environment:</p> <pre><code>python3 -m venv /path/to/new/venv --system-site-packages\nsource /path/to/new/venv/bin/activate\n</code></pre> </li> <li> <p>Install DeepSpeed:</p> <pre><code>pip install deepspeed\n</code></pre> </li> <li> <p>Clone microsoft/DeepSpeedExamples and navigate into the directory:</p> <pre><code>git clone https://github.com/microsoft/DeepSpeedExamples.git\ncd DeepSpeedExamples/training/cifar\n</code></pre> </li> </ol> <p>Launching DeepSpeed</p> <p>In both examples, the 'train_batch_size' variable needs to be modified from 16 to 12 in the DeepSpeed config embedded in the function <code>get_ds_config()</code> from the Python file <code>cifar10_deepspeed.py</code>. This is because the default of 16 is not compatible with the 12 ranks per node we are launching with. DeepSpeed features can be further modified in the DeepSpeed config, and the full feature set is described in the DeepSpeed documentation.</p> Launching with MPICHLaunching with DeepSpeed <ol> <li> <p>Get the total number of available GPUs:</p> <ol> <li>Count the number of lines in <code>$PBS_NODEFILE</code> (1 host per line)</li> <li>Count the number of GPUs available on the current host</li> <li><code>NGPUS=\"$((${NHOSTS}*${NGPU_PER_HOST}))\"</code> <pre><code>NHOSTS=$(wc -l &lt; \"${PBS_NODEFILE}\")\nNGPU_PER_HOST=12\nNGPUS=\"$((${NHOSTS}*${NGPU_PER_HOST}))\"\n</code></pre></li> </ol> </li> <li> <p>Launch with <code>mpiexec</code>: <pre><code>mpiexec \\\n  --verbose \\\n  --envall \\\n  -n \"${NGPUS}\" \\\n  --ppn \"${NGPU_PER_HOST}\" \\\n  --hostfile=\"${PBS_NODEFILE}\" \\\n  python3 \\\n    cifar10_deepspeed.py\n</code></pre></p> </li> </ol> <ol> <li> <p>Create a DeepSpeed compliant <code>hostfile</code>, specifying the <code>hostname</code> and number of GPUs (<code>slots</code>) for each of our available workers (more info here): <pre><code>cat $PBS_NODEFILE &gt; hostfile\nsed -e 's/$/ slots=12/' -i hostfile\n</code></pre></p> </li> <li> <p>Create a <code>.deepspeed_env</code> (more info here) containing the environment variables our workers will need access to: <pre><code>echo \"PATH=${PATH}\" &gt;&gt; .deepspeed_env\necho \"LD_LIBRARY_PATH=${LD_LIBRARY_PATH}\" &gt;&gt; .deepspeed_env\necho \"http_proxy=${http_proxy}\" &gt;&gt; .deepspeed_env\necho \"https_proxy=${https_proxy}\" &gt;&gt; .deepspeed_env\n</code></pre></p> </li> </ol> <p>Warning</p> <p>The <code>.deepspeed_env</code> file expects each line to be of the form <code>KEY=VALUE</code>. Each of these will then be set as environment variables on each available worker specified in our <code>hostfile</code>.</p> <p>We can then run the <code>cifar10_deepspeed.py</code> module using DeepSpeed: <pre><code>deepspeed --hostfile=hostfile cifar10_deepspeed.py \\\n    --deepspeed \n</code></pre></p> <code>AssertionError: Micro batch size per gpu: 0 has to be greater than 0</code> <p>Depending on the details of your specific job, it may be necessary to modify the provided <code>ds_config.json</code>.</p> <p>If you encounter an error: <pre><code>x3202c0s31b0n0: AssertionError: Micro batch size per gpu: 0 has to be greater than 0\n</code></pre> you can modify the <code>\"train_batch_size\": 16</code> variable in the provided <code>ds_config.json</code> to the (total) number of available GPUs, and explicitly set <code>\"gradient_accumulation_steps\": 1</code>, as shown below. <pre><code>$ export NHOSTS=$(wc -l &lt; \"${PBS_NODEFILE}\")\n$ export NGPU_PER_HOST=$(nvidia-smi -L | wc -l)\n$ export NGPUS=\"$((${NHOSTS}*${NGPU_PER_HOST}))\"\n$ echo $NHOSTS $NGPU_PER_HOST $NGPUS\n24 4 96\n$ # replace \"train_batch_size\" with $NGPUS in ds_config.json\n$ # and write to `ds_config-polaris.json`\n$ sed \\\n    \"s/$(cat ds_config.json| grep batch | cut -d ':' -f 2)/ ${NGPUS},/\" \\\n    ds_config.json \\\n    &gt; ds_config-polaris.json\n$ cat ds_config-polaris.json\n{\n    \"train_batch_size\": 96,\n    \"gradient_accumulation_steps\": 1,\n    ...\n}\n</code></pre></p>"},{"location":"aurora/data-science/frameworks/gpytorch/","title":"GPyTorch on Aurora","text":""},{"location":"aurora/data-science/frameworks/gpytorch/#1-login-and-queue-a-job","title":"1. Login and Queue a Job","text":"<p>Login to Aurora:</p> <pre><code>ssh &lt;username&gt;@aurora.alcf.anl.gov\n</code></pre> <p>Refer to Getting Started on Aurora for additional information. In particular, you need to set the environment variables that provide access to the proxy host.</p> <p>Note</p> <p>The instructions below should be run directly from a compute node.</p> <p>Explicitly, to request an interactive job (from <code>aurora-uan</code>):</p> <pre><code>qsub -I -q &lt;your_Queue&gt; -l select=1,walltime=60:00 -A &lt;your_ProjectName&gt; -l filesystems=&lt;fs1:fs2&gt;\n</code></pre> <p>Refer to job scheduling and execution for additional information.</p>"},{"location":"aurora/data-science/frameworks/gpytorch/#2-once-on-a-compute-node-load-modules","title":"2. Once on a Compute Node, Load Modules","text":"<pre><code>module use /soft/modulefiles\nmodule load frameworks\npython3 -m venv --system-site-packages env_gpytorch\nsource env_gpytorch/bin/activate\npython3 -m pip install gpytorch\n</code></pre>"},{"location":"aurora/data-science/frameworks/gpytorch/#optional","title":"Optional","text":"<p>Create an <code>activation_env.sh</code> file that contains the following lines:</p> <pre><code>module use /soft/modulefiles\nmodule load frameworks\nsource env_gpytorch/bin/activate\n</code></pre> <p>and run <code>source activation_env.sh</code> to activate your environment for subsequent runs.</p>"},{"location":"aurora/data-science/frameworks/gpytorch/#3-running-on-gpus","title":"3. Running on GPUs","text":"<p>To run on GPUs, add the following to your code:</p> <pre><code>import intel_extension_for_pytorch as ipex\n</code></pre> <p>Set the device as follows in the code:</p> <pre><code>if torch.cuda.is_available():\n    device = torch.device('cuda')\nelif torch.xpu.is_available():\n    device = torch.device('xpu')\nelse: \n    device = torch.device('cpu')\n</code></pre> <p>(You might need to install an earlier version of GPyTorch for multiple GPU usage.)</p>"},{"location":"aurora/data-science/frameworks/jax/","title":"Jax on Aurora","text":""},{"location":"aurora/data-science/frameworks/megatron-deepspeed/","title":"Megatron-DeepSpeed","text":"<p>Megatron-DeepSpeed is a scalable, highly performant library for training large language models on any GPU<sup>2</sup>.</p> <p>In particular, it retains the core 4D parallelism<sup>1</sup> functionality of the NVIDIA / <code>Megatron-LM</code> library, while leveraging the microsoft / <code>DeepSpeed</code> library for efficient scaling and \ud83c\udf4b saforem2 / <code>ezpz</code> for automated device + backend selection.</p>"},{"location":"aurora/data-science/frameworks/megatron-deepspeed/#getting-started","title":"Getting Started","text":"<ol> <li> <p>Clone the argonne-lcf / <code>Megatron-DeepSpeed</code> repository:</p> <pre><code>git clone https://github.com/argonne-lcf/Megatron-DeepSpeed\ncd Megatron-DeepSpeed\n</code></pre> </li> <li> <p>Set up your environment:</p> <pre><code>export PBS_O_WORKDIR=$(pwd)\nsource &lt;(curl -s https://raw.githubusercontent.com/saforem2/ezpz/refs/heads/main/src/ezpz/bin/utils.sh)\nezpz_setup_env\n</code></pre> </li> <li> <p>Install dependencies:</p> <ol> <li> <p>\ud83c\udf4b saforem2 / <code>ezpz</code>:</p> <pre><code>python3 -m pip install -e \"git+https://github.com/saforem2/ezpz#egg=ezpz\" --require-virtualenv\n</code></pre> </li> <li> <p>microsoft / <code>DeepSpeed</code>:</p> <pre><code>python3 -m pip install deepspeed\n</code></pre> </li> </ol> </li> <li> <p>Launch training:</p> <pre><code># Before launching, `PBS_O_WORKDIR` should be set to Megatron-DeepSpeed's PATH\n# and venv inside Megatron-DeepSpeed/venv should be activated.\nTP=2 NLAYERS=10 DATA_FILE_LIST=ALCF/data-lists/aurora/books.txt bash train_aGPT_7B.sh\n</code></pre> <p>This will launch a distributed pre-training run with:</p> <ul> <li> <p><code>NLAYERS=10</code>: Llama style model consisting of 10 layers</p> </li> <li> <p><code>TP=2</code>: Split across 2 Tensor Parallel groups</p> </li> <li> <p><code>DATA_FILE_LIST</code>: Using the Books corpus of the Dolma dataset</p> </li> </ul> Overridable Options <p>This is a simple subset of the overridable options.</p> <p>The full list (as well as their default values) can be found in ALCF / <code>helpers.sh</code></p> <ul> <li><code>DTYPE</code>: Data type</li> <li><code>DATA_FILE_LIST</code>: Data file list</li> <li><code>FFN_HIDDEN_SIZE</code>: Feedforward Neural Network projection size</li> <li><code>GRAD_ACC_STEPS</code>: Gradient accumulation steps</li> <li><code>HEADS</code>: Number of attention heads</li> <li><code>HIDDEN</code>: Hidden size</li> <li><code>MICRO_BATCH</code>: Micro batch size</li> <li><code>NO_FLASH_ATTN</code>: No Flash Attention</li> <li><code>NLAYERS</code>: Number of layers</li> <li><code>NUM_KV_HEAD</code>: Number of key-value heads</li> <li><code>OPT</code>: Optimizer<ul> <li><code>adam</code></li> <li><code>adam8bit</code></li> <li><code>adamw</code></li> <li><code>adamwschedulefree</code></li> <li><code>apex.adam</code></li> <li><code>apex.sgd</code></li> <li><code>ds.fusedlamb</code></li> <li><code>ds.onebitlamb</code></li> <li><code>galoreadamw</code></li> <li><code>galoreadamw8bit</code></li> <li><code>galoreadamw8bitperlayer</code></li> <li><code>ipex.fusedlamb</code></li> <li><code>ipex.lamb</code></li> <li><code>shampoo</code></li> <li><code>sgd</code></li> <li><code>sgdschedulefree</code></li> <li><code>sophiag</code></li> </ul> </li> <li><code>PP</code>: Pipeline parallelism degree</li> <li><code>SEQ</code>: Sequence length</li> <li><code>SP</code>: Sequence parallelism (Ulysses) degree</li> <li><code>TP</code>: Tensor parallelism degree</li> <li><code>TRAIN_TOKENS</code>: Number of training tokens</li> <li><code>TRAIN_ITERS</code>: Number of training iterations</li> <li><code>USE_ACTIVATION_CHECKPOINTING</code>: Use activation checkpointing</li> <li><code>WEIGHT_DECAY</code>: Weight decay</li> <li><code>ZERO_STAGE</code>: Zero stage</li> </ul> </li> </ol> <ol> <li> <p>4D parallelism refers to data (DP), tensor (TP), pipeline (PP), and sequence (SP) parallelism degrees of freedom.\u00a0\u21a9</p> </li> <li> <p>Megatron-DeepSpeed is designed to work on any GPU, including NVIDIA GPUs (NCCL), AMD GPUs (RCCL), and Intel XPUs (CCL).\u00a0\u21a9</p> </li> </ol>"},{"location":"aurora/data-science/frameworks/oneCCL/","title":"oneCCL","text":"<p>oneAPI Collective Communications Library (oneCCL) provides an efficient implementation of communication patterns used in deep learning. oneCCL is governed by the UXL Foundation and is an implementation of the oneAPI specification.</p> <p>oneCCL can be used through:</p> <ol> <li>Native C++ SYCL mode</li> <li>Horovod</li> <li>PyTorch Distributed Data Parallel (DDP)</li> </ol>"},{"location":"aurora/data-science/frameworks/oneCCL/#aurora-oneccl-environment","title":"Aurora oneCCL environment","text":"<pre><code>kaushikvelusamy@aurora-uan-0012:~&gt; module load frameworks\n(/opt/aurora/24.180.0/frameworks/aurora_nre_models_frameworks-2024.2.1_u1) kaushikvelusamy@aurora-uan-0012:~&gt; echo $CCL_ROOT\n/opt/aurora/24.180.0/CNDA/oneapi/ccl/2021.13.1_20240808.145507\n</code></pre> <p>oneCCL environment variables</p> <p>We have identified a set of environment settings that typically provide better performance or address potential application hangs and crashes at large scale. This particular setup is still experimental, and it might change as the environment variable settings are refined. Users are encouraged to check this page regularly.</p> <pre><code>export CCL_PROCESS_LAUNCHER=pmix  \nexport CCL_ATL_TRANSPORT=mpi\nexport CCL_ALLREDUCE_SCALEOUT=direct:0-1048576;rabenseifner:1048577-max  # currently best allreduce algorithm at large scale\nexport CCL_BCAST=double_tree # currently best bcast algorithm at large scale\n\nexport CCL_KVS_MODE=mpi\nexport CCL_CONFIGURATION_PATH=\"\"\nexport CCL_CONFIGURATION=cpu_gpu_dpcpp\nexport CCL_KVS_CONNECTION_TIMEOUT=600 \n\nexport CCL_ZE_CACHE_OPEN_IPC_HANDLES_THRESHOLD=1024\nexport CCL_KVS_USE_MPI_RANKS=1\n\nexport MPI_PROVIDER=$FI_PROVIDER\nunset MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE\nunset MPIR_CVAR_CH4_COLL_SELECTION_TUNING_JSON_FILE\nunset MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE\n</code></pre> <p>The following additional set of environment variable setups might be application-dependent. Users are encouraged to try to set them and see whether they help their applications.</p> <pre><code>ulimit -c unlimited\nexport FI_MR_ZE_CACHE_MONITOR_ENABLED=0\nexport FI_MR_CACHE_MONITOR=disabled\nexport FI_CXI_RX_MATCH_MODE=hybrid\nexport FI_CXI_OFLOW_BUF_SIZE=8388608\nexport FI_CXI_DEFAULT_CQ_SIZE=1048576\nexport FI_CXI_CQ_FILL_PERCENT=30\nexport INTELGT_AUTO_ATTACH_DISABLE=1\nexport PALS_PING_PERIOD=240\nexport PALS_RPC_TIMEOUT=240\nexport MPIR_CVAR_GATHERV_INTER_SSEND_MIN_PROCS=-1 # to solve the sync send issue in Horovod seg fault\nexport CCL_ATL_SYNC_COLL=1 # to avoid potential hang at large scale\nexport CCL_OP_SYNC=1 # to avoid potential hang at large scale\n</code></pre> <p>Algorithm selection</p> <p><pre><code>export CCL_COLLECTIVENAME=topo\nexport CCL_COLLECTIVENAME_SCALEOUT=ALGORITHM_NAME\n</code></pre> More info on Algorithm selection: oneCCL Environment Variables</p> <pre><code>export CCL_ALLREDUCE=topo\nexport CCL_ALLREDUCE_SCALEOUT=rabenseifner \n</code></pre>"},{"location":"aurora/data-science/frameworks/oneCCL/#native-c-sycl-mode","title":"Native C++ SYCL mode","text":"<p>You can compile examples from the oneCCL Git repository and use the library from the system default instead of local builds. More information at: oneCCL Benchmark User Guide</p> <p>To build the C++ benchmark examples:</p> <pre><code>cd oneccl\nmkdir build\ncd build\nmodule load cmake\ncmake .. -DCMAKE_C_COMPILER=icx-cc -DCMAKE_CXX_COMPILER=icpx -DCOMPUTE_BACKEND=dpcpp -DCMAKE_INSTALL_PREFIX=/lus/flare/projects/Aurora_deployment/kaushik/all_reduce_frameworks/gitrepos/oneCCL/build/\nmake -j install\n\nrm -rf _install/bin/* _install/lib/*mpi* _install/lib/*fabric* _install/opt/\n</code></pre> <p>To run from a job script:</p> <p><pre><code>#!/bin/bash -x\n# qsub -l nodes=2:ncpus=208 -q workq  -l walltime=02:00:00 -l filesystems=lustre_scaling -A  prod ./pbs_job_\n#PBS -A &lt;ProjectName&gt;\n#PBS -k doe\n\nmodule load frameworks \ncd $PBS_O_WORKDIR\necho Jobid: $PBS_JOBID\necho Running on nodes `cat $PBS_NODEFILE`\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nRANKS_PER_NODE=12          # Number of MPI ranks per node\nNRANKS=$(( NNODES * RANKS_PER_NODE ))\necho \"NUM_OF_NODES=${NNODES}  TOTAL_NUM_RANKS=${NRANKS}  RANKS_PER_NODE=${RANKS_PER_NODE}\"\n\nCPU_BINDING1=list:4:9:14:19:20:25:56:61:66:71:74:79\nEXT_ENV=\"--env FI_CXI_DEFAULT_CQ_SIZE=1048576\"\nAPP1=/lus/flare/projects/Aurora_deployment/kaushik/all_reduce_frameworks/gitrepos/oneCCL/build/_install/examples/benchmark/benchmark\n\necho $CCL_ROOT\nexport LD_LIBRARY_PATH=$CCL_ROOT/lib:$LD_LIBRARY_PATH\nexport CPATH=$CCL_ROOT/include:$CPATH\nexport LIBRARY_PATH=$CCL_ROOT/lib:$LIBRARY_PATH\n\nexport CCL_PROCESS_LAUNCHER=pmix  \nexport CCL_ATL_TRANSPORT=mpi\nexport CCL_ALLREDUCE=topo\nexport CCL_ALLREDUCE_SCALEOUT=rabenseifner \n\nexport CCL_KVS_MODE=mpi\nexport CCL_CONFIGURATION_PATH=\"\"\nexport CCL_CONFIGURATION=cpu_gpu_dpcpp\nexport CCL_KVS_CONNECTION_TIMEOUT=600 \n\nwhich python\n\nmkdir -p ./out_${PBS_JOBID}/c_oneccl_gpu\nfor NNODES in 4 8 16 32 64 \ndo \nRANKS_PER_NODE=12          # Number of MPI ranks per node\nNRANKS=$(( NNODES * RANKS_PER_NODE ))\n\n    for BUF_SIZE in 1 2 4 8 16 32 64 128 256 512 1024 2048 4096 8192 16384 32768 65536 131072 262144 524288 1048576 2097152 4194304 8388608 16777216  33554432 67108864 134217728 268435456\n    do\n        date\n        mpiexec ${EXT_ENV}  --env CCL_LOG_LEVEL=info  --env CCL_PROCESS_LAUNCHER=pmix  --env CCL_ATL_TRANSPORT=mpi \\\n                            --np ${NRANKS} -ppn ${RANKS_PER_NODE} --cpu-bind  $CPU_BINDING1    $APP1   \\\n                            --elem_counts ${BUF_SIZE},${BUF_SIZE},${BUF_SIZE}  \\\n                            --coll allreduce -j off -i 1 -w 0  --backend sycl  --sycl_dev_type gpu &gt;  ./out_${PBS_JOBID}/c_oneccl_gpu/${PBS_JOBID}_${NNODES}_${NRANKS}_${RANKS_PER_NODE}_${BUF_SIZE}_sycl_ccl_gpu_out_w1.txt\n        date\n    echo ${BUF_SIZE}\n\n    done\ndone\n\n# For CPU only, change benchmark options to : --backend host --sycl_dev_type host\n</code></pre> For more information on oneCCL benchmark, please refer to: oneCCL Benchmark User Guide</p>"},{"location":"aurora/data-science/frameworks/oneCCL/#horovod","title":"Horovod","text":"<p>TensorFlow Horovod example:</p> <pre><code>import datetime\nfrom time import perf_counter_ns\nimport sys\n\nimport tensorflow as tf\nimport horovod.tensorflow as hvd\nimport intel_extension_for_tensorflow as itex\nprint(itex.__version__)\nhvd.init()\n\nhvd_local_rank = hvd.local_rank()\nhvd_size = hvd.size()\nprint(\"hvd_local_rank = %d  hvd_size = %d\" % (hvd_local_rank, hvd_size))\n\nxpus = tf.config.experimental.list_physical_devices('XPU')\nlogical_gpus = tf.config.experimental.set_visible_devices(xpus[hvd.local_rank()], 'XPU')\nprint(xpus)\ntf.debugging.set_log_device_placement(True)\n\ndim_size = int(int(sys.argv[1]) / 4)\nelapsed1 = []\n\nfor _ in range(5):\n    with tf.device(f\"XPU:{hvd_local_rank % 12}\"):\n        x = tf.ones([1, dim_size], dtype=tf.float32)\n        # print(x)\n        t5 = perf_counter_ns() \n        y = hvd.allreduce(x, average=False)\n        t6 = perf_counter_ns()\n        elapsed1.append(t6 - t5)\n\nif hvd.rank() == 0:\n    for e in elapsed1:\n        print(e)\n</code></pre> <p>PyTorch Horovod example:</p> <pre><code>from time import perf_counter_ns\nimport sys\nimport intel_extension_for_pytorch  # Added Extra\nimport torch.nn.parallel\nimport horovod.torch as hvd\nhvd.init()\nhvd_local_rank = hvd.local_rank()\nhvd_size = hvd.size()\n# print(\"hvd_local_rank = %d  hvd_size = %d\" % (hvd_local_rank, hvd_size))\n\ndef get_default_device():\n    if torch.xpu.is_available():\n        return torch.device(f\"xpu:{hvd_local_rank % 12}\")\n    else:\n        return torch.device('cpu')\n\ndevice = get_default_device()\n\ndim_size = int(int(sys.argv[1]) / 4)\nelapsed1 = []\n\nfor _ in range(50):\n    x = torch.ones([1, dim_size], dtype=torch.float32).to(device, non_blocking=True)\n    # print(x)\n    t5 = perf_counter_ns() \n    y = hvd.allreduce(x, average=False)\n    t6 = perf_counter_ns()\n    elapsed1.append(t6 - t5)\n\nif hvd.rank() == 0:\n    for e in elapsed1:\n        print(e)\n</code></pre>"},{"location":"aurora/data-science/frameworks/oneCCL/#pytorch-ddp","title":"PyTorch DDP","text":"<pre><code>import datetime\nfrom time import perf_counter_ns\nimport sys\nimport os\nimport socket\nfrom mpi4py import MPI\nimport intel_extension_for_pytorch  # Added Extra\nimport torch.nn.parallel\nimport torch.distributed as dist\nimport oneccl_bindings_for_pytorch\n\nMPI.COMM_WORLD.Barrier()\n\nos.environ['RANK'] = str(os.environ.get('PMI_RANK', 0))\nos.environ['WORLD_SIZE'] = str(os.environ.get('PMI_SIZE', 1))\nmpi_world_size = MPI.COMM_WORLD.Get_size()\nmpi_my_rank = MPI.COMM_WORLD.Get_rank()\n\nif mpi_my_rank == 0:\n   master_addr = socket.gethostname()\n   sock = socket.socket()\n   sock.bind(('', 0))\n   # master_port = sock.getsockname()[1] \n   master_port = 2345\nelse:\n   master_addr = None\n   master_port = None\n\nmaster_addr = MPI.COMM_WORLD.bcast(master_addr, root=0)\nmaster_port = MPI.COMM_WORLD.bcast(master_port, root=0)\nos.environ[\"MASTER_ADDR\"] = master_addr\nos.environ[\"MASTER_PORT\"] = str(master_port)\n\nMPI.COMM_WORLD.Barrier()\ndist.init_process_group(backend=\"ccl\", init_method='env://', world_size=mpi_world_size, rank=mpi_my_rank, timeout=datetime.timedelta(seconds=3600))\nMPI.COMM_WORLD.Barrier()\n\ndist_my_rank = dist.get_rank()\ndist_world_size = dist.get_world_size()\n\ndef get_default_device():\n    if torch.xpu.is_available():\n        return torch.device(f\"xpu:{dist_my_rank % 12}\")\n    else:\n        return torch.device('cpu')\n\ndevice = get_default_device()\n\ndim_size = int(int(sys.argv[1]) / 4)\nMPI.COMM_WORLD.Barrier()\n\nelapsed1 = []\n\nfor _ in range(50):\n    x = torch.ones([1, dim_size], dtype=torch.float32).to(device, non_blocking=True)\n    # print(x)\n    t5 = perf_counter_ns() \n    dist.all_reduce(x, op=dist.ReduceOp.SUM)  # Added Extra op\n    MPI.COMM_WORLD.Barrier()\n    t6 = perf_counter_ns()\n    elapsed1.append(t6 - t5)\n\nif mpi_my_rank == 0:\n    for e in elapsed1:\n        print(e)\n</code></pre> <p>References</p> <ol> <li>oneCCL Environment Variables</li> <li>oneCCL GitHub Repository</li> <li>Intel Torch CCL</li> <li>Argonne LCF DL Scaling</li> <li>oneCCL Benchmark User Guide</li> </ol>"},{"location":"aurora/data-science/frameworks/pyg/","title":"PyTorch Geometric (PyG)","text":"<p>PyTorch Geometric (PyG) is a Python library built on top of PyTorch for deep learning on graphs. It provides tools for working with graph-structured data and implementations of many Graph Neural Networks (GNNs).</p>"},{"location":"aurora/data-science/frameworks/pyg/#pyg-on-aurora","title":"PyG on Aurora","text":"<p>PyTorch Geometric includes a base library, called <code>torch_geometric</code>, and a number of optional dependencies: - <code>torch_scatter</code> - <code>torch_sparse</code> - <code>torch_cluster</code> - <code>torch_spline_conv</code> - <code>pyg_lib</code></p> <p>The base library, <code>torch_geometric</code>, relies solely on PyTorch and can utilize Intel GPUs through the <code>xpu</code> device specification.</p> <p>The optional dependencies include optimized operations that are relevant to GNNs and are missing in PyTorch. They have only CPU and CUDA implementations and can therefore only run on Aurora CPUs.</p>"},{"location":"aurora/data-science/frameworks/pyg/#pyg-base-library","title":"PyG base library","text":"<p>Here we explain how to install the base library, <code>torch_geometric</code>, on Aurora and show a simple example of training a PyG model on Intel GPUs.</p>"},{"location":"aurora/data-science/frameworks/pyg/#installing-torch_geometric-on-aurora","title":"Installing <code>torch_geometric</code> on Aurora","text":"<p>We are going to install <code>torch_geometric</code> in a virtual environment that inherits PyTorch and the other libraries from the base frameworks module:</p> <pre><code>module load frameworks\npython3 -m venv --clear venv --system-site-packages\nsource venv/bin/activate\npip install torch_geometric\n</code></pre>"},{"location":"aurora/data-science/frameworks/pyg/#example","title":"Example","text":"<p>The following example is inspired by the <code>gcn.py</code> example on the PyG repository.</p> <ul> <li>Copy the following Python script into a file called <code>gcn.py</code>.   <pre><code># gcn.py\nimport argparse\nimport time\nimport torch\nimport intel_extension_for_pytorch as ipex\nimport torch_geometric\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--dataset', type=str, default='Cora')\nparser.add_argument('--hidden_channels', type=int, default=256)\nparser.add_argument('--lr', type=float, default=0.01)\nparser.add_argument('--epochs', type=int, default=20)\nparser.add_argument('--device', choices=['auto', 'cpu'], default='auto', help='device to use')\nparser.add_argument('--num_nodes', type=int, default=10000)\nparser.add_argument('--num_edges', type=int, default=5000000)\nparser.add_argument('--num_features', type=int, default=32)\nparser.add_argument('--num_classes', type=int, default=100)\nargs = parser.parse_args()\n\ndevice = torch.device(args.device)\nprint(f\"device: {device}\")\n\nedge_index = torch.randint(args.num_nodes, size=(args.num_edges, 2), dtype=torch.long)\nx = torch.randn(size=(args.num_nodes, args.num_features))\ny = torch.randint(args.num_classes, size=(args.num_nodes,), dtype=torch.long)\ndata = torch_geometric.data.Data(x=x, edge_index=edge_index.t().contiguous(), y=y, num_classes=(y.max()+1).item())\ndata = data.to(device)\n\nclass GCN(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels):\n        super().__init__()\n        self.conv1 = torch_geometric.nn.GCNConv(in_channels, hidden_channels)\n        self.conv2 = torch_geometric.nn.GCNConv(hidden_channels, out_channels)\n\n    def forward(self, x, edge_index, edge_weight=None):\n        x = torch.nn.functional.dropout(x, p=0.5, training=self.training)\n        x = self.conv1(x, edge_index, edge_weight).relu()\n        x = torch.nn.functional.dropout(x, p=0.5, training=self.training)\n        x = self.conv2(x, edge_index, edge_weight)\n        return x\n\nmodel = GCN(\n    in_channels=data.num_features,\n    hidden_channels=args.hidden_channels,\n    out_channels=data.num_classes,\n).to(device)\n\noptimizer = torch.optim.Adam([\n    dict(params=model.conv1.parameters(), weight_decay=5e-4),\n    dict(params=model.conv2.parameters(), weight_decay=0)\n], lr=args.lr)  # Only perform weight-decay on first convolution.\nmodel, optimizer = ipex.optimize(model, optimizer=optimizer)\n\ndef train():\n    model.train()\n    optimizer.zero_grad()\n    out = model(data.x, data.edge_index, data.edge_attr)\n    loss = torch.nn.functional.cross_entropy(out, data.y)\n    loss.backward()\n    optimizer.step()\n    return float(loss)\n\ntimes = []\nfor epoch in range(1, args.epochs + 1):\n    start = time.time()\n    loss = train()\n    times.append(time.time() - start)\nprint(f'Median time per epoch: {torch.tensor(times).median():.4f}s')\n</code></pre></li> <li>Start an interactive job on one node.</li> <li>Load the frameworks module, activate the virtual environment, and run the script:   <pre><code>module load frameworks\nsource venv/bin/activate\npython gcn.py\n</code></pre>   The output should look like:   <pre><code>device: xpu\nMedian time per epoch: 0.2721s\n</code></pre></li> <li>To run on the CPU, use <code>python gcn.py --device cpu</code>.</li> </ul>"},{"location":"aurora/data-science/frameworks/pyg/#optional-dependencies","title":"Optional dependencies","text":"<p>Use the following script to create a virtual environment and install the base library and the CPU versions of all the optional dependencies:</p> <pre><code>#!/bin/bash \n\nmodule load frameworks\n\nTORCH_LIB=$(python -c \"import torch; print(torch.__file__)\" | sed 's/__init__.py/lib/')\nTORCH_VERSION=`python -c \"import torch; print(torch.__version__)\" | sed 's/^\\([0-9.]*\\).*/\\1/'`\n\npython3 -m venv --clear venv --system-site-packages\nsource venv/bin/activate\n\nexport LD_LIBRARY_PATH=${TORCH_LIB}:$LD_LIBRARY_PATH\n\n# PyTorch Geometric and utils\npip install torch_geometric\n\nlibs=(\\\n\"https://github.com/rusty1s/pytorch_scatter.git\" \\\n\"https://github.com/rusty1s/pytorch_sparse.git\" \\\n\"https://github.com/rusty1s/pytorch_cluster.git\" \\\n\"https://github.com/rusty1s/pytorch_spline_conv.git\")\n\nfor lib in \"${libs[@]}\"; do\n    LIB_NAME=\"$(basename \"$lib\" .git)\"\n    git clone ${lib} &amp;&amp; cd \"$LIB_NAME\"\n    git submodule update --init --recursive\n    # get library version compatible with pytorch\n    LIB_VERSION=`wget -O - https://data.pyg.org/whl/torch-${TORCH_VERSION}%2Bcpu.html 2&gt;/dev/null | \\\n        grep -m1 $(echo ${LIB_NAME} | sed 's/pytorch_\\(.*\\)/\\1/') | \\\n        sed 's/.*torch_[a-z_]*-\\([^+-]*\\)[%+-].*/\\1/'`\n    git checkout ${LIB_VERSION} \n    # Force to install without OpenMP backend\n    sed \"s|if ('backend: OpenMP' in info and 'OpenMP not found' not in info|if (False|g\" setup.py &gt; tmp &amp;&amp; \\\n        mv tmp setup.py\n    pip install .\n    cd ..\n    rm -rf \"$LIB_NAME\"\ndone\n</code></pre>"},{"location":"aurora/data-science/frameworks/pytorch/","title":"PyTorch on Aurora","text":"<p>PyTorch is a popular, open-source deep learning framework developed and  released by Facebook. The PyTorch home page, has more information about PyTorch, which you can refer to. For troubleshooting on  Aurora, please contact support@alcf.anl.gov.</p>"},{"location":"aurora/data-science/frameworks/pytorch/#provided-installation","title":"Provided Installation","text":"<p>PyTorch is already installed on Aurora with GPU support and available through the frameworks module.  To use it from a compute node, please load the following modules:</p> <p><pre><code>module load frameworks\n</code></pre> Then, you can <code>import</code> PyTorch in Python as usual (below showing results from the <code>frameworks/2024.2.1_u1</code>  module): <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; torch.__version__\n'2.3.1+cxx11.abi'\n</code></pre> A simple but useful check could be to use PyTorch to get device information on a compute node. You can do this the following way:</p> get-device-info.py<pre><code>import torch\nimport intel_extension_for_pytorch as ipex\n\nprint(f\"GPU availability: {torch.xpu.is_available()}\")\nprint(f'Number of tiles = {torch.xpu.device_count()}')\ncurrent_tile = torch.xpu.current_device()\nprint(f'Current tile = {current_tile}')\nprint(f'Current device ID = {torch.xpu.device(current_tile)}')\nprint(f'Device properties = {torch.xpu.get_device_properties()}')\n</code></pre> Example output: <pre><code>GPU availability: True\nNumber of tiles = 12\nCurrent tile = 0\nCurrent device ID = &lt;intel_extension_for_pytorch.xpu.device object at 0x1540a9f25790&gt;\nDevice properties = _XpuDeviceProperties(name='Intel(R) Data Center GPU Max 1550', platform_name='Intel(R) Level-Zero', \\\ntype='gpu', driver_version='1.3.30872', total_memory=65536MB, max_compute_units=448, gpu_eu_count=448, \\\ngpu_subslice_count=56, max_work_group_size=1024, max_num_sub_groups=64, sub_group_sizes=[16 32], has_fp16=1, has_fp64=1, \\\nhas_atomic64=1)\n</code></pre> <p>Each Aurora node has 6 GPUs (also called \"Devices\" or \"cards\") and each GPU is composed of two tiles (also called \"Sub-device\"). By default, each tile is mapped to one PyTorch device, giving a total of 12 devices per node in the above output. </p> <p><code>import intel_extension_for_pytorch as ipex</code></p> <p>Along with importing the <code>torch</code> module, you need to import the <code>intel_extension_for_pytorch</code> module in order to detect Intel GPUs as <code>xpu</code> devices. </p> <p>Warning</p> <p>It is highly recommended to import <code>intel_extension_for_pytorch</code> right after <code>import torch</code>, prior to importing other packages, (from Intel's \"Getting Started\" doc).</p> Using GPU Devices as PyTorch devices <p>By default, each tile is mapped to one PyTorch device, giving a total of 12 devices per node, as seen above.  To map a PyTorch device to one particular GPU Device out of the 6 available on a compute node, these  environmental variables should be set</p> <p><pre><code>export ZE_FLAT_DEVICE_HIERARCHY=COMPOSITE\nexport ZE_AFFINITY_MASK=0\n\n# or, equivalently, following the syntax `Device.Sub-device`\nexport ZE_AFFINITY_MASK=0.0,0.1\n</code></pre> In the example given above, an application is targeting the <code>Device:0</code>  and <code>Sub-devices: 0, 1</code>, i.e. the two tiles of the GPU:0. This is  particularly important in setting a performance benchmarking baseline. Setting the above environmental variables after loading the frameworks modules, you can check that each PyTorch device is now mapped to one GPU: <pre><code>import torch\nimport intel_extension_for_pytorch as ipex\ntorch.xpu.device_count()\ntorch.xpu.get_device_properties()\n</code></pre></p> Example output <pre><code>1\n_XpuDeviceProperties(name='Intel(R) Data Center GPU Max 1550', platform_name='Intel(R) Level-Zero', type='gpu', driver_version='1.3.30872', total_memory=131072MB, max_compute_units=896, gpu_eu_count=896, gpu_subslice_count=112, max_work_group_size=1024, max_num_sub_groups=64, sub_group_sizes=[16 32], has_fp16=1, has_fp64=1, has_atomic64=1)\n</code></pre> <p>More information and details are available through the Level Zero Specification Documentation - Affinity Mask</p>"},{"location":"aurora/data-science/frameworks/pytorch/#code-changes-to-run-pytorch-on-aurora-gpus","title":"Code changes to run PyTorch on Aurora GPUs","text":"<p>Intel Extension for PyTorch (IPEX) is an open-source project that extends PyTorch with optimizations for extra performance boost on Intel CPUs and enables the use of Intel GPUs.</p> <p>Here we list some common changes that you may need to do to your PyTorch code in order to use Intel GPUs. Please consult Intel's IPEX Documentation for additional details and useful tutorials.</p> <ol> <li>Import the <code>intel_extension_for_pytorch</code> right after importing <code>torch</code>:    <pre><code>import torch\nimport intel_extension_for_pytorch as ipex\n</code></pre></li> <li>All the <code>API</code> calls involving <code>torch.cuda</code>, should be replaced with <code>torch.xpu</code>. For example:    <pre><code>- torch.cuda.device_count()\n+ torch.xpu.device_count()\n</code></pre></li> <li>When moving tensors and model to GPU, replace <code>\"cuda\"</code> with <code>\"xpu\"</code>. For example:    <pre><code>- model = model.to(\"cuda\")\n+ model = model.to(\"xpu\")\n</code></pre></li> <li>Convert model and loss criterion to <code>xpu</code>, and then call <code>ipex.optimize</code> for additional performance boost:    <pre><code>device = torch.device('xpu')\nmodel = model.to(device)\ncriterion = criterion.to(device)\nmodel, optimizer = ipex.optimize(model, optimizer=optimizer)\n</code></pre></li> </ol> <p>Tip</p> <p>A more portable solution to select the appropriate device is the following: <pre><code>if torch.cuda.is_available():\n    device = torch.device('cuda')\nelif torch.xpu.is_available():\n    device = torch.device('xpu')\nelse: \n    device = torch.device('cpu')\nmodel = model.to(device)\n</code></pre></p>"},{"location":"aurora/data-science/frameworks/pytorch/#example-training-a-pytorch-model-on-a-single-gpu-tile","title":"Example: training a PyTorch model on a single GPU tile","text":"<p>Here is a simple code to train a dummy PyTorch model on CPU:</p> pytorch_cpu.py<pre><code>import torch\n\ntorch.manual_seed(0)\n\nsrc = torch.rand((2048, 1, 512))\ntgt = torch.rand((2048, 20, 512))\ndataset = torch.utils.data.TensorDataset(src, tgt)\nloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n\nmodel = torch.nn.Transformer(batch_first=True)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = torch.nn.CrossEntropyLoss()\nmodel.train()\n\nfor epoch in range(10):\n    for source, targets in loader:\n        optimizer.zero_grad()\n\n        output = model(source, targets)\n        loss = criterion(output, targets)\n\n        loss.backward()\n        optimizer.step()\n</code></pre> <p>And here is the code to train the same model on a single GPU tile on Aurora, with new or modified lines highlighted:</p> pytorch_xpu.py<pre><code>import torch\nimport intel_extension_for_pytorch as ipex\ndevice = torch.device('xpu')\n\ntorch.manual_seed(0)\n\nsrc = torch.rand((2048, 1, 512))\ntgt = torch.rand((2048, 20, 512))\ndataset = torch.utils.data.TensorDataset(src, tgt)\nloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n\nmodel = torch.nn.Transformer(batch_first=True)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = torch.nn.CrossEntropyLoss()\nmodel.train()\nmodel = model.to(device)\ncriterion = criterion.to(device)\nmodel, optimizer = ipex.optimize(model, optimizer=optimizer)\n\nfor epoch in range(10):\n    for source, targets in loader:\n        source = source.to(device)\n        targets = targets.to(device)\n        optimizer.zero_grad()\n\n        output = model(source, targets)\n        loss = criterion(output, targets)\n\n        loss.backward()\n        optimizer.step()\n</code></pre> <p>Here are the steps to run the above code on Aurora:</p> <ol> <li>Login to Aurora:     <pre><code>ssh &lt;username&gt;@aurora.alcf.anl.gov\n</code></pre></li> <li>Request a one-node interactive job for 30 minutes:    <pre><code>qsub -q debug -A &lt;your_project_name&gt; -l select=1,walltime=30:00 -l filesystems=home:flare -k doe -j oe -I\n</code></pre></li> <li>Copy the above Python script into a file called <code>pytorch_xpu.py</code> and make it executable with <code>chmod a+x pytorch_xpu.py</code>.</li> <li>Load the frameworks module:    <pre><code>module load frameworks\n</code></pre></li> <li>Run the script:    <pre><code>python pytorch_xpu.py\n</code></pre></li> </ol>"},{"location":"aurora/data-science/frameworks/pytorch/#pytorch-best-practices-on-aurora","title":"PyTorch Best Practices on Aurora","text":"<p>When running PyTorch applications, we have found the following practices to be  generally, if not universally, useful and encourage you to try some of these  techniques to boost performance of your own applications.</p> <ol> <li> <p>Use Reduced Precision. Reduced Precision is available on Intel Max 1550 and  is supported with PyTorch operations. In general, the way to do this is via the  PyTorch Automatic Mixed Precision package (AMP), as described in the  mixed precision documentation. In  PyTorch, users generally need to manage casting and loss scaling manually,  though context managers and function decorators can provide easy tools to do  this.</p> </li> <li> <p>PyTorch has a <code>JIT</code> module as well as backends to support op fusion, similar to TensorFlow's <code>tf.function</code> tools. See TorchScript for more information.</p> </li> <li> <p><code>torch.compile</code> will be available through the next framework release.</p> </li> <li> <p>In order to run an application with <code>TF32</code> precision type, one must set the following environmental parameter: <code>export IPEX_FP32_MATH_MODE=TF32</code>. This allows calculations using <code>TF32</code> as opposed to the default <code>FP32</code>, and done through <code>intel_extension_for_pytorch</code> module.</p> </li> <li> <p>For convolutional neural networks, using <code>channels_last</code> (NHWC) memory format gives better performance. More info here and here</p> </li> </ol>"},{"location":"aurora/data-science/frameworks/pytorch/#distributed-training-on-multiple-gpus","title":"Distributed Training on multiple GPUs","text":"<p>Distributed training with PyTorch on Aurora is facilitated through both Distributed Data Parallel (DDP) and Horovod, with comparable performance.  We recommend using native PyTorch DDP to perform Data Parallel training on Aurora. </p>"},{"location":"aurora/data-science/frameworks/pytorch/#distributed-data-parallel-ddp","title":"Distributed Data Parallel (DDP)","text":"<p>DDP training is accelerated using oneAPI Collective Communications Library Bindings for Pytorch (<code>oneccl_bindings_for_pytorch</code>). The extension supports FP32 and BF16 data types.  More detailed information and examples are available at the Intel oneCCL repo, formerly known as <code>torch-ccl</code>.</p>"},{"location":"aurora/data-science/frameworks/pytorch/#code-changes-to-train-on-multiple-gpus-using-ddp","title":"Code changes to train on multiple GPUs using DDP","text":"<p>The key steps in performing distributed training are:</p> <ol> <li>Load the <code>oneccl_bindings_for_pytorch</code> module, which enables efficient distributed deep learning training in PyTorch using Intel's oneCCL library, implementing collectives like <code>allreduce</code>, <code>allgather</code>, <code>alltoall</code>.</li> <li>Initialize PyTorch's <code>DistributedDataParallel</code></li> <li>Use <code>DistributedSampler</code> to partition the training data among the ranks</li> <li>Pin each rank to a GPU</li> <li>Wrap the model in DDP to keep it in sync across the ranks </li> <li>Rescale the learning rate</li> <li>Use <code>set_epoch</code> for shuffling data across epochs</li> </ol> <p>Here is the code to train the same dummy PyTorch model on multiple GPUs, where new or modified lines have been highlighted:</p> pytorch_ddp.py<pre><code>from mpi4py import MPI\nimport os, socket\nimport torch\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nimport intel_extension_for_pytorch as ipex\nimport oneccl_bindings_for_pytorch as torch_ccl\n\n# DDP: Set environmental variables used by PyTorch\nSIZE = MPI.COMM_WORLD.Get_size()\nRANK = MPI.COMM_WORLD.Get_rank()\nLOCAL_RANK = os.environ.get('PALS_LOCAL_RANKID')\nos.environ['RANK'] = str(RANK)\nos.environ['WORLD_SIZE'] = str(SIZE)\nMASTER_ADDR = socket.gethostname() if RANK == 0 else None\nMASTER_ADDR = MPI.COMM_WORLD.bcast(MASTER_ADDR, root=0)\nos.environ['MASTER_ADDR'] = f\"{MASTER_ADDR}.hsn.cm.aurora.alcf.anl.gov\"\nos.environ['MASTER_PORT'] = str(2345)\nprint(f\"DDP: Hi from rank {RANK} of {SIZE} with local rank {LOCAL_RANK}. {MASTER_ADDR}\")\n\n# DDP: initialize distributed communication with nccl backend\ntorch.distributed.init_process_group(backend='ccl', init_method='env://', rank=int(RANK), world_size=int(SIZE))\n\n# DDP: pin GPU to local rank.\ntorch.xpu.set_device(int(LOCAL_RANK))\ndevice = torch.device('xpu')\ntorch.manual_seed(0)\n\nsrc = torch.rand((2048, 1, 512))\ntgt = torch.rand((2048, 20, 512))\ndataset = torch.utils.data.TensorDataset(src, tgt)\n# DDP: use DistributedSampler to partition the training data\nsampler = torch.utils.data.distributed.DistributedSampler(dataset, shuffle=True, num_replicas=SIZE, rank=RANK, seed=0)\nloader = torch.utils.data.DataLoader(dataset, sampler=sampler, batch_size=32)\n\nmodel = torch.nn.Transformer(batch_first=True)\n# DDP: scale learning rate by the number of GPUs.\noptimizer = torch.optim.Adam(model.parameters(), lr=(0.001*SIZE))\ncriterion = torch.nn.CrossEntropyLoss()\nmodel.train()\nmodel = model.to(device)\ncriterion = criterion.to(device)\nmodel, optimizer = ipex.optimize(model, optimizer=optimizer)\n# DDP: wrap the model in DDP\nmodel = DDP(model)\n\nfor epoch in range(10):\n    # DDP: set epoch to sampler for shuffling\n    sampler.set_epoch(epoch)\n\n    for source, targets in loader:\n        source = source.to(device)\n        targets = targets.to(device)\n        optimizer.zero_grad()\n\n        output = model(source, targets)\n        loss = criterion(output, targets)\n\n        loss.backward()\n        optimizer.step()\n\n# DDP: cleanup\ntorch.distributed.destroy_process_group()\n</code></pre> <p>Here are the steps to run the above code on Aurora:</p> <ol> <li>Login to Aurora:     <pre><code>ssh &lt;username&gt;@aurora.alcf.anl.gov\n</code></pre></li> <li>Request an interactive job on two nodes for 30 minutes:    <pre><code>qsub -q debug -A &lt;your_project_name&gt; -l select=2,walltime=30:00 -l filesystems=home:flare -k doe -j oe -I\n</code></pre></li> <li>Copy the above Python script into a file called <code>pytorch_ddp.py</code> and make it executable with <code>chmod a+x pytorch_ddp.py</code>.</li> <li>Load the frameworks module:    <pre><code>module load frameworks\n</code></pre></li> <li>Run the script on 24 tiles, 12 per node:    <pre><code>mpiexec -n 24 -ppn 12 python pytorch_ddp.py\n</code></pre></li> </ol> <p>Settings for training beyond 16 nodes</p> <p>When training at medium and large scales, we recommend using the module <code>frameworks_optimized</code>, which provides an optimized setup based on observed performance. To use this optimized setup, the last two steps of the above instructions should be replaced with the following ones:</p> <ol> <li>Load the <code>frameworks_optimized</code> module:    <pre><code>module use /soft/datascience/frameworks_optimized/\nmodule load frameworks_optimized\n</code></pre></li> <li>Run the script on 24 tiles, 12 per node:    <pre><code>mpiexec -n 24 -ppn 12 --cpu-bind=${CPU_BIND} python pytorch_ddp.py\n</code></pre></li> </ol> Setting the CPU Affinity <p>The CPU affinity can be set manually through mpiexec.  You can do this the following way (after having loaded all needed modules):</p> <pre><code>export CPU_BIND=\"verbose,list:2-4:10-12:18-20:26-28:34-36:42-44:54-56:62-64:70-72:78-80:86-88:94-96\"\nmpiexec ... --cpu-bind=${CPU_BIND}\n</code></pre> <p>These bindings should be used along with the following oneCCL and Horovod  environment variable settings:</p> <pre><code>HOROVOD_THREAD_AFFINITY=\"4,12,20,28,36,44,56,64,72,80,88,96\"\nCCL_WORKER_AFFINITY=\"5,13,21,29,37,45,57,65,73,81,89,97\"\n</code></pre> <p>When running 12 ranks per node with these settings the <code>framework</code>s use 3 cores,  with Horovod tightly coupled with the <code>framework</code>s using one of the 3 cores, and  oneCCL using a separate core for better performance, eg. with rank 0 the  <code>framework</code>s would use cores 2,3,4, Horovod would use core 4, and oneCCL would  use core 5.</p> <p>Each workload may perform better with different settings.  The criteria for choosing the cpu bindings are:</p> <ul> <li>Binding for GPU and NIC affinity \u2013 To bind the ranks to cores on the proper      socket or NUMA nodes.</li> <li>Binding for cache access \u2013 This is the part that will change per application      and some experimentation is needed.</li> </ul> <p>Important: This setup is a work in progress, and based on observed  performance. The recommended settings are likely to changed with new <code>framework</code> releases.</p>"},{"location":"aurora/data-science/frameworks/pytorch/#distributed-training-with-multiple-ccss","title":"Distributed Training with Multiple CCSs","text":"<p>The Intel PVC GPUs contain 4 Compute Command Streamers (CCSs) on each tile, which can be used to group Execution Units (EUs) into common pools.  These pools can then be accessed by separate processes thereby enabling distributed training with multiple MPI processes per tile.  This feature on PVC is similar to MPS on NVIDIA GPUs  and can be beneficial for increasing computational throughput when training or performing inference with smaller models which do not require the entire memory of a PVC tile. For more information, see the section on using multiple CCSs under the Running Jobs on Aurora page.</p> <p>For both DDP and Horovod, distributed training with multiple CCSs can be enabled programmatically within the user code by explicitly setting the <code>xpu</code> device in PyTorch, for example</p> <pre><code>import os\nfrom argparse import ArgumentParser\nimport torch\nimport intel_extension_for_pytorch\nimport oneccl_bindings_for_pytorch\n\nparser = ArgumentParser(description='CCS Test')\nparser.add_argument('--ppd', default=1, type=int, choices=[1,2,4], \n                    help='Number of MPI processes per GPU device') # (1)!\nargs = parser.parse_args()\n\nlocal_rank = int(os.environ.get('PALS_LOCAL_RANKID'))\nif torch.xpu.is_available():\n    xpu_id = local_rank//args.ppd if torch.xpu.device_count()&gt;1 else 0\n    assert xpu_id&gt;=0 and xpu_id&lt;torch.xpu.device_count(), \\\n           f\"Assert failed: xpu_id={xpu_id} and {torch.xpu.device_count()} available devices\"\n    torch.xpu.set_device(xpu_id)\n</code></pre> <ol> <li>PVC GPU allow the use of 1, 2 or 4 CCSs on each tile</li> </ol> <p>and then adding the proper environment variables and <code>mpiexec</code> settings in the run script.  For example, to run distributed training with 48 MPI processes per node exposing 4 CCSs per tile, set</p> <pre><code>export ZEX_NUMBER_OF_CCS=0:4,1:4,2:4,3:4,4:4,5:4,6:4,7:4,8:4,9:4,10:4,11:4\nBIND_LIST=\"1:2:4:6:8:10:12:14:16:18:20:22:24:26:28:30:32:34:36:38:40:42:44:46:53:54:56:58:60:62:64:66:68:70:72:74:76:78:80:82:84:86:88:90:92:94:96:98\"\nmpiexec --pmi=pmix --envall -n 48 --ppn 48 \\\n    --cpu-bind=verbose,list:${BIND_LIST} \\\n    python training_script.py --ppd=4\n</code></pre> <p>Alternatively, users can use the following modified GPU affinity script in their <code>mpiexec</code> command in order to bind multiple MPI processes to each tile by setting <code>ZE_AFFINITY_MASK</code></p> gpu_affinity_ccs.sh<pre><code>#!/bin/bash\n\nnum_ccs=$1 # (1)!\nshift\ngpu_id=$(( PALS_LOCAL_RANKID / num_ccs ))\nexport ZE_AFFINITY_MASK=$gpu_id\nexec \"$@\"\n</code></pre> <ol> <li>Note that the script takes the number of CCSs exposed as a command line argument</li> </ol> <p>Checking PVC usage with <code>xpu-smi</code></p> <p>Users are invited to check correct placement of the MPI ranks on the different tiles by connecting to the compute node being used and executing  <pre><code>module load xpu-smi\nwatch -n 0.1 xpu-smi stats -d &lt;GPU_ID&gt; # (1)!\n</code></pre></p> <ol> <li>In this case, GPU_ID refers to the 6 GPU on each node, not an individual tile</li> </ol> <p>and checking the GPU and memory utilization of both tiles.</p> <p>Multiple CCSs and oneCCL</p> <ul> <li>When performing distributed training exposing multiple CCSs, the collective communications with the oneCCL backend are delegated to the CPU. This is done in the background by oneCCL, so no change to the users' code is required to move data between host and device, however it may impact the performance of the collectives at scale.</li> <li>When using PyTorch DDP, the model must be offloaded to the XPU device after calling the <code>DDP()</code> wrapper on the model to avoid hangs.</li> </ul>"},{"location":"aurora/data-science/frameworks/scikit-learn/","title":"scikit-learn on Aurora","text":"<p>scikit-learn is a popular open-source Python library for machine learning. It has wide coverage of machine learning algorithms (other than neural networks), such as k-means clustering and random forests.</p> <p>scikit-learn (abbreviated \"sklearn\") is built for CPUs. However, Intel(R) Extension for Scikit-learn (abbreviated \"sklearnex\") is a free Python package that speeds up scikit-learn on Intel CPUs &amp; GPUs and adds support for additional functionality, such as incremental and distributed algorithms. For more information, see the scikit-learn-intelex GitHub page, the documentation, or Intel's website.</p>"},{"location":"aurora/data-science/frameworks/scikit-learn/#environment-setup","title":"Environment Setup","text":"<p>Intel Extension for Scikit-learn is already pre-installed on Aurora, available in the <code>frameworks</code> module. You can load the frameworks module as described here, which will activate a conda environment.</p>"},{"location":"aurora/data-science/frameworks/scikit-learn/#usage","title":"Usage","text":""},{"location":"aurora/data-science/frameworks/scikit-learn/#patching","title":"Patching","text":"<p>To accelerate existing scikit-learn code with minimal code changes, Intel Extension for Scikit-learn uses patching: replacing stock scikit-learn algorithms with versions that utilize Intel(R) oneAPI Data Analytics Library (oneDAL).</p> <p>Note that patching only affects supported algorithms and parameters. To see the current support, check Intel's page here. Otherwise, the Intel Extension will fall back on stock scikit-learn, which has to run on the CPU. To know which version is being used, enable Verbose Mode, for example, with the environment variable <code>SKLEARNEX_VERBOSE=INFO</code>. However, verbose mode is only available for supported algorithms.</p> <p>There are multiple ways to patch scikit-learn with the Intel Extension, as Intel documents here. For example, you can patch within the script, like this:</p> <pre><code>from sklearnex import patch_sklearn\npatch_sklearn()\n</code></pre> <p>It is important to note that this needs to happen before importing scikit-learn. To explicitly only patch certain estimators, you can import particular functions from <code>sklearnex</code> instead of <code>sklearn</code>, like this:</p> <pre><code>from sklearnex.neighbors import NearestNeighbors\n</code></pre>"},{"location":"aurora/data-science/frameworks/scikit-learn/#gpu-acceleration","title":"GPU Acceleration","text":"<p>Intel Extension for Scikit-learn can execute algorithms on the GPU via the dpctl package, which should be included in the frameworks module. (If not, see the Python page.) dpctl implements oneAPI concepts like queues and devices.</p> <p>As described in more detail in Intel's documentation here, there are two ways to run on the GPU.</p> <ol> <li>Pass the input data to the algorithm as <code>dpctl.tensor.usm_ndarray</code>. Then the algorithm will run on the same device as the data and return the result as a usm_array on the same device.</li> <li>Configure Intel Extension for Scikit-learn, for example, by setting a context: <code>sklearnex.config_context</code>.</li> </ol> <p>Patching (described above) can be helpful in the case of functionality that already exists in scikit-learn because you can import the functions from <code>sklearn</code> instead of <code>sklearnex</code>.</p>"},{"location":"aurora/data-science/frameworks/scikit-learn/#distributed-mode","title":"Distributed Mode","text":"<p>To distribute an <code>sklearnex</code> algorithm across multiple GPUs, we need several ingredients demonstrated in an example below. We recommend using the MPI backend rather than the CCL backend since it is tested more thoroughly on Aurora.</p> <p>Warning</p> <p>The current version of Intel Extension to scikit-learn does not scale well to multiple GPUs. The cause has been identified, and we're waiting on a fix. However, if you use the oneDAL C++ API, the scaling is much better.</p> <ol> <li>Use dpctl to create a SYCL queue (connection to the GPU devices you choose).</li> <li>Using dpctl and your queue, move your data to the GPU devices.</li> <li>Run the algorithm on that data. The compute will happen where the data is. The algorithm should be from <code>sklearnex.spmd</code>.</li> </ol> <p>Since you are importing the algorithm from <code>sklearnex</code> instead of <code>sklearn</code>, patching is not necessary here.</p>"},{"location":"aurora/data-science/frameworks/scikit-learn/#an-example-python-script","title":"An Example Python Script","text":"<p>This example is adapted from an example in Intel's scikit-learn-intelex GitHub repo.</p> <pre><code>import dpctl\nimport dpctl.tensor as dpt\nfrom mpi4py import MPI\nfrom sklearn.datasets import make_classification\nfrom sklearnex.spmd.neighbors import KNeighborsClassifier\n\n# Create a GPU SYCL queue to store data on device.\nq = dpctl.SyclQueue(\"gpu\")\n\n# mpi4py is one way to handle arranging data across ranks.\n# For the sake of a concise demo, each rank is generating different random training data.\ncomm = MPI.COMM_WORLD\nrank = comm.Get_rank()\nX, y = make_classification(n_samples=100000, n_features=8, random_state=rank)\n\n# Move the data to the GPU devices.\ndpt_X = dpt.asarray(X, usm_type=\"device\", sycl_queue=q)\ndpt_y = dpt.asarray(y, usm_type=\"device\", sycl_queue=q)\n\n# Run the algorithm.\nmodel_spmd = KNeighborsClassifier(\n    algorithm=\"brute\", n_neighbors=20, weights=\"uniform\", p=2, metric=\"minkowski\"\n)\nmodel_spmd.fit(dpt_X, dpt_y)\n</code></pre>"},{"location":"aurora/data-science/frameworks/scikit-learn/#an-example-job-script","title":"An Example Job Script","text":"<p>Below we give an example job script. Note that we are using Aurora MPICH (the default MPI library on Aurora) and not using oneCCL, so we don't need special oneCCL settings. For more about pinning ranks to CPU cores and GPUs, see the Running Jobs page.</p> example_scikit-learn_distributed.sh<pre><code>module use /soft/modulefiles\nmodule load frameworks\n\n# This is to resolve an issue due to a package called \"numexpr\".\n# It sets the variable\n# 'numexpr.nthreads' to available number of threads by default, in this case\n# to 208. However, the 'NUMEXPR_MAX_THREADS' is also set to 64 as a package\n# default. The solution is to either set the 'NUMEXPR_NUM_THREADS' to less than\n# or equal to '64' or to increase the 'NUMEXPR_MAX_THREADS' to the available\n# number of threads. Both of these variables can be set manually.\nexport NUMEXPR_NUM_THREADS=64\n\nexport CPU_BIND=\"verbose,list:2-4:10-12:18-20:26-28:34-36:42-44:54-56:62-64:70-72:78-80:86-88:94-96\"\n\n# Launch the script\nmpiexec -np 12 -ppn 12 --cpu-bind ${CPU_BIND} gpu_tile_compact.sh python knn_mpi4py_spmd.py\n</code></pre>"},{"location":"aurora/data-science/frameworks/tensorflow/","title":"TensorFlow on Aurora","text":"<p>TensorFlow is a popular, open-source deep learning framework developed and  released by Google. The  TensorFlow home page has more information about  TensorFlow, which you can refer to. For troubleshooting on Polaris, please  contact support@alcf.anl.gov.</p>"},{"location":"aurora/data-science/frameworks/tensorflow/#provided-installation","title":"Provided Installation","text":"<p>TensorFlow is already preinstalled on Aurora, available in the <code>frameworks</code>  module. To use it from a compute node, load the module: <pre><code>module use /soft/modulefiles/\nmodule load frameworks\n</code></pre></p> <p>Then you can <code>import</code> TensorFlow as usual, the following is an output from the  <code>frameworks</code> module:</p> <p><pre><code>import tensorflow as tf\ntf.__version__\n</code></pre> <pre><code>'2.14.1'\n</code></pre> This import will fail on login nodes because there is no XPU on login nodes. </p> <p>A simple but useful check could be to use TensorFlow to get device information  on a compute node. You can do this the following way:</p> <p><pre><code>tf.config.list_physical_devices()\n</code></pre> <pre><code>[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), \nPhysicalDevice(name='/physical_device:XPU:0', device_type='XPU'), \nPhysicalDevice(name='/physical_device:XPU:1', device_type='XPU'), \nPhysicalDevice(name='/physical_device:XPU:2', device_type='XPU'), \nPhysicalDevice(name='/physical_device:XPU:3', device_type='XPU'), \nPhysicalDevice(name='/physical_device:XPU:4', device_type='XPU'), \nPhysicalDevice(name='/physical_device:XPU:5', device_type='XPU'), \nPhysicalDevice(name='/physical_device:XPU:6', device_type='XPU'), \nPhysicalDevice(name='/physical_device:XPU:7', device_type='XPU'), \nPhysicalDevice(name='/physical_device:XPU:8', device_type='XPU'), \nPhysicalDevice(name='/physical_device:XPU:9', device_type='XPU'), \nPhysicalDevice(name='/physical_device:XPU:10', device_type='XPU'), \nPhysicalDevice(name='/physical_device:XPU:11', device_type='XPU')]\n</code></pre></p> <p>Note that, here <code>tf.config</code> return 12 tiles of 6 cards (the number of GPU  resources on an Aurora compute node), and treat each tile as a device. The user can choose to set the environmental variable <code>ZE_FLAT_DEVICE_HIERARCHY</code> with  appropriate values to achieve desired behavior, as described in the  Level Zero Specification documentation. This environment variable is equivalent to the <code>ITEX_TILE_AS_DEVICE</code>, which is to be deprecated soon.</p> <p>Intel extension for TensorFLow has been made publicly available as an  open-source project at  GitHub.</p> <p>Please consult the following resources for additional details and useful tutorials:</p> <ul> <li>Intel's Documentation</li> <li>Intel's Examples</li> <li>Intel's ITEX Features Guide</li> <li>Intel's Practice Guide</li> </ul>"},{"location":"aurora/data-science/frameworks/tensorflow/#tensorflow-best-practices-on-aurora","title":"TensorFlow Best Practices on Aurora","text":""},{"location":"aurora/data-science/frameworks/tensorflow/#single-device-performance","title":"Single Device Performance","text":"<p>To expose one particular device out of the 6 available on a compute node,  this environmental variable should be set</p> <p><pre><code>export ZE_AFFINITY_MASK=0.0,0.1\n\n# The values taken by this variable follows the syntax `Device.Sub-device`\n</code></pre> In the example given above, an application is targeting the  Device:0 and Sub-devices: 0, 1, i.e. the two tiles of the GPU:0.  This is particularly important in setting a performance benchmarking baseline.</p> <p>More information and details are available through Level Zero Specification Documentation - Affinity Mask</p>"},{"location":"aurora/data-science/frameworks/tensorflow/#single-node-performance","title":"Single Node Performance","text":"<p>When running TensorFlow applications, we have found the following practices to  be generally, if not universally, useful and encourage you to try some of these  techniques to boost performance of your own applications.</p>"},{"location":"aurora/data-science/frameworks/tensorflow/#reduced-precision","title":"Reduced Precision","text":"<p>Use Reduced Precision, whenever the application allows. Reduced Precision is  available on Intel Max 1550 and is supported with TensorFlow operations. In  general, the way to do this is via the <code>tf.keras.mixed_precision</code> Policy, as  described in the  mixed precision documentation Intel's extension for TensorFlow is fully compatible with the Keras mixed  precision API in TensorFlow. It also provides an advanced auto mixed precision  feature. For example, you can just set two environment variables to get the  performance benefit from low-precision data type <code>FP16</code>/<code>BF16</code> without changing the  application code.</p> <p><pre><code>export ITEX_AUTO_MIXED_PRECISION=1\nexport ITEX_AUTO_MIXED_PRECISION_DATA_TYPE=\"BFLOAT16\" # or \"FLOAT16\"\n</code></pre> If you use a custom training loop (and not <code>keras.Model.fit</code>), you will also  need to apply loss scaling.</p>"},{"location":"aurora/data-science/frameworks/tensorflow/#tensorflows-graph-api","title":"TensorFlow's graph API","text":"<p>Use TensorFlow's graph API to improve efficiency of operations. TensorFlow is,  in general, an imperative language but with function decorators like  <code>@tf.function</code> you can trace functions in your code. Tracing replaces your  python function with a lower-level, semi-compiled TensorFlow Graph. More  information about the <code>tf.function</code> interface is available  here.  When possible, use <code>jit_compile</code>, but be aware of sharp bits when using  <code>tf.function</code>: python expressions that aren't tensors are often replaced as  constants in the graph, which may or may not be your intention.</p> <p>There is an experimental feature, which allows for aggressive fusion of kernels through  oneDNN Graph API. Intel's extension for TensorFlow can offload performance critical graph  partitions to oneDNN library to get more aggressive graph optimizations. It can be done by setting this environmental variable:</p> <p><pre><code>export ITEX_ONEDNN_GRAPH=1\n</code></pre> This feature is experimental, and actively under development.</p>"},{"location":"aurora/data-science/frameworks/tensorflow/#tf32-math-mode","title":"<code>TF32</code> Math Mode","text":"<p>The Intel Xe Matrix Extensions (Intel XMX) engines in Intel Max 1550 Xe-HPC  GPUs natively support <code>TF32</code> math mode. Through intel extension for tensorflow you can enable it by setting the following environmental variable:</p> <pre><code>export ITEX_FP32_MATH_MODE=\"TF32\"\n</code></pre>"},{"location":"aurora/data-science/frameworks/tensorflow/#xla-compilation-plannedupcoming","title":"XLA Compilation (Planned/Upcoming)","text":"<p>XLA is the Accelerated Linear Algebra library that is available in TensorFlow  and critical in software like JAX. XLA will compile a <code>tf.Graph</code> object,  generated with <code>tf.function</code> or similar, and perform optimizations like  operation-fusion. XLA can give impressive performance boosts with almost no  user changes except to set an environment variable <code>TF_XLA_FLAGS=--tf_xla_auto_jit=2</code>.  If your code is complex, or has dynamically sized tensors (tensors where the  shape changes every iteration), XLA can be detrimental: the overhead for  compiling functions can be large enough to mitigate performance improvements.  XLA is particularly powerful when combined with reduced precision,  yielding speedups &gt; 100% in some models. </p> <p>Intel provides initial intel GPU support for TensorFlow models with XLA  acceleration through  Intel Extension for OpenXLA. Full TensorFlow and PyTorch support is planned for development.</p>"},{"location":"aurora/data-science/frameworks/tensorflow/#a-simple-example","title":"A simple example","text":"<p>A simple example on how to use Intel GPU with TensorFlow is the following:</p> intel-xpu-tf-example.py<pre><code>import tensorflow as tf   # TensorFlow registers PluggableDevices here.\ntf.config.list_physical_devices()  # XPU device is visible to TensorFlow.\n\n#Section 1 Run implicitly\na = tf.random.normal(shape=[5], dtype=tf.float32)  # Runs on XPU.\nb = tf.nn.relu(a)         # Runs on XPU .\n\n#Section 2 Run with explicit device setting\nwith tf.device(\"/XPU:0\"):  # Users can also use 'with tf.device' syntax.\n  c = tf.nn.relu(a)        # Runs on XPU.\nwith tf.device(\"/CPU:0\"):\n  c = tf.nn.relu(a)        # Runs on CPU.\n\n#Section 3 Run with graph mode\n@tf.function  # Defining a tf.function\ndef run():\n  d = tf.random.uniform(shape=[100], dtype=tf.float32)\n  e = tf.nn.relu(d)\nrun()  # PluggableDevices also work with tf.function and graph mode. Runs on XPU\n</code></pre>"},{"location":"aurora/data-science/frameworks/tensorflow/#multi-gpu-multi-node-scale-up","title":"Multi-GPU / Multi-Node Scale Up","text":"<p>TensorFlow is compatible with scaling up to multiple GPUs per node, and across  multiple nodes. Good performance with tensorFlow has been seen with horovod in  particular. For details, please see the  Horovod documentation. Some Aurora specific details might be helpful to you.</p>"},{"location":"aurora/data-science/frameworks/tensorflow/#environment-variables","title":"Environment Variables","text":"<p>The following environmental variables should be set on the batch submission  script (PBSPro script) in the case of attempting to run beyond 16 nodes.</p> <p>oneCCL environment variables</p> <p>We have identified a set of environment settings that typically provide better performance or address potential application hangs and crashes at large scale. This particular setup is still experimental, and it might change as the environment variable settings are refined. Users are encouraged to check this page regularly.</p> <pre><code>export CCL_PROCESS_LAUNCHER=pmix  \nexport CCL_ATL_TRANSPORT=mpi\nexport CCL_ALLREDUCE_SCALEOUT=direct:0-1048576;rabenseifner:1048577-max  # currently best allreduce algorithm at large scale\nexport CCL_BCAST=double_tree # currently best bcast algorithm at large scale\n\nexport CCL_KVS_MODE=mpi\nexport CCL_CONFIGURATION_PATH=\"\"\nexport CCL_CONFIGURATION=cpu_gpu_dpcpp\nexport CCL_KVS_CONNECTION_TIMEOUT=600 \n\nexport CCL_ZE_CACHE_OPEN_IPC_HANDLES_THRESHOLD=1024\nexport CCL_KVS_USE_MPI_RANKS=1\n\nexport MPI_PROVIDER=$FI_PROVIDER\nunset MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE\nunset MPIR_CVAR_CH4_COLL_SELECTION_TUNING_JSON_FILE\nunset MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE\n</code></pre> <p>The following additional set of environment variable setups might be application-dependent. Users are encouraged to try to set them and see whether they help their applications.</p> <pre><code>ulimit -c unlimited\nexport FI_MR_ZE_CACHE_MONITOR_ENABLED=0\nexport FI_MR_CACHE_MONITOR=disabled\nexport FI_CXI_RX_MATCH_MODE=hybrid\nexport FI_CXI_OFLOW_BUF_SIZE=8388608\nexport FI_CXI_DEFAULT_CQ_SIZE=1048576\nexport FI_CXI_CQ_FILL_PERCENT=30\nexport INTELGT_AUTO_ATTACH_DISABLE=1\nexport PALS_PING_PERIOD=240\nexport PALS_RPC_TIMEOUT=240\nexport MPIR_CVAR_GATHERV_INTER_SSEND_MIN_PROCS=-1 # to solve the sync send issue in Horovod seg fault\nexport CCL_ATL_SYNC_COLL=1 # to avoid potential hang at large scale\nexport CCL_OP_SYNC=1 # to avoid potential hang at large scale\n</code></pre>"},{"location":"aurora/data-science/frameworks/tensorflow/#cpu-affinity","title":"CPU Affinity","text":"<p>The CPU affinity should be set manually through mpiexec.  You can do this the following way:</p> <pre><code>export CPU_BIND=\"verbose,list:2-4:10-12:18-20:26-28:34-36:42-44:54-56:62-64:70-72:78-80:86-88:94-96\"\nmpiexec ... --cpu-bind=${CPU_BIND}\n</code></pre> <p>These bindings should be used along with the following oneCCL and Horovod  environment variable settings:</p> <pre><code>HOROVOD_THREAD_AFFINITY=\"4,12,20,28,36,44,56,64,72,80,88,96\"\nCCL_WORKER_AFFINITY=\"5,13,21,29,37,45,57,65,73,81,89,97\"\n</code></pre> <p>When running 12 ranks per node with these settings the <code>framework</code>s use 3 cores,  with Horovod tightly coupled with the <code>framework</code>s using one of the 3 cores, and  oneCCL using a separate core for better performance, eg. with rank 0 the  <code>framework</code>s would use cores 2,3,4, Horovod would use core 4, and oneCCL would  use core 5.</p> <p>Each workload may perform better with different settings.  The criteria for choosing the cpu bindings are:</p> <ul> <li>Binding for GPU and NIC affinity \u2013 To bind the ranks to cores on the proper      socket or NUMA nodes.</li> <li>Binding for cache access \u2013 This is the part that will change per application      and some experimentation is needed.</li> </ul> <p>Note</p> <p>This setup is a work in progress, and based on observed performance. The recommended settings are likely to change with new <code>framework</code> releases.</p>"},{"location":"aurora/data-science/frameworks/tensorflow/#distributed-training","title":"Distributed Training","text":"<p>Distributed training with TensorFlow  on Aurora is facilitated through Horovod, using Intel Optimization for Horovod.</p> <p>The key steps in performing distributed training are laid out in the following example:</p> <ul> <li>Tensorflow examples with Intel Optimization for Horovod</li> </ul> <p>Detailed implementation of the same example is here:</p> <ul> <li>TensorFlow with Keras and Horovod</li> </ul> <p>A suite of detailed and well documented examples is part of Intel's optimization for Horovod repository:</p> <ul> <li>Distributed Training Example Suite</li> </ul>"},{"location":"aurora/data-science/frameworks/tensorflow/#a-simple-job-script","title":"A simple Job Script","text":"<p>Below we give a simple job script:</p> <pre><code>#!/bin/bash -l\n#PBS -l select=512                              # selecting 512 Nodes\n#PBS -l place=scatter\n#PBS -l walltime=1:59:00\n#PBS -q prod                                   # a specific queue\n#PBS -A &lt;ProjectName&gt;                          # project allocation\n#PBS -l filesystems=&lt;fs1:fs2&gt;                   # specific filesystem, can be a list separated by :\n#PBS -k doe\n#PBS -e /home/$USER/path/to/errordir\n#PBS -o /home/$USER/path/to/outdir              # path to `stdout` or `.OU` files\n#PBS -j oe                                      # output and error placed in the `stdout` file\n#PBS -N a.name.for.the.job\n\n#####################################################################\n# This block configures the total number of ranks, discovering\n# it from PBS variables.\n# 12 Ranks per node, if doing rank/tile\n#####################################################################\n\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS_PER_NODE=12\nlet NRANKS=${NNODES}*${NRANKS_PER_NODE}\n\n# This is a fix for running over 16 nodes:\nexport FI_CXI_DEFAULT_CQ_SIZE=131072\nexport FI_CXI_OFLOW_BUF_SIZE=8388608\nexport FI_CXI_CQ_FILL_PERCENT=20\n# These are workaround for a known Cassini overflow issue\n\nexport FI_LOG_LEVEL=warn\n#export FI_LOG_PROV=tcp\nexport FI_LOG_PROV=cxi\n# These allow for logging from a specific provider (libfabric)\n\nexport MPIR_CVAR_ENABLE_GPU=0\n\n#####################################################################\n# FRAMEWORK Variables that make a performance difference\n#####################################################################\n\n# Toggle tf32 on (or don't):\nexport ITEX_FP32_MATH_MODE=TF32\n\n#####################################################################\n# End of perf-adjustment section\n#####################################################################\n\n#####################################################################\n# Environment set up, using the latest frameworks drop\n#####################################################################\n\nmodule use /soft/modulefiles\nmodule load frameworks\n\nexport NUMEXPR_NUM_THREADS=64\n# This is to resolve an issue due to a package called \"numexpr\".\n# It sets the variable\n# 'numexpr.nthreads' to available number of threads by default, in this case\n# to 208. However, the 'NUMEXPR_MAX_THREADS' is also set to 64 as a package\n# default. The solution is to either set the 'NUMEXPR_NUM_THREADS' to less than\n# or equal to '64' or to increase the 'NUMEXPR_MAX_THREADS' to the available\n# number of threads. Both of these variables can be set manually.\n\n\n## CCL setup\nexport FI_CXI_DEFAULT_CQ_SIZE=131072\nexport FI_CXI_OVFLOW_BUF_SIZE=8388608\nexport FI_CXI_CQ_FILL_PERCENT=20\n\nexport FI_LOG_LEVEL=warn\n#export FI_LOG_PROV=tcp\nexport FI_LOG_PROV=cxi\n\nexport CCL_KVS_GET_TIMEOUT=600\n\nexport LD_LIBRARY_PATH=$CCL_ROOT/lib:$LD_LIBRARY_PATH\nexport CPATH=$CCL_ROOT/include:$CPATH\nexport LIBRARY_PATH=$CCL_ROOT/lib:$LIBRARY_PATH\n\nexport CCL_PROCESS_LAUNCHER=pmix  \nexport CCL_ATL_TRANSPORT=mpi\nexport CCL_ALLREDUCE=topo\nexport CCL_ALLREDUCE_SCALEOUT=rabenseifner  # currently best allreduce algorithm at large scale\nexport CCL_BCAST=double_tree # currently best bcast algorithm at large scale\n\nexport CCL_KVS_MODE=mpi\nexport CCL_CONFIGURATION_PATH=\"\"\nexport CCL_CONFIGURATION=cpu_gpu_dpcpp\nexport CCL_KVS_CONNECTION_TIMEOUT=600 \n\nexport CCL_ZE_CACHE_OPEN_IPC_HANDLES_THRESHOLD=1024\nexport CCL_KVS_USE_MPI_RANKS=1\n\n#####################################################################\n# End of environment setup section\n#####################################################################\n\n#####################################################################\n# JOB LAUNCH\n######################################################################\n\n\nexport CCL_LOG_LEVEL=\"WARN\"\nexport CPU_BIND=\"verbose,list:2-4:10-12:18-20:26-28:34-36:42-44:54-56:62-64:70-72:78-80:86-88:94-96\"\nHOROVOD_THREAD_AFFINITY=\"4,12,20,28,36,44,56,64,72,80,88,96\"\nCCL_WORKER_AFFINITY=\"5,13,21,29,37,45,57,65,73,81,89,97\"\n\nulimit -c 0\n\n# Launch the script\nmpiexec -np ${NRANKS} -ppn ${NRANKS_PER_NODE} \\\n--cpu-bind ${CPU_BIND} \\\npython path/to/application.py\n</code></pre>"},{"location":"aurora/data-science/inference/libtorch/","title":"LibTorch C++ Library","text":"<p>LibTorch is a C++ library for Torch, with many of the APIs that are available in PyTorch. Users can find more information in the PyTorch documentation. This is useful for integrating the Torch ML framework into traditional HPC simulation codes and therefore enables training and inference of ML models. On Aurora, the Intel Extension for PyTorch (IPEX) library is needed to access the Max 1550 GPU, which has the device name <code>kXPU</code> in LibTorch. During compilation, Intel optimizations will be activated automatically once the IPEX dynamic library is linked.</p>"},{"location":"aurora/data-science/inference/libtorch/#environment-setup","title":"Environment Setup","text":"<p>To use LibTorch on Aurora, load the ML frameworks module:</p> <pre><code>module load frameworks\n</code></pre> <p>This will also load the consistent oneAPI SDK (version 2024.2) and <code>cmake</code>.</p>"},{"location":"aurora/data-science/inference/libtorch/#torch-and-ipex-libraries","title":"Torch and IPEX libraries","text":"<p>With the ML frameworks module loaded as shown above, run:</p> <pre><code>python -c 'import torch; print(torch.__path__[0])'\npython -c 'import torch; print(torch.utils.cmake_prefix_path)'\n</code></pre> <p>to find the path to the Torch libraries, include files, and CMake files.</p> <p>For the path to the IPEX dynamic library, run:</p> <pre><code>python -c 'import torch; print(torch.__path__[0].replace(\"torch\",\"intel_extension_for_pytorch\"))'\n</code></pre>"},{"location":"aurora/data-science/inference/libtorch/#linking-libtorch-and-ipex-libraries","title":"Linking LibTorch and IPEX Libraries","text":"<p>When using the CMake build system, LibTorch and IPEX libraries can be linked to an example C++ application using the following <code>CMakeLists.txt</code> file:</p> <pre><code>cmake_minimum_required(VERSION 3.5 FATAL_ERROR)\ncmake_policy(SET CMP0074 NEW)\nproject(project-name)\n\nfind_package(Torch REQUIRED)\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS} -Wl,--no-as-needed\")\nset(TORCH_LIBS ${TORCH_LIBRARIES})\n\nfind_library(IPEX_LIB intel-ext-pt-gpu PATHS ${INTEL_EXTENSION_FOR_PYTORCH_PATH}/lib NO_DEFAULT_PATH REQUIRED)\nset(TORCH_LIBS ${TORCH_LIBS} ${IPEX_LIB})\ninclude_directories(SYSTEM ${INTEL_EXTENSION_FOR_PYTORCH_PATH}/include)\n\nadd_executable(exe main.cpp)\ntarget_link_libraries(exe ${TORCH_LIBS})\n\nset_property(TARGET exe PROPERTY CXX_STANDARD 17)\n</code></pre> <p>and configuring the build with:</p> <pre><code>cmake \\\n    -DCMAKE_PREFIX_PATH=`python -c 'import torch; print(torch.utils.cmake_prefix_path)'` \\\n    -DINTEL_EXTENSION_FOR_PYTORCH_PATH=`python -c 'import torch; print(torch.__path__[0].replace(\"torch\",\"intel_extension_for_pytorch\"))'` \\\n    ./\nmake\n</code></pre>"},{"location":"aurora/data-science/inference/libtorch/#device-introspection","title":"Device Introspection","text":"<p>Similarly to PyTorch, LibTorch provides APIs to perform introspection on the devices available on the system. The simple code below shows how to check if XPU devices are available, how many are present, and how to loop through them to discover some properties.</p> <pre><code>#include &lt;torch/torch.h&gt;\n#include &lt;c10/xpu/XPUFunctions.h&gt;\n\nint main(int argc, const char* argv[])\n{\n  torch::DeviceType device;\n  int num_devices = 0;\n  if (torch::xpu::is_available()) {\n    std::cout &lt;&lt; \"XPU devices detected\" &lt;&lt; std::endl;\n    device = torch::kXPU;\n\n    num_devices = torch::xpu::device_count();\n    std::cout &lt;&lt; \"Number of XPU devices: \" &lt;&lt; num_devices &lt;&lt; std::endl;\n\n    for (int i = 0; i &lt; num_devices; ++i) {\n      c10::xpu::set_device(i);\n      std::cout &lt;&lt; \"Device \" &lt;&lt; i &lt;&lt; \":\" &lt;&lt; std::endl;\n\n      c10::xpu::DeviceProp device_prop{};\n      c10::xpu::get_device_properties(&amp;device_prop, i);\n      std::cout &lt;&lt; \"  Name: \" &lt;&lt; device_prop.name &lt;&lt; std::endl;\n      std::cout &lt;&lt; \"  Total memory: \" &lt;&lt; device_prop.global_mem_size / (1024 * 1024) &lt;&lt; \" MB\" &lt;&lt; std::endl;\n    }\n  } else {\n    device = torch::kCPU;\n    std::cout &lt;&lt; \"No XPU devices detected, setting device to CPU\" &lt;&lt; std::endl;\n  }\n\n  return 0;\n}\n</code></pre>"},{"location":"aurora/data-science/inference/libtorch/#model-inferencing-using-the-torch-api","title":"Model Inferencing Using the Torch API","text":"<p>This example shows how to perform inference with the ResNet50 model using LibTorch. First, get a JIT-traced version of the model by executing <code>python resnet50_trace.py</code> (shown below) on a compute node.</p> <pre><code>import torch\nimport torchvision\nimport intel_extension_for_pytorch as ipex\nfrom time import perf_counter\n\ndevice = 'xpu'\n\nmodel = torchvision.models.resnet50()\nmodel.to(device)\nmodel.eval()\n\ndummy_input = torch.rand(1, 3, 224, 224).to(device)\n\nmodel_jit = torch.jit.trace(model, dummy_input)\ntic = perf_counter()\npredictions = model_jit(dummy_input)\ntoc = perf_counter()\nprint(f\"Inference time: {toc-tic}\")\n\ntorch.jit.save(model_jit, f\"resnet50_jit.pt\")\n</code></pre> <p>Then, build <code>inference-example.cpp</code> (shown below):</p> <pre><code>#include &lt;torch/torch.h&gt;\n#include &lt;torch/script.h&gt;\n\nint main(int argc, const char* argv[]) {\n  torch::jit::script::Module model;\n  try {\n    model = torch::jit::load(argv[1]);\n    std::cout &lt;&lt; \"Loaded the model\\n\";\n  }\n  catch (const c10::Error&amp; e) {\n    std::cerr &lt;&lt; \"error loading the model\\n\";\n    return -1;\n  }\n\n  model.to(torch::Device(torch::kXPU));\n  std::cout &lt;&lt; \"Model offloaded to GPU\\n\\n\";\n\n  auto options = torch::TensorOptions()\n                      .dtype(torch::kFloat32)\n                      .device(torch::kXPU);\n  torch::Tensor input_tensor = torch::rand({1,3,224,224}, options);\n  assert(input_tensor.dtype() == torch::kFloat32);\n  assert(input_tensor.device().type() == torch::kXPU);\n  std::cout &lt;&lt; \"Created the input tensor on GPU\\n\";\n\n  torch::Tensor output = model.forward({input_tensor}).toTensor();\n  std::cout &lt;&lt; \"Performed inference\\n\\n\";\n\n  std::cout &lt;&lt; \"Slice of predicted tensor is : \\n\";\n  std::cout &lt;&lt; output.slice(/*dim=*/1, /*start=*/0, /*end=*/10) &lt;&lt; '\\n';\n\n  return 0;\n}\n</code></pre> <p>and execute it with <code>./inference-example ./resnet50_jit.pt</code>.</p>"},{"location":"aurora/data-science/inference/libtorch/#libtorch-interoperability-with-sycl-pipelines","title":"LibTorch Interoperability with SYCL Pipelines","text":"<p>The LibTorch API can be integrated with data pipelines using SYCL to operate on input and output data already offloaded to the Intel Max 1550 GPU. The code below extends the above example of performing inference with the ResNet50 model by first generating the input data on the CPU, then offloading it to the GPU with SYCL, and finally passing the device pointer to LibTorch for inference using <code>torch::from_blob()</code>, which creates a Torch tensor from a data pointer with zero-copy.</p> <p>The source code for <code>inference-example.cpp</code> is modified as follows:</p> <pre><code>#include &lt;torch/torch.h&gt;\n#include &lt;torch/script.h&gt;\n#include &lt;iostream&gt;\n#include \"sycl/sycl.hpp\"\n#include &lt;vector&gt;\n\nconst int N_BATCH = 1;\nconst int N_CHANNELS = 3;\nconst int N_PIXELS = 224;\nconst int INPUTS_SIZE = N_BATCH*N_CHANNELS*N_PIXELS*N_PIXELS;\nconst int OUTPUTS_SIZE = N_BATCH*N_CHANNELS;\n\nint main(int argc, const char* argv[]) {\n  torch::jit::script::Module model;\n  try {\n    model = torch::jit::load(argv[1]);\n    std::cout &lt;&lt; \"Loaded the model\\n\";\n  }\n  catch (const c10::Error&amp; e) {\n    std::cerr &lt;&lt; \"error loading the model\\n\";\n    return -1;\n  }\n\n  model.to(torch::Device(torch::kXPU));\n  std::cout &lt;&lt; \"Model offloaded to GPU\\n\\n\";\n\n  // Create the input data on the host\n  std::vector&lt;float&gt; inputs(INPUTS_SIZE);\n  srand(12345);\n  for (int i=0; i&lt;INPUTS_SIZE; i++) {\n    inputs[i] = static_cast &lt;float&gt; (rand()) / static_cast &lt;float&gt; (RAND_MAX);\n  }\n  std::cout &lt;&lt; \"Generated input data on the host \\n\\n\";\n\n  // Move input data to the device with SYCL\n  sycl::queue Q(sycl::gpu_selector_v);\n  std::cout &lt;&lt; \"SYCL running on \"\n            &lt;&lt; Q.get_device().get_info&lt;sycl::info::device::name&gt;()\n            &lt;&lt; \"\\n\\n\";\n  float *d_inputs = sycl::malloc_device&lt;float&gt;(INPUTS_SIZE, Q);\n  Q.memcpy((void *) d_inputs, (void *) inputs.data(), INPUTS_SIZE*sizeof(float));\n  Q.wait();\n\n  // Pre-allocate the output array on device and fill with a number\n  double *d_outputs = sycl::malloc_device&lt;double&gt;(OUTPUTS_SIZE, Q);\n  Q.submit([&amp;](sycl::handler &amp;cgh) {\n    cgh.parallel_for(OUTPUTS_SIZE, [=](sycl::id&lt;1&gt; idx) {\n      d_outputs[idx] = 1.2345;\n    });\n  });\n  Q.wait();\n  std::cout &lt;&lt; \"Offloaded input data to the GPU \\n\\n\";\n\n  // Convert input array to Torch tensor\n  auto options = torch::TensorOptions()\n                      .dtype(torch::kFloat32)\n                      .device(torch::kXPU);\n  torch::Tensor input_tensor = torch::from_blob(\n                                 d_inputs,\n                                 {N_BATCH,N_CHANNELS,N_PIXELS,N_PIXELS},\n                                 options);\n  assert(input_tensor.dtype() == torch::kFloat32);\n  assert(input_tensor.device().type() == torch::kXPU);\n  std::cout &lt;&lt; \"Created the input Torch tensor on GPU\\n\\n\";\n\n  // Perform inference\n  torch::NoGradGuard no_grad; // equivalent to \"with torch.no_grad():\" in PyTorch\n  torch::Tensor output = model.forward({input_tensor}).toTensor();\n  std::cout &lt;&lt; \"Performed inference\\n\\n\";\n\n  // Copy the output Torch tensor to the SYCL pointer\n  auto output_tensor_ptr = output.contiguous().data_ptr();\n  Q.memcpy((void *) d_outputs, (void *) output_tensor_ptr, OUTPUTS_SIZE*sizeof(double));\n  Q.wait();\n  std::cout &lt;&lt; \"Copied output Torch tensor to SYCL pointer\\n\";\n\n  return 0;\n}\n</code></pre> <p>Note that an additional C++ flag is needed in this case, as shown below in the <code>cmake</code> command:</p> <pre><code>cmake \\\n    -DCMAKE_CXX_FLAGS=\"-std=c++17 -fsycl\" \\\n    -DCMAKE_PREFIX_PATH=`python -c 'import torch; print(torch.utils.cmake_prefix_path)'` \\\n    -DINTEL_EXTENSION_FOR_PYTORCH_PATH=`python -c 'import torch; print(torch.__path__[0].replace(\"torch\",\"intel_extension_for_pytorch\"))'` \\\n    ./\n</code></pre>"},{"location":"aurora/data-science/inference/openvino/","title":"Model Inference with OpenVINO","text":"<p>OpenVINO is a library developed by Intel specifically designed for accelerating inference of ML models on their CPU and GPU hardware.  This page contains build and run instructions for Python, but please refer to the OpenVINO GitHub page for more information.</p>"},{"location":"aurora/data-science/inference/openvino/#installing-the-openvino-python-runtime-and-cli-tools","title":"Installing the OpenVINO Python Runtime and CLI Tools","text":"<p>OpenVINO does not come with the default frameworks module on Aurora, but it can be installed manually within a Python virtual environment as shown below: <pre><code>module load frameworks\npython -m venv --clear /path/to/_ov_env --system-site-packages\nsource /path/to/_ov_env/bin/activate\npip install openvino==2024.4.0\npip install openvino-dev==2024.4.0\n</code></pre></p> <p>It is recommended that the path to the virtual environment be in the user's project space on Flare.</p>"},{"location":"aurora/data-science/inference/openvino/#model-converter","title":"Model Converter","text":"<p>The first suggested step is to convert the model from one of the ML frameworks into OpenVINO's Intermediate Representation (IR).  This consists of an <code>.xml</code> file which describes the network topology and a <code>.bin</code> file which contains the weights and biases in binary format.  The conversion can be done from the command line with <code>ovc</code> or using the Python API <code>openvino.convert_model()</code>. Note that PyTorch models cannot be converted directly with <code>ovc</code> and need to be converted to ONNX format first. You can find more information on the conversion process on OpenVINO's documentation page.</p> <p>The following code snippet demonstrates how to use the Python API to convert the ResNet50 model from TorchVision and save the OpenVINO IR. <pre><code>import openvino as ov\nimport torch\nfrom torchvision.models import resnet50\n\nmodel = resnet50(weights='DEFAULT')\ninput_data = torch.rand(1, 3, 224, 224)\n\nov_model = ov.convert_model(model, example_input=input_data)\nov.save_model(ov_model, 'resnet50.xml')\n</code></pre></p> <p>Information on using the CLI conversion tool can be found by running <code>ovc -h</code>, which will save the model in IR format by default.</p> <p>Note that by default, both <code>ovc</code> and <code>openvino.save_model()</code> perform compression of the model weights to FP16. This reduces the memory needed to store the model and can provide an increase in performance.  To disable this feature, use: <pre><code>ov.save_model(ov_model, 'resnet50.xml', compress_to_fp16=False)\n</code></pre></p> <p>or</p> <pre><code>ovc model.onnx --compress_to_fp16=False\n</code></pre>"},{"location":"aurora/data-science/inference/openvino/#benchmark-app","title":"Benchmark App","text":"<p>Before writing a script or program to perform inference with the OpenVINO runtime, the performance of the model can be tested with the CLI tool <code>benchmark_app</code>. </p> <p>A minimal example to run on a single Intel Max 1550 tile is shown below: <pre><code>benchmark_app -m resnet50.xml -hint latency -d GPU.0 -data_shape [1,3,224,224]\n</code></pre></p> <p>which returns a series of information on the parameters set for the benchmark tests and the performance of the tests. The last few lines of the output are shown below.</p> <pre><code>[ INFO ] Execution Devices:['GPU.0']\n[ INFO ] Count:            42847 iterations\n[ INFO ] Duration:         60001.96 ms\n[ INFO ] Latency:\n[ INFO ]    Median:        1.38 ms\n[ INFO ]    Average:       1.38 ms\n[ INFO ]    Min:           1.35 ms\n[ INFO ]    Max:           21.31 ms\n[ INFO ] Throughput:   714.09 FPS\n</code></pre> <p>Note that <code>benchmark_app</code> takes a number of additional configuration options which are listed by running <code>benchmark_app -h</code>. </p>"},{"location":"aurora/data-science/inference/openvino/#inference-with-python-openvino-api","title":"Inference with Python OpenVINO API","text":"<p>Inference can be performed by invoking the compiled model directly or using the OpenVINO Runtime API explicitly to create inference requests.</p> <p>An example of performing direct inference with the compiled model is shown below.  This leads to compact code, but it performs a single synchronous inference request.  Future calls to the model will reuse the same inference request created, thus experiencing less overhead. <pre><code>import openvino as ov\nimport torch\n\ncore = ov.Core()\ncompiled_model = core.compile_model(\"resnet50.xml\")\n\ninput_data = torch.rand((1, 3, 224, 224))\nresults = compiled_model(input_data)[0]\n</code></pre></p> <p>By default, OpenVINO performs inference with FP16 precision on GPU, but the precision and device can be selected with hints, such as: <pre><code>import openvino.properties.hint as hints\ncore.set_property(\n    \"GPU.0\",\n    {hints.execution_mode: hints.ExecutionMode.ACCURACY},\n)\n</code></pre></p> <p>More information on the available hints can be found on the OpenVINO documentation page.</p> <p>Other than the direct call to the model, the Runtime API can be used to create inference requests and control their execution. For this approach, we refer the user to the OpenVINO documentation page.</p>"},{"location":"aurora/debugging/ddt-aurora/","title":"Debugging on Aurora with DDT","text":"<p>We have licenses for Linaro DDT on Aurora, a parallel debugger that is able to use gdb-oneapi as its underlying engine on Aurora. This is not a tutorial on DDT; for that, you should use Linaro's documentation, such as the documentation bundled with their client programs. Here we provide specific information on using DDT interactively in the GUI client-server mode on Aurora.</p>"},{"location":"aurora/debugging/ddt-aurora/#client","title":"Client","text":"<p>Download and install the latest Linaro Forge client for your desktop/laptop system from the Linaro website. This is available for Linux, macOS, and Windows systems.</p>"},{"location":"aurora/debugging/ddt-aurora/#configuring-the-remote-client","title":"Configuring the Remote Client","text":"<p>Before you can start a DDT debugging session on Aurora compute nodes, you must set your client for remote connection from Aurora compute nodes. Your client window should look something like this:</p> <p></p> <p>Click the Remote Launch pull-down and click Configure to create a connector for Aurora:</p> <p></p> <p>Click the Add button on the Configure Remote Connections screen:</p> <p></p> <p>Create a configuration named \"aurora,\" and set it up like this example, replacing \"username\" with your actual ALCF login name:</p> <p></p> <p>The path in the Remote Installation Directory field changes when new versions of DDT are installed on Aurora. To find the correct path, use the <code>which</code> command. After you've loaded the <code>forge</code> module, this will show you the path to use (remove the <code>/bin/ddt</code> portion when entering it into the DDT client Remote Installation Directory field):</p> <pre><code>aurora-uan-0011&gt; which ddt\n/opt/aurora/24.180.3/support/tools/forge/24.1.1/bin/ddt\naurora-uan-0011&gt;\n</code></pre> <p>You may want to test the configuration. To do that, click the Test Remote Launch button. If you see a login prompt like the following example, use your usual ALCF one-time password:</p> <p></p> <p>If the test is successful, you are ready to proceed from an Aurora compute node.</p>"},{"location":"aurora/debugging/ddt-aurora/#invoking-the-ddt-server-from-aurora","title":"Invoking the DDT Server from Aurora","text":"<p>To run DDT interactively from Aurora, start up an interactive PBS job. You'll need to load a module to access DDT:</p> <pre><code>module load forge\n</code></pre> <p>If you are using a wrapper script to map MPI ranks to PVC GPU tiles, you must set this environment variable to the full path to that wrapper script:</p> <pre><code>export FORGE_DEBUGGER_WRAPPER=/opt/aurora/24.180.3/support/tools/mpi_wrapper_utils/gpu_tile_compact.sh\n</code></pre> <p>(The path to the default <code>gpu_tile_compact.sh</code> script changes when there's a new default software module update. You may find the up-to-date path using the command <code>which gpu_tile_compact.sh</code>.)</p> <p>As discussed with respect to gdb-oneapi, you must explicitly enable GPU debugging on all the PVC GPUs you are using, on all the nodes you are using. One way to do this is to create a script and execute it across all your compute nodes using <code>mpiexec</code>. Here is an example script, which takes an argument <code>1</code> to enable debugging or <code>0</code> to disable it:</p> helper_toggle_eu_debug.sh<pre><code>#!/usr/bin/env bash\n# helper_toggle_eu_debug.sh\n\nexport MY_RANK=${PMIX_RANK}\nexport MY_NODE=${PALS_NODEID}\nexport MY_LOCAL_RANK=${PALS_LOCAL_RANKID}\n\neu_debug_toggle() {\n  for f in /sys/class/drm/card*/prelim_enable_eu_debug\n  do\n    echo $1 &gt; $f\n  done\n  echo \"INFO: EU debug state on rank-${MY_RANK}: $(cat /sys/class/drm/card*/prelim_enable_eu_debug | tr '\\n' ' ')\"\n  # sleep 10\n}\n\n# One rank per node toggles eu debug:\nif [ ${MY_LOCAL_RANK} -eq 0 ]; then\n    eu_debug_toggle $1\nfi\n</code></pre> <p>From the interactive prompt on your lead Aurora compute node, issue</p> <pre><code>export NNODES=`wc -l &lt; $PBS_NODEFILE`\nmpiexec -n $NNODES ./helper_toggle_eu_debug.sh 1\nZET_ENABLE_PROGRAM_DEBUGGING=1\n</code></pre> <p>To start the DDT server and connect to your client, make sure your client is running and you have selected the remote connection to Aurora you created as shown above. On the Aurora compute node shell prompt, issue the command to debug your binary like this example, which starts up DDT on 16 nodes, with 12 MPI ranks per node:</p> <pre><code>ddt --np=192 --connect --mpi=generic --mpiargs=\"-l --ppn 12 --cpu-bind verbose,list:0-7,104-111:8-15,112-119:16-23,120-127:24-31,128-135:32-39,136-143:40-47,144-151:52-59,156-163:60-67,164-171:68-75,172-179:76-83,180-187:84-91,188-195:92-99,196-203 -envall\" ./a.out\n</code></pre> <p>On the client, you should see a connection pop-up like this:</p> <p></p> <p>Click the Connect button. This should bring up a DETAILS pane that looks like the following example. Make sure the Implementation is set to \"Generic\" (change it to Generic if it isn't). Likewise, confirm and adjust the number of OpenMP threads and other parameters to be correct for your run. For PVC debugging, the Intel Xe box should be checked:</p> <p></p> <p>When you are satisfied with the details, click the Run button. This should pop up a window that shows the multiple processes starting up. If that startup completes normally, the pop-up will disappear and your client window should reveal the full DDT debugging GUI interface, something like this example:</p> <p></p> <p>From here, you should be able to control starting and stopping processes, ranks, and threads (CPU and GPU threads). If you set a breakpoint or otherwise stop in the source code for a GPU-offloaded kernel, you should be able to click the Thread radio button and see threads with a \"GPU\" badge on them. As mentioned above, this is not meant to be full documentation on how to use DDT. A good place to start with that is to open the User Guide from the Help menu in the client application.</p>"},{"location":"aurora/debugging/debugging-overview/","title":"Debugging on Aurora - Overview","text":"<p>There are 3 debuggers available on Aurora:</p> <ol> <li>gdb-oneapi - This is Intel's version of gdb augmented to allow debugging kernels executing on the PVC GPUs.</li> <li>DDT - The Linaro parallel debugger. This is the same parallel debugger that we have on Polaris. It supports a client-server mode and (via using <code>gdb-oneapi</code> internally) debugging kernels executing on the PVC GPUs.</li> <li>gdb4hpc - An alternative for CPU debugging only that will apply commands to all threads in the MPI process group.</li> </ol>"},{"location":"aurora/debugging/gdb-oneapi/","title":"Debugging on Aurora with <code>gdb-oneapi</code>","text":"<p>The <code>gdb-oneapi</code> tool is part of Intel's oneAPI software and is available via the default modules loaded on Aurora. It provides the ability to debug kernels offloaded to the PVC GPUs, as well as CPU code debugging. It does not provide multiprocess or multinode debugging; it is not integrated with MPI. For parallel debugging, we recommend using DDT. You may also use noninteractive debugging for all or selected MPI ranks.</p> <p>You may find it useful to peruse the Intel\u00ae Distribution for GDB* Documentation. For generic documentation on <code>gdb</code>, refer to the FSF guide Debugging with GDB.</p>"},{"location":"aurora/debugging/gdb-oneapi/#preliminaries","title":"Preliminaries","text":"<p>To use <code>gdb-oneapi</code> effectively, you need to compile and link your application with <code>-g</code>. To get anywhere with GPU debugging, the current best practice is to compile and link with <code>-g -O0</code>.</p> <p>Before you debug with <code>gdb-oneapi</code>, you must explicitly enable GPU debugging on all the PVC GPUs you are using, on all the nodes you are using. One way to do this is to create a script and execute it across all your compute nodes using <code>mpiexec</code>. Here is an example script, which takes an argument <code>1</code> to enable debugging or <code>0</code> to disable it:</p> <pre><code>#!/usr/bin/env bash\n# helper_toggle_eu_debug.sh\n\nexport MY_RANK=${PMIX_RANK}\nexport MY_NODE=${PALS_NODEID}\nexport MY_LOCAL_RANK=${PALS_LOCAL_RANKID}\n\neu_debug_toggle() {\n  for f in /sys/class/drm/card*/prelim_enable_eu_debug\n  do\n    echo $1 &gt; $f\n  done\n  echo \"INFO: EU debug state on rank-${MY_RANK}: $(cat /sys/class/drm/card*/prelim_enable_eu_debug | tr '\\n' ' ')\"\n  # sleep 10\n}\n\n# One rank per node toggles eu debug:\nif [ ${MY_LOCAL_RANK} -eq 0 ]; then\n    eu_debug_toggle $1\nfi\n</code></pre> <p>From the interactive prompt on your lead Aurora compute node, issue</p> <pre><code>export NNODES=`wc -l &lt; $PBS_NODEFILE`\nmpiexec -n $NNODES ./helper_toggle_eu_debug.sh 1\nZET_ENABLE_PROGRAM_DEBUGGING=1\n</code></pre>"},{"location":"aurora/debugging/gdb-oneapi/#notes-on-gpu-debugging","title":"Notes on GPU Debugging","text":"<p>The man page and <code>gdb-oneapi --help</code> do not include any information about GPU debugging\u2014only the generic <code>gdb</code> information. The current build of <code>gdb-oneapi</code> does support the TUI (Text User Interface) mode via the <code>--tui</code> command-line switch. The <code>help</code> command from the <code>(gdb)</code> command prompt command-line interface does not offer any insights into GPU debugging, since the commands to use are really just the normal gdb commands. The key is that it provides access to GPU threads, not just CPU threads. If you query the threads, you will see CPU threads (such as OpenMP threads) and example GPU threads if there are any scheduled. The GPU threads look like the last line in this example output, in which 2.481 is a single GPU thread id running on that GPU. All the other threads in this example are CPU threads, which are mostly waiting for GPU kernels to complete:</p> Example gdb output<pre><code>    (gdb) info threads -s\n    Id      Target Id                                          Frame\n    1.1     Thread 0x155523298880 (LWP 25335) \"xgc-es-cpp-gpu\" 0x000015552d310407 in sched_yield () from /lib64/libc.so.6\n    1.3     Thread 0x15551b307700 (LWP 27775) \"xgc-es-cpp-gpu\" 0x000015552d2efba1 in clock_nanosleep@GLIBC_2.2.5 () from /lib64/libc.so.6\n    1.4     Thread 0x155515e9b700 (LWP 27809) \"xgc-es-cpp-gpu\" 0x000015552d32bcdf in epoll_wait () from /lib64/libc.so.6\n    1.5     Thread 0x155505c17780 (LWP 28039) \"xgc-es-cpp-gpu\" 0x000015552d41a70c in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\n    1.6     Thread 0x155505815800 (LWP 28046) \"xgc-es-cpp-gpu\" 0x000015552d41a70c in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\n    1.7     Thread 0x155505413880 (LWP 28056) \"xgc-es-cpp-gpu\" 0x000015552d41a70c in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\n    1.8     Thread 0x155505011900 (LWP 28062) \"xgc-es-cpp-gpu\" 0x000015552d41a70c in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\n    1.9     Thread 0x155504c0f980 (LWP 28065) \"xgc-es-cpp-gpu\" 0x000015552d41a70c in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\n    1.10    Thread 0x15550480da00 (LWP 28070) \"xgc-es-cpp-gpu\" 0x000015552d41a70c in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\n    1.11    Thread 0x15550440ba80 (LWP 28075) \"xgc-es-cpp-gpu\" 0x000015552d41a70c in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\n    1.12    Thread 0x155504009b00 (LWP 28080) \"xgc-es-cpp-gpu\" 0x000015552d41a70c in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\n    1.13    Thread 0x155503c07b80 (LWP 28096) \"xgc-es-cpp-gpu\" 0x000015552d41a70c in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\n    1.14    Thread 0x155503805c00 (LWP 28110) \"xgc-es-cpp-gpu\" 0x000015552d41a70c in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\n    1.15    Thread 0x155503403c80 (LWP 28121) \"xgc-es-cpp-gpu\" 0x000015552d41a70c in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\n    1.16    Thread 0x155503001d00 (LWP 28137) \"xgc-es-cpp-gpu\" 0x000015552d41a70c in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\n    1.17    Thread 0x155502bffd80 (LWP 28151) \"xgc-es-cpp-gpu\" 0x000015552d41a70c in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\n    1.18    Thread 0x1555027fde00 (LWP 28153) \"xgc-es-cpp-gpu\" 0x000015552d41a70c in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\n    1.19    Thread 0x1555023fbe80 (LWP 28155) \"xgc-es-cpp-gpu\" 0x000015552d41a70c in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\n    1.20    Thread 0x155501ffa700 (LWP 28160) \"xgc-es-cpp-gpu\" 0x000015552d41a70c in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\n    * 2.481:0 ZE 0.7.4.0                                       get_f0_grid&lt;Kokkos::Device&lt;Kokkos::Experimental::SYCL, Kokkos::Experimental::SYCLDeviceUSMSpace&gt; &gt;\n     (grid=..., magnetic_field=..., species=..., vgrid=..., pol_decomp=...,\n     part=..., grid_wts0=..., f0_ptl=...) at getf0.tpp:22\n</code></pre> <p>You may use the <code>thread apply</code> command followed by a specific thread number, followed by a <code>gdb</code> command, to execute that command on the specific thread. For example:</p> <pre><code>thread apply 2.481 where\n</code></pre> <p>This will show the call stack for that GPU thread, which should show the GPU kernel function calls.</p> <p>To set a mode where the stepping commands such as <code>next</code> and <code>stepi</code> only apply to a single thread, use:</p> <pre><code>set scheduler-locking step\n</code></pre> <p>You may find it useful to look at PVC assembly code. In stepping through GPU code, you may use:</p> <pre><code>disassemble $pc - 0x20, $pc + 0x20\n</code></pre> <p>This shows the assembly code for a range of instructions before and after the current step (program counter). Adjust the hex value larger or smaller than <code>0x20</code> to increase/decrease the range of assembly instructions displayed.</p> <p>When debugging in GPU code, you should be able to use the usual <code>gdb</code> inspection commands such as <code>print</code> to look at GPU data structures, variables, and registers.</p>"},{"location":"aurora/debugging/gdb-oneapi/#stopping-at-gpu-segmentation-faults-aka-page-faults","title":"Stopping at GPU Segmentation Faults (a.k.a. \"Page Faults\")","text":"<p>GPU segmentation faults are a common reason for debugging. To make <code>gdb-oneapi</code> stop where they occur, use</p> <pre><code>handle all stop print\n</code></pre> <p>before the first <code>run</code> command (or sometime before you expect the fault to happen).</p>"},{"location":"aurora/debugging/gdb-oneapi/#noninteractive-debugging","title":"Noninteractive Debugging","text":"<p>For MPI programs run using a wrapper script to map ranks to GPUs, you may use a modified wrapper script to invoke a set of predetermined <code>gdb-oneapi</code> commands on some or all of the ranks. For example: mpi-wrapper-gdb-oneapi.sh<pre><code>#!/bin/bash\ndisplay_help() {\n  echo \" Will map MPI ranks to gpu tiles in compact and then round-robin fashion\"\n  echo \" Usage:\"\n  echo \"   mpiexec --np N $gpu_tile_compact_gdb-oneapi ./a.out\"\n  exit 1\n}\n\nnum_gpu=6\nnum_tile=2\n\nif [ \"$#\" -eq 0 ] || [ \"$1\" == \"--help\" ] || [ \"$1\" == \"-h\" ] || [ \"$num_gpu\" = 0 ]; then\n  display_help\nfi\n\ngpu_id=$(( (PALS_LOCAL_RANKID / num_tile ) % num_gpu ))\ntile_id=$((PALS_LOCAL_RANKID % num_tile))\n\nunset EnableWalkerPartition\nexport EnableImplicitScaling=0\nexport ZE_ENABLE_PCI_ID_DEVICE_ORDER=1\nexport ZE_AFFINITY_MASK=$gpu_id.$tile_id\n\nexport ZET_ENABLE_PROGRAM_DEBUGGING=1 # needed for gdb-oneapi:\n\ngdb-oneapi -batch -ex \"handle all stop print\" -ex run -ex \"thread apply all bt\" --args $* &gt;out.${PBS_JOBID%.*}.$PALS_RANKID 2&gt;err.${PBS_JOBID%.*}.$PALS_RANKID\n</code></pre></p> <p>This example prints a backtrace where GPU segmentation violations or other types of errors occur, and pipes the output into file names including the MPI rank number.</p>"},{"location":"aurora/debugging/gdb4hpc/","title":"HPE gdb4hpc on Aurora","text":"<p>The gdb4hpc is not a GPU-aware debugger but can be used to debug general code problems at scale. This debugger will apply commands to all threads in the MPI process group.</p>"},{"location":"aurora/debugging/gdb4hpc/#attaching-to-a-running-job","title":"Attaching to a running job","text":"<p>Determine the <code>jobid</code> of interest:</p> <pre><code>qstat -u $USER\n</code></pre> <pre><code>harms@aurora-uan-0009:~/working/all2all&gt; qstat -u $USER\n\naurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov: \n                                                            Req'd  Req'd   Elap\nJob ID          Username Queue    Jobname    SessID NDS TSK Memory Time  S Time\n--------------- -------- -------- ---------- ------ --- --- ------ ----- - -----\n127750.aurora-* harms    workq    all2all       --    4   4    --  00:30 R   -- \n</code></pre> <p>Next find a node the job is running on. Choose the first node in the list of <code>vnodes</code>: <pre><code>qstat -f 127750 | grep exec_vnode\n</code></pre></p> <pre><code>harms@aurora-uan-0009:~/working/all2all&gt; qstat -f 127750 | grep exec_vnode\nexec_vnode = (x4305c2s6b0n0:ncpus=1)+(x4305c2s7b0n0:ncpus=1)+(x4305c4s0b0n0\n</code></pre> <p>Log in to this node, find your <code>mpiexec</code> process id, and run <code>gdb4hpc</code>:</p> <pre><code>ssh x4305c2s6b0n0\nps -eaf | grep mpiexec\nmodule load gdb4hpc\nCTI_WLM_IMPL=ssh gdb4hpc\n</code></pre> Example output <pre><code>harms@aurora-uan-0009:~/working/all2all&gt; ssh x4305c2s6b0n0\nharms@x4305c2s6b0n0:~&gt; ps -eaf | grep mpiexec\nharms    108581 108569  0 16:05 ?        00:00:00 mpiexec -l --no-transfer --line-buffer --np 16 -ppn 4 --cpu-bind core ./a2a-p2p\nharms    109440 109354  0 16:11 pts/4    00:00:00 grep --color=auto mpiexec\nharms@x4305c2s6b0n0:~&gt; module load gdb4hpc\nharms@x4305c2s6b0n0:~&gt; CTI_WLM_IMPL=ssh gdb4hpc\n\ngdb4hpc 4.14.7 - Cray Line Mode Parallel Debugger\nWith Cray Comparative Debugging Technology.\nCopyright 2007-2022 Hewlett Packard Enterprise Development LP.\nCopyright 1996-2016 University of Queensland. All Rights Reserved.\n\nType \"help\" for a list of commands.\nType \"help &lt;cmd&gt;\" for detailed help about a command.\ndbg all&gt;\n</code></pre> <p>Now attach to the <code>mpiexec</code> process:</p> <pre><code>  dbg all&gt; attach $a &lt;pid&gt;\n</code></pre> Example output <pre><code>dbg all&gt; attach $a 108581\n0/16 ranks connected... (timeout in 299 seconds)\n0/16 ranks connected... (timeout in 298 seconds)\n...\n12/16 ranks connected... (timeout in 300 seconds)\n16/16 ranks connected.\nCreated network...\nConnected to application...\nCurrent rank location:\na{0}: #0  0x00001472aba12699 in MPIDI_progress_test\n... backtrace ...\n</code></pre>"},{"location":"aurora/hardware-overview/machine-overview/","title":"Aurora Machine Overview","text":"<p>Aurora is a 10,624-node HPE Cray-Ex based system. It has 166 racks with 21,248 CPUs and 63,744 GPUs. Each node consists of 2 Intel Xeon CPU Max Series (codename Sapphire Rapids or SPR) with on-package HBM and 6 Intel Data Center GPU Max Series (codename Ponte Vecchio or PVC). Each Xeon CPU has 52 physical cores supporting 2 hardware threads per core and 64 GB of HBM. Each CPU socket has 512 GB of DDR5 memory. The GPUs are connected all-to-all with Intel X<sup>e</sup> Link interfaces. Each node has 8 HPE Slingshot-11 NICs, and the system is connected in a Dragonfly topology. The GPUs may send messages directly to the NIC via PCIe, without the need to copy into CPU memory.</p> <p></p> <p>Figure 1: Summary of the compute, memory, and communication hardware contained within a single Aurora node.</p> <p>The Intel Data Center GPU Max Series is based on X<sup>e</sup> Core. Each X<sup>e</sup> core consists of 8 vector engines and 8 matrix engines with 512 KB of L1 cache that can be configured as cache or Shared Local Memory (SLM). 16 X<sup>e</sup> cores are grouped together to form a slice. 4 slices are combined along with a large L2 cache and 4 HBM2E memory controllers to form a stack or tile. One or more stacks/tiles can then be combined on a socket to form a GPU. More detailed information about node architecture can be found here.</p>"},{"location":"aurora/hardware-overview/machine-overview/#aurora-compute-node","title":"Aurora Compute Node","text":"NODE COMPONENT DESCRIPTION PER NODE AGGREGATE Processor 2000 MHz 2 21,248 Cores/Threads Intel Xeon CPU Max 9470C Series 104/208 1,104,896/2,209,792 CPU HBM HBM2e 64x2 GiB 1.328 PiB CPU DRAM DDR5 512x2 GiB 10.375 PiB GPUs Intel Data Center Max 1550 Series 6 63,744 GPU HBM HBM2e 768 GiB 7.968 PiB"},{"location":"aurora/hardware-overview/machine-overview/#aurora-gpu-architecture-summary","title":"Aurora GPU Architecture Summary","text":"GPU COMPONENT DESCRIPTION COUNT CAPABILITY Stack a.k.a. Tile 2 X<sup>e</sup> Vector Engine a.k.a. EU (execution unit) 512 per Stack (448 active) 8 threads, 512b SIMD X<sup>e</sup> Matrix Engine a.k.a. systolic part of EU 512 per Stack (448 active) Register 512-bit register 128 per thread X<sup>e</sup> Core a.k.a. subslice; unit of 8 EUs 64 per Stack 128 per GPU L1 cache 128 KiB Last Level cache a.k.a. RAMBO cache 384 MiB per GPU <p>See Aurora Overview for more information.</p>"},{"location":"aurora/node-performance-overview/node-performance-overview/","title":"Single node \"GPU-Peak\" benchmarks","text":"<p>This work was done on a pre-production supercomputer with early versions of the Aurora software development kit.</p> <p>This page aims to give you a high-level overview of key performance numbers for a single Aurora node.</p> <ul> <li>We are providing both 1 Tile and Full Node numbers.</li> <li>The Full Node numbers are the weak scaling version of the single node one.</li> <li>The Full Node numbers have been achieved by one rank per tile, 12 ranks.</li> <li>All benchmarks' source code and launch options are included so you can tweak them as needed.</li> <li>We are not exhaustive. Please assume we cherry-picked the correct size to get the best numbers.</li> <li>We will not compare the results to some \u201ctheoretical\u201d value. Theoretical values are full of assumptions, and we want to keep this page short.</li> <li>We will not compare the results to other hardware. Feel free to do it yourself \ud83d\ude42</li> <li>To improve reproducibility, only the \u201cbest\u201d numbers are reported (e.g., we take the minimum time of repetition step). When doing \"real\" science, please perform better statistical analysis.</li> <li>The code will use a mixture of OpenMP and SYCL in C++ (sorry, Fortran, Python, and Level Zero lovers).</li> </ul> <p>The asterisk (<code>*</code>) means that the data was collected on Sunspot with an older software stack.</p>"},{"location":"aurora/node-performance-overview/node-performance-overview/#micro-benchmarks","title":"Micro-benchmarks","text":"One Tile Full Node Scaling Single Precision Peak Flops 23 TFlop/s 267 TFlop/s 11.8 Double Precision Peak Flops 17 TFlop/s 187 TFlop/s 10.9 Memory Bandwidth (triad) 1 TB/s 12 TB/s 11.9 PCIe Unidirectional Bandwidth (H2D) 54 GB/s 329 GB/s 6.1 PCIe Unidirectional Bandwidth (D2H) 55 GB/s 263 GB/s 4.8 PCIe Bidirectional Bandwidth 76 GB/s 357 GB/s 4.7 Tile2Tile Unidirectional Bandwidth 196 GB/s 1 TB/s 6.0 Tile2Tile Bidirectional Bandwidth 287 GB/s 2 TB/s 5.9 GPU2GPU Unidirectional Bandwidth 15 GB/s 95 GB/s 6.3 GPU2GPU Bidirectional Bandwidth 23 GB/s 142 GB/s 6.2"},{"location":"aurora/node-performance-overview/node-performance-overview/#benchmark-description","title":"Benchmark description","text":"<ul> <li>Double Precision Peak Flops: Chain of FMA.</li> <li>Memory Bandwidth (triad): Triad, 2 loads, 1 store.</li> <li>PCIe Unidirectional Bandwidth (H2D): Host to Device data transfer.</li> <li>PCIe Unidirectional Bandwidth (D2H): Device to Host data transfer.</li> <li>PCIe Bidirectional Bandwidth: Concurrent Host to Device and Device to Host data transfer.</li> <li>Tile2Tile Unidirectional Bandwidth: MPI Rank 0 (GPU N, Tile 0) will send a GPU buffer to Rank 1 (GPU N, Tile 1).</li> <li>Tile2Tile Bidirectional Bandwidth: MPI Rank 0 (GPU N, Tile 0) will send a GPU buffer to Rank 1 (GPU N, Tile 1). Concurrently, Rank 1 will also send a buffer to Rank 0.</li> <li>GPU2GPU Unidirectional Bandwidth: MPI Rank 0 (GPU 0, Tile 0) will send a GPU buffer to Rank 1 (GPU 1, Tile 0).</li> <li>GPU2GPU Bidirectional Bandwidth: MPI Rank 0 (GPU 0, Tile 0) will send a GPU buffer to Rank 1 (GPU 1, Tile 0). Concurrently, Rank 1 will also send a buffer to Rank 0.</li> </ul>"},{"location":"aurora/node-performance-overview/node-performance-overview/#gemm","title":"GEMM","text":"One Tile Full Node Scaling DGEMM 15 TFlop/s 179 TFlop/s 11.9 SGEMM 22 TFlop/s 258 TFlop/s 11.7 HGEMM 263 TFlop/s 2606 TFlop/s 9.9 BF16GEMM 273 TFlop/s 2645 TFlop/s 9.7 TF32GEMM 110 TFlop/s 1311 TFlop/s 11.9 I8GEMM 577 TFlop/s 5394 TFlop/s 9.4"},{"location":"aurora/node-performance-overview/node-performance-overview/#fft","title":"FFT","text":"One Tile Full Node Scaling Single-precision FFT C2C 1D 3 TFlop/s 34 TFlop/s 10.8 Single-precision FFT C2C 2D 3 TFlop/s 35 TFlop/s 10.4 <p>Don't hesitate to contact ALCF staff (via email or Slack) for complaints, bug reports, or praise.</p>"},{"location":"aurora/performance-tools/advisor/","title":"Advisor","text":""},{"location":"aurora/performance-tools/advisor/#references","title":"References","text":"<p>Intel Advisor User Guide</p> <p>Intel Advisor Performance Optimization Cookbook</p>"},{"location":"aurora/performance-tools/advisor/#introduction","title":"Introduction","text":"<p>Intel\u00ae Advisor is a design and analysis tool for developing performant code. The tool supports C, C++, Fortran, SYCL, OpenMP, OpenCL\u2122 code, and Python. It helps with the following:</p> <ul> <li>Performant CPU Code: Design your application for efficient threading, vectorization, and memory use.</li> <li>Efficient GPU Offload: Identify parts of the code that can be profitably offloaded. Optimize the code for compute and memory.</li> <li>Flow Graph Design and Analysis: Create, visualize, and analyze task and dependency computation for heterogeneous algorithms. </li> </ul>"},{"location":"aurora/performance-tools/advisor/#roofline-and-performance-insights-for-gpus","title":"Roofline and Performance Insights for GPUs","text":"<p>Get actionable advice for performant GPU code. In addition to the Roofline Analysis for kernels, you can:</p> <ul> <li>Get specific, actionable recommendations to design code that runs optimally on GPUs.</li> <li>See the CPU and GPU code performance side-by-side with a unified dashboard.</li> <li>Discover GPU application performance characterization, such as bandwidth sensitivity, instruction mix, and cache-line use.</li> </ul>"},{"location":"aurora/performance-tools/advisor/#offload-modeling","title":"Offload Modeling","text":"<p>Understand if your code benefits from GPU porting or how much performance acceleration your GPU code can get from moving to a next-generation GPU. You can:</p> <ul> <li>Pinpoint offload opportunities where it pays off the most.</li> <li>Project the performance on a GPU.</li> <li>Identify bottlenecks and potential performance gains.</li> <li>Get guidance for optimizing data transfer between host and target devices.</li> </ul>"},{"location":"aurora/performance-tools/advisor/#a-quick-instruction-for-advisor-roofline-analysis-on-intel-gpus","title":"A quick instruction for Advisor roofline analysis on Intel GPUs","text":"<p>Step 1: Setting the environments</p> <pre><code>$ module load oneapi\n$ export PRJ=&lt;your_project_dir&gt;\n</code></pre> <p>Step 2-a: Collecting the GPU Roofline data on a single GPU (Survey analysis and Trip Count with FLOP analysis)</p> <pre><code>$ advisor --collect=roofline --profile-gpu --project-dir=$PRJ -- &lt;your_executable&gt; &lt;your_arguments&gt;\n</code></pre> <p>Step 2-b: Collecting the GPU Roofline data on one of MPI ranks (Survey analysis and Trip Count with FLOP analysis)</p> <pre><code>$ mpirun -n 1 gpu_tile_compact.sh advisor --collect=survey --profile-gpu --project-dir=$PRJ -- &lt;your_executable&gt; &lt;your_arguments&gt; : -n 1 gpu_tile_compact.sh &lt;your_executable&gt; &lt;your_arguments&gt;\n$ mpirun -n 1 gpu_tile_compact.sh advisor --collect=tripcounts --profile-gpu --flop --no-trip-counts --project-dir=$PRJ -- &lt;your_executable&gt; &lt;your_arguments&gt; : -n 1 gpu_tile_compact.sh &lt;your_executable&gt; &lt;your_arguments&gt;\n</code></pre> <p>Step 3-a: Generate a GPU Roofline report, and then review the HTML report</p> <pre><code>$ advisor --report=all --project-dir=$PRJ --report-output=${PRJ}/roofline_all.html\n</code></pre> <p>Step 3-b: Download the project folder to your local system and open it with the stand-alone Advisor Client</p>"},{"location":"aurora/performance-tools/advisor/#simple-examples","title":"Simple examples","text":""},{"location":"aurora/performance-tools/advisor/#advisor-roofline-analysis-for-one-mpi-rank-out-of-12-mpi-ranks","title":"Advisor roofline analysis for one MPI rank out of 12 MPI ranks","text":"<pre><code>$ mpiexec -n 1 gpu_tile_compact.sh advisor --collect=survey --profile-gpu --project-dir=Advisor_results -- ./Comp_GeoSeries_omp_mpicxx_DP 2048 1000 : -n 11 gpu_tile_compact.sh ./Comp_GeoSeries_omp_mpicxx_DP 2048 1000\n$ mpiexec -n 1 gpu_tile_compact.sh advisor --collect=tripcounts --profile-gpu --flop --no-trip-counts --project-dir=Advisor_results -- ./Comp_GeoSeries_omp_mpicxx_DP 2048 1000 : -n 11 gpu_tile_compact.sh ./Comp_GeoSeries_omp_mpicxx_DP 2048 1000\n$ advisor --report=all --project-dir=Advisor_results --report-output=Advisor_results/roofline_all.html\n</code></pre>"},{"location":"aurora/performance-tools/performance-overview/","title":"Performance Tools Overview","text":""},{"location":"aurora/performance-tools/vtune/","title":"VTune","text":""},{"location":"aurora/performance-tools/vtune/#references","title":"References","text":"<p>Intel VTune Profiler User Guide</p> <p>Downloadable documents for VTune Profiler</p>"},{"location":"aurora/performance-tools/vtune/#introduction","title":"Introduction","text":"<p>Intel VTune Profiler can be used to find and fix performance bottlenecks quickly. There are several options (i.e., GPU Hotspots analysis, GPU Offload analysis, and HPC Performance Characterization analysis) available for Intel CPUs and GPUs on Aurora.</p> <p>Intel\u00ae VTune\u2122 Profiler is a performance analysis tool for serial, multithreaded, GPU-accelerated applications. Use VTune Profiler to analyze your choice of algorithm. Identify potential benefits for your application on Intel CPUs and GPUs on Aurora.</p> <p>Use VTune Profiler to locate or determine:</p> <ul> <li>The most time-consuming (hot) functions in your application and/or on the whole system</li> <li>Sections of code that do not effectively utilize available processor time</li> <li>The best sections of code to optimize for sequential performance and for threaded performance</li> <li>Synchronization objects that affect the application performance</li> <li>Whether, where, and why your application spends time on input/output operations</li> <li>Whether your application is CPU or GPU bound and how effectively it offloads code to the GPU</li> <li>The performance impact of different synchronization methods, different numbers of threads, or different algorithms</li> <li>Thread activity and transitions</li> <li>Hardware-related issues in your code such as data sharing, cache misses, branch misprediction, and others</li> </ul>"},{"location":"aurora/performance-tools/vtune/#vtune-analysis-types-for-intel-gpus","title":"VTune analysis types for Intel GPUs","text":""},{"location":"aurora/performance-tools/vtune/#gpu-offload","title":"GPU offload","text":"<pre><code>vtune \u2013collect gpu-offload &lt;target&gt;\n</code></pre> <p>This analysis enables you to: * Identify how effectively your application uses SYCL, OpenMP, or OpenCL kernels and explore them further with GPU Compute/Media Hotspots analysis * Analyze execution of Intel Media SDK tasks over time * Explore GPU usage and analyze a software queue for GPU engines at each moment of time</p>"},{"location":"aurora/performance-tools/vtune/#gpu-computemedia-hotspots","title":"GPU Compute/Media Hotspots","text":"<pre><code>vtune \u2013collect gpu-hotspots &lt;target&gt;\n</code></pre> <p>Use the GPU Compute/Media Hotspots analysis to: * Explore GPU kernels with high GPU utilization, estimate the effectiveness of this utilization, identify possible reasons for stalls or low occupancy, and options. * Explore the performance of your application per selected GPU metrics over time. * Analyze the hottest SYCL* standards or OpenCL\u2122 kernels for inefficient kernel code algorithms or incorrect work item configuration.</p> <p>The GPU Compute/Media Hotspots analysis is a good next step if you have already run the GPU Offload analysis and identified: * a performance-critical kernel for further analysis and optimization; * a performance-critical kernel that is tightly connected with other kernels in the program and may slow down their performance.</p> <p>For source-level in-kernel profiling, applications should be built with -fdebug-info-for-profiling -gline-tables-only.</p>"},{"location":"aurora/performance-tools/vtune/#a-quick-instruction-for-vtune-analysis-on-intel-gpus","title":"A quick instruction for VTune analysis on Intel GPUs","text":"<p>GPU hotspots analysis can be used as the first step. Without special knobs, its overhead is minimal, and it provides useful performance data such as kernel time, instance count, SIMD width, EU Array active/stalled/idle ratio, EU occupancy, GPU barriers/atomic, and so on. The following are simple instructions on Intel GPUs:</p>"},{"location":"aurora/performance-tools/vtune/#running-an-application-with-vtune-on-intel-gpus","title":"Running an application with VTune on Intel GPUs","text":"<pre><code>module load oneapi\n\n### To run an application on a single stack of a GPU\nZE_AFFINITY_MASK=0.0 vtune -collect gpu-hotspots -r VTune_results_1S -- ./a.out\n\n### To run an application on two stacks of a single GPU\nZE_AFFINITY_MASK=0 vtune -collect gpu-hotspots -r VTune_results_2S -- ./a.out\n\n### To run an MPI application (e.g., 24 MPI ranks on two Aurora nodes)\nmpirun -n 24 gpu_tile_compact.sh vtune -collect gpu-hotspots -r VTune_results_MPI -- ./a.out\n\n### To run an MPI application with VTune on a select MPI (e.g., MPI rank 5 out of 24 ranks)\nmpirun -n 5 gpu_tile_compact.sh ./a.out : -n 1 gpu_tile_compact.sh vtune -collect gpu-hotspots -r VTune_results_MPI_5 -- ./a.out : -n 18 ./a.out \n</code></pre>"},{"location":"aurora/performance-tools/vtune/#checking-if-vtune-collection-is-successful-or-not","title":"Checking if VTune collection is successful or not","text":"<p>After successful VTune analysis, VTune provides Hottest GPU Computing Tasks with High Sampler Usage with non-zero data. The following is an example from a GeoSeries benchmark:</p> <pre><code>Hottest GPU Computing Tasks with High Sampler Usage\nComputing Task                                                                                                                         Total Time\n-------------------------------------------------------------------------------------------------------------------------------------  ----------\nComp_Geo(cl::sycl::queue, double*, double*, int, int)::{lambda(cl::sycl::handler&amp;)#1}::operator()(cl::sycl::handler&amp;) const::Comp_Geo      0.627s\nzeCommandListAppendMemoryCopy         \n</code></pre>"},{"location":"aurora/performance-tools/vtune/#after-collecting-the-performance-data-vtune-profiler-web-server-can-be-used-for-the-post-processing","title":"After collecting the performance data, VTune profiler web server can be used for the post-processing.","text":"<p>Step 1: Open a new terminal and log into an Aurora login node (no X11 forwarding required) <pre><code>ssh &lt;username&gt;@bastion.alcf.anl.gov\nssh &lt;username&gt;@login.aurora.alcf.anl.gov\n</code></pre> Step 2: Start VTune server on an Aurora login node after loading the oneAPI module and setting the corresponding environmental variables for VTune <pre><code>module load oneapi\nvtune-backend --data-directory=&lt;location of precollected VTune results&gt;\n</code></pre> Step 3: Open a new terminal with SSH port forwarding enabled <pre><code>ssh -L 127.0.0.1:&lt;port printed by vtune-backend&gt;:127.0.0.1:&lt;port printed by vtune-backend&gt; &lt;username&gt;@aurora.alcf.anl.gov\n</code></pre></p> <p>Step 4: Check if the login nodes of Step 2 and Step 3 are the same or not. If not (e.g., <code>aurora-uan-0009</code> from Step 2 and <code>aurora-uan-0010</code> from Step 3), run <code>ssh</code> on the terminal for Step 3 to the login node of Step 2 <pre><code>ssh -L 127.0.0.1:&lt;port printed by vtune-backend&gt;:127.0.0.1:&lt;port printed by vtune-backend&gt; aurora-uan-xxxx\n</code></pre></p> <p>Step 5: Open the URL printed by VTune server in Firefox web browser on your local computer. For a security warning, click \"Advanced...\" and then \"Accept the Risk and Continue\".</p> <ul> <li> <p>Accept VTune server certificate: When you open VTune GUI, your web browser will complain about VTune self-signed certificate. You either need to tell the web browser to proceed or install the VTune server certificate on your client machine so that the browser trusts it. To install the certificate, note the path to the public part of the certificate printed by VTune server in the output, copy it to your client machine, and add it to the trusted certificates.</p> </li> <li> <p>Set the passphrase: When you run the server for the first time, the URL that it outputs contains a one-time token. When you open such a URL in the browser, VTune server prompts you to set a passphrase. Other users can't access your VTune server without knowing this passphrase. The hash of the passphrase will be persisted on the server. Also, a secure HTTP cookie will be stored in your browser so that you do not need to enter the passphrase each time you open VTune GUI.</p> </li> </ul> <p></p> <p></p>"},{"location":"aurora/performance-tools/vtune/#simple-examples","title":"Simple examples","text":""},{"location":"aurora/performance-tools/vtune/#vtune-gpu-offload-analysis","title":"VTune gpu-offload analysis","text":"<pre><code>mpiexec -n 12 gpu_tile_compact.sh vtune -collect gpu-offload -r VTune_gpu-offload ./Comp_GeoSeries_omp_mpicxx_DP 2048 1000\n</code></pre>"},{"location":"aurora/performance-tools/vtune/#vtune-gpu-hotspots-analysis","title":"VTune gpu-hotspots analysis","text":"<pre><code>mpiexec -n 12 gpu_tile_compact.sh vtune -collect gpu-hotspots -r VTune_gpu-hotspots ./Comp_GeoSeries_omp_mpicxx_DP 2048 1000\n</code></pre>"},{"location":"aurora/performance-tools/vtune/#vtune-instruction-count-analysis","title":"VTune instruction count analysis","text":"<pre><code>mpiexec -n 12 gpu_tile_compact.sh vtune -collect gpu-hotspots -knob characterization-mode=instruction-count -r VTune_inst-count ./Comp_GeoSeries_omp_mpicxx_DP 2048 1000\n</code></pre>"},{"location":"aurora/performance-tools/vtune/#vtune-source-analysis","title":"VTune source analysis","text":"<pre><code>mpiexec -n 12 gpu_tile_compact.sh vtune -collect gpu-hotspots -knob profiling-mode=source-analysis -r VTune_source ./Comp_GeoSeries_omp_mpicxx_DP 2048 1000\n</code></pre>"},{"location":"aurora/performance-tools/vtune/#vtune-memory-latency-analysis","title":"VTune memory latency analysis","text":"<pre><code>mpiexec -n 12 gpu_tile_compact.sh vtune -collect gpu-hotspots -knob profiling-mode=source-analysis -knob source-analysis=mem-latency -r VTune_mem-latency ./Comp_GeoSeries_omp_mpicxx_DP 2048 1000\n</code></pre>"},{"location":"aurora/programming-models/kokkos-aurora/","title":"Kokkos","text":""},{"location":"aurora/programming-models/kokkos-aurora/#kokkos","title":"Kokkos","text":"<p>Kokkos Core implements a programming model in C++ for writing performance-portable applications targeting all major HPC platforms. For that purpose, it provides abstractions for both parallel execution of code and data management. Kokkos is designed to target complex node architectures with N-level memory hierarchies and multiple types of execution resources. It currently can use Serial and OpenMP (threads) for CPU execution spaces (\"backends\") and CUDA, HIP, SYCL, and OpenMPTarget for GPU execution spaces. By convention, Kokkos only allows one GPU backend at a time.</p>"},{"location":"aurora/programming-models/kokkos-aurora/#kokkos-documentation","title":"Kokkos Documentation","text":"<ul> <li>Kokkos-core Wiki</li> <li>Kokkos GitHub</li> </ul>"},{"location":"aurora/programming-models/kokkos-aurora/#kokkos-on-aurora","title":"Kokkos on Aurora","text":"<p>The prebuilt Kokkos on Aurora includes 3 backends: Serial and OpenMP for CPU execution and SYCL for GPU execution (with ahead-of-time (AOT) compilation, not just-in-time (JIT) compilation). To use it, run:</p> <pre><code>module load kokkos\n</code></pre> <p>This sets the following environment variables, some of which are used by <code>cmake</code>:</p> <ul> <li><code>KOKKOS_ROOT</code> - path to the <code>lib64/</code>, <code>include/</code> files installed</li> <li><code>LIBRARY_PATH</code> - prepends <code>$KOKKOS_ROOT/lib64</code> to this variable used by <code>cmake</code></li> <li><code>CPATH</code> - prepends <code>$KOKKOS_ROOT/include</code> to this variable used by <code>cmake</code></li> <li><code>LD_LIBRARY_PATH</code> - prepends <code>$KOKKOS_ROOT/lib64</code> to this variable</li> </ul>"},{"location":"aurora/programming-models/kokkos-aurora/#building-a-kokkos-application-using-cmake","title":"Building a Kokkos Application Using <code>cmake</code>","text":"<p>Add these lines to <code>CMakeLists.txt</code>:</p> <pre><code>find_package(Kokkos REQUIRED)\ntarget_link_libraries(myTarget Kokkos::kokkoscore)\n</code></pre> <p>Here is a simple example <code>CMakeLists.txt</code> to compile an example program:</p> <pre><code>cmake_minimum_required(VERSION 3.22)\nproject(buildExample)\nfind_package(Kokkos REQUIRED)\n\nset(buildExample_SOURCE_DIR \".\")\n\nset(top_SRCS\n  ${buildExample_SOURCE_DIR}/example1.cpp)\n\nset(SOURCE_FILES ${top_SRCS})\n\nadd_executable(example1_sycl_aot ${SOURCE_FILES})\ntarget_link_libraries(example1_sycl_aot Kokkos::kokkoscore)\ntarget_include_directories(example1_sycl_aot PUBLIC ${buildExample_SOURCE_DIR})\n</code></pre> <p>Configure and build it like this:</p> <pre><code>mkdir build\ncd build\ncmake -DCMAKE_CXX_COMPILER=CC -DCMAKE_C_COMPILER=cc ..\nmake\n</code></pre>"},{"location":"aurora/programming-models/kokkos-aurora/#building-a-kokkos-application-using-make","title":"Building a Kokkos Application Using <code>make</code>","text":"<p>Here's an example <code>Makefile</code>:</p> <pre><code># KOKKOS_ROOT set via:\n#   module load kokkos\n# \n# You can look at the first lines of\n# $KOKKOS_ROOT/lib64/cmake/Kokkos/KokkosConfigCommon.cmake to see the flags\n# used in cmake configuration of the kokkos library build. The default Kokkos\n# module on Aurora was built with the default oneAPI module and includes\n# Serial, OpenMP (threads) and SYCL backends. So you should have that\n# environment module loaded and include compiler flags for sycl and openmp:\n\n# Aurora MPICH wrapper for C++ and C compilers:\nCXX=\"mpic++ -cxx=icpx\"\nCC=\"mpicc -cc=icx\"\n\nSYCL_AOT_CPPFLAGS=-fsycl -fsycl-targets=spir64_gen -fno-sycl-id-queries-fit-in-int -fsycl-dead-args-optimization -fsycl-unnamed-lambda -std=c++17\nSYCL_AOT_LDFLAGS=-Xsycl-target-backend \"-device pvc\"\n\nCPPFLAGS=-g -O2 -fiopenmp -I $(KOKKOS_ROOT)/include $(SYCL_AOT_CPPFLAGS) -Wno-deprecated-declarations -Wno-tautological-constant-compare -Wno-unknown-attributes -ffp-model=precise\n\nLDFLAGS=$(CPPFLAGS) $(SYCL_AOT_LDFLAGS)\nLDLIBS=-L$(KOKKOS_ROOT)/lib64 -lkokkoscore -lkokkossimd -lkokkoscontainers -lpthread\n\nSRCS=example1.cpp\nOBJS=$(subst .cpp,.o,$(SRCS))\n\nall: example1_aurora\n\nexample1_aurora: $(OBJS)\n    $(CXX) $(LDFLAGS) -o example1_aurora $(OBJS) $(LDLIBS)\n\nexample1.o: example1.cpp\n\nclean:\n    rm -f $(OBJS)\n\ndistclean: clean\n    rm -f example1_aurora\n</code></pre>"},{"location":"aurora/programming-models/kokkos-aurora/#configuring-your-own-kokkos-build-on-aurora","title":"Configuring Your Own Kokkos Build on Aurora","text":"<p>Here are recommended environment settings and configuration to build your own Kokkos libraries on Aurora:</p>"},{"location":"aurora/programming-models/kokkos-aurora/#environment","title":"Environment","text":"<p>To match what was done in the centrally-built Kokkos associated with the modules discussed above, use the same oneAPI version as indicated in <code>module help kokkos</code> and use the Aurora MPICH wrapper <code>mpic++ -cxx=icpx</code> as the C++ compiler (or just use <code>icpx</code> if you're not using MPI). To build Kokkos, you'll need CMake.</p>"},{"location":"aurora/programming-models/kokkos-aurora/#cmake-configuration","title":"CMake Configuration","text":"<p>This example builds three backends: OpenMP, Serial, and SYCL.</p> <pre><code>git clone git@github.com:kokkos/kokkos.git\ncd kokkos\nmkdir build\ncd build\n\ncmake\\\n    -DCMAKE_BUILD_TYPE=RelWithDebInfo\\\n    -DCMAKE_CXX_COMPILER=icpx\\\n    -DCMAKE_CXX_EXTENSIONS=OFF\\\n    -DCMAKE_CXX_STANDARD=17\\\n    -DKokkos_ENABLE_TESTS=OFF\\\n    -DKokkos_ENABLE_SERIAL=ON\\\n    -DKokkos_ENABLE_OPENMP=ON\\\n    -DKokkos_ENABLE_SYCL=ON\\\n    -DKokkos_ARCH_INTEL_GEN=ON\\\n    -DKokkos_ENABLE_SYCL_RELOCATABLE_DEVICE_CODE=OFF\\\n    -DKokkos_ENABLE_IMPL_SYCL_DEVICE_GLOBAL_SUPPORTED=OFF\\\n    -DBUILD_SHARED_LIBS=ON\\\n    -DKokkos_ENABLE_DEPRECATED_CODE_3=ON\\\n    -DKokkos_ENABLE_DEBUG=OFF\\\n    -DKokkos_ENABLE_EXAMPLES=OFF\\\n    -DCMAKE_CXX_FLAGS=\"-Wno-deprecated-declarations -Wno-tautological-constant-compare -ffp-model=precise -Xsycl-target-backend \\\"-device pvc\\\"\"\\\n    -DCMAKE_EXE_LINKER_FLAGS=\"-ffp-model=precise -fsycl-max-parallel-link-jobs=20 -fno-sycl-rdc\"\\\n    -DCMAKE_VERBOSE_MAKEFILE=OFF\\\n    -DCMAKE_INSTALL_PREFIX=/path/to/your/install/directory\\\n    ..\n\nmake -j16 -l16 install\n</code></pre>"},{"location":"aurora/programming-models/level-0/","title":"Level-Zero on Aurora","text":""},{"location":"aurora/programming-models/level-0/#overview","title":"Overview","text":"<p>The objective of the \u2018oneAPI\u2019 Level-Zero Application Programming Interface (API) is to provide direct-to-metal interfaces to offload accelerator devices. Its programming interface can be tailored to any device's needs and can be adapted to support a broader set of language features such as function pointers, virtual functions, unified memory, and I/O capabilities.</p>"},{"location":"aurora/programming-models/level-0/#setting-the-environment-to-use-level-zero-on-aurora","title":"Setting the environment to use Level-Zero on Aurora","text":"<p>The Intel Programming Environment is the main environment on Aurora. The Intel Compute Runtime is part of this environment and grants access to Level-Zero. The Intel Compute Runtime is loaded by default in your environment.</p> <pre><code>&gt; module list\n\nCurrently Loaded Modules:\n  1) gcc/11.2.0                    3) intel_compute_runtime/release/agama-devel-551   5) libfabric/1.15.2.0   7) cray-libpals/1.3.3\n  2) mpich/51.2/icc-all-pmix-gpu   4) oneapi/eng-compiler/2022.12.30.003              6) cray-pals/1.3.3\n</code></pre>"},{"location":"aurora/programming-models/level-0/#building-on-aurora","title":"Building on Aurora","text":"<p>Level-Zero is a C API that can be used in your application by including the <code>ze_api.h</code> file:</p> <pre><code>#include &lt;ze_api.h&gt;\n</code></pre> <p>Applications that use the Level-Zero API need to be linked to the Level-Zero loader library by using the <code>-lze_loader</code> linker flag.</p>"},{"location":"aurora/programming-models/level-0/#level-zero-documentation","title":"Level-Zero Documentation","text":"<p>The Level-Zero documentation can be found here: Level-Zero Specification.</p>"},{"location":"aurora/programming-models/opencl-aurora/","title":"OpenCL","text":""},{"location":"aurora/programming-models/opencl-aurora/#overview","title":"Overview","text":"<p>OpenCL\u2122 (Open Computing Language) is an open, royalty-free standard for cross-platform, parallel programming of diverse accelerators found in supercomputers, cloud servers, personal computers, mobile devices, and embedded platforms. OpenCL greatly improves the speed and responsiveness of a wide spectrum of applications in numerous market categories, including professional creative tools, scientific and medical software, vision processing, and neural network training and inferencing.</p>"},{"location":"aurora/programming-models/opencl-aurora/#setting-the-environment-to-use-opencl-on-aurora","title":"Setting the environment to use OpenCL on Aurora","text":"<p>The Intel Programming Environment is the main environment on Aurora. The Intel Compute Runtime is part of this environment and grants access to OpenCL. The Intel Compute Runtime is loaded by default in your environment.</p> <pre><code>&gt; module list\n\nCurrently Loaded Modules:\n  1) gcc/11.2.0                    3) intel_compute_runtime/release/agama-devel-551   5) libfabric/1.15.2.0   7) cray-libpals/1.3.3\n  2) mpich/51.2/icc-all-pmix-gpu   4) oneapi/eng-compiler/2022.12.30.003              6) cray-pals/1.3.3\n</code></pre>"},{"location":"aurora/programming-models/opencl-aurora/#building-on-aurora","title":"Building on Aurora","text":"<p>OpenCL is a C API that can be used in your application by including the <code>CL/opencl.h</code> file:</p> <pre><code>#include &lt;CL/opencl.h&gt;\n</code></pre> <p>Applications that use the OpenCL API need to be linked to the OpenCL loader library by using the <code>-lOpenCL</code> linker flag when using Make. For CMake-based builds, you can find the OpenCL package and then link it to your targets as shown below:</p> <pre><code>find_package(OpenCL REQUIRED)\ntarget_link_libraries(my_target PRIVATE OpenCL::OpenCL)\n</code></pre> <p>During the configure step, CMake may need help finding the OpenCL library. You can provide hints to CMake by setting the <code>OpenCL_LIBRARY</code> and <code>OpenCL_INCLUDE_DIR</code> variables. If you use <code>icx</code> or <code>icpx</code> on Aurora, you can do the following during the configure step:</p> <pre><code>export OPENCL_BASE_DIR=$(dirname $(which icx))/..\ncmake -DOpenCL_LIBRARY=${OPENCL_BASE_DIR}/lib/libOpenCL.so \\\n      -DOpenCL_INCLUDE_DIR=${OPENCL_BASE_DIR}/include/sycl \\\n      &lt;other_cmake_options&gt;\n</code></pre> <p>C++ bindings exist and can be used in C++ applications by including the <code>CL/opencl.hpp</code> file:</p> <pre><code>#include &lt;CL/opencl.hpp&gt;\n</code></pre>"},{"location":"aurora/programming-models/opencl-aurora/#opencl-documentation","title":"OpenCL Documentation","text":"<p>The OpenCL Specification and the OpenCL Reference Pages are provided by Khronos.</p> <p>Documentation for the C++ bindings is available here: OpenCL C++ Bindings.</p>"},{"location":"aurora/programming-models/openmp-aurora/","title":"OpenMP on Aurora","text":""},{"location":"aurora/programming-models/openmp-aurora/#overview","title":"Overview","text":"<p>The OpenMP API is an open standard for parallel programming. The specification document can be found here: OpenMP Specification. The specification describes directives, runtime routines, and environment variables that allow an application developer to express parallelism (e.g., shared memory multiprocessing and device offloading). Many compiler vendors provide implementations of the OpenMP specification (OpenMP Specifications).</p>"},{"location":"aurora/programming-models/openmp-aurora/#setting-the-environment-to-use-openmp-on-aurora","title":"Setting the environment to use OpenMP on Aurora","text":"<p>The Intel oneAPI Programming Environment is the main environment on Aurora to maximally use the hardware. oneAPI has OpenMP support for both CPU threads and GPU devices. The oneAPI module is loaded by default in your environment:</p> <pre><code>&gt; module list\n\nCurrently Loaded Modules:\n  1) gcc/11.2.0                    3) intel_compute_runtime/release/agama-devel-551   5) libfabric/1.15.2.0   7) cray-libpals/1.3.3\n  2) mpich/51.2/icc-all-pmix-gpu   4) *oneapi/eng-compiler/2022.12.30.003*              6) cray-pals/1.3.3\n</code></pre> <p>However, additional versions of oneAPI with newer compiler versions can be found by adding additional modules to your path:</p> <pre><code>&gt; module use /soft/modulefiles/\n&gt; module avail oneapi\n\n-------------------------------------------------------------------------------- /soft/modulefiles ---------------------------------------------------------------------------------\n   oneapi/eng-compiler/2023.05.15.003    oneapi/eng-compiler/2023.10.15.002        oneapi/release/2023.10.15.001        spack-pe-oneapi/0.5-rc1 (D)\n   oneapi/eng-compiler/2023.05.15.006    oneapi/eng-compiler/2023.12.15.002 (D)    oneapi/release/2023.12.15.001 (D)\n   oneapi/eng-compiler/2023.05.15.007    oneapi/release/2023.05.15.001             spack-pe-oneapi/0.4-rc1\n\n------------------------------------------------------------------------- /opt/aurora/23.073.0/modulefiles -------------------------------------------------------------------------\n   oneapi/eng-compiler/2022.12.30.003 (L)    oneapi/release/2022.12.30.001\n</code></pre> <p>The additional oneAPI modules can be loaded with <code>module load oneapi/eng-compiler/2023.10.15.002</code>, for example.</p>"},{"location":"aurora/programming-models/openmp-aurora/#building-on-aurora","title":"Building on Aurora","text":"<p>The following table shows the compiler and flags:</p> Language MPI Wrapper Compiler (Underlying Compiler) Flag to Turn on OpenMP Support and Target CPU Threads Additional Flags to Target GPU Devices Fortran mpifort (ifx) <code>-fiopenmp</code> <code>-fopenmp-targets=spir64_gen -Xopenmp-target-backend \"-device pvc\"</code> C mpicc (icx) <code>-fiopenmp</code> <code>-fopenmp-targets=spir64_gen -Xopenmp-target-backend \"-device pvc\"</code> C++ mpicxx (icpx) <code>-fiopenmp</code> <code>-fopenmp-targets=spir64_gen -Xopenmp-target-backend \"-device pvc\"</code>"},{"location":"aurora/programming-models/openmp-aurora/#running-on-aurora","title":"Running on Aurora","text":"<p>To run, you can execute the produced executable or use <code>mpiexec</code> in a job script, and then submit the script to an Aurora queue, like:</p> <pre><code>$ cat submit.sh\n#!/bin/bash -l\n#PBS -l select=1\n#PBS -l walltime=0:30:00\n#PBS -l filesystems=&lt;fs1:fs2&gt;\n#PBS -q &lt;queue&gt; \n#PBS -A &lt;ProjectName&gt;\n\ncd ${PBS_O_WORKDIR}\nmpiexec -n 1 ./executable\n$ # submit to the queue:\n$ qsub -l select=1 -l walltime=0:30:00 -q EarlyAppAccess -A Project ./submit.sh\n</code></pre> <p>In the above, having the PBS options in the script and on the command line is redundant, but we put it there to show both ways of launching. This submits the script to one node in the <code>EarlyAppAccess</code> queue on Aurora, requesting 30 minutes. It will charge project <code>Project</code> for the time. You should replace it with your project name.</p> <p>More details for setting up the job script are in the Job Scheduling and Execution section.</p>"},{"location":"aurora/programming-models/openmp-aurora/#example","title":"Example","text":"<pre><code>$ cat hello.cpp\n#include &lt;stdio.h&gt;\n#include &lt;omp.h&gt;\n\nint main(int argc, char** argv) {\n  printf(\"Number of devices: %d\\n\", omp_get_num_devices());\n\n  #pragma omp target\n  {\n    if (!omp_is_initial_device())\n      printf(\"Hello world from accelerator.\\n\");\n    else\n      printf(\"Hello world from host.\\n\");\n  }\n  return 0;\n}\n</code></pre> <pre><code>$ cat hello.F90\nprogram main\n  use omp_lib\n  implicit none\n  integer flag\n\n  write(*,*) \"Number of devices:\", omp_get_num_devices()\n\n  !$omp target map(from:flag)\n    if (.not. omp_is_initial_device()) then\n      flag = 1\n    else\n      flag = 0\n    endif\n  !$omp end target\n\n  if (flag == 1) then\n    print *, \"Hello world from accelerator\"\n  else\n    print *, \"Hello world from host\"\n  endif\nend program main\n</code></pre>"},{"location":"aurora/programming-models/openmp-aurora/#to-compile","title":"To compile","text":"<pre><code>$ mpicxx -fiopenmp -fopenmp-targets=spir64_gen -Xopenmp-target-backend \"-device pvc\" hello.cpp -o c_test\n$ mpifort -fiopenmp -fopenmp-targets=spir64_gen -Xopenmp-target-backend \"-device pvc\" hello.F90 -o f_test\n</code></pre>"},{"location":"aurora/programming-models/openmp-aurora/#to-run","title":"To run","text":"<pre><code>$ mpiexec -n 1 ./c_test\nNumber of devices: 6\nHello world from accelerator.\n$ mpiexec -n 1 ./f_test\nNumber of devices:            6\nHello world from accelerator\n</code></pre>"},{"location":"aurora/programming-models/raja-aurora/","title":"Raja","text":"<p>Placeholder</p>"},{"location":"aurora/programming-models/sycl-aurora/","title":"SYCL on Aurora","text":""},{"location":"aurora/programming-models/sycl-aurora/#overview","title":"Overview","text":"<p>SYCL is an open, royalty-free, cross-platform abstraction layer that enables code for heterogeneous and offload processors to be written using modern ISO C++. It provides APIs and abstractions to find devices (CPUs, GPUs, FPGAs, etc.) on which code can be executed and to manage data resources and code execution on those devices.</p> <p>The specification can be found here: SYCL 2020 Specification</p>"},{"location":"aurora/programming-models/sycl-aurora/#setting-the-environment-to-use-sycl-on-aurora","title":"Setting the environment to use SYCL on Aurora","text":"<p>The Intel oneAPI Programming Environment is the main environment on Aurora. oneAPI has SYCL support. The oneAPI module is loaded by default in your environment:</p> <pre><code>$ module list\n\nCurrently Loaded Modules:\n  1) gcc-runtime/12.2.0-267awrk   5) gcc/12.2.0                             9) libfabric/1.20.1\n  2) gmp/6.2.1-yctcuid            6) intel_compute_runtime/release/996.26  10) cray-pals/1.4.0\n  3) mpfr/4.2.1-fhgnwe7           7) oneapi/eng-compiler/2024.07.30.002    11) cray-libpals/1.4.0\n  4) mpc/1.3.1-ygprpb4            8) mpich/icc-all-pmix-gpu/20240717\n</code></pre>"},{"location":"aurora/programming-models/sycl-aurora/#building-on-aurora","title":"Building on Aurora","text":"<p>Simply use <code>-fsycl</code>. For CMake, use <code>find_package(IntelSYCL REQUIRED)</code>. See <code>cat $CMPLR_ROOT/lib/cmake/IntelSYCL/IntelSYCLConfig.cmake</code> for more details.</p>"},{"location":"aurora/programming-models/sycl-aurora/#example","title":"Example","text":"<pre><code>$ cat hello_sycl.cpp\n#include &lt;sycl/sycl.hpp&gt;\nint main(int argc, char **argv) {\n  int global_range = 10;\n  // Default Queue\n  sycl::queue Q;\n  // Queue introspection\n  std::cout &lt;&lt; \"Running on \" &lt;&lt; Q.get_device().get_info&lt;sycl::info::device::name&gt;() &lt;&lt; std::endl;\n\n  // Allocate device memory\n  int *A = sycl::malloc_device&lt;int&gt;(global_range, Q);\n  // Blocking kernel that uses the memory\n  Q.parallel_for(global_range, [=](auto id) { A[id] = id; }).wait();\n  // Allocate Host Memory\n  std::vector&lt;int&gt; A_host(global_range);\n  // Blocking copy the device memory to the host\n  Q.copy(A, A_host.data(), global_range).wait();\n  // Free Device Memory\n  sycl::free(A, Q);\n\n  for (size_t i = 0; i &lt; global_range; i++)\n    std::cout &lt;&lt; \"A_host[ \" &lt;&lt; i &lt;&lt; \" ] = \" &lt;&lt; A_host[i] &lt;&lt; std::endl;\n  return 0;\n}\n$ icpx -fsycl hello_sycl.cpp\n$ ./a.out\n</code></pre> <p>More examples can be found here: SYCL Training Examples</p>"},{"location":"aurora/services/gitlab-ci/","title":"Continuous Integration via Gitlab-CI For Aurora","text":""},{"location":"aurora/services/gitlab-ci/#changes-from-the-general-documentation-needed-for-aurora","title":"Changes from the general documentation needed for Aurora:","text":"<p>Currently, https://gitlab-sunspot.alcf.anl.gov must be accessed via a proxy.</p> <p>The following command will connect to an Aurora login node from your local system and establish the required proxy: <code>ssh login.aurora.alcf.anl.gov -D localhost:25565</code></p> <p>(<code>25565</code> is the proxy port, it may be changed as needed.)</p> <p>In order to use the proxy, configure your web browser to use a SOCKS proxy.</p>"},{"location":"aurora/services/gitlab-ci/#instead-of-gitlab-cialcfanlgov-use-gitlab-sunspotalcfanlgov","title":"Instead of gitlab-ci.alcf.anl.gov, use gitlab-sunspot.alcf.anl.gov.","text":""},{"location":"aurora/services/gitlab-ci/#note-the-specific-variables-for-auroras-scheduler","title":"Note the specific variables for Aurora's scheduler:","text":"Cluster Scheduler Variable Name Support docs Aurora PBS ANL_AURORA_SCHEDULER_PARAMETERS Aurora Getting Started"},{"location":"aurora/services/gitlab-ci/#instructions-for-firefox-other-browsers-are-similar","title":"Instructions for Firefox (other browsers are similar):","text":"<ol> <li>Open Firefox settings</li> <li>Navigate to \"General\" &gt; \"Network Settings\" &gt; \"Settings\"      (at the bottom of the General settings page.)</li> <li>Ensure \"Manual proxy configuration\" is selected</li> <li>Fill the \"SOCKS Host\" field with 'localhost'</li> <li>Fill the associated port field with '25565' (or an alternate port you specified in your ssh command)</li> <li>Ensure \"SOCKS v5\" is selected</li> <li>Ensure \"Proxy DNS when using SOCKS v5\" is selected\"</li> <li>Select \"OK\"</li> </ol> <p>NOTE: You will not have internet access in Firefox while using the proxy. Select \"No proxy\" to re-enable internet access.</p> <p>For ease of use, many users have had success using extensions like FoxyProxy, or using a separate web browser for accessing resources that require the proxied connection.</p>"},{"location":"aurora/services/gitlab-ci/#examples-of-gitlab-ciyml-files-for-aurora","title":"Examples of <code>.gitlab-ci.yml</code> files for Aurora:","text":"<p>See the large-example .gitlab-ci.yml file for additional examples.</p> <p>Example: A <code>.gitlab-ci.yml</code> file for an Aurora project</p> <pre><code># this include allows us to reference defaults in anl/ci-resource/defaults\ninclude:\n  - project: 'anl/ci-resources/defaults'\n    ref: main\n    file:\n      - '/runners.yml'\n\nvariables:\n  ANL_AURORA_SCHEDULER_PARAMETERS: \"-A ProjectName -l walltime=0:30:00  -q AuroraQueueName\"\nstages:\n  - stage1\n  - stage2\nshell_test1:\n  stage: stage1\n  extends: .aurora-shell-runner\n  script:\n    - echo \"Shell Job 1\"\nbatch_test:\n  stage: stage2\n  tags:\n  extends: .aurora-batch-runner\n  script:\n    - echo \"Job 2 start\"\n    - echo \"Job end\"\n</code></pre> <p>Example: Running a batch job on the Aurora HPC</p> <pre><code># this include allows us to reference defaults in anl/ci-resource/defaults\ninclude:\n  - project: 'anl/ci-resources/defaults'\n    ref: main\n    file:\n      - '/runners.yml'\n\nvariables:\n ANL_AURORA_SCHEDULER_PARAMETERS: \"-A ProjectName -l walltime=0:30:00  -q AuroraQueueName\"\n\nbatch_test:\n  extends: .aurora-batch-runner\n  script:\n    - echo \"Job start\"\n    - echo \"Job end\"\n</code></pre> <p>Example: Aurora pipeline with custom stages</p> <pre><code># this include allows us to reference defaults in anl/ci-resource/defaults\ninclude:\n  - project: 'anl/ci-resources/defaults'\n    ref: main\n    file:\n      - '/runners.yml'\n\nvariables:\n ANL_AURORA_SCHEDULER_PARAMETERS: \"-A ProjectName -l walltime=0:30:00  -q AuroraQueueName\"\n\nstages:\n  - stage1\n  - stage2\n\ntest1:\n  stage: stage1\n  extends: .aurora-shell-runner\n  script:\n    - export\n    - id\n    - hostname\n    - echo \"Running on aurora with shell runner\" \n    - echo test &gt; test.txt\ntest2:\n  stage: stage2\n  extends: .aurora-batch-runner\n  script:\n    - echo \"Job 2 start\"\n    - echo \"Job 2 end\"\n</code></pre> <p>Example: Gitlab job designed to only run on merge requests</p> <pre><code># this include allows us to reference defaults in anl/ci-resource/defaults\ninclude:\n  - project: 'anl/ci-resources/defaults'\n    ref: main\n    file:\n      - '/runners.yml'\ntest1:\n  rules:\n    - if: $CI_COMMIT_TAG                    # Do not execute jobs for tag context\n      when: never\n    - if: $CI_COMMIT_BRANCH == \"master\"     # Do not run on master, since will run on the merge request just prior\n      when: never\n    - if: $CI_MERGE_REQUEST_IID             # CI_MERGE_REQUEST_IID exists, so run job\n  stage: stage1\n  extends: .aurora-batch-runner\n  script:\n    - echo \"Run test 1\"\n</code></pre>"},{"location":"aurora/services/jupyterhub/","title":"JupyterHub","text":""},{"location":"aurora/visualization/paraview/","title":"Paraview on Aurora","text":""},{"location":"aurora/workflows/adios/","title":"ADIOS2","text":"<p>The Adaptable Input/Output (I/O) System (ADIOS2) is a framework for I/O and streaming of scientific data developed as part of the U.S. DOE Exascale Computing Project. ADIOS2 conveniently provides C, C++, Fortran, and Python APIs for traditional file system I/O, as well as APIs for transporting data between applications running concurrently on HPC systems. Data transport with ADIOS2 can be performed via the file system, wide-area networks (WAN), remote direct memory access (RDMA), or MPI to construct a variety of workflows, such as in-situ (or in-transit) visualization, data analysis, and ML training and inference from ongoing simulations.</p> <p>Users are invited to find more information about ADIOS2 on their GitHub page and their documentation.</p>"},{"location":"aurora/workflows/adios/#accessing-adios2-on-aurora","title":"Accessing ADIOS2 on Aurora","text":"<p>Pre-built modules are available to all users, enabling access to the latest version of ADIOS2 (v2.10). These modules can be displayed with <code>module avail adios2</code> and comprise a CPU-only build and a SYCL build of the library, with the SYCL build being the default. Note that both ADIOS2 modules also load a Spack installation of Python 3.10 with the <code>numpy</code>, <code>mpi4py</code>, and <code>adios2</code> packages. For instance, the default SYCL build can be loaded by executing</p> <pre><code>module load adios2\n</code></pre> <p>A custom build of ADIOS2 is also possible on Aurora. In this case, we recommend users start with the following install script for a base build:</p> install_adios2.sh<pre><code>export CRAYPE_LINK_TYPE=dynamic\ngit clone https://github.com/ornladios/ADIOS2.git ADIOS2\nmkdir adios2-build &amp;&amp; cd adios2-build\ncmake \\\n    -DCMAKE_INSTALL_PREFIX=${PWD}/install \\\n    -DADIOS2_BUILD_EXAMPLES=ON \\\n    -DADIOS2_USE_MPI=ON \\\n    -DADIOS2_HAVE_MPI_CLIENT_SERVER=ON \\\n    -DADIOS2_USE_SST=ON \\\n    -DADIOS2_USE_SSC=ON \\\n    -DADIOS2_USE_Python=OFF \\\n    -DADIOS2_USE_HDF5=OFF \\\n    -DADIOS2_USE_BZip2=OFF \\\n    -DCMAKE_BUILD_TYPE=RelWithDebInfo \\\n    ../ADIOS2 2&gt;&amp;1 | tee adios2_config.log\nmake -j 8 2&gt;&amp;1 | tee adios2_build.log\nmake install 2&gt;&amp;1 | tee adios2_install.log\ncd ..\n</code></pre> <p>Building the Python bindings</p> <p>The Python bindings for ADIOS2 can be built by setting <code>ADIOS2_USE_Python=ON</code>; however, this requires a Python 3 installation to be found. We recommend users load the Python AI/ML module with <code>module load frameworks</code> and build ADIOS2 under this environment. This will require users to augment their Python path with <code>export PYTHONPATH=$PYTHONPATH:/path/to/adios2-build/install/lib/python3.10/site-packages</code> in order to use the <code>adios2</code> package. Alternatively, users can use a custom Python installation, but note that ADIOS2 requires <code>numpy</code> and <code>mpi4py</code> as well.</p> <p>A full list of CMake options is available in the documentation.</p>"},{"location":"aurora/workflows/adios/#mixed-c-and-python-hello-world-example","title":"Mixed C++ and Python Hello World Example","text":"<p>Here we show a basic example of using ADIOS2 to stream data between a C++ data producer (e.g., a simulation) and a Python data consumer (e.g., a data analysis or ML component). Both applications are MPI programs. In this simple workflow, each application loops over a workflow iteration loop, in which the producer writes data to the stream and the consumer reads the data. The ADIOS2 IO engine is set to SST for data streaming, and the engine parameters are set to force the producer to pause execution until the consumer has read the data for a given step. This is not a requirement and can be modified with the <code>RendezvousReaderCount</code>, <code>QueueFullPolicy</code>, and <code>QueueLimit</code> parameters. More information on the SST engine can be found in the documentation as well as in the provided examples.</p> producer.cpp<pre><code>#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;adios2.h&gt;\n#include &lt;mpi.h&gt;\n#include &lt;unistd.h&gt;\n\ntemplate &lt;class T&gt;\nvoid PrintData(const std::vector&lt;T&gt; &amp;data, const int rank, const size_t step)\n{\n    std::cout &lt;&lt; \"\\tProducer Rank[\" &lt;&lt; rank &lt;&lt; \"]: send data [\";\n    for (size_t i = 0; i &lt; data.size(); ++i)\n    {\n        std::cout &lt;&lt; data[i] &lt;&lt; \" \";\n    }\n    std::cout &lt;&lt; \"]\" &lt;&lt; std::endl;\n}\n\nint main(int argc, char *argv[])\n{\n\n    // MPI_THREAD_MULTIPLE is only required if you enable the SST MPI_DP\n    int rank, size, provide;\n    MPI_Init_thread(&amp;argc, &amp;argv, MPI_THREAD_MULTIPLE, &amp;provide);\n    MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &amp;size);\n\n    // Create a new communicator\n    int color = 3130, arank, asize;\n    MPI_Comm app_comm;\n    MPI_Comm_split(MPI_COMM_WORLD, color, rank, &amp;app_comm);\n    MPI_Comm_rank(app_comm, &amp;arank);\n    MPI_Comm_size(app_comm, &amp;asize);\n\n    // ADIOS IO Setup\n    adios2::ADIOS adios(app_comm);\n    adios2::IO sstIO = adios.DeclareIO(\"myIO\");\n    sstIO.SetEngine(\"Sst\");\n    adios2::Params params;\n    params[\"RendezvousReaderCount\"] = \"1\";\n    params[\"QueueFullPolicy\"] = \"Block\";\n    params[\"QueueLimit\"] = \"1\";\n    params[\"DataTransport\"] = \"RDMA\";\n    params[\"OpenTimeoutSecs\"] = \"600\";\n    sstIO.SetParameters(params);\n\n    // Setup the data to send\n    std::vector&lt;float&gt; myArray = {0.0, 1.0, 2.0, 3.0, 4.0};\n    const std::size_t Nx = myArray.size();\n    for (size_t k = 0; k &lt; myArray.size(); ++k) {\n        myArray[k] = myArray[k] + static_cast&lt;float&gt;(Nx * arank);\n    }\n    const float increment = (float)(Nx * asize * 1.0);\n\n    // Define variable and local size\n    auto bpFloats = sstIO.DefineVariable&lt;float&gt;(\"y\", {asize * Nx}, {arank * Nx}, {Nx});\n\n    int workflow_steps = 2;\n    adios2::Engine sstWriter = sstIO.Open(\"data_stream\", adios2::Mode::Write);\n    for (size_t i = 0; i &lt; workflow_steps; ++i) {\n        sleep(3);\n        if (arank == 0)\n            std::cout &lt;&lt; \"\\n Iteration \" &lt;&lt; i &lt;&lt; std::endl;\n        sstWriter.BeginStep();\n        sstWriter.Put&lt;float&gt;(bpFloats, myArray.data());\n        PrintData(myArray, rank, i);\n        sstWriter.EndStep();\n        for (size_t k = 0; k &lt; myArray.size(); ++k) {\n            myArray[k] += increment;\n        }\n    }\n    sstWriter.Close();\n\n    MPI_Finalize();\n\n    return 0;\n}\n</code></pre> consumer.py<pre><code>from mpi4py import MPI\nimport numpy as np\nfrom adios2 import Stream, Adios, bindings\n\n# MPI Init\nCOMM = MPI.COMM_WORLD\nRANK = COMM.Get_rank()\nSIZE = COMM.Get_size()\n\nif __name__ == '__main__':\n    # Create new communicator (needed for launch on MPMD mode)\n    color = 3230\n    app_comm = COMM.Split(color, RANK)\n    asize = app_comm.Get_size()\n    arank = app_comm.Get_rank()\n    adios = Adios(app_comm)\n\n    # ADIOS IO Setup\n    io = adios.declare_io(\"myIO\")\n    io.set_engine(\"SST\")\n    parameters = {\n        'RendezvousReaderCount': '1', # options: 1 for sync, 0 for async\n        'QueueFullPolicy': 'Block', # options: Block, Discard\n        'QueueLimit': '1', # options: 0 for no limit\n        'DataTransport': 'RDMA', # options: MPI, WAN, UCX, RDMA\n        'OpenTimeoutSecs': '600', # number of seconds SST is to wait for a peer connection on Open()\n    }\n    io.set_parameters(parameters)\n\n    # Loop over workflow steps and read data at each step\n    workflow_steps = 2\n    with Stream(io, \"data_stream\", \"r\", app_comm) as stream:\n        for istep in range(workflow_steps):\n            stream.begin_step()\n            var = stream.inquire_variable(\"y\")\n            shape = var.shape()\n            count = int(shape[0] / asize)\n            start = count * arank\n            if arank == asize - 1:\n                count += shape[0] % asize\n            data = stream.read(\"y\", [start], [count])\n            print(f\"\\tConsumer [{arank}]: received data {data}\",flush=True)\n            stream.end_step()\n</code></pre> <p>To build the C++ producer, use the following CMake file:</p> CMakeLists.txt<pre><code>cmake_minimum_required(VERSION 3.12)\nproject(ADIOS2HelloExample)\n\nif(NOT TARGET adios2_core)\n  set(_components CXX)\n\n  find_package(MPI COMPONENTS C)\n  if(MPI_FOUND)\n    # Workaround for various MPI implementations forcing the link of C++ bindings\n    add_definitions(-DOMPI_SKIP_MPICXX -DMPICH_SKIP_MPICXX)\n\n    list(APPEND _components MPI)\n  endif()\n\n  find_package(ADIOS2 REQUIRED COMPONENTS ${_components})\nendif()\n\nadd_executable(producer producer.cpp)\ntarget_link_libraries(producer adios2::cxx11_mpi MPI::MPI_CXX)\n\ninstall(TARGETS producer RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR})\n</code></pre> <p>and execute the following commands:</p> <pre><code>module load adios2\nmodule load cmake\n\ncmake ./\nmake\n</code></pre> <p>The example can be run from an interactive session with the following script, which runs the producer and consumer with two ranks per node and places the producer on socket 0 and the consumer on socket 1 of each node. The producer and consumer can also be run on separate nodes by specifying the <code>--hostfile</code> or <code>--hostlist</code> in the <code>mpiexec</code> commands.</p> run_adios_example.sh<pre><code>#!/bin/bash\n\nmodule load adios2\n\nexport OMP_PROC_BIND=spread\nexport OMP_PLACES=threads\n\nNODES=$(cat $PBS_NODEFILE | wc -l)\nPROCS_PER_NODE=2\nPROCS=$((NODES * PROCS_PER_NODE))\n\n# Run Python example\nmpiexec -n $PROCS --ppn $PROCS_PER_NODE --cpu-bind list:1:2 producer &amp;\nmpiexec -n $PROCS --ppn $PROCS_PER_NODE --cpu-bind list:53:54 python consumer.py\nwait\n</code></pre> <p>Selecting the SST Data Transport Plane</p> <p>The SST data transport plane can be selected with the parameter <code>DataTransport</code>. We recommend using RDMA; however, note that it requires running the applications on more than one node. The WAN data plane can also be used, but it may result in slower data transfer performance at scale. The MPI data plane is currently not available, but we are working on resolving the issue with the ADIOS2 team.</p>"},{"location":"aurora/workflows/balsam/","title":"Balsam on Aurora","text":"<p>Balsam is a toolkit for managing large computational campaigns on HPC systems. Balsam helps users execute large numbers of jobs with inter-job dependencies, track job outcomes, and manage post-processing analysis. The command line interface and Python API make it easy for users to adopt: after wrapping the command line for an application in a few lines of Python code, users can describe jobs with accompanying options. These jobs are stored persistently in the Balsam database. Balsam is especially well-suited for executing large ensembles of MPI tasks with a variety of sizes.</p> <p>A user's Balsam service consists of a Balsam Site process that runs on a login node that orchestrates the execution of work, and a Balsam Site directory space where job and workflow results are stored. When the user submits a batch job to PBS through Balsam, the Site process pulls Balsam jobs from the database and executes them within the PBS batch job, achieving high throughput while incurring only a single wait-time in the queue.</p> <p>The full Balsam documentation covers all functionality for users, including additional examples, and describes the Balsam architecture for potential developers.</p>"},{"location":"aurora/workflows/balsam/#setup-and-installation","title":"Setup and installation","text":"<p>Balsam requires Python 3.7+. To install Balsam on Aurora, first set up a virtual Python environment:</p> <pre><code>module load frameworks\npython -m venv _env\nsource _env/bin/activate\npip install --upgrade pip\npip install --pre balsam\n</code></pre> <p>Alternatively, Balsam can be installed in a conda environment also with pip.</p> <p>The Balsam command line tool will now be in your path. To get information on how to use the command line tool, you can type <code>balsam --help</code> in your shell.</p> <p>To use Balsam, users need an account on the Balsam server. Users can get an account by contacting the ALCF Help Desk. Once a user has an account, they can log in and make a new site. A Balsam site is a project space for your workflow. You will be prompted to select what machine (Aurora) you are working on when creating a new site:</p> <pre><code>balsam login\nbalsam site init -n new-site new-site\ncd new-site\nbalsam site start\n</code></pre>"},{"location":"aurora/workflows/balsam/#aurora-specific-notes","title":"Aurora specific notes","text":"<p>In the Balsam configuration for Aurora, a Balsam <code>gpu</code> refers to an Aurora node GPU tile. Setting the Balsam job option <code>gpus_per_rank = 1</code> will place one rank per GPU tile. Setting <code>gpus_per_rank = 2</code> will place one rank per GPU.</p>"},{"location":"aurora/workflows/balsam/#simple-mpi-ensemble-on-aurora-with-balsam","title":"Simple MPI ensemble on Aurora with Balsam","text":"<p>Here is an example that runs an application <code>hello_affinity</code> from our getting started guide in <code>mpi-mode</code>, which will execute the application with <code>mpiexec</code>. We also show an example of executing an echo command that takes an argument and runs on a single GPU tile.</p> <p>Warning</p> <p>Ensembles of tasks launched with <code>mpiexec</code> on multiple nodes are currently limited to 1000 total tasks run per batch job. This means when <code>mpiexec</code> calls return, the nodes they used can refill only a limited number of times, rather than an arbitrary number of times like on Polaris. This is due to a known issue with Slingshot and will be fixed in the future. Users running MPI application ensembles on Aurora with Balsam should take this into account when configuring their workflows.</p> balsam_job_ensemble.py<pre><code>from balsam.api import ApplicationDefinition, Job\n\n# Define an application that runs an echo command and takes an input\nclass EchoHello(ApplicationDefinition):\n    site = \"test_aurora\"\n    command_template = \"echo Hello, {{ say_hello_to }}! ZE_AFFINITY_MASK=$ZE_AFFINITY_MASK OMP_NUM_THREADS=$OMP_NUM_THREADS\"\nEchoHello.sync()\n\n# Define an application that runs a compiled executable and is wrapped by the gpu affinity script\nclass HelloAffinity(ApplicationDefinition):\n    site = \"test_aurora\"\n    command_template = \"$HOME/GettingStarted/HelperScripts/Aurora/set_affinity_gpu_aurora.sh $HOME/GettingStarted/Examples/Aurora/affinity_gpu/sycl/hello_affinity\"\nHelloAffinity.sync()\n\n# Create a Balsam job that runs hello_affinity on one node with twelve ranks per node\naffinity_job = Job(site_name=\"test_aurora\",\n                   app_id=\"HelloAffinity\",\n                   workdir=f\"demo/hello_affinity\",\n                   tags={\"workflow\":\"demo\"},\n                   num_nodes=1,\n                   node_packing_count=1,\n                   ranks_per_node=12,\n                   gpus_per_rank=1)\n\n# This call saves a single Balsam job to the database\naffinity_job.save()\n\n# Create many Balsam jobs that run the EchoHello app each on a single tile\necho_jobs = [Job( site_name=\"test_aurora\", \n         app_id=\"EchoHello\", \n         workdir=f\"demo/echo_hello{n}\", \n         parameters={\"say_hello_to\": f\"world {n}!\"},\n         tags={\"workflow\":\"demo\"}, \n         node_packing_count=12, # this allows for 12 of these jobs to run concurrently on a node\n         gpus_per_rank=1,\n         ranks_per_node=1,)       \n    for n in range(24)\n]\n\n# This call saves a list of jobs to the database\necho_jobs = Job.objects.bulk_create(echo_jobs)\n</code></pre> <p>After execution of this script, your site will have two registered apps and several Balsam jobs. Use the Balsam CLI tool to query them:</p> <p>To check apps registered in a site: <pre><code>balsam app ls\n</code></pre></p> <p>To check the status of jobs in the site: <pre><code>balsam job ls\n</code></pre></p> <p>To submit a batch job to PBS to execute the Balsam jobs in your site, you can do so at the command line from within your site directory: <pre><code>balsam queue submit -n 2 -t 10 -q debug-scaling -A &lt;project_name&gt; -j mpi\n</code></pre> This will submit a 2-node job (<code>-n</code> option) to the <code>debug-scaling</code> queue in mpi mode (<code>-j</code> option). MPI-mode batch jobs like this one will execute applications with <code>mpiexec</code>.  The time limit for the batch job is set to 10 minutes (<code>-t</code> option).</p> <p>You can also submit jobs with the Python API: submit_mpi_mode.py<pre><code>from balsam.api import BatchJob, Site\n\n# Get the site id\nsite = Site.objects.get(\"test_aurora\")\nsite_id = site.id\n\n# Submit batch job to PBS\nBatchJob.objects.create(\n   site_id=site_id,\n   num_nodes=2,\n   wall_time_min=10,\n   job_mode=\"mpi\", # This mode will execute the application with mpiexec\n   queue=\"debug-scaling\",\n   project=\"Aurora_deployment\", # put your &lt;project_name&gt; here\n)\n</code></pre></p> <p>To check the status of batch jobs that Balsam is tracking: <pre><code>balsam queue ls\n</code></pre> and batch jobs it has completed: <pre><code>balsam queue ls --history\n</code></pre></p> <p>The standard output (stdout) will be written to each job's workdir in the data directory to a file called <code>job.out</code> and can be accessed like this: <pre><code>cat data/demo/*/job.out\n</code></pre></p> <p>Batch jobs created by Balsam will have a name beginning with <code>qlaunch</code> when queried with the <code>PBS</code> command <code>qstat</code>.</p> <p>Balsam has additional features that will submit work to PBS elastically, a special app type for native Python code, and a <code>serial</code> job mode for executing tasks that are single core/GPU that do not require MPI launching. More information can be found in the Balsam documentation.</p>"},{"location":"aurora/workflows/balsam/#troubleshooting","title":"Troubleshooting","text":"<p>If Balsam is failing to submit batch jobs to PBS, check the <code>settings.yml</code> file in the Balsam site directory and look for the section <code>allowed_queues</code>.  The queue you are submitting to must appear in this section of the settings.  If it does not, add it and restart the site process with: <pre><code>balsam site stop\nbalsam site start\n</code></pre></p> <p>If the queue does appear, get more information about the batch jobs Balsam is submitting to PBS with: <pre><code>balsam queue ls -v --history\n</code></pre></p>"},{"location":"aurora/workflows/deephyper/","title":"DeepHyper","text":""},{"location":"aurora/workflows/libensemble/","title":"libEnsemble on Aurora","text":"<p>libEnsemble is a Python toolkit for running dynamic ensembles of calculations.</p> <p>Users provide generator and simulator functions to express their ensembles, where the generator can steer the ensemble based on previous results. These functions can portably submit user executables at any scale.</p> <p>System details are detected, and dynamic resource management is provided. This includes automatically detecting, assigning, and reassigning GPUs for ensemble members.</p> <p>libEnsemble can be used in a consistent manner on laptops, clusters, and supercomputers with minimal required dependencies.</p>"},{"location":"aurora/workflows/libensemble/#configuring-python-and-installation","title":"Configuring Python and Installation","text":"<p>To obtain Python use:</p> <pre><code>module use /soft/modulefiles\nmodule load frameworks\n</code></pre> <p>To obtain libEnsemble::</p> <pre><code>pip install libensemble\n</code></pre> <p>See the ALCF docs for more details on using Python on Aurora.</p>"},{"location":"aurora/workflows/libensemble/#example","title":"Example","text":"<p>To run the forces_gpu tutorial on Aurora.</p> <p>To obtain the example, you can clone the <code>libEnsemble</code> repository\u2014although only the <code>forces</code> sub-directory is needed:</p> <pre><code>git clone https://github.com/Libensemble/libensemble\ncd libensemble/libensemble/tests/scaling_tests/forces/forces_app\n</code></pre> <p>To compile <code>forces</code> (a C application with OpenMP target):</p> <pre><code>mpicc -DGPU -O3 -fiopenmp -fopenmp-targets=spir64 -o forces.x forces.c\n</code></pre> <p>Now go to the <code>forces_gpu</code> directory:</p> <pre><code>cd ../forces_gpu\n</code></pre> <p>To use all available GPUs, open <code>run_libe_forces.py</code> and adjust the exit criteria to perform more simulations. The following will run two simulations per worker:</p> <pre><code># Instruct libEnsemble to exit after this many simulations\nensemble.exit_criteria = ExitCriteria(sim_max=nsim_workers*2)\n</code></pre> <p>Now grab an interactive session on two nodes (or use the batch script at <code>../submission_scripts/submit_pbs_aurora.sh</code>):</p> <pre><code>qsub -A &lt;myproject&gt; -l select=2 -l walltime=15:00 -lfilesystems=home -q EarlyAppAccess -I\n</code></pre> <p>Once in the interactive session, you may need to reload the frameworks module:</p> <pre><code>cd $PBS_O_WORKDIR\nmodule use /soft/modulefiles\nmodule load frameworks\n</code></pre> <p>Then, run:</p> <pre><code>python run_libe_forces.py --comms local --nworkers 13\n</code></pre> <p>This provides twelve workers for running simulations (one for each GPU across two nodes). An extra worker runs the persistent generator. GPU settings for each worker simulation are printed.</p> <p>Looking at <code>libE_stats.txt</code> will provide a summary of the runs.</p>"},{"location":"aurora/workflows/libensemble/#using-tiles-as-gpus","title":"Using Tiles as GPUs","text":"<p>To treat each tile as its own GPU, add the <code>use_tiles_as_gpus=True</code> option to the <code>libE_specs</code> block in <code>run_libe_forces.py</code>:</p> <pre><code>ensemble.libE_specs = LibeSpecs(\n    num_resource_sets=nsim_workers,\n    sim_dirs_make=True,\n    use_tiles_as_gpus=True,\n)\n</code></pre> <p>Now, rerun with twice the workers:</p> <pre><code>python run_libe_forces.py --comms local --nworkers 25\n</code></pre> <p>The <code>forces</code> example will automatically use the GPUs available to each worker (one MPI rank per GPU). If fewer workers are provided, multiple GPUs will be used per simulation.</p> <p>Also, see <code>forces_gpu_var_resources</code> and <code>forces_multi_app</code> examples for cases using varying processor/GPU counts per simulation.</p>"},{"location":"aurora/workflows/libensemble/#demonstration","title":"Demonstration","text":"<p>A video demonstration of the <code>forces_gpu</code> example on Frontier is available. The workflow is identical when running on Aurora, except for different compiler options and numbers of workers due to differing GPU counts per node.</p> <p>More details: - libEnsemble Documentation - libEnsemble github page - libEnsemble Documentation Aurora page</p>"},{"location":"aurora/workflows/parsl/","title":"Parsl on Aurora","text":"<p>Parsl is a parallel programming library for Python.  It can be used to deploy large numbers of tasks in parallel and with complex dependencies on ALCF machines, and is particularly well suited to run high-throughput workflows.  While Parsl is a Python library, it can execute tasks that run any compiled application.  Parsl can also execute tasks that run mpi applications.</p> <p>Parsl uses Python's concurrent futures module to create functions that return a Python futures object.  A Parsl workflow operates by creating futures for tasks that the Parsl executor will then fulfill by running them on available compute resources.</p> <p>A Parsl workflow contains two parts:</p> <ul> <li>The workflow logic of applications, tasks and task dependencies</li> <li>The configuration of compute resources that execute tasks</li> </ul> <p>Here we sketch out some possible configurations for executing workflows on Aurora.  </p> <p>These docs were written for Parsl 2025.1.13.</p>"},{"location":"aurora/workflows/parsl/#installation-and-setup","title":"Installation and Setup","text":"<p>Parsl is a Python library and can be installed with <code>pip</code>.  For example, in a Python virtual environment:</p> <pre><code>python -m venv $HOME/_env\nsource $HOME/_env/bin/activate\npip install parsl\n</code></pre> <p>Python on Aurora</p> <p>To get Python on Aurora, users can either load the AI frameworks module with <code>module load frameworks</code> or the basic Python 3.10 module with <code>module load python/3.10.13</code></p> <p>When using Parsl to distribute work over many PBS Jobs (first two examples below), your workflow script will be executed on a login node and will not return until all tasks are completed.  In this situation, it is advisable to run your script in a screen session on the login node.</p>"},{"location":"aurora/workflows/parsl/#parsl-config-for-a-large-ensemble-of-single-tile-tasks-run-over-many-pbs-jobs","title":"Parsl Config for a Large Ensemble of Single Tile tasks run over many PBS Jobs","text":"<p>A common use case is to run a large ensemble of tasks that each require one GPU tile on Aurora and to spread this workload over multiple PBS Jobs.  The reason for spreading this workload over many PBS Jobs may be the size of the ensemble and/or the runtime of the tasks.</p> <p>The <code>Config</code> object for this case is defined like this:</p> config.py<pre><code># config.py\nimport os\nfrom parsl.config import Config\n\n# PBSPro is the right provider for ALCF:\nfrom parsl.providers import PBSProProvider\n# The high throughput executor is for scaling large single core/tile/gpu tasks on HPC system:\nfrom parsl.executors import HighThroughputExecutor\n# Use the MPI launcher to launch worker processes:\nfrom parsl.launchers import MpiExecLauncher\n\n# These options will run work in 1 node batch jobs run one at a time\nnodes_per_job = 1\nmax_num_jobs = 1\ntile_names = [f'{gid}.{tid}' for gid in range(6) for tid in range(2)]\n\n# The config will launch workers from this directory\nexecute_dir = os.getcwd()\n\naurora_single_tile_config = Config(\n    executors=[\n        HighThroughputExecutor(\n            # Ensures one worker per GPU tile on each node\n            available_accelerators=tile_names,\n            max_workers_per_node=12,\n            # Distributes threads to workers/tiles in a way optimized for Aurora\n            cpu_affinity=\"list:0-7,104-111:8-15,112-119:16-23,120-127:24-31,128-135:32-39,136-143:40-47,144-151:52-59,156-163:60-67,164-171:68-75,172-179:76-83,180-187:84-91,188-195:92-99,196-203\",\n            # Increase if you have many more tasks than workers\n            prefetch_capacity=0,\n            # Options that specify properties of PBS Jobs\n            provider=PBSProProvider(\n                # Project name\n                account=\"Aurora_deployment\",\n                # Submission queue\n                queue=\"debug\",\n                # Commands run before workers launched\n                # Make sure to activate your environment where Parsl is installed\n                worker_init=f'''source $HOME/_env/bin/activate; cd {execute_dir}''',\n                # Wall time for batch jobs\n                walltime=\"0:30:00\",\n                # Change if data/modules located on other filesystem\n                scheduler_options=\"#PBS -l filesystems=home:flare\",\n                # Ensures 1 manger per node; the manager will distribute work to its 12 workers, one per tile\n                launcher=MpiExecLauncher(bind_cmd=\"--cpu-bind\", overrides=\"--ppn 1\"),\n                # options added to #PBS -l select aside from ncpus\n                select_options=\"\",\n                # Number of nodes per PBS job\n                nodes_per_block=nodes_per_job,\n                # Minimum number of concurrent PBS jobs running workflow\n                min_blocks=0,\n                # Maximum number of concurrent PBS jobs running workflow\n                max_blocks=max_num_jobs,\n                # Hardware threads per node\n                cpus_per_node=208,\n            ),\n        ),\n    ],\n    # How many times to retry failed tasks\n    # this is necessary if you have tasks that are interrupted by a PBS job ending\n    # so that they will restart in the next job\n    retries=1,\n)\n</code></pre> <p>Import this <code>Config</code> object and use in a workflow script, e.g.:</p> <p>my_parsl_workflow.py<pre><code># my_parsl_workflow.py\nimport os\nimport parsl\nfrom parsl import bash_app, python_app\n\nfrom config import aurora_single_tile_config\n\n# Bash apps are for executing compiled applications or other shell commands\n@bash_app\ndef hello_affinity(stdout='hello.stdout', stderr='hello.stderr'):\n    return f'$HOME/GettingStarted/Examples/Aurora/affinity_gpu/sycl/hello_affinity'\n\n# Python apps are for executing native python functions\n@python_app\ndef hello_world(message, sleep_time=1):\n    import time\n    time.sleep(sleep_time)\n    return f\"Hello {message}\"\n\nworking_directory = os.getcwd()\n\nprint(\"Starting my_parsl_workflow\")\n\nwith parsl.load(aurora_single_tile_config):\n\n    # Create 12 hello_world tasks\n    hello_world_futures = [hello_world(f\"Aurora {i}\") for i in range(12)]\n    print(f\"Created {len(hello_world_futures)} hello_world tasks\")\n\n    # Create 12 hello_affinity tasks\n    hello_affinity_futures = [hello_affinity(stdout=f\"{working_directory}/output/hello_{i}.stdout\",\n                                             stderr=f\"{working_directory}/output/hello_{i}.stderr\")\n                              for i in range(12)]\n    print(f\"Created {len(hello_world_futures)} hello_affinity tasks\")\n\n    # This line will block until all hello_world results are returned\n    hello_world_results = [tf.result() for tf in hello_world_futures]\n    print(\"hello_world tasks complete\")\n    print(f\"python apps like hello_world return the function result, e.g. {hello_world_results[0]=}\")\n\n    # This line will block until all hello_affinity results are returned\n    hello_affinity_results = [tf.result() for tf in hello_affinity_futures]\n    print(\"hello_affinity tasks complete\")\n    print(f\"bash apps like hello_affinity return the return code of the executable, e.g. {hello_affinity_results[0]=}\")\n\n    print(f\"Read results of hello_affinity from stdout file:\")\n    for i,tf in enumerate(hello_affinity_futures):\n        with open(f\"{working_directory}/output/hello_{i}.stdout\", \"r\") as f:\n            outputs = f.readlines()\n            print(outputs)\n\n    print(\"Tasks done!\")\n</code></pre> Note that a Parsl workflow script must block at some point on the result of all tasks that are created in order to ensure that the tasks complete.</p> <p>To run this workflow script: <pre><code>source $HOME/_env/bin/activate\npython my_parsl_workflow.py\n</code></pre></p> <p>When executing this script, the script will block until all tasks are completed. You may wish to check the scheduler to verify that Parsl queues a job to execute the tasks.</p>"},{"location":"aurora/workflows/parsl/#parsl-config-for-ensemble-of-multinode-mpi-tasks-tasks-run-over-many-pbs-jobs","title":"Parsl Config for Ensemble of Multinode MPI tasks tasks run over many PBS Jobs","text":"<p>In the previous example, <code>mpiexec</code> was used as a launcher, rather than an executor.  In order to run applications that have MPI communication, <code>mpiexec</code> has to be used a different way by Parsl.  To run MPI applications, use the <code>SimpleLauncher</code> and the <code>MPIExecutor</code>.  Note that the configuration has to set <code>max_workers_per_block</code> to align with the resource needs of the application.  The <code>MPIExecutor</code> can only run tasks that use more than one node.</p> <p>Warning</p> <p>Ensembles of tasks launched with <code>mpiexec</code> on multiple nodes are currently limited to 1000 total tasks run per batch job.  This means when <code>mpiexec</code> calls return, the nodes they used can refill only a limited number of times, rather than an arbitrary number of times like on Polaris.  This is due to a known issue with Slingshot and will be fixed in the future.  Users running MPI application ensembles on Aurora with Parsl should take this into account when configuring their workflows.</p> <p>This example <code>Config</code> object can be used to execute MPI tasks that use two nodes each:</p> config.py<pre><code>import parsl\nimport os\nfrom parsl.config import Config\n# PBSPro is the right provider for ALCF:\nfrom parsl.providers import PBSProProvider\n# The MPIExecutor is for running MPI applications:\nfrom parsl.executors import MPIExecutor\n# Use the Simple launcher\nfrom parsl.launchers import SimpleLauncher\n\n# These options will run work in 10 node batch jobs run one at a time\nnodes_per_task = 2\nnodes_per_job = 10\nmax_num_jobs = 1\n\n# We will save outputs in the current working directory\nworking_directory = os.getcwd()\n\nmpi_ensemble_config = Config(\n    executors=[\n        MPIExecutor(\n            # This creates 1 worker for each multinode task slot\n            max_workers_per_block=nodes_per_job//nodes_per_task,\n            provider=PBSProProvider(\n                account=\"Aurora_deployment\",\n                worker_init=f\"\"\"source $HOME/_env/bin/activate; \\\n                                cd {working_directory}\"\"\",\n                walltime=\"0:30:00\",\n                queue=\"lustre_scaling\",\n                scheduler_options=\"#PBS -l filesystems=home:flare\",\n                launcher=SimpleLauncher(),\n                select_options=\"\",\n                nodes_per_block=nodes_per_job,\n                max_blocks=1,\n                cpus_per_node=208,\n            ),\n        ),\n    ],\n    retries=1,\n)\n</code></pre> <p>This example workflow uses this <code>Config</code> to run an ensemble of 2-node MPI tasks:</p> my_parsl_workflow.py<pre><code># my_parsl_workflow.py\nimport os\nimport parsl\nfrom parsl import bash_app\n\nfrom config import mpi_ensemble_config\n\n# This app will run the hello_affinity application with mpiexec\n# Using the set_affinity_gpu_aurora.sh script will bind each mpi rank to a gpu tile\n@bash_app\ndef mpi_hello_affinity(parsl_resource_specification, depth=8, stdout='mpi_hello.stdout', stderr='mpi_hello.stderr'):\n    APP_DIR = \"$HOME/GettingStarted\"\n    # PARSL_MPI_PREFIX will resolve to `mpiexec -n num_ranks -ppn ranks_per_node -hosts NODE001,NODE002`\n    return f\"$PARSL_MPI_PREFIX --cpu-bind depth --depth={depth} {APP_DIR}/HelperScripts/Aurora/set_affinity_gpu_aurora.sh {APP_DIR}/Examples/Aurora/affinity_gpu/sycl/hello_affinity\"\n\nprint(\"Starting my_parsl_workflow\")\n\nworking_directory = os.getcwd()\n\nwith parsl.load(mpi_ensemble_config):\n\n    task_futures = []\n\n    # Create 2-node tasks\n    # We set 12 ranks per node to match the number of gpu tiles on an aurora node\n    resource_specification = {'num_nodes': 2, # Number of nodes required for the application instance\n                              'ranks_per_node': 12, # Number of ranks / application elements to be launched per node\n                              'num_ranks': 24, # Number of ranks in total\n                             }\n\n    print(f\"Creating mpi tasks with {resource_specification['num_nodes']} nodes per task, {resource_specification['num_ranks']} ranks per task, and {resource_specification['ranks_per_node']} ranks per node\")\n    task_futures += [mpi_hello_affinity(\n                            parsl_resource_specification=resource_specification,\n                            stdout=f\"{working_directory}/mpi_output/{i}/hello.stdout\",\n                            stderr=f\"{working_directory}/mpi_output/{i}/hello.stderr\")\n                        for i in range(10)]\n\n    # This loop will block until all task results are returned\n    print(f\"{len(task_futures)} tasks created, wating for completion\")\n    for tf in task_futures:\n        tf.result()\n\n    print(\"Tasks done!\")\n</code></pre>"},{"location":"aurora/workflows/parsl/#run-parsl-workflow-within-a-single-pbs-job","title":"Run Parsl Workflow within a single PBS Job","text":"<p>If your tasks can be run within a single PBS job, Parsl can be configured to run inside the PBS job, instead of submitting multiple jobs to the scheduler as shown in the examples above.</p> <p>To run the single tile task ensemble from above in this alternate mode, use this <code>Config</code> object in the workflow script: config.py<pre><code># config.py\nimport os\nfrom parsl.config import Config\n\n# Use LocalProvider to launch workers within a submitted batch job\nfrom parsl.providers import LocalProvider\n# The high throughput executor is for scaling large single core/tile/gpu tasks on HPC system:\nfrom parsl.executors import HighThroughputExecutor\n# Use the MPI launcher to launch worker processes:\nfrom parsl.launchers import MpiExecLauncher\n\ntile_names = [f'{gid}.{tid}' for gid in range(6) for tid in range(2)]\n\n# The config will launch workers from this directory\nexecute_dir = os.getcwd()\n\n# Get the number of nodes:\nnode_file = os.getenv(\"PBS_NODEFILE\")\nwith open(node_file,\"r\") as f:\n    node_list = f.readlines()\n    num_nodes = len(node_list)\n\naurora_single_tile_config = Config(\n    executors=[\n        HighThroughputExecutor(\n            # Ensures one worker per GPU tile on each node\n            available_accelerators=tile_names,\n            max_workers_per_node=12,\n            # Distributes threads to workers/tiles in a way optimized for Aurora\n            cpu_affinity=\"list:0-7,104-111:8-15,112-119:16-23,120-127:24-31,128-135:32-39,136-143:40-47,144-151:52-59,156-163:60-67,164-171:68-75,172-179:76-83,180-187:84-91,188-195:92-99,196-203\",\n            # Increase if you have many more tasks than workers\n            prefetch_capacity=0,\n            # Options that specify properties of PBS Jobs\n            provider=LocalProvider(\n                # Number of nodes job\n                nodes_per_block=num_nodes,\n                launcher=MpiExecLauncher(bind_cmd=\"--cpu-bind\", overrides=\"--ppn 1\"),\n                init_blocks=1,\n                max_blocks=1,\n            ),\n        ),\n    ],\n)\n</code></pre></p> <p>Then submit the the workflow with a PBS batch script: <pre><code>#!/bin/bash -l\n#PBS -l select=1\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -q debug\n#PBS -A &lt;ProjectName&gt;\n\ncd ${PBS_O_WORKDIR}\n\nsource $HOME/_env/bin/activate\npython my_workflow_script.py\n</code></pre></p> <p>Note that if the workflow does not complete before the end of the PBS job, outstanding tasks will not complete.</p>"},{"location":"aurora/workflows/smartsim/","title":"SmartSim and SmartRedis","text":"<p>SmartSim is an open-source tool developed by Hewlett Packard Enterprise (HPE) designed to facilitate the integration of traditional HPC simulation applications with machine learning workflows. There are two core components to SmartSim:</p> <ul> <li>Infrastructure Library (IL)</li> <li>Provides an API to start, stop, and monitor HPC applications from Python</li> <li>Interfaces with the PBSpro scheduler to launch jobs</li> <li>Deploys a distributed in-memory database called the Orchestrator</li> <li>SmartRedis Client Library</li> <li>Provides clients that connect to the Orchestrator from Fortran, C, C++, and Python code</li> <li>The client API library enables data transfer to/from the database and the ability to load and run JIT-traced Python and ML runtimes acting on stored data</li> </ul> <p>For more resources on SmartSim, follow the links below:</p> <ul> <li>Source code</li> <li>Documentation</li> <li>Zoo of examples</li> <li>Fall 2023 ALCF User Hands-On Workshop</li> <li>NekRS-ML</li> </ul>"},{"location":"aurora/workflows/smartsim/#installation","title":"Installation","text":"<p>Create a Python virtual environment based on the ML frameworks module:</p> <pre><code>module load frameworks/2024.2.1_u1\npython -m venv --clear /path/to/_ssim_env --system-site-packages\nsource /path/to/_ssim_env/bin/activate\n</code></pre> <p>It is recommended that the venv is installed in a user's project space on the Flare parallel file system.</p> <p>Install SmartSim:</p> <pre><code>git clone https://github.com/rickybalin/SmartSim.git\ncd SmartSim\ngit checkout rollback_aurora\npip install -e .\ncd ..\n</code></pre> <p>Install the RedisAI PyTorch backend for the CPU:</p> <pre><code>export TORCH_CMAKE_PATH=$( python -c 'import torch;print(torch.utils.cmake_prefix_path)' )\nexport TORCH_PATH=$( python -c 'import torch; print(torch.__path__[0])' )\nexport LD_LIBRARY_PATH=$TORCH_PATH/lib:$LD_LIBRARY_PATH\nsmart build -v --device cpu --torch_dir $TORCH_CMAKE_PATH --no_tf\nsmart validate --device cpu\n</code></pre> <p>Install the SmartRedis library:</p> <pre><code>git clone https://github.com/rickybalin/SmartRedis.git\ncd SmartRedis\npip install -e .\nmake lib\ncd ..\n</code></pre> <p>Known Issues:</p> <ul> <li>Pip installing SmartSim returns some warnings which can be safely ignored.</li> <li>The <code>smart build -v --device cpu</code> command builds the RedisAI backend for the CPU. This enables ML model inferencing on the CPU with SmartSim and SmartRedis. Due to a limitation with RedisAI, the backend cannot be built for the Intel Max 1550 GPU.</li> <li>The RedisAI backend requires an older version of TensorFlow relative to what is loaded with the frameworks module on Aurora. If you need the TensorFlow backend, please contact us at support@alcf.anl.gov.</li> <li>When running a workload with SmartSim, please include the following in your run or submit scripts:</li> </ul> <pre><code>export TORCH_PATH=$( python -c 'import torch; print(torch.__path__[0])' )\nexport LD_LIBRARY_PATH=$TORCH_PATH/lib:$LD_LIBRARY_PATH\n</code></pre>"},{"location":"crux/getting-started/","title":"Getting Started on Crux","text":""},{"location":"crux/getting-started/#logging-into-crux","title":"Logging Into Crux","text":"<p>To log into Crux: <pre><code>ssh &lt;username&gt;@crux.alcf.anl.gov\n</code></pre> Then, type in the password from your CRYPTOCard/MobilePASS+ token. Once logged in, you land on one of the Crux login nodes (<code>crux-login-01</code>, <code>crux-login-02</code>).</p>"},{"location":"crux/getting-started/#hardware-overview","title":"Hardware Overview","text":"<p>An overview of the Crux system, including details on the compute node architecture, is available on the Machine Overview page.</p>"},{"location":"crux/getting-started/#compiling-applications","title":"Compiling Applications","text":"<p>For all code building and development, please use Crux compute nodes, especially for large parallel builds. Please read through the Compiling and Linking Overview page and corresponding pages depending on the target compiler and programming model.</p>"},{"location":"crux/getting-started/#accessing-additional-software","title":"Accessing Additional Software","text":"<p>ALCF installs additional software in <code>/soft</code>, which can be accessed via module commands by altering your <code>$MODULEPATH</code>: <pre><code>module use /soft/modulefiles\n</code></pre> The available software can then be queried with <code>module avail</code>. In particular, loading the <code>spack-pe-base</code> module provides access to additional software and build tools. For example, <code>cmake</code> is available via the following:</p> <pre><code>module use /soft/modulefiles\nmodule load spack-pe-base\nmodule load cmake\n</code></pre>"},{"location":"crux/getting-started/#submitting-and-running-jobs","title":"Submitting and Running Jobs","text":"<p>Please read through the Running Jobs with PBS at the ALCF page for information on using the PBS scheduler and preparing job submission scripts.</p> <p>For more information on Crux queues and job submission, visit: Running Jobs on Crux.</p>"},{"location":"crux/getting-started/#lustre-file-striping","title":"Lustre File Striping","text":"<p>In addition to the content above, here is a document on Lustre File Striping Basics:</p> <ul> <li>Lustre File Striping Basics</li> </ul>"},{"location":"crux/getting-started/#proxy","title":"Proxy","text":"<p>If the node you are on doesn\u2019t have outbound network connectivity, add the following to your <code>~/.bash_profile</code> file to access the proxy host:</p> <pre><code># proxy settings\nexport HTTP_PROXY=\"http://proxy.alcf.anl.gov:3128\"\nexport HTTPS_PROXY=\"http://proxy.alcf.anl.gov:3128\"\nexport http_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport https_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport ftp_proxy=\"http://proxy.alcf.anl.gov:3128\"\n</code></pre>"},{"location":"crux/getting-started/#getting-assistance","title":"Getting Assistance","text":"<p>Please direct all questions, requests, and feedback to support@alcf.anl.gov.</p>"},{"location":"crux/getting-started/#-","title":"---","text":""},{"location":"crux/compiling-and-linking/compiling-and-linking-overview/","title":"Compiling and Linking on Crux","text":""},{"location":"crux/compiling-and-linking/compiling-and-linking-overview/#overview","title":"Overview","text":"<p>Crux has AMD processors on the login nodes (crux-login-01,02) and AMD processors on the compute nodes (see Machine Overview page). The login nodes can be used to compile software, create containers, and launch jobs. For larger, parallel builds, it will be beneficial to compile those directly on the compute nodes.</p> <p>To launch an interactive job and acquire a compute node for compiling, use:</p> <pre><code>qsub -I -q workq -A myProjectShortName -n 1 -t HH:MM:SS\n</code></pre> <p>The default programming environment on the Crux compute nodes is currently Cray: <code>PrgEnv-cray</code>. The GNU programming environment <code>PrgEnv-gnu</code> is also available to users. It is recommended that the Cray MPI wrappers are used for building applications.</p> <ul> <li><code>cc</code> - C compiler</li> <li><code>CC</code> - C++ compiler</li> <li><code>ftn</code> - Fortran compiler</li> </ul> <p>Each of these wrappers will select the corresponding vendor compiler based on the PrgEnv module loaded in the environment. The following are some helpful options to understand what the compiler wrapper is invoking:</p> <ul> <li><code>--craype-verbose</code>: Print the command which is forwarded to the compiler invocation</li> <li><code>--cray-print-opts=libs</code>: Print library information</li> <li><code>--cray-print-opts=cflags</code>: Print include information</li> </ul> <p>Further documentation and options are available via <code>man cc</code> and similar.</p>"},{"location":"crux/compiling-and-linking/compiling-and-linking-overview/#modules-on-crux","title":"Modules on Crux","text":"<p>Available modules can be listed via the command:</p> <pre><code>module avail\n</code></pre> <p>Loaded modules in your environment can be listed via the command:</p> <pre><code>module list\n</code></pre> <p>To load new modules, use:</p> <pre><code>module load &lt;module_name&gt;\n</code></pre>"},{"location":"crux/containers/containers/","title":"Containers on Crux","text":"<p>Apptainer will be supported on Crux at a future date.</p>"},{"location":"crux/containers/containers/#recipe-based-container-building","title":"Recipe-Based Container Building","text":"<p>As mentioned earlier, you can build Apptainer containers from recipe files. Instructions are available here. See available containers for more recipes.</p> <p>Note: You can also build custom recipes by bootstrapping from prebuilt images. For example, the first two lines in a recipe to use our custom TensorFlow implementation would be <code>Bootstrap: oras</code> followed by <code>From: ghcr.io/argonne-lcf/tf2-mpich-nvidia-gpu:latest</code>.</p>"},{"location":"crux/containers/containers/#available-containers","title":"Available containers","text":"<p>If you just want to know what containers are available, here you go:</p> <ul> <li> <p>Examples for running MPICH containers can be found here.</p> </li> <li> <p>Examples for running databases can be found here.</p> </li> <li> <p>For using shpc - that allows for running containers as modules. It can be found here.</p> </li> </ul> <p>The latest containers are updated periodically. If you have trouble using containers or request a newer or a different container, please contact ALCF support at <code>support@alcf.anl.gov</code>.</p>"},{"location":"crux/containers/containers/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":"<p>Permission Denied Error: If you encounter permission errors during the build:</p> <ul> <li> <p>Check your quota and delete any unnecessary files.</p> </li> <li> <p>Clean up the Apptainer cache, <code>~/.apptainer/cache</code>, and set the Apptainer tmp and cache directories as below. If your home directory is full and if you are building your container on a compute node, then set the tmpdir and cachedir to local scratch:</p> </li> </ul> <pre><code>export BASE_SCRATCH_DIR=/local/scratch/ # FOR POLARIS\n#export BASE_SCRATCH_DIR=/raid/scratch/ # FOR SOPHIA\nexport APPTAINER_TMPDIR=$BASE_SCRATCH_DIR/apptainer-tmpdir\nmkdir $APPTAINER_TMPDIR\nexport APPTAINER_CACHEDIR=$BASE_SCRATCH_DIR/apptainer-cachedir/\nmkdir $APPTAINER_CACHEDIR\n</code></pre> <ul> <li> <p>Make sure you are not in a directory accessed with a symbolic link, i.e., check if <code>pwd</code> and <code>pwd -P</code> return the same path.</p> </li> <li> <p>If any of the above doesn't work, try running the build in your home directory.</p> </li> </ul> <p>Mapping to rank 0 on all nodes: Ensure that the container's MPI aligns with the system MPI. For example, follow the additional steps outlined in the container registry documentation for MPI on Polaris.</p> <p>libmpi.so.40 not found: This can happen if the container's application has an OpenMPI dependency, which is not currently supported on Polaris. It can also spring up if the container's base environment is not a Debian-based architecture such as Ubuntu. Ensure the application has an MPICH implementation as well. Also, try removing <code>.conda/</code>, <code>.cache/</code>, and <code>.local/</code> folders from your home directory and rebuilding the container.</p> <p>Disabled Port mapping, user namespace, and [network virtualization] Network virtualization is disabled for the container due to security constraints. See issue #2533.</p> <p>Apptainer instance errors with version 1.3.2</p> <p>Use <code>nohup</code> and <code>&amp;</code> as an alternative if you want to run Apptainer as a background process. See below for an example of running Postgres as a background process: <pre><code> nohup apptainer run \n -B pgrun:/var/run/postgresql \\\n -B pgdata:/var/lib/postgresql/data \\\n --env-file pg.env \\\n postgres.sing postgres &amp;\n\n # 3) Capture its PID so we can kill it later\n echo $! &gt; postgres_pid.txt\n echo \"Started Postgres in the background with PID $(cat postgres_pid.txt)\"\n\n# 4) Perform whatever work you need while Postgres is running\n#    In this demo, we just sleep for 30 minutes (1800 seconds).\nsleep 1800\n\n# 5) Kill the background process at the end of the job\nkill \"$(cat postgres_pid.txt)\"\nrm postgres_pid.txt\n</code></pre></p>"},{"location":"crux/data-science/python/","title":"Python","text":"<p>At a future date, we will provide prebuilt <code>conda</code> environments containing CPU-optimized builds of <code>torch</code>, <code>tensorflow</code> (both with <code>horovod</code> support for multi-node calculations), <code>jax</code>, and many other commonly used Python modules.</p> <p>In the meantime, users should be able to create their own local environments to begin work on Crux. In this example, <code>mpi4py</code> is installed.</p> <pre><code>python3 -m venv ~/_test_env\n. ~/_test_env/bin/activate\npip install mpi4py\n</code></pre> <p>This new virtual environment can then be used in a batch job as in this simple hello_world example available in the GettingStarted repo.</p> <pre><code>$ cat hello_world.py\nfrom mpi4py import MPI\nimport sys\nimport socket\n\ncomm = MPI.COMM_WORLD\nrank = comm.Get_rank()\nsize = comm.Get_size()\n\nprint(\"Hello World from rank {} of {} on {}\".format(rank, size, socket.gethostname()))\n\nsys.exit()\n</code></pre> <pre><code>$ qsub ./submit.sh\n12345.crux-pbs-0001.head.cm.crux.alcf.anl.gov\n\n$ cat submit.sh.o12345\nNUM_OF_NODES= 2 TOTAL_NUM_RANKS= 8 RANKS_PER_NODE= 4 THREADS_PER_RANK= 1\nHello World from rank 0 of 8 on x1000c0s0b1n1\nHello World from rank 1 of 8 on x1000c0s0b1n1\nHello World from rank 2 of 8 on x1000c0s0b1n1\nHello World from rank 3 of 8 on x1000c0s0b1n1\nHello World from rank 4 of 8 on x1000c0s1b0n0\nHello World from rank 5 of 8 on x1000c0s1b0n0\nHello World from rank 6 of 8 on x1000c0s1b0n0\nHello World from rank 7 of 8 on x1000c0s1b0n0\n</code></pre>"},{"location":"crux/hardware-overview/machine-overview/","title":"Crux Machine Overview","text":"<p>Crux is an HPE Cray EX Liquid Cooled system with a peak performance of 1.18 PF, comprised of 64 compute blades connected via Slingshot. Each blade has 4 compute nodes for a total of 256 nodes in the system. Each compute node has dual AMD EPYC 7742 64-Core Processors. Each CPU core supports up to two hyperthreads for a total of 256 threads possible per node. Each CPU has 128 GB of DDR4 memory for a total of 256 GB per node.</p>"},{"location":"crux/hardware-overview/machine-overview/#node-architecture","title":"Node Architecture","text":"<p>The output of <code>numactl --hardware</code> is very helpful in understanding the connectivity of the CPU cores in each compute node. Each of the CPUs consists of four NUMA domains containing 16 cores and connected directly to 1/4 of the DDR channels. The following information will be useful in understanding how best to affinitize processes to CPUs on each node. For example, users running multiple applications per node will likely want to localize within a set of NUMA domains in a CPU.</p> <p>For CPU 0:</p> <ul> <li>NUMA 0: cores 0-15,128-143</li> <li>NUMA 1: cores 16-31,144-159</li> <li>NUMA 2: cores 32-47,160-175</li> <li>NUMA 3: cores 48-63,176-191</li> </ul> <p>For CPU 1:</p> <ul> <li>NUMA 4: cores 64-79,192-207</li> <li>NUMA 5: cores 80-95,208-223</li> <li>NUMA 6: cores 96-111,224-239</li> <li>NUMA 7: cores 112-127,240-255</li> </ul>"},{"location":"crux/queueing-and-running-jobs/running-jobs/","title":"Running Jobs on Crux","text":""},{"location":"crux/queueing-and-running-jobs/running-jobs/#queues","title":"Queues","text":"<p>There are five production queues you can target in your qsub (<code>-q &lt;queue name&gt;</code>):</p> Queue Name Node Min Node Max Time Min Time Max Notes debug 1 4 5 min 2 hr max 8 nodes in use by this queue at any given time; Only 8 nodes are exclusive (see Note below) workq-route 1 512 5 min 24 hrs Routing queue; 100 jobs max per project; See below <p>Note: The debug queue has 8 exclusively dedicated nodes.</p> <p><code>workq-route</code> is a routing queue and routes your job to one of the following execution queues (currently just one):</p> Queue Name Node Min Node Max Time Min Time Max Notes workq 1 512 5 min 24 hrs 20 jobs queue or running/10 jobs running per project"},{"location":"crux/queueing-and-running-jobs/running-jobs/#running-mpiopenmp-applications","title":"Running MPI+OpenMP Applications","text":"<p>Note: For OpenMP-enabled applications, it is extremely important to set the number of OpenMP threads to an appropriate value. As on most systems, the default value for <code>OMP_NUM_THREADS</code> is set to the maximum possible, which is 256 on the Crux compute nodes.</p> <p>Once a submitted job is running, calculations can be launched on the compute nodes using <code>mpiexec</code> to start an MPI application. Documentation is accessible via <code>man mpiexec</code>, and some helpful options follow.</p> <ul> <li><code>-n</code> total number of MPI ranks</li> <li><code>-ppn</code> number of MPI ranks per node</li> <li><code>--cpu-bind</code> CPU binding for application</li> <li><code>--depth</code> number of CPUs per rank (useful with <code>--cpu-bind</code>)</li> <li><code>--env</code> set environment variables (<code>--env OMP_NUM_THREADS=2</code>)</li> <li><code>--hostfile</code> indicate file with hostnames (the default is <code>--hostfile $PBS_NODEFILE</code>)</li> </ul> <p>A sample submission script with directives is below for a 4-node job with 8 MPI ranks on each node and 8 OpenMP threads per rank. Each hardware thread runs a single OpenMP thread since there are 64 hardware threads on the CPU (2 per core). You can download and compile <code>hello_affinity</code> from this link.</p> <pre><code>#!/bin/bash -l\n#PBS -N AFFINITY\n#PBS -l select=4:system=crux\n#PBS -l place=scatter\n#PBS -l walltime=0:10:00\n#PBS -q debug\n#PBS -A Catalyst  # Replace with your project\n#PBS -l filesystems=home:eagle\n\n# MPI+OpenMP example w/ 64 MPI ranks per node and threads spread evenly across cores\n# There are two 32-core CPUs on each node. This will run 32 MPI ranks per CPU, 2 OpenMP threads per rank, and each thread bound to a single core.\n\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS_PER_NODE=64 # Number of MPI ranks to spawn per node\nNDEPTH=2 # Number of hardware threads per rank (i.e. spacing between MPI ranks)\nNTHREADS=2 # Number of software threads per rank to launch (i.e. OMP_NUM_THREADS)\n\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))\n\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}\"\n\n# Change the directory to work directory, which is the directory you submit the job.\ncd $PBS_O_WORKDIR\n\nMPI_ARGS=\"-n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth \"\nOMP_ARGS=\"--env OMP_NUM_THREADS=${NTHREADS} --env OMP_PROC_BIND=true --env OMP_PLACES=cores \"\n\nmpiexec ${MPI_ARGS} ${OMP_ARGS} ./hello_affinity\n</code></pre> <p>The <code>hello_affinity</code> program is a compiled C++ code, which is built via <code>make clean ; make</code> in the linked directory after cloning the Getting Started repository.</p>"},{"location":"crux/queueing-and-running-jobs/running-jobs/#running-multiple-mpi-applications-on-a-single-node","title":"Running Multiple MPI Applications on a Single Node","text":"<p>Multiple applications can be run simultaneously on a node by launching several <code>mpiexec</code> commands and backgrounding them. For performance, it will likely be necessary to ensure that each application runs on a distinct set of CPU resources. One can provide a list of CPUs using the <code>--cpu-bind</code> option to explicitly assign CPU resources on a node to each application. Output from the <code>numactl --hardware</code> command is useful for understanding how to localize applications within NUMA domains on the two CPUs of each node.</p> <p>In the example below, eight instances of the application are simultaneously running on a single node, with each application localized to a single NUMA domain. Each application here is bound to 16 CPU cores with a single process running on each core (i.e. no hyperthreads). In the first instance, the application is spawning 16 MPI ranks on cores 0-15 in the first CPU.</p> <pre><code>  MPI_ARG=\"-n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE}\"\n  OMP_ARG=\"--env OMP_NUM_THREADS=${NTHREADS} \"\n\n  # Socket 0\n  mpiexec ${MPI_ARG} ${OMP_ARG} --cpu-bind list:0:1:2:3:4:5:6:7:8:9:10:11:12:13:14:15 ./hello_affinity &amp;\n  mpiexec ${MPI_ARG} ${OMP_ARG} --cpu-bind list:16:17:18:19:20:21:22:23:24:25:26:27:28:29:30:31 ./hello_affinity &amp;\n  mpiexec ${MPI_ARG} ${OMP_ARG} --cpu-bind list:32:33:34:35:36:37:38:39:40:41:42:43:44:45:46:47 ./hello_affinity &amp;\n  mpiexec ${MPI_ARG} ${OMP_ARG} --cpu-bind list:48:49:50:51:52:53:54:55:56:57:58:59:60:61:62:63 ./hello_affinity &amp;\n\n  # Socket 1\n  mpiexec ${MPI_ARG} ${OMP_ARG} --cpu-bind list:64:65:66:67:68:69:70:71:72:73:74:75:76:77:78:79 ./hello_affinity &amp;\n  mpiexec ${MPI_ARG} ${OMP_ARG} --cpu-bind list:80:81:82:83:84:85:86:87:88:89:90:91:92:93:94:95 ./hello_affinity &amp;\n  mpiexec ${MPI_ARG} ${OMP_ARG} --cpu-bind list:96:97:98:99:100:101:102:103:104:105:106:107:108:109:110:111 ./hello_affinity &amp;\n  mpiexec ${MPI_ARG} ${OMP_ARG} --cpu-bind list:112:113:114:115:116:117:118:119:120:121:122:123:124:125:126:127 ./hello_affinity &amp;\n\nwait\n</code></pre>"},{"location":"crux/queueing-and-running-jobs/running-jobs/#running-multiple-mpi-applications-on-multiple-nodes","title":"Running Multiple MPI Applications on Multiple Nodes","text":"<p>An important detail missing from the prior example was specifying the hostfile. When not specified, the default hostfile ${PBS_NODEFILE} is used for all invocations of <code>mpiexec</code>, meaning all applications will include identical sets of nodes. This is fine for single-node jobs, but appropriate hostfiles need to be created and passed to <code>mpiexec</code> when running applications across subsets of nodes in a large job.</p> <p>The following example first splits the hostfile ${PBS_NODEFILE} into separate hostfiles each containing the requested number of nodes (in this case just 1 per file). The separate hostfiles are then used in each batch of <code>mpiexec</code> calls to launch applications on different compute nodes.</p> <pre><code># MPI example w/ multiple runs per batch job\nNNODES=`wc -l &lt; $PBS_NODEFILE`\n\n# Settings for each run: 8 runs each with 16 MPI ranks per node spread evenly across specified subset of cores\nNUM_NODES_PER_MPI=1\nNRANKS_PER_NODE=16\nNTHREADS=1\n\nNTOTRANKS=$(( NUM_NODES_PER_MPI * NRANKS_PER_NODE ))\necho \"NUM_OF_NODES= ${NNODES} NUM_NODES_PER_MPI= ${NUM_NODES_PER_MPI} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}\"\n\n# Increase value of suffix-length if more than 99 jobs\nsplit --lines=${NUM_NODES_PER_MPI} --numeric-suffixes=1 --suffix-length=2 $PBS_NODEFILE local_hostfile.\n\nfor lh in local_hostfile*\ndo\n  echo \"Launching mpiexec w/ ${lh}\"\n  MPI_ARG=\"-n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --hostfile ${lh} \"\n  OMP_ARG=\"--env OMP_NUM_THREADS=${NTHREADS} \"\n\n  # Socket 0\n  mpiexec ${MPI_ARG} ${OMP_ARG} --cpu-bind list:0:1:2:3:4:5:6:7:8:9:10:11:12:13:14:15 ./hello_affinity &amp;\n  mpiexec ${MPI_ARG} ${OMP_ARG} --cpu-bind list:16:17:18:19:20:21:22:23:24:25:26:27:28:29:30:31 ./hello_affinity &amp;\n  mpiexec ${MPI_ARG} ${OMP_ARG} --cpu-bind list:32:33:34:35:36:37:38:39:40:41:42:43:44:45:46:47 ./hello_affinity &amp;\n  mpiexec ${MPI_ARG} ${OMP_ARG} --cpu-bind list:48:49:50:51:52:53:54:55:56:57:58:59:60:61:62:63 ./hello_affinity &amp;\n\n  # Socket 1\n  mpiexec ${MPI_ARG} ${OMP_ARG} --cpu-bind list:64:65:66:67:68:69:70:71:72:73:74:75:76:77:78:79 ./hello_affinity &amp;\n  mpiexec ${MPI_ARG} ${OMP_ARG} --cpu-bind list:80:81:82:83:84:85:86:87:88:89:90:91:92:93:94:95 ./hello_affinity &amp;\n  mpiexec ${MPI_ARG} ${OMP_ARG} --cpu-bind list:96:97:98:99:100:101:102:103:104:105:106:107:108:109:110:111 ./hello_affinity &amp;\n  mpiexec ${MPI_ARG} ${OMP_ARG} --cpu-bind list:112:113:114:115:116:117:118:119:120:121:122:123:124:125:126:127 ./hello_affinity &amp;\n\n  sleep 1s\ndone\n\nwait\n\nrm -f local_hostfile.*\n</code></pre> <p>Ensemble <code>examples</code> for several cases are provided to help users with crafting job submission scripts.</p>"},{"location":"crux/queueing-and-running-jobs/running-jobs/#compute-node-access-to-the-internet","title":"Compute Node Access to the Internet","text":"<p>Currently, the only access to the internet is via a proxy. Here are the proxy environment variables for Crux:</p> <pre><code>export http_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport https_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport ftp_proxy=\"http://proxy.alcf.anl.gov:3128\"\n</code></pre>"},{"location":"data-management/acdc/acdc-overview/","title":"ALCF Community Data Co-Op (ACDC)","text":""},{"location":"data-management/acdc/acdc-overview/#overview-of-the-alcf-community-data-co-op-acdc","title":"Overview of the ALCF Community Data Co-Op (ACDC)","text":"<p>The ALCF Community Data Co-Op (ACDC) powers data-driven research by providing a platform for data access and sharing, and value-added services for data discovery and analysis.</p> <p>A fundamental aspect of ACDC is a data fabric that allows programmatic data access and straightforward large-scale data sharing with collaborators via Globus services. This provides a platform to build out different modalities for data access and use, such as indexing of data for discovery, data portals for interactive search and access, and accessible analysis services. ACDC will continue to be expanded to deliver ALCF users the platform to build customizable and accessible services towards the goal of supporting data-driven discoveries.</p>"},{"location":"data-management/acdc/acdc-overview/#data-access-and-sharing","title":"Data access and sharing","text":"<p>ALCF project PIs can share data on Eagle with their collaborators, making facility accounts unnecessary. With this service, the friction of data sharing amongst collaborators is eliminated \u2013 there is no need to create copies of data for sharing, or allocation and accounts just to access data. ALCF PIs can grant access to data, at read-only or read/write access levels. Non-ALCF users throughout the scientific community, who have been granted permissions, can access the data on the Eagle filesystem using Globus.</p> <p>Access to the data for ALCF users and collaborators is supported via bulk transfer (Globus transfer) or direct browser-based access (HTTP/S). Direct connections to high-speed external networks permit data access at many gigabytes per second. Management of permissions and access is via a web application or command line clients, or directly via an Application Programming Interface (APIs). The interactivity permitted by the APIs distinguishes ACDC from the ALCF\u2019s previous storage systems and presents users with many possibilities for data control and distribution.</p>"},{"location":"data-management/acdc/acdc-overview/#data-portal-for-discovery-and-access","title":"Data portal for discovery and access","text":"<p>ACDC\u2019s fully supported production environment is the next step in the expansion of edge services that blur the boundaries between experimental laboratories and computing facilities. The use and prominence of such services at the ALCF are only expected to increase as they become more integral to the facility\u2019s ability to deliver data-driven scientific discoveries.</p> <p>ACDC includes several project-specific data portals that enable search and discovery of the data hosted on Eagle. The portals allow users to craft queries and filters to find specific sets of data that match their criteria and use faceted search for the discovery of data. Portals also provide the framework for other interfaces including data processing capabilities, all secured with authentication and configured authorization policy.</p> <p>The ACDC portal is a deployment of Django Globus Portal Framework customized for a variety of different projects. For most of these projects, the search metadata links directly to data on Eagle, with browser-based download, preview, and rendering of files, and bulk data access.</p>"},{"location":"data-management/acdc/acdc-overview/#getting-started","title":"Getting Started","text":"<ol> <li>Request an allocation: Researchers or PIs request an allocation on Eagle, and a project allocation is created upon request acceptance.</li> <li>Manage Access: PIs can manage the space independently or assign other users to manage the space, as well as provide other users with read or read/write access for folders in the space. Globus groups and identities are used to manage such access.</li> <li>Authentication: Globus is used for authentication and identity needed to access the system. As Globus has built-in support for federated logins, users can access ACDC using their campus or institution federated username and passcode.</li> </ol> <p>If you are new to the ALCF, follow these instructions on how to transfer your data to ACDC: Transferring Data to Eagle</p> <p>If you already have an ALCF account, follow these instructions on how to share your data: Sharing Data to Eagle</p>"},{"location":"data-management/acdc/eagle-data-sharing/","title":"Sharing Data on Eagle Using Globus Guest Collections","text":""},{"location":"data-management/acdc/eagle-data-sharing/#overview","title":"Overview","text":"<p>Collaborators throughout the scientific community have the ability to write data to and read scientific data from the Eagle filesystem using Globus sharing capability. This capability provides PIs with a natural and convenient storage space for collaborative work.</p> <p>Note</p> <p>The project PI needs to have an active ALCF account to set up Globus guest collections on Eagle and set permissions for collaborators to access data. If the PI does not have an account or has an inactive account, they will not be able to create a Globus guest collection. If a PI's account goes inactive after the Globus guest collection was created and shared, the collection will become inaccessible until the PI's account is reactivated. Only the project PI has the ability to create a collection; project proxies cannot create a collection.</p> <p>Globus is a service that provides research data management, including managed transfer and sharing. It makes it easy to move, sync, and share large amounts of data. Globus will manage file transfers, monitor performance, retry failures, recover from faults automatically when possible, and report the status of your data transfer. Globus supports GridFTP for bulk and high-performance file transfer, and direct HTTPS for download. The service allows the user to submit a data transfer request and performs the transfer asynchronously in the background. For more information, see Globus data transfer and Globus data sharing.</p>"},{"location":"data-management/acdc/eagle-data-sharing/#logging-into-globus-with-your-alcf-login","title":"Logging into Globus with your ALCF Login","text":"<p>ALCF researchers can use their ALCF Login username and password to access Globus. Go to the Globus website and click on Log In in the upper right corner of the page.</p> <p> </p> Logging into Globus <p>Type or scroll down to \"Argonne LCF\" in the \"Use your existing organizational login\" box, and then click \"Continue\".</p> <p> </p> Select Organization Argonne LCF <p>You will be taken to a familiar-looking page for ALCF login. Enter your ALCF login username and password.</p>"},{"location":"data-management/acdc/eagle-data-sharing/#accessing-your-eagle-project-directory","title":"Accessing your Eagle Project Directory","text":"<p>Note</p> <p>Specifically for PIs with Eagle 'Data-Only' projects (no compute allocations), logging in through Globus is the only way to access the project directory.</p> <p>PIs with data and compute allocations will have access to the required compute-system login nodes (along with the Globus Web Interface) to access their project directory.</p>"},{"location":"data-management/acdc/eagle-data-sharing/#creating-a-guest-collection","title":"Creating a Guest Collection","text":"<p>A project PI needs to have an 'active' ALCF account in place to create and share guest collections with collaborators. Please note that ONLY a PI has the ability to create guest collections.</p> <p>Info</p> <p>PIs with an \"Inactive/Deleted\" ALCF account should submit a reactivation request by filling out this form: Re-activation Form</p> <p>Info</p> <p>PIs without an ALCF account should submit an ALCF account request by filling out this form: Account Request Form</p>"},{"location":"data-management/acdc/eagle-data-sharing/#navigate-to-the-collections-tab","title":"Navigate to the Collections tab","text":"<p>There are multiple ways to navigate to the Collections tab in \"Endpoints\": 1. Click the link to get started. It will take you to the Collections tab for Eagle. OR 2. Click on 'Endpoints' located in the left panel of the Globus web app. Type \"alcf#dtn_eagle\" (for Eagle) in the search box located at the top of the page and click the magnifying glass to search. Click on the Managed Public Endpoint \"alcf#dtn_eagle\" from the search results. Click on the Collections tab. OR 3. Click on 'File Manager' located in the left panel of the Globus web app. Search for 'alcf#dtn_eagle' and select it in the Collection field. Select your project directory or a subdirectory that you would like to share with collaborators as a Globus guest collection. Click on 'Share' on the right side of the panel, which will take you to the Collections tab.</p> <p>Note: When you select an endpoint to transfer data to/from, you may be asked to authenticate with that endpoint. Follow the instructions on screen to activate the endpoint and to authenticate. You may also have to provide Authentication/Consent for the Globus web app to manage collections on this endpoint.</p>"},{"location":"data-management/acdc/eagle-data-sharing/#adding-a-guest-collection","title":"Adding a Guest Collection","text":"<p>In the Collections tab, click 'Add a Guest Collection' located at the top right-hand corner.</p> <ol> <li> <p>Fill out the form:</p> <ol> <li>If the path to the directory is not pre-populated, click the browse button, navigate and select the directory. Note that you can create a single guest collection and set permissions for folders within a guest collection. There is no reason to create multiple guest collections to share for a single project.</li> <li>Give the collection a Display Name (choose a descriptive name)</li> </ol> </li> <li> <p>Click \"Create Collection\"</p> </li> </ol> <p> </p> Create New Guest Collection"},{"location":"data-management/acdc/eagle-data-sharing/#sharing-data-with-collaborators-using-guest-collections","title":"Sharing Data with Collaborators Using Guest Collections","text":"<p>Your data in the Guest Collections can be easily shared with collaborators at ALCF or elsewhere. You have full control over which files your collaborators can access, and whether they have read-only or read-write permissions.</p> <p>To share data with collaborators (that either have a Globus account or an ALCF account), click on 'Endpoints', select your newly created Guest Collection (as described in the section above), and go to the 'Permissions' tab. Click on 'Add Permissions - Share With':</p> <p> </p> Add Permissions <p>You can share with other Globus users or Globus Groups (for more information on Groups, scroll down to Groups). You can give the collaborators read, write, or read+write permissions. Once the options have been selected, click 'Add Permission'.</p> <p> </p> Add Permissions - Share With <p>PI can also choose to share their data with 'Public' with anonymous read access (and anonymous write disabled). This allows anyone that has access to the data to read and/or download it without authorizing the request.</p> <p> </p> Add Permissions - Share With <p>You should then see the share and the people you have shared it with. You can repeat this process for any number of collaborators. At any time, you can terminate access to the directory by clicking the trash can next to the user.</p> <p> </p> List of people that you have shared with"},{"location":"data-management/acdc/eagle-data-sharing/#additional-information-on-globus-guest-collections","title":"Additional information on Globus Guest Collections","text":"<ol> <li>ONLY a project PI can create guest collections and make them accessible to collaborators. Project proxies cannot create guest collections.</li> <li>You can only share directories, not individual files.</li> <li>Globus allows directory trees to be shared as either read or read/write. This means that any subdirectories within that tree also have the same permissions. Globus supports setting permissions at a folder level, so there is no need to create multiple guest collections for a project. You can create a guest collection at the top level and share sub-directories with the collaborators by assigning the appropriate permissions.</li> <li>When you create a guest collection endpoint and give access to one or more Globus users, you can select whether each person has read or read/write access. If they have write access, they can also delete files within that directory tree, so you should be careful about providing write access.</li> <li>Globus guest collections are created and managed by project PIs. If the PI of a project changes, the new PI will have to create a new guest collection and share them with the users. Contact ALCF Support (support@alcf.anl.gov) in such cases. Globus guest collections' ownership cannot be transferred.</li> <li>Guest collections are active as long as the project directory is available and the PI's ALCF account is active. If the PI's ALCF account goes inactive, the collections become inaccessible to all its collaborators. Access is restored once the PI's account is reactivated.</li> <li>All RW actions are performed as the PI when using Guest Collections. If a PI does not have permissions to read or write a file or a directory, then the Globus guest collection users won't either.</li> </ol>"},{"location":"data-management/acdc/eagle-data-sharing/#creating-a-group","title":"Creating a group","text":"<ol> <li>Go to Groups on the left panel</li> <li>Click on \u2018Create a new group\u2019 at the top</li> <li>Give the group a descriptive name and add a Description for more information</li> <li>Make sure you select the \u2018group members only\u2019 radio button</li> <li>Click on \u2018Create Group\u2019</li> </ol> Create new group"},{"location":"data-management/acdc/eagle-data-sharing/#transferring-data-from-eagle","title":"Transferring data from Eagle","text":"<p>Log in to Globus using your ALCF credentials. After authenticating, you will be taken to the Globus File Manager tab. In the 'Collection' box, type the name of the Eagle managed endpoint (<code>alcf#dtn_eagle</code>). Navigate to the folder/file you want to transfer. HTTPS access (read-only) is enabled so you can download files by clicking the \"Download\" button.</p> <p>Click on 'Download' to download the required file.</p> <p> </p> Download the required file <p>To transfer files to another Globus endpoint, in the \"collection\" search box in the RHS panel, enter the destination endpoint (which could also be your Globus Connect Personal endpoint).</p> <p> </p> Transferring files to another Globus endpoint <p>To transfer files, select a file or directory on one endpoint, and click the blue 'Start' button.</p> <p> </p> Transferring files <p>If the transfer is successful, you should see the following message:</p> <p> </p> A Successful Transfer <p>Click on 'View details' to display task detail information.</p> <p> </p> Transfer completed <p>You will also receive an email when the transfer is complete.</p> <p> </p> Email confirmation"},{"location":"data-management/acdc/eagle-data-sharing/#deleting-a-guest-collection","title":"Deleting a guest collection","text":"<p>To see all guest collections you have shared, go to 'Endpoints' in the left-hand navigation bar, then 'Administered by You'. Select the guest collection endpoint you wish to delete, and click on 'Delete endpoint'.</p> <p> </p> Deleting a guest collection"},{"location":"data-management/acdc/eagle-data-sharing/#what-to-tell-your-collaborators","title":"What to tell your Collaborators","text":"<p>If you set up a shared endpoint and want your collaborator to download the data, this is what you need to tell them.</p> <p>First, the collaborator needs to get a Globus account. The instructions for setting up a Globus account are as described above. This account is free. They may already have Globus access via their institution.</p> <p>If the collaborator is downloading the data to his/her personal workstation, they need to install the Globus Connect client. Globus connect clients are available for Mac, Windows, or Linux systems and are free.</p> <p>If you clicked on the 'notify users via email' button when you added access for this user, they should have received a message that looks like this:</p> <p> </p> Click on the 'notify users via email' button for collaborators to receive an email <p>You can, of course, also send an email to your collaborators yourself, telling them you've shared a folder with them. The collaborator should click on the link, which will require logging in with their institutional or Globus login username and password. They should then be able to see the files you shared with them. External collaborator's view of the shared collection is shown below:</p> <p> </p> Collaborator transfer or sync to <p>They should click on the files they want to transfer, then 'Transfer or Sync to', enter their own endpoint name and desired path, and click the 'Start' button near the bottom to start the transfer.</p> <p> </p> Choosing transfer path"},{"location":"data-management/acdc/eagle-data-sharing/#encryption-and-security","title":"Encryption and Security","text":"<p>Data can be encrypted during Globus file transfers. In some cases, encryption cannot be supported by an endpoint, and Globus Online will signal an error.</p> <p>For more information, see How does Globus ensure my data is secure?</p> <p>In the Transfer Files window, click on 'More options' at the bottom of the 2 panes. Check the 'encrypt transfer' checkbox in the options.</p> <p> </p> Encrypting the transfer <p>Alternatively, you can encrypt the files before transfer using any method on your local system, then transfer them using Globus, then unencrypt on the other end.</p> <p>Note: Encryption and verification will slow down the data transfer.</p>"},{"location":"data-management/acdc/eagle-data-sharing/#faqs","title":"FAQs","text":""},{"location":"data-management/acdc/eagle-data-sharing/#general-faqs","title":"General FAQs:","text":"<p>1. What is the Eagle file system?</p> <p>They are Lustre file systems residing on an HPE ClusterStor E1000 platform equipped with 100 Petabytes of usable capacity across 8480 disk drives. Each ClusterStor platform also provides 160 Object Storage Targets and 40 Metadata Targets with an aggregate data transfer rate of 650GB/s.</p> <p>2. What is the difference between a Guest, Shared, and Mapped collection?</p> <ul> <li>Guest collections: A Guest collection is a logical construct that a PI sets up on their project directory in Globus that makes it accessible to collaborators. The PI creates a guest collection at or below their project and shares it with the Globus account holders.</li> <li>Shared collection: A guest collection becomes a shared collection when it is shared with a user/group.</li> <li>Mapped Collections: Mapped Collections are created by the endpoint administrators. In the case of Eagle, these are created by ALCF.</li> </ul> <p>3. Who can create Guest collections?</p> <p>ONLY a project PI (or project owner) can create guest collections and make them accessible to collaborators.</p> <p>Project Proxy (on the POSIX side) or Access Manager (on the Globus side) do not have the ability to create guest collections.</p> <p>4. Who is an Access Manager?</p> <p>Access Manager is someone who can act as a Proxy on behalf of the PI to manage the collection. The Access Manager has the ability to add users, remove users, grant or revoke read/write access privileges for those users on that particular guest collection. However, Access Managers DO NOT have permissions to create guest collections.</p> <p>5. What are Groups?</p> <p>Groups are constructs that enable multi-user data collaboration. A PI (and an Access Manager) can create new groups, add members to them, and share a guest collection with a group of collaborators.</p> <p>Note: Members of groups do not need to have ALCF accounts.</p> <p>6. What are some of the Common Errors you see and what do they mean?</p> <pre><code>- EndpointNotFound   -  Wrong endpoint name \n- PermissionDenied    -  If you do not have permissions to view or modify the collection on &lt;endpoint&gt;\n- ServiceUnavailable  -  If the service is down for maintenance\n</code></pre>"},{"location":"data-management/acdc/eagle-data-sharing/#pi-faqs","title":"PI FAQs:","text":"<p>1. How can a PI request for a data-only, Eagle storage allocation?</p> <p>A project PI can request an allocation by filling out the Director\u2019s Discretionary Allocation Request form: Request an allocation. The allocations committee reviews the proposals and provides its decision in 1-2 weeks.</p> <p>2. Does a PI need to have an ALCF account to create a Globus guest collection?</p> <p>Yes. The PI needs to have an 'active' ALCF account in place to create and share guest collections with collaborators.</p> <ul> <li>PIs with an \"Inactive/Deleted\" ALCF account should submit a reactivation request by filling out this form: Re-activation Form</li> <li>PIs without an ALCF account should submit an ALCF account request by filling out this form: Account Request Form</li> </ul> <p>3. What endpoint should the PI use?</p> <p><code>alcf#dtn_eagle</code> (project on Eagle)</p> <p>4. What are the actions a PI can perform?</p> <ul> <li>Create and delete guest collections, groups</li> <li>Create, delete, and share the data with ALCF users and external collaborators</li> <li>Specify someone as a Proxy (Access Manager) for the guest collections</li> <li>Transfer data between the guest collection on Eagle and other Globus endpoints/collections</li> </ul> <p>5. How can a PI specify someone as a Proxy on the Globus side?</p> <p>Go to alcf#dtn_eagle -&gt; collections -&gt; shared collection -&gt; roles -&gt; select 'Access Manager'</p> <p> </p> To specify someone as a Proxy, click on \"Roles\" <p> </p> Choose Access Manager and \"Add Role\" <p>6. What is the high-level workflow for setting up a guest collection?</p> <ol> <li>PI requests a compute or data-only allocation project.</li> <li>Once the request is approved, ALCF staff sets up a project, unixgroup, and project directory.</li> <li>A Globus sharing policy is created for the project with appropriate access controls, provided the PI has an active ALCF account.</li> <li> <p>PI creates a guest collection for the project, using the Globus mapped collection for the file system (alcf#dtn_eagle)</p> <ul> <li>Note: PI needs to have an active ALCF Account and will need to log in to Globus using their ALCF credentials.</li> <li>If PI has an existing Globus account, it needs to be linked to their ALCF account.</li> </ul> </li> <li> <p>PI adds collaborators to the guest collection. Collaborators can be ALCF users and external collaborators and can be added with Read only or Read-Write permissions</p> </li> </ol> <p>7. How can project members with ALCF accounts access the project directory via Globus?</p> <p>Users that have active ALCF accounts and are part of the project in the ALCF Account and Project Management system will automatically have access to the project directory which they can access by browsing the Globus endpoint <code>alcf#dtn_eagle</code>. If they want to access the files using the Globus guest collection set up by the PI, the PI will need to explicitly give them permissions to that guest collection. The purpose of Globus guest collections is to share the data with collaborators that don't have ALCF accounts or are not part of the project in the ALCF Account and Project Management system.</p> <p>8. Who has the permissions to create a guest collection?</p> <p>Only the PI has the ability to create a guest collection. The Access Manager, along with the PI, has permissions to share it with collaborators (R-only or R-W permissions as needed).</p> <p>9. I am the project PI. Why do I see a \"Permission Denied\" error when I try to CREATE a guest collection?</p> <p>If you are a PI and you see this error, it could mean that a sharing policy for the project is missing. Please contact support@alcf.anl.gov so they can set one up.</p> <p>10. If a PI added a member as a project proxy on the POSIX-side, is it safe to assume that the Proxy can create guest collections?</p> <p>No, project proxies cannot create guest collections, only the PI can.</p> <p>11. Who can create groups?</p> <p>A PI (and an Access Manager) can create new groups, add members to them, and share a guest collection with a group of collaborators. For more information, refer to: Creating a group</p> <p>12. What happens when the PI of a project changes? What happens to the guest collection endpoint?</p> <p>The new PI will need to create new guest collections and share it with collaborators again. Guest collections are tied to a PI's account and cannot be transferred.</p> <p>13. I noticed that I am the owner of all the files that were transferred by external collaborators using the guest collection. Why is that?</p> <p>When collaborators read files from or write files to the guest collection, they do so on behalf of the PI. All writes show up as having been carried by the PI. Additionally, if the PI does not have permission to read or write to a file or folder in the directory, then the collaborators will not have those permissions either.</p> <p>14. What happens to the guest collections when the PI's account goes inactive?</p> <p>The collections go inactive and will remain in that state until the PI's account is re-activated.</p> <p>15. How long does it take for the endpoint to become accessible to collaborators after a PI's account is re-activated?</p> <p>Right away. The page needs to be refreshed and sometimes you may have to log out and log back in.</p>"},{"location":"data-management/acdc/eagle-data-sharing/#access-manager-faqs","title":"Access Manager FAQs:","text":"<p>1. What are the actions an Access Manager can perform?</p> <p>Access Manager should be able to see the collection under \"Shared with you\" and \"Shareable by you\" tabs. They have permissions to add and/or delete collaborators on the shared collection and restrict their R-W access as needed.</p> <p>2. Does an Access Manager need to have an ALCF account?</p> <p>Not necessary. However, if they need to manage the membership on the POSIX side (or in the ALCF Account and Project Management system), they will need an ALCF account and be a Proxy on the project.</p> <p>3. What is the difference between an ALCF project Proxy and a guest collection Access Manager?</p> <p>An ALCF Project Proxy has permissions to manage project membership on the POSIX side whereas a guest collection Access Manager has permissions to manage the project membership specific to that guest collection, created by the PI, on the Globus side.</p> <p>4. I am an 'Access Manager' on the collection. Why do I see a 'Permission Denied' error when I try to SHARE a guest collection created by the PI?</p> <p>If you are a non-PI who is able to access the guest collection but unable to share it, it means that your role on this guest collection is limited to a \"Member\". If you want the ability to share folders and sub-folders from the collections that are shared with you, please talk to the PI. They will need to set your role to an \"Access Manager\" for the collection within Globus.</p> <p>5. Can an Access Manager give external collaborators access to the collections that are shared with them?</p> <p>Yes, an Access Manager will see the \"Permissions\" tab at the top of the shared collection page and can share it with collaborators and/or a group.</p> <p>6. Can an Access Manager create collections using the shared endpoint?</p> <p>No. An access manager cannot create a collection, only a PI can do that. The access manager can however share folders and sub-folders from the collections that are shared with them.</p> <p>7. Can an Access Manager leave a globus group or withdraw membership request for collaborators?</p> <p>Yes. [Go to alcf#dtn_eagle -&gt; Groups &gt; group_name -&gt; Members -&gt; click on specific user -&gt; Role &amp; Status -&gt; Set the appropriate status]</p> <p> </p> If you get this error, you do not have read permissions. <p>8. Can an Access Manager delete guest collections created by PI?</p> <p>No. Access managers cannot delete guest collections.</p>"},{"location":"data-management/acdc/eagle-data-sharing/#guest-collection-collaborators-faqs","title":"Guest Collection Collaborators FAQs:","text":"<p>1. What actions can collaborators perform?</p> <ol> <li>Collaborators can read files from a collection*</li> <li>Collaborators can write to a collection**</li> <li>Collaborators can delete files in a collection**</li> </ol> <p>*If the PI has read permissions for those files on the POSIX side and the collaborator is given read permissions in Globus for the guest collection.</p> <p>**If the PI has write permissions for those files on the POSIX side and the collaborator is given write permissions in Globus for the guest collection.</p> <p>2. I am a collaborator. Why do I see a 'Permission Denied' error when I try to ACCESS a guest collection created by the PI?</p> <p>If you are a non-PI and you see this error while trying to access the collection, it means that you do not have read permissions to access the guest collection. Please contact the PI for required access.</p> <p> </p> If you get this error, you do not have read permissions."},{"location":"data-management/acdc/transferring-data-to-eagle/","title":"Transferring Data to Eagle","text":""},{"location":"data-management/acdc/transferring-data-to-eagle/#evolution-of-the-petrel-data-service-to-the-alcf-community-data-co-op","title":"Evolution of the Petrel Data Service to the ALCF Community Data Co-Op","text":"<p>The Petrel data service is evolving into a more mature service called the ALCF Community Data Co-Op (ACDC), which will be launched later this year.</p> <p>In preparation for this shift, all current Petrel project PIs will need to move their project data to ALCF's Eagle filesystem by December 2021.</p> <p>For detailed instructions on how to move your data, please follow the steps outlined below. You will need to follow the order of the steps as listed.</p> <p>If you have any questions, please email: support@alcf.anl.gov.</p>"},{"location":"data-management/acdc/transferring-data-to-eagle/#transferring-data-to-eagle_1","title":"Transferring data to Eagle","text":""},{"location":"data-management/acdc/transferring-data-to-eagle/#1-request-a-dd-project-on-eagle-filesystem","title":"1. Request a DD project on Eagle Filesystem","text":"<p>All Petrel project owners/PIs should request a Director's Discretionary project on the Eagle filesystem by filling out the form at https://my.alcf.anl.gov/accounts/#/allocationRequests. Select \"New Project\" and then \"Eagle\" as the resource and fill out the rest of the form. In the \"Project and Justification Summary\" section, along with the requested details, you should also state that you are migrating your data from Petrel.</p> <p>Once the submission is reviewed and approved by the allocations committee, your project will be created on the Eagle filesystem, and you will be notified via email. The approval process may take 1-2 weeks. Once the project is approved, proceed to the next step.</p>"},{"location":"data-management/acdc/transferring-data-to-eagle/#2-apply-for-an-alcf-account","title":"2. Apply for an ALCF account","text":"<p>A project PI will need an active ALCF account to:</p> <ul> <li>Transfer their data from Petrel to the Eagle filesystem</li> <li>Enable data sharing on their Eagle project (See section \"4 Share your data on Eagle using Globus Guest Collections\" for more details)</li> </ul> <p>NOTE: A collaborator does not need an ALCF account to access data that is shared on Eagle (as a Globus Guest Collection). They can sign into Globus with their institutional identity to access the data. The first time they log in, they will need to accept terms and conditions.</p>"},{"location":"data-management/acdc/transferring-data-to-eagle/#to-apply-for-an-alcf-account","title":"To apply for an ALCF account:","text":"<ul> <li>Visit https://my.alcf.anl.gov/ and click on \"Request An Account\".</li> <li>When prompted for project name, please select the project on Eagle that was created for your Petrel data as a result of Step 1: Request a DD project on Eagle (you have to wait for your project to be created before you can apply for an account).</li> <li>If you don't have one, please follow the directions under \"Step 1: Request a DD project on Eagle\" (above).</li> <li>For more details on the ALCF account request process, visit the webpage Request an account.</li> <li>Once your account is created and you have the cryptocard/mobile token to log in to Eagle, proceed to the next step to transfer the data from Petrel to Eagle.</li> </ul>"},{"location":"data-management/acdc/transferring-data-to-eagle/#3-transfer-data-from-your-source-endpoint-to-eagle-using-globus","title":"3. Transfer data from your source endpoint to Eagle using Globus","text":"<p>You can use the Globus web app to transfer data or the CLI. See Using CLI for instructions on how to use the CLI to transfer data. The following set of instructions uses the Globus web app, using alcf#dtn_eagle (path /projectname) as the destination to transfer data from your source endpoint.</p> <p>NOTE: Anonymous HTTPS read access is enabled on Eagle.</p> <p>Step 1: Log into https://app.globus.org/file-manager?destination_id=05d2c76a-e867-4f67-aa57-76edeb0beda0, which opens two panes in the Globus File Manager, with ALCF Eagle on the right-hand side.</p> <ul> <li>Enter the name of your source endpoint in the pane on the left-hand side.</li> </ul> <p> </p> File Manager <p> </p> Enter the name of your source endpoint <p>Step 2: You may have to log in and link your ALCF identity to your Globus account.</p> <p> </p> Log in and link your ALCF identity to your Globus account <p>Step 3: Log in using your ALCF credentials.</p> <p> </p> Use ALCF credentials <p>Step 4: If the login is successful, the folders and files on the Eagle file system will be displayed in the project/file viewer.</p> <p> </p> Eagle file system in the project/file viewer <p>Step 5: Navigate to the correct destination (project folder) on the Eagle file system. Choose the files/folders to transfer in the left-hand side panel (Petrel endpoint).</p> <p>NOTE: Before clicking the \"Start\" button, click on the Transfer and Sync Options and check the \"sync\" checkbox and then click start.</p> <p> </p> Choose the files/folders <p>Step 6: Click on the \"Activity\" tab on the left-hand side navigation panel to view the status and details of your transfers.</p> <p> </p> Activity tab <p>Step 7: Once the transfer is successful, you should see the files and folders on the Eagle file system. You will also receive an email notification from Globus letting you know that your transfer was successful.</p> <p> </p> Files and folders on the Eagle file system"},{"location":"data-management/acdc/transferring-data-to-eagle/#migrating-permissions-from-petrel-to-eagle","title":"Migrating permissions from Petrel to Eagle:","text":"<p>For PIs who had previously stored data on Petrel and are migrating to Eagle, the following tool automates the step of copying the permissions set on Petrel to Eagle. The tool, <code>migrate_permissions.py</code> at https://github.com/globus/globus-tool-examples, takes the source endpoint (your shared endpoint on Petrel in this case) and destination endpoint (the guest collection on Eagle that has the data) and copies over all the permissions. The tool assumes the data was copied over as is from source to destination.</p> <p>If you have any questions on the tool or need further support, please contact support@globus.org.</p>"},{"location":"data-management/acdc/transferring-data-to-eagle/#4-share-your-data-on-eagle-using-globus-guest-collections","title":"4. Share your data on Eagle using Globus Guest Collections","text":"<p>Your data on the Eagle file system can easily be shared with collaborators who are at ALCF or elsewhere. You have full control over which files your collaborator can access and whether they have read-only or read-write permissions.</p> <p>See below for step-by-step instructions on how to share data from Eagle using Globus Guest Collections:</p> <p>https://docs.alcf.anl.gov/data-management/acdc/eagle-data-sharing/</p> <p>NOTE: Guest Collections are tied to the project PI's account, so if the PI's account becomes inactive, the Guest Collections will also become inactive. Once the PI's account is reactivated, access to the Guest Collections is restored.</p>"},{"location":"data-management/acdc/transferring-data-to-eagle/#using-globus-cli-tool","title":"Using Globus CLI tool:","text":"<p>To copy data and permissions from a source collection, PIs can use a Globus CLI tool that automates the step of copying the permissions set on the source collection and applies them to the collection on Eagle. This is especially useful for PIs who had previously stored data on Petrel. See https://github.com/globus/globus-tool-examples for more information.</p> <p>The tool, <code>migrate_permissions.py</code> in the GitHub repo, takes the source endpoint (the shared endpoint on Petrel, for example) and destination endpoint (the guest collection on Eagle that has the data) and copies over all the permissions. The tool assumes the data was copied over as is from source to destination. Note that you need to have a guest collection set up for your project on Eagle to use the CLI command and tool. See this page for instructions on how to set up guest collections.</p> <p>If you have any questions on the tool or need further support, please contact support@globus.org.</p> <p>Existing data portals:</p> <p>To reconfigure and update your existing data portals to point to your guest collections on Eagle, please work directly with the developer/maintainer of the portal.</p>"},{"location":"data-management/acdc/transferring-data-to-eagle/#faqs-for-migrating-petrel-data-to-eagle","title":"FAQs for migrating Petrel data to Eagle:","text":""},{"location":"data-management/acdc/transferring-data-to-eagle/#1-is-it-important-for-a-petrel-project-ownerpi-to-obtain-an-alcf-account","title":"1. Is it important for a Petrel project owner/PI to obtain an ALCF account?","text":"<p>Yes, the data from Petrel needs to be moved to an ALCF project directory on the Eagle filesystem. The PI will need an ALCF account to log into Globus and move the data to their Eagle project directory.</p>"},{"location":"data-management/acdc/transferring-data-to-eagle/#2-what-is-the-workflow-for-migrating-data-from-petrel-and-giving-access-to-collaborators-on-eagle","title":"2. What is the workflow for migrating data from Petrel and giving access to collaborators on Eagle?","text":"<ol> <li>PI requests an Eagle allocation project.</li> <li>Allocations Committee reviews and approves requests.</li> <li>Once the allocation request is approved, the project is created and associated with a UNIX group and project directory on Eagle.</li> <li>PI requests an ALCF account (if they don't have one).</li> <li>Once the ALCF account is created and tied to the project on Eagle, the PI moves the data from Petrel to Eagle using Globus.</li> <li>PI creates guest collections for the project on Eagle, using the Globus web app using the mapped collection/endpoint for Eagle (alcf#dtn_eagle). Note that:</li> <li>The PI needs to have an active ALCF Account and will need to log in to Globus using their ALCF credentials.</li> <li>Only the PI (and not a proxy) can create guest collections.</li> <li>If the PI already has a Globus account, it needs to be linked to their ALCF account.</li> <li>PI adds collaborators to the guest collection.</li> <li>Added with read-only or read-write permissions.</li> <li>Note: Anonymous HTTPS write is disabled, and only anonymous HTTPS read is allowed.</li> <li>Existing data portals on Petrel should be updated to point to the new guest collection on Eagle. Please work directly with the developer/maintainer of the portal.</li> </ol>"},{"location":"data-management/acdc/transferring-data-to-eagle/#3-what-endpoints-should-the-pi-use-to-move-data-from-petrel","title":"3. What endpoints should the PI use to move data from Petrel?","text":"<ul> <li>Source: Globus endpoint on Petrel for the Petrel allocation</li> <li>Destination: Globus endpoint on the Eagle filesystem and the path to the directory (alcf#dtn_eagle, path /) OR the name of the guest collection on Eagle"},{"location":"data-management/data-transfer/sftp-scp/","title":"SFTP and SCP","text":"<p>These standard utilities are available for local area transfers of small files; they are not recommended for use with large data transfers due to poor performance and excess resource utilization on the login nodes.</p> <p>See Globus for performing large data transfers.</p>"},{"location":"data-management/data-transfer/using-globus/","title":"Using Globus","text":"<p>Globus addresses the challenges faced by researchers in moving, sharing, and archiving large volumes of data among distributed sites. With Globus, you hand off data movement tasks to a hosted service that manages the entire operation. It monitors performance and errors, retries failed transfers, corrects problems automatically whenever possible, and reports status to keep you informed and focused on your research.</p> <p>Command line and web-based interfaces are available. The command line interface, which requires only SSH to be installed on the client, is the method of choice for script-based workflows. Globus also provides a REST-style transfer API for advanced use cases that require scripting and automation.</p>"},{"location":"data-management/data-transfer/using-globus/#getting-started","title":"Getting Started","text":"<p>Basic documentation for getting started with Globus can be found at the following URL: https://docs.globus.org/how-to/</p>"},{"location":"data-management/data-transfer/using-globus/#data-transfer-node","title":"Data Transfer Node","text":"<p>Several data transfer nodes (DTNs) for <code>/home</code>, Eagle, Grand, and HPSS are available to ALCF users, allowing users to perform wide and local area data transfers. Access to the DTNs is provided via the following Globus endpoints.</p>"},{"location":"data-management/data-transfer/using-globus/#alcf-globus-endpoints","title":"ALCF Globus Endpoints","text":"<p>The Globus endpoint and the path to use depend on where your data resides. If your data is on:</p> <ul> <li><code>/home</code>, which is where your home directory resides for Polaris, Sophia, and Crux systems: <code>alcf#dtn_home</code> for accessing <code>/home</code> (i.e., home directories on the agile-home filesystem). Use the path <code>/&lt;username&gt;</code></li> <li>HPSS: <code>alcf#dtn_hpss</code></li> <li>Eagle filesystem: <code>alcf#dtn_eagle</code> for accessing <code>/lus/eagle/projects</code> or <code>/eagle</code> (i.e., project directories on the Eagle filesystem). Use the path <code>/eagle/&lt;project name&gt;</code></li> <li>Grand filesystem: <code>alcf#dtn_grand</code> for accessing <code>/lus/grand/projects</code> or <code>/grand</code> (i.e., project directories on the Grand filesystem). Use the path <code>/grand/&lt;project name&gt;</code></li> <li>Flare filesystem: <code>alcf#dtn_flare</code> for accessing <code>/lus/flare/projects</code> or <code>/flare</code> (i.e., project directories on the Flare filesystem) on Aurora. Use the path <code>/flare/&lt;project name&gt;</code></li> </ul> <p>After registering, simply use the appropriate ALCF endpoint, as well as other sources or destinations. Use your ALCF credentials (your OTP generated by the CryptoCARD token with PIN or Mobilepass app) to activate the ALCF endpoint.</p> <p>Globus Connect Personal allows users to add laptops or desktops as an endpoint to Globus in just a few steps. After you set up Globus Connect Personal, Globus can be used to transfer files to and from your computer.</p>"},{"location":"data-management/data-transfer/using-globus/#references","title":"References","text":"<p>Research Data Management with Globus (2019) </p>"},{"location":"data-management/filesystem-and-storage/data-storage/","title":"ALCF Data Storage","text":""},{"location":"data-management/filesystem-and-storage/data-storage/#disk-storage","title":"Disk Storage","text":"<p>The ALCF operates a number of file systems that are mounted globally across all of our production systems.</p>"},{"location":"data-management/filesystem-and-storage/data-storage/#home","title":"Home","text":"<p>A Lustre file system residing on a DDN AI-400X NVMe Flash platform. It has 24 NVMe drives with 7 TB each, providing 123 TB of usable space. It provides 8 Object Storage Targets and 4 Metadata Targets.</p>"},{"location":"data-management/filesystem-and-storage/data-storage/#eagle","title":"Eagle","text":"<p>A Lustre file system residing on an HPE ClusterStor E1000 platform equipped with 100 Petabytes of usable capacity across 8,480 disk drives. This ClusterStor platform provides 160 Object Storage Targets and 40 Metadata Targets with an aggregate data transfer rate of 650 GB/s. The primary use of Eagle is data sharing with the research community. Eagle has community sharing capabilities which allow PIs to share their project data with external collaborators using Globus. Eagle can also be used for compute campaign storage.</p> <p>Also see ALCF Data Policies and Data Transfer.</p>"},{"location":"data-management/filesystem-and-storage/data-storage/#tape-storage","title":"Tape Storage","text":"<p>ALCF operates three 10,000-slot Spectralogic tape libraries. We are currently running a combination of LTO6 and LTO8 tape technology. The LTO tape drives have built-in hardware compression which typically achieves compression ratios between 1.25:1 and 2:1 depending on the data, yielding an effective capacity of approximately 65 PB.</p>"},{"location":"data-management/filesystem-and-storage/data-storage/#hpss","title":"HPSS","text":"<p>HPSS is a data archive and retrieval system that manages large amounts of data on disk and robotic tape libraries. It provides hierarchical storage management services that allow it to migrate data between those storage platforms.</p> <p>HPSS is currently configured with a disk and tape tier. The disk tier has a capacity of 1.2 PB on a DataDirect Networks SFA12K-40 storage array. By default, all archived data is initially written to the disk tier. The tape tier consists of 3 SpectraLogic T950 robotic tape libraries containing a total of 72 LTO6 tape drives with a total uncompressed capacity of 64 PB. Archived data is migrated to the tape tier at regular intervals, then deleted from the disk tier to create space for future archives.</p> <p>Access to HPSS is provided by various client components. Currently, ALCF supports access through two command-line clients, HSI and HTAR. In order for the client to authenticate with HPSS, the user must have a keytab file that should be located in their home directory under the subdirectory .hpss. The file name will be in the format .ktb_."},{"location":"data-management/filesystem-and-storage/data-storage/#hsi-general-usage","title":"HSI General Usage","text":"<p>HSI can be invoked by simply entering <code>hsi</code> at your normal shell prompt. Once authenticated, you will enter the HSI command shell environment:</p> <pre><code>&gt; hsi\n[HSI]/home/username-&gt;\n</code></pre> <p>You may enter \"help\" to display a brief description of available commands.</p> <p>If archiving from or retrieving to Eagle, you must disable the Transfer Agent with <code>-T off</code>.</p> <p>Example archive: <pre><code>[HSI]/home/username-&gt; put mydatafile                # same name on HPSS\n[HSI]/home/username-&gt; put local.file : hpss.file    # different name on HPSS\n[HSI]/home/username-&gt; put -T off mydatafile\n</code></pre></p> <p>Example retrieval: <pre><code>[HSI]/home/username-&gt; get mydatafile\n[HSI]/home/username-&gt; get local.file : hpss.file\n[HSI]/home/username-&gt; get -T off mydatafile\n</code></pre></p> <p>Most of the usual shell commands will work as expected in the HSI command environment. For example, checking what files are archived:</p> <p><code>[HSI]/home/username-&gt; ls -l</code></p> <p>And organizing your archived files:</p> <pre><code>[HSI]/home/username-&gt; mkdir dataset1\n[HSI]/home/username-&gt; mv hpss.file dataset1\n[HSI]/home/username-&gt; ls dataset1\n[HSI]/home/username-&gt; rm dataset1/hpss.file\n</code></pre> <p>It may be necessary to use single or double quotes around metacharacters to avoid having the shell prematurely expand them. For example:</p> <pre><code>[HSI]/home/username-&gt; get *.c\n</code></pre> <p>will not work, but</p> <pre><code>[HSI]/home/username-&gt; get \"*.c\"\n</code></pre> <p>will retrieve all files ending in .c.</p> <p>Following normal shell conventions, other special characters in filenames such as whitespace and semicolon also need to be escaped with \"\\\" (backslash). For example:</p> <pre><code>[HSI]/home/username-&gt; get \"data\\ file\\ \\;\\ version\\ 1\"\n</code></pre> <p>retrieves the file named \"data file ; version 1\".</p> <p>HSI can also be run as a command line or embedded in a script as follows:</p> <pre><code>hsi -O log.file \"put local.file\"\n</code></pre>"},{"location":"data-management/filesystem-and-storage/data-storage/#htar-general-usage","title":"HTAR General Usage","text":"<p>HTAR is a tar-like utility that creates tar-format archive files directly in HPSS. It can be run as a command line or embedded in a script.</p> <p>Example archive: <pre><code>htar -cf hpssfile.tar localfile1 localfile2 localfile3\n</code></pre></p> <p>Example retrieval:</p> <pre><code>htar -xf hpssfile.tar localfile2\n</code></pre> <p>NOTE: The current version of HTAR has a 64 GB file size limit as well as a path length limit. The recommended client is HSI.</p>"},{"location":"data-management/filesystem-and-storage/data-storage/#globus","title":"Globus","text":"<p>In addition, HPSS is accessible through the Globus endpoint <code>alcf#dtn_hpss</code>. As with HSI and HTAR, you must have a keytab file before using this endpoint. For more information on using Globus, please see Using Globus.</p>"},{"location":"data-management/filesystem-and-storage/data-storage/#keytab-file-missing","title":"Keytab File Missing","text":"<p>If you see an error like this:</p> <pre><code>*** HSI: (KEYTAB auth method) - keytab file missing or inaccessible: /\n home/username/.hpss/.ktb_username\n Error - authentication/initialization failed\n</code></pre> <p>it means that your account is not enabled to use the HPSS yet. Please contact support to have it set up.</p>"},{"location":"data-management/filesystem-and-storage/disk-quota/","title":"Disk Quota","text":""},{"location":"data-management/filesystem-and-storage/disk-quota/#overview","title":"Overview","text":"<p>Disk quotas are enabled on project directories. ALCF's HPC systems use the agile-home file system located at <code>/lus/agile/home</code>, where quotas are also enforced. Details on the home file system are listed in file systems. Below are descriptions and examples for the home file system, as well as the Eagle project filesystems.</p>"},{"location":"data-management/filesystem-and-storage/disk-quota/#home-directory-quotas","title":"Home Directory Quotas","text":"<p>By default, each home directory is assigned a quota of 50GB. File ownership determines disk space usage.</p> <p>To check the home directory usage, enter this command:</p> <pre><code>myquota\n</code></pre> <pre><code>Name                           Type     Filesystem        Used               Quota          Grace\n=========================================================================================================\nuserX                         User     /lus/agile         44.13G          50.00G             none\n</code></pre>"},{"location":"data-management/filesystem-and-storage/disk-quota/#project-directory-quotas","title":"Project Directory Quotas","text":"<p>The amount of data stored under <code>/lus/grand/projects/PROJECT_NAME</code> cannot exceed the approved project quota limit set during the allocation period. The total data usage under the project directory is used to calculate the disk quota.</p> <p>To check project quota usage on the file systems, enter this command:</p> <pre><code>myprojectquotas\n</code></pre> <pre><code>Lustre : Current Project Quota information for projects you're a member of:\n\nName                       Type        Filesystem          Used             Quota           Grace\n==============================================================================================================\nprojectX                  Project      eagle                1.87T             1000T            -\n</code></pre>"},{"location":"data-management/filesystem-and-storage/disk-quota/#requesting-a-new-eagle-allocation","title":"Requesting a New Eagle Allocation","text":"<p>To request a new project with an allocation on Eagle (with or without a compute allocation), please fill out the Director's Discretionary allocation form. Note that all new compute projects will have the default file system.</p>"},{"location":"data-management/filesystem-and-storage/disk-quota/#quota-increases","title":"Quota Increases","text":"<p>If you need a quota increase for Director's Discretionary allocations, please fill out the Director's Discretionary allocation form.</p> <p>If you need a quota increase for your INCITE/ALCC/ESP project directory, please send an email to support@alcf.anl.gov with the machine, project name, new quota amount, and reason for the increase.</p>"},{"location":"data-management/filesystem-and-storage/file-systems/","title":"ALCF File Systems","text":"<p>Our HPC systems store project data in file systems called Eagle and Flare. Eagle and Flare are Lustre file systems mounted as <code>/eagle</code> and <code>/flare</code>, respectively. For more information on the Lustre file systems, here is a document on Lustre File Striping Basics.</p> <ul> <li>Lustre File Striping Basics</li> </ul> <p>For information on the AI Testbed storage systems, refer to the AI Testbed storage page: https://argonne-lcf.github.io/ai-testbed-userdocs/common/storage/</p> <p>Our HPC systems also mount a Lustre home file system, either agile-home or gecko-home. The home file system is mounted as <code>/home</code> and should generally be used for small files and any binaries to be run on Polaris or Aurora. The performance of this file system is reasonable, but using it for intensive I/O from the compute nodes is discouraged because I/O from the compute nodes uses the project data file systems, which are fast parallel systems and have far more storage space and greater I/O performance than the home directory space.</p> <p>The agile-home file system is regularly backed up to tape. The data file system is not backed up. It is the user\u2019s responsibility to ensure that copies of any critical data on the data file system have either been archived to tape or stored elsewhere.</p> Name Accessible From Type Path Production Backed-up Usage agile-home Polaris Lustre /home or /lus/agile/home Yes Yes General use gecko-home Aurora Lustre /home or /lus/gecko/home Yes No General use Eagle Polaris Lustre /eagle or /lus/eagle/projects Yes No Community sharing via Globus;  Intensive job output, large files Flare Aurora Lustre /flare or /lus/flare/projects Yes No Community sharing via Globus;  Intensive job output, large files Node SSD  (Compute node only) Polaris xfs /local/scratch (Polaris) Yes No Local node scratch during run"},{"location":"data-management/filesystem-and-storage/file-systems/#available-directories","title":"Available Directories","text":""},{"location":"data-management/filesystem-and-storage/file-systems/#home-directories","title":"Home Directories","text":"<ul> <li>Created when an account is created</li> <li>Located under /home</li> <li>Each home directory is subject to a quota based on user file ownership. The default quota is 50 GB</li> </ul>"},{"location":"data-management/filesystem-and-storage/file-systems/#sharing-home-directory-files-or-subdirectories-with-others","title":"Sharing Home Directory Files or Subdirectories with Others","text":"<p>If you need to share files or subdirectories (folders) under your home directory with collaborators (other ALCF users), you need to change file permissions from their defaults. You must change permissions of your top-level <code>/home/username</code> directory, even if you only want to share certain files/directories within it. Using normal Linux file permissions control is good enough to give access to all other users and is simple. For more fine-grained control over specific users, you need to use Linux access control list (ACL) commands.</p>"},{"location":"data-management/filesystem-and-storage/file-systems/#simple-method-permission-to-all-users","title":"Simple Method: Permission to All Users","text":"<p>First, a one-time-only change to your top-level <code>/home/username</code> directory.</p> <pre><code>chmod o+x /home/username\n</code></pre> <p>Then you may permission individual files and/or subdirectories with read access. For example, to recursively change permissions on <code>/home/username/subdirectoryname</code> so that all files in that subdirectory and any subdirectory trees within it are world-readable, you would use</p> <pre><code>chmod -R o+Xr /home/username/subdirectoryname\n</code></pre>"},{"location":"data-management/filesystem-and-storage/file-systems/#refined-method-use-acl-to-give-permission-to-specific-users","title":"Refined Method: Use ACL to Give Permission to Specific Users","text":"<p>First, a one-time-only change to your top-level <code>/home/username</code> directory. To share files/directories with user gilgamesh, for example:</p> <pre><code>setfacl -m u:gilgamesh:X /home/username\n</code></pre> <p>Then you may permission individual files and/or subdirectories with read access. For example, to recursively change permissions on <code>/home/username/subdirectoryname</code> so that all files in that subdirectory and any subdirectory trees within it are readable to user gilgamesh, you would use</p> <pre><code>setfacl -R -m u:gilgamesh:rX /home/username/subdirectoryname\n</code></pre>"},{"location":"data-management/filesystem-and-storage/file-systems/#project-directories","title":"Project Directories","text":"<ul> <li>Directories on Eagle or Flare are created when an allocation (INCITE, ALCC, Discretionary, etc.) is awarded. Directories can be created as stand-alone allocations. Use the allocation request form to submit requests for an allocation on Eagle. </li> <li>Directory paths:<ul> <li>Eagle: <code>/eagle</code> or <code>/lus/eagle/projects</code></li> <li>Flare: <code>/flare</code> or <code>/lus/flare/projects</code></li> </ul> </li> </ul> <p>These project spaces do not have user quotas but a directory quota, meaning that ALL files contained within a project directory, regardless of the username, cannot exceed the disk space allocation granted to the project. For more information on quotas, see the Disk Quota page.</p>"},{"location":"data-management/filesystem-and-storage/file-systems/#local-node-ssd","title":"Local Node SSD","text":"<p>Access to SSDs is enabled by default on Polaris.</p>"},{"location":"data-management/filesystem-and-storage/file-systems/#ssd-information","title":"SSD Information","text":"<ul> <li>Local scratch SSD storage on compute nodes for running jobs</li> <li>Completely local non-parallel filesystem</li> <li>Located at /local/scratch on Polaris computes</li> <li>Wiped between Cobalt/PBS Pro jobs</li> <li>No automatic backups provided</li> <li>Information on the current SSD drives in use is below:</li> </ul> <p>Polaris SSD Specs</p> <p>Model PM1725a drives specifications</p> Model PM1725a drives ------- Capacity 1.6 TB Sequential Read 3300 MB/s Sequential Write 3300 MB/s"},{"location":"data-management/filesystem-and-storage/hpss/","title":"Using HPSS","text":""},{"location":"data-management/filesystem-and-storage/hpss/#overview","title":"Overview","text":"<p>HPSS is a data archive and retrieval system that manages large amounts of data on disk and robotic tape libraries. It provides hierarchical storage management services that allow it to migrate data between those storage platforms.</p> <p>HPSS is currently configured with a disk and tape tier. The disk tier has a capacity of 1.2PB on a DataDirect Networks SFA12K-40 storage array. By default, all archived data is initially written to the disk tier. The tape tier consists of 3 SpectraLogic T950 robotic tape libraries containing a total of 72 LTO6 tape drives with a total uncompressed capacity of 64 PB. Archived data is migrated to the tape tier at regular intervals, then deleted from the disk tier to create space for future archives.</p> <p>Access to HPSS is provided by various client components. Currently, ALCF supports access through two command-line clients: HSI and HTAR. These are installed on the login nodes of Theta, Cooley, and Polaris. In order for the client to authenticate with HPSS, the user must have a keytab file that should be located in their home directory under the subdirectory <code>.hpss</code>. The file name will be in the format <code>.ktb_&lt;userid&gt;</code>.</p>"},{"location":"data-management/filesystem-and-storage/hpss/#hsi-general-usage","title":"HSI General Usage","text":"<p>HSI can be invoked by simply entering <code>hsi</code> at your normal shell prompt. Once authenticated, you will enter the HSI command shell environment: <pre><code>&gt; hsi\n[HSI]/home/username-&gt;\n</code></pre></p> <p>You may enter \"help\" to display a brief description of available commands.</p> <p>Example archive: <pre><code>[HSI]/home/username-&gt; put mydatafile                # same name on HPSS\n[HSI]/home/username-&gt; put local.file : hpss.file    # different name on HPSS\n</code></pre></p> <p>Example retrieval: <pre><code>[HSI]/home/username-&gt; get mydatafile\n[HSI]/home/username-&gt; get local.file : hpss.file\n</code></pre></p> <p>Most of the usual shell commands will work as expected in the HSI command environment.</p> <p>For example, checking what files are archived: <pre><code>[HSI]/home/username-&gt; ls -l\n</code></pre></p> <p>And organizing your archived files: <pre><code>[HSI]/home/username-&gt; mkdir dataset1\n[HSI]/home/username-&gt; mv hpss.file dataset1\n[HSI]/home/username-&gt; ls dataset1\n[HSI]/home/username-&gt; rm dataset1/hpss.file\n</code></pre></p> <p>It may be necessary to use single or double quotes around metacharacters to avoid having the shell prematurely expand them.</p> <p>For example: <pre><code>[HSI]/home/username-&gt; get *.c\n</code></pre> will not work, but <pre><code>[HSI]/home/username-&gt; get \"*.c\"\n</code></pre> will retrieve all files ending in .c.</p> <p>Following normal shell conventions, other special characters in filenames such as whitespace and semicolon also need to be escaped with \"\\\" (backslash). For example: <pre><code>[HSI]/home/username-&gt; get \"data\\ file\\ \\;\\ version\\ 1\"\n</code></pre> retrieves the file named \"data file ; version 1\".</p> <p>HSI can also be run as a command line or embedded in a script as follows: <pre><code>hsi -O log.file \"put local.file\"\n</code></pre></p>"},{"location":"data-management/filesystem-and-storage/hpss/#htar-general-usage","title":"HTAR General Usage","text":"<p>HTAR is a tar-like utility that creates tar-format archive files directly in HPSS. It can be run as a command line or embedded in a script.</p> <p>Example archive: <pre><code>htar -cf hpssfile.tar localfile1 localfile2 localfile3\n</code></pre></p> <p>Example retrieval: <pre><code>htar -xf hpssfile.tar localfile2\n</code></pre></p> <p>Note: - On Theta, you must first load the HSI module to make HSI and HTAR available. Use <code>module load hsi</code>. - The current version of HTAR has a 64GB file size limit as well as a path length limit. The recommended client is HSI.</p>"},{"location":"data-management/filesystem-and-storage/hpss/#globus","title":"Globus","text":"<p>In addition, HPSS is accessible through the Globus endpoint <code>alcf#dtn_hpss</code>. As with HSI and HTAR, you must have a keytab file before using this endpoint. For more information on using Globus, please see Using Globus.</p>"},{"location":"data-management/filesystem-and-storage/hpss/#common-problems","title":"Common Problems","text":""},{"location":"data-management/filesystem-and-storage/hpss/#keytab-file-missing","title":"Keytab File Missing","text":"<p>If you see an error like this: <pre><code>*** HSI: (KEYTAB auth method) - keytab file missing or inaccessible: /\n home/username/.hpss/.ktb_username\n Error - authentication/initialization failed\n</code></pre> it means that your account is not enabled to use the HPSS yet. Please contact support to have it set up.</p>"},{"location":"polaris/contacting-support/","title":"Support Issues &amp; Software Requests","text":""},{"location":"polaris/contacting-support/#contacting-support","title":"Contacting Support","text":"<p>For user support issues or questions, please direct all questions, requests, and feedback to support@alcf.anl.gov.</p> <p>Please be aware there is a list of known issues on Polaris that can be found here.</p> <p>When contacting support, please include the following information:</p> <ul> <li>Your ALCF Username</li> <li>Your project name</li> <li>The system you're on (Polaris, Sophia, etc.)</li> </ul>"},{"location":"polaris/contacting-support/#job-failures","title":"Job Failures","text":"<p>If you are having issues running your job, or your job is failing, please include the following in your email to support:</p> <ul> <li>All job IDs of the failures</li> <li>Your <code>qsub</code> submission script if you're submitting a batch job, or your full <code>qsub</code> command if you're submitting an interactive job</li> <li>A list of all modules loaded while running your job. Please provide the list via the <code>module list</code> command and NOT a list of your <code>module load &lt;module&gt;</code> commands</li> <li>The <code>*.e</code> (error) and <code>*.o</code> (output) files from at least one of your job failures</li> <li>Any errors displayed on the command line</li> </ul> <p>Info</p> <p>Support does not have access to your home directory or your project directory. Please do not include directory paths as a means for Support to access your submission script. It must be attached to the ticket.</p>"},{"location":"polaris/contacting-support/#python-issues","title":"Python Issues","text":"<p>If you need to open a ticket related to Python, please be sure to include the following in your email to support:</p> <ul> <li>Your <code>qsub</code> submission script</li> <li>Which base conda module and environment you are using</li> <li>The output from <code>module list</code></li> <li>Whether you have extended the base environment via <code>venv</code>, <code>conda clone</code>, etc.</li> <li>Have you installed any new packages, or removed existing ones? If so, please include your script and commands necessary to recreate the issue</li> <li>Whether you're attempting to run on a login node or a compute node</li> </ul> <p>By including the above information, this will help ALCF Support staff quickly route your ticket to the correct subject-matter expert (SME), resulting in a quicker resolution.</p> <p>Tip</p> <p>We encourage the use of the pre-installed <code>conda</code> environment. Any custom environments are supported on a best-effort basis only.</p>"},{"location":"polaris/contacting-support/#installation-compiling-issues","title":"Installation &amp; Compiling Issues","text":"<p>If you are having issues installing and/or compiling your app, please include the following in your email to support:</p> <ul> <li>The output from <code>module list</code></li> <li>If you are on a login node or a compute node</li> <li>A link to the app you are attempting to install (if possible)</li> <li>The full command you're using to compile</li> <li>Any other necessary steps Support will need to recreate the issue</li> </ul>"},{"location":"polaris/contacting-support/#software-requests","title":"Software Requests","text":"<p>Warning</p> <p>Software install requests can take several months to receive approval before testing and installation. Therefore, it is strongly encouraged to attempt to install the app yourself before contacting support for it to be installed system-wide.</p> <p>If you want to request a package be installed on Polaris, please include the following when reaching out to support:</p> <ul> <li>A link to the package/app you want to be installed</li> <li>The reason you need the package installed</li> <li>Why the currently provided applications/modules will not work for your workflow (if applicable)</li> </ul>"},{"location":"polaris/getting-started/","title":"Getting Started on Polaris","text":""},{"location":"polaris/getting-started/#logging-into-polaris","title":"Logging Into Polaris","text":"<p>To log into Polaris: <pre><code>ssh &lt;username&gt;@polaris.alcf.anl.gov\n</code></pre> Then, type in the password from your CRYPTOCard/MobilePASS+ token.</p>"},{"location":"polaris/getting-started/#hardware-overview","title":"Hardware Overview","text":"<p>An overview of the Polaris system, including details on the compute node architecture, is available on the Machine Overview page.</p>"},{"location":"polaris/getting-started/#compiling-applications","title":"Compiling Applications","text":"<p>Users are encouraged to read through the Compiling and Linking Overview page and corresponding pages depending on the target compiler and programming model.</p>"},{"location":"polaris/getting-started/#accessing-additional-software","title":"Accessing Additional Software","text":"<p>In addition to the Cray PE, ALCF installs software in <code>/soft</code>, which can be accessed via module commands by altering your <code>$MODULEPATH</code>: <pre><code>module use /soft/modulefiles\n</code></pre> The available software can then be queried with <code>module avail</code>.</p> <p>Additionally, a suite of software packages is provided via Spack deployments, detailed on the Spack PE page.</p>"},{"location":"polaris/getting-started/#submitting-and-running-jobs","title":"Submitting and Running Jobs","text":"<p>Users are encouraged to read through the Running Jobs with PBS at the ALCF page for information on using the PBS scheduler and preparing job submission scripts. Some example job submission scripts are available on the Example Job Scripts page as well.</p>"},{"location":"polaris/getting-started/#lustre-file-striping","title":"Lustre File Striping","text":"<p>In addition to the content above, here is a document on Lustre File Striping Basics:</p> <ul> <li>Lustre File Striping Basics</li> </ul>"},{"location":"polaris/getting-started/#proxy","title":"Proxy","text":"<p>If the node you are on doesn\u2019t have outbound network connectivity, add the following to your <code>~/.bash_profile</code> file to access the proxy host:</p> <pre><code># proxy settings\nexport HTTP_PROXY=\"http://proxy.alcf.anl.gov:3128\"\nexport HTTPS_PROXY=\"http://proxy.alcf.anl.gov:3128\"\nexport http_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport https_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport ftp_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport no_proxy=\"admin,polaris-adminvm-01,localhost,*.cm.polaris.alcf.anl.gov,polaris-*,*.polaris.alcf.anl.gov,*.alcf.anl.gov\"\n</code></pre>"},{"location":"polaris/getting-started/#getting-assistance","title":"Getting Assistance","text":"<p>Please direct all questions, requests, and feedback to support@alcf.anl.gov.</p>"},{"location":"polaris/known-issues/","title":"Known Issues","text":"<p>This is a collection of known issues that have been encountered on Polaris. Documentation will be updated as issues are resolved. Users are encouraged to email support@alcf.anl.gov to report issues.</p>"},{"location":"polaris/known-issues/#submitting-jobs","title":"Submitting Jobs","text":"<ol> <li> <p>For batch job submissions, if the parameters within your submission script do not meet the parameters of any of the execution queues (<code>small</code>, ..., <code>backfill-large</code>), you might not receive the \"Job submission\" error on the command line at all, and the job will never appear in the history <code>qstat -xu &lt;username&gt;</code> (current bug in PBS). For example, if a user submits a script to the <code>prod</code> routing queue requesting 10 nodes for 24 hours, exceeding the \"Time Max\" of 6 hours of the <code>small</code> execution queue (which handles jobs with 10-24 nodes), then it may behave as if the job was never submitted.</p> </li> <li> <p>Job scripts are copied to temporary locations after <code>qsub</code>, and any changes to the original script while the job is queued will not be reflected in the copied script. Furthermore, <code>qalter</code> requires <code>-A &lt;allocation name&gt;</code> when changing job properties. Currently, there is a request for a <code>qalter</code>-like command to trigger a re-copy of the original script to the temporary location.</p> </li> </ol>"},{"location":"polaris/known-issues/#compiling-running-applications","title":"Compiling &amp; Running Applications","text":"<ol> <li>If your job fails to start with an <code>RPC launch</code> message like the one below, please forward the complete messages to support@alcf.anl.gov.    <pre><code>launch failed on x3104c0s1b0n0: Couldn't forward RPC launch(ab751d77-e80a-4c54-b1c2-4e881f7e8c90) to child x3104c0s31b0n0.hsn.cm.polaris.alcf.anl.gov: Resource temporarily unavailable\n</code></pre></li> <li>The message below is an XALT-related warning that can be ignored when running <code>apptainer</code>. For other commands, please forward the complete message to support@alcf.anl.gov so we are aware of your use case.    <pre><code>ERROR: ld.so: object '/soft/xalt/3.0.2-202408282050/lib64/libxalt_init.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n</code></pre></li> </ol>"},{"location":"polaris/known-issues/#sshing-between-polaris-compute-nodes","title":"<code>ssh</code>'ing between Polaris Compute Nodes","text":"<ol> <li> <p>You should be able to <code>ssh</code> freely (without needing a password) between your assigned compute nodes on Polaris. If you are running into <code>ssh</code> issues, check for the following causes:</p> </li> <li> <p>Your <code>/home/&lt;username&gt;</code> directory permissions should be set to <code>700</code> (<code>chmod 700 /home/&lt;username&gt;</code>).</p> </li> <li>Confirm the following files exist in your <code>.ssh</code> directory and the permissions are set to the following:<ol> <li><code>-rw-------  (600)  authorized_keys</code></li> <li><code>-rw-r--r--  (644)  config</code></li> <li><code>-rw-------  (600)  id_rsa</code></li> <li><code>-rw-r--r--  (644)  id_rsa.pub</code></li> </ol> </li> <li>If you do not have the files mentioned above, you will need to create them.<ol> <li>You can generate an <code>id_rsa</code> file with the following command: <code>ssh-keygen -t rsa</code></li> </ol> </li> <li>Copy the contents of your <code>.ssh/id_rsa.pub</code> file to <code>.ssh/authorized_keys</code>.</li> </ol>"},{"location":"polaris/running-jobs/","title":"Running Jobs on Polaris","text":""},{"location":"polaris/running-jobs/#queues","title":"Queues","text":"<p>There are five production queues you can target in your qsub (<code>-q &lt;queue name&gt;</code>):</p> Queue Name Node Min Node Max Time Min Time Max Notes debug 1 2 5 min 1 hr max 24 nodes in use by this queue at any given time; Only 8 nodes are exclusive (see Note below) debug-scaling 1 10 5 min 1 hr max 1 job running/accruing/queued per-user prod 10 496 5 min 24 hrs Routing queue; See below preemptable 1 10 5 min 72 hrs Please be aware that jobs in the preemptable queue can be killed at any time if jobs are submitted to the demand queue. Max 20 jobs running/accruing/queued per-project; see Note below demand 1 56 5 min 1 hr By request only; max 100 jobs running/accruing/queued per-project <p>Note: Please be aware that jobs in the preemptable queue can be killed at any time if jobs are submitted to the demand queue. Jobs in the demand queue take priority over jobs in the preemptable queue. This means jobs in the preemptable queue may be preempted (killed without any warning) if there are jobs in the demand queue. Unfortunately, there's always an inherent risk of jobs being killed when using the preemptable queue.  Please use the following command to view details of a queue: <code>qstat -Qf &lt;queuename&gt;</code></p> <p>To make your job rerunnable, add the following PBS directive: <code>#PBS -r y</code>. This will ensure your job will restart once the demand job is complete. </p> <p>Note: The debug queue has 8 exclusively dedicated nodes. If there are free nodes in production, then debug jobs can take another 16 nodes for a total of 24.</p> <p><code>prod</code> is a routing queue and routes your job to one of the following six execution queues:</p> Queue Name Node Min Node Max Time Min Time Max Notes small 10 24 5 min 3 hrs medium 25 99 5 min 6 hrs large 100 496 5 min 24 hrs backfill-small 10 24 5 min 3 hrs low priority, negative project balance backfill-medium 25 99 5 min 6 hrs low priority, negative project balance backfill-large 100 496 5 min 24 hrs low priority, negative project balance <ul> <li>Note 1: You cannot submit to these queues directly; you can only submit to the routing queue \"<code>prod</code>\".</li> <li>Note 2: All of these queues have a limit of ten (10) jobs running/accruing per-project.</li> <li>Note 3: All of these queues have a limit of one hundred (100) jobs queued (not accruing score) per-project.</li> <li>Note 4: As of January 2023, it is recommended to submit jobs with a maximum node count of 476-486 nodes given current rates of downed nodes (larger jobs may sit in the queue indefinitely).</li> </ul>"},{"location":"polaris/running-jobs/#running-mpiopenmp-applications","title":"Running MPI+OpenMP Applications","text":"<p>Once a submitted job is running, calculations can be launched on the compute nodes using <code>mpiexec</code> to start an MPI application. Documentation is accessible via <code>man mpiexec</code>, and some helpful options follow.</p> <ul> <li><code>-n</code> total number of MPI ranks</li> <li><code>-ppn</code> number of MPI ranks per node</li> <li><code>--cpu-bind</code> CPU binding for application</li> <li><code>--depth</code> number of CPUs per rank (useful with <code>--cpu-bind</code>)</li> <li><code>--env</code> set environment variables (<code>--env OMP_NUM_THREADS=2</code>)</li> <li><code>--hostfile</code> indicate file with hostnames (the default is <code>--hostfile $PBS_NODEFILE</code>)</li> </ul> <p>A sample submission script with directives is below for a 4-node job with 8 MPI ranks on each node and 8 OpenMP threads per rank. Each hardware thread runs a single OpenMP thread since there are 64 hardware threads on the CPU (2 per core). You can download and compile <code>hello_affinity</code> from this link.</p> <pre><code>#!/bin/bash -l\n#PBS -N AFFINITY\n#PBS -l select=4:ncpus=256\n#PBS -l walltime=0:10:00\n#PBS -q debug-scaling\n#PBS -A Catalyst  # Replace with your project\n\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS=8 # Number of MPI ranks to spawn per node\nNDEPTH=8 # Number of hardware threads per rank (i.e. spacing between MPI ranks)\nNTHREADS=8 # Number of software threads per rank to launch (i.e. OMP_NUM_THREADS)\n\nNTOTRANKS=$(( NNODES * NRANKS ))\n\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS} THREADS_PER_RANK= ${NTHREADS}\"\n\n# Change the directory to work directory, which is the directory you submit the job.\ncd $PBS_O_WORKDIR\nmpiexec --np ${NTOTRANKS} -ppn ${NRANKS} -d ${NDEPTH} --cpu-bind depth -env OMP_NUM_THREADS=${NTHREADS} ./hello_affinity\n</code></pre>"},{"location":"polaris/running-jobs/#running-gpu-enabled-applications","title":"Running GPU-enabled Applications","text":"<p>GPU-enabled applications will similarly run on the compute nodes using the above example script. - The environment variable <code>MPICH_GPU_SUPPORT_ENABLED=1</code> needs to be set if your application requires MPI-GPU support whereby the MPI library sends and receives data directly from GPU buffers. In this case, it will be important to have the <code>craype-accel-nvidia80</code> module loaded both when compiling your application and during runtime to correctly link against a GPU Transport Layer (GTL) MPI library. Otherwise, you'll likely see <code>GPU_SUPPORT_ENABLED is requested, but GTL library is not linked</code> errors during runtime. - If running on a specific GPU or subset of GPUs is desired, then the <code>CUDA_VISIBLE_DEVICES</code> environment variable can be used. For example, if one only wanted an application to access the first two GPUs on a node, then setting <code>CUDA_VISIBLE_DEVICES=0,1</code> could be used.</p>"},{"location":"polaris/running-jobs/#binding-mpi-ranks-to-gpus","title":"Binding MPI ranks to GPUs","text":"<p>The Cray MPI on Polaris does not currently support binding MPI ranks to GPUs. For applications that need this support, this instead can be handled by use of a small helper script that will appropriately set <code>CUDA_VISIBLE_DEVICES</code> for each MPI rank. One example is available here where each MPI rank is similarly bound to a single GPU with round-robin assignment.</p> <p>An example <code>set_affinity_gpu_polaris.sh</code> script follows where GPUs are assigned round-robin to MPI ranks.</p> <p><pre><code>#!/bin/bash -l\nnum_gpus=4\n# need to assign GPUs in reverse order due to topology\n# See Polaris Device Affinity Information:\n# https://www.alcf.anl.gov/support/user-guides/polaris/hardware-overview/machine-overview/index.html\ngpu=$((${num_gpus} - 1 - ${PMI_LOCAL_RANK} % ${num_gpus}))\nexport CUDA_VISIBLE_DEVICES=$gpu\necho \"RANK= ${PMI_RANK} LOCAL_RANK= ${PMI_LOCAL_RANK} gpu= ${gpu}\"\nexec \"$@\"\n</code></pre> This script can be placed just before the executable in the <code>mpiexec</code> command like so. <pre><code>mpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth ./set_affinity_gpu_polaris.sh ./hello_affinity\n</code></pre> Users with different needs, such as assigning multiple GPUs per MPI rank, can modify the above script to suit their needs.</p>"},{"location":"polaris/running-jobs/#interactive-jobs-on-compute-nodes","title":"Interactive Jobs on Compute Nodes","text":"<p>Here is how to submit an interactive job to, for example, edit/build/test an application on Polaris compute nodes: <pre><code>qsub -I -l select=1 -l filesystems=home:eagle -l walltime=1:00:00 -q debug -A &lt;project_name&gt;\n</code></pre></p> <p>This command requests 1 node for a period of 1 hour in the debug queue, requiring access to the /home and eagle filesystems. After waiting in the queue for a node to become available, a shell prompt on a compute node will appear. You may then start building applications and testing GPU affinity scripts on the compute node.</p> <p>NOTE: If you want to <code>ssh</code> or <code>scp</code> to one of your assigned compute nodes, you will need to make sure your <code>$HOME</code> directory and your <code>$HOME/.ssh</code> directory permissions are both set to <code>700</code>.</p>"},{"location":"polaris/running-jobs/#running-multiple-mpi-applications-on-a-node","title":"Running Multiple MPI Applications on a Node","text":"<p>Multiple applications can be run simultaneously on a node by launching several <code>mpiexec</code> commands and backgrounding them. For performance, it will likely be necessary to ensure that each application runs on a distinct set of CPU resources and/or targets specific GPUs. One can provide a list of CPUs using the <code>--cpu-bind</code> option, which when combined with <code>CUDA_VISIBLE_DEVICES</code> provides a user with specifying exactly which CPU and GPU resources to run each application on. In the example below, four instances of the application are simultaneously running on a single node. In the first instance, the application is spawning MPI ranks 0-7 on CPUs 24-31 and using GPU 0. This mapping is based on output from the <code>nvidia-smi topo -m</code> command and pairs CPUs with the closest GPU.</p> <pre><code>export CUDA_VISIBLE_DEVICES=0\nmpiexec -n 8 --ppn 8 --cpu-bind list:24:25:26:27:28:29:30:31 ./hello_affinity &amp;\n\nexport CUDA_VISIBLE_DEVICES=1\nmpiexec -n 8 --ppn 8 --cpu-bind list:16:17:18:19:20:21:22:23 ./hello_affinity &amp;\n\nexport CUDA_VISIBLE_DEVICES=2\nmpiexec -n 8 --ppn 8 --cpu-bind list:8:9:10:11:12:13:14:15 ./hello_affinity &amp;\n\nexport CUDA_VISIBLE_DEVICES=3\nmpiexec -n 8 --ppn 8 --cpu-bind list:0:1:2:3:4:5:6:7 ./hello_affinity &amp;\n\nwait\n</code></pre>"},{"location":"polaris/running-jobs/#compute-node-access-to-the-internet","title":"Compute Node Access to the Internet","text":"<p>Currently, the only access to the internet is via a proxy. Here are the proxy environment variables for Polaris:</p> <pre><code>export http_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport https_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport ftp_proxy=\"http://proxy.alcf.anl.gov:3128\"\n</code></pre> <p>In the future, though we don't have a timeline on this because it depends on future features in Slingshot and internal software development, we intend to have public IP addresses be a schedulable resource. For instance, if only your head node needed public access, your select statement might look something like: <code>-l select=1:pubnet=True+63</code>.</p>"},{"location":"polaris/running-jobs/#controlling-where-your-job-runs","title":"Controlling Where Your Job Runs","text":"<p>If you wish to have your job run on specific nodes, form your select like this: <code>-l select=1:vnode=&lt;node name1&gt;+1:vnode=&lt;node name2&gt;...</code>. Obviously, that gets tedious for large jobs.</p> <p>If you want to control the location of a few nodes, for example, 2 out of 64, but the rest don't matter, you can do something like this: <code>-l select=1:vnode=&lt;node name1&gt;+1:vnode=&lt;node name2&gt;+62:system=foo</code>.</p> <p>Every node has a PBS resource called <code>tier0</code> with a rack identifier and <code>tier1</code> with a dragonfly group identifier. If you want all your nodes grouped in a rack, you can add the group specifier <code>-l select=8:system=foo,place=scatter:group=tier0</code>. If you wanted everything in the same dragonfly group, replace <code>tier0</code> with <code>tier1</code>. Note that you have to also explicitly specify the place when you use group. If you wanted a specific rack or dragonfly group instead of any of them, you are back to the select: <code>-l select 10:tier0=x3001-g0</code>.</p>"},{"location":"polaris/running-jobs/#network-rack-and-dragonfly-group-mappings","title":"Network: Rack and Dragonfly Group Mappings","text":"<ul> <li>Racks contain (7) 6U chassis; each chassis has 2 nodes for 14 nodes per rack</li> <li>The hostnames are of the form xRRPPc0sUUb[0|1]n0 where:<ul> <li>RR is the row {30, 31, 32}</li> <li>PP is the position in the row {30 goes 1-16, 31 and 32 go 1-12}</li> <li>c is chassis and is always 0</li> <li>s stands for slot, but in this case is the RU in the rack and values are {1,7,13,19,25,31,37}</li> <li>b is BMC controller and is 0 or 1 (each node has its own BMC)</li> <li>n is node, but is always 0 since there is only one node per BMC</li> </ul> </li> <li>So, 16+12+12 = 40 racks * 14 nodes per rack = 560 nodes.</li> <li>Note that in production group 9 (the last 4 racks) will be the designated on-demand racks</li> <li>The management racks are x3000 and X3100 and are dragonfly group 10</li> <li>The TDS rack is x3200 and is dragonfly group 11</li> <li>Each compute node will have a PBS resource named <code>tier0</code> which will be equal to the values in the table below. This allows you to group your jobs within a rack if you wish. There is also a resource called <code>tier1</code> which will be equal to the column headings. This allows you to group your jobs within a dragonfly group if you wish.</li> </ul> g0 g1 g2 g3 g4 g5 g6 g7 g8 g9 x3001-g0 x3005-g1 x3009-g2 x3013-g3 x3101-g4 x3105-g5 x3109-g6 x3201-g7 x3205-g8 x3209-g9 x3002-g0 x3006-g1 x3010-g2 x3014-g3 x3102-g4 x3106-g5 x3110-g6 x3202-g7 x3206-g8 x3210-g9 x3003-g0 x3007-g1 x3011-g2 x3015-g3 x3103-g4 x3107-g5 x3111-g6 x3203-g7 x3207-g8 x3211-g9 x3004-g0 x3008-g1 x3012-g2 x3016-g3 x3104-g4 x3108-g5 x3112-g6 x3204-g7 x3208-g8 x3212-g9"},{"location":"polaris/system-updates/","title":"Polaris System Updates","text":""},{"location":"polaris/system-updates/#2024-09-09","title":"2024-09-09","text":""},{"location":"polaris/system-updates/#xalt","title":"XALT","text":"<p>The XALT library tracking software has been enabled for all Polaris users. More information can be found on the XALT page.</p>"},{"location":"polaris/system-updates/#2024-04-22","title":"2024-04-22","text":"<p>The management software on Polaris has been upgraded to HPCM 1.10. The following version changes are in place with the upgrade to HPCM 1.10:</p> <ul> <li>HPE Cray Programming Environment (CPE) 23.12</li> <li>SlingShot version 2.1.2</li> <li>NVIDIA SDK 23.9</li> <li>NVIDIA driver version 535.154.05</li> <li>CUDA 12.2</li> <li>SUSE 15 SP5</li> </ul>"},{"location":"polaris/system-updates/#releasing-jobs","title":"Releasing Jobs","text":"<p>Jobs that were queued before the upgrade have been restored to the appropriate queues but are placed on user hold. Jobs are not expected to complete successfully due to the changes made to the system and software environments resulting from the upgrade. We recommend you review your jobs and either release the hold (<code>qrls &lt;jobid&gt;</code>) or delete it (<code>qdel &lt;jobid&gt;</code>) and resubmit as appropriate.</p> <ul> <li>Users need to rebuild for the new PE environment and major OS upgrade. Existing binaries are unlikely to run successfully.</li> <li>We have held all jobs submitted prior to the upgrade as a user hold. Users may release their existing jobs with <code>qrls</code> to run after they have rebuilt their binaries.</li> <li>PBS does cache the job execution script. If a change to the script is required due to a path changing post-rebuild, the job will have to be resubmitted.</li> <li>All application binaries should be rebuilt prior to further job submissions.</li> </ul>"},{"location":"polaris/system-updates/#re-building-user-codes","title":"Re-building User Codes","text":"<p>Many user codes will need to be re-built and/or re-linked against the newer version of the programming environment (23.12) and Spack-provided dependencies.</p>"},{"location":"polaris/system-updates/#changes-to-the-user-software-environment","title":"Changes to the User Software Environment","text":"<p>In addition to the system upgrades, several changes have been made to the user software environment which may impact user workflows.</p>"},{"location":"polaris/system-updates/#older-pe-versions-removed","title":"Older PE Versions Removed","text":"<p>Older versions of the Cray PE (older than 23.12) are deprecated as they are incompatible with the upgraded system stack and are no longer available for use.</p>"},{"location":"polaris/system-updates/#datascience-anaconda-module-updates","title":"Datascience Anaconda Module Updates","text":"<p>We have updated the datascience Anaconda module and built various packages and libraries with CUDA 12.4.1 to be compatible with the new Polaris NVIDIA GPU hardware driver (CUDA 12.2) and to use the latest MPI, NCCL, cuDNN, TensorRT, etc. libraries. PyTorch 2.3.0 and TensorFlow 2.16.1 are now available as part of this module.</p> <p>To use the new environment, type: <pre><code>module use /soft/modulefiles \nmodule load conda; conda activate\n</code></pre></p>"},{"location":"polaris/system-updates/#soft-refresh-and-default-modulepath-change","title":"<code>/soft</code> Refresh and Default <code>$MODULEPATH</code> Change","text":"<p>Due to the new system software stack, <code>/soft</code> has been purged to allow for software to be rebuilt. In addition, <code>/soft/modulefiles</code> is no longer in the default <code>$MODULEPATH</code>. To access modules installed in <code>/soft</code>, users should run <code>module use /soft/modulefiles</code>.</p> <p>Adding <code>module use /soft/modulefiles</code> to your profile should approximate the old behavior.</p>"},{"location":"polaris/system-updates/#modules-removed","title":"Modules Removed","text":"<p>The following modules have been removed:</p> <pre><code>   aocl/3.2.0                                                        hpctoolkit/2022.07.27\n   aocl/4.0                                                   (D)    hpctoolkit/2023.03.27                                                    (D)\n   ascent/develop/2024-01-08-492f9b0                                 imagemagick/imagemagick-7.1.1-11\n   boost/1.80.0                                                      kokkos/kokkos-3.6.01\n   boost/1.81.0                                               (D)    kokkos/3.7.00-cuda\n   cabana/cabana-20220723                                            kokkos/3.7.00-sycl\n   cabana/PrgEnv-gnu/8.3.3/gnu/11.2.0/cuda_cudatoolkit_11.8.0 (D)    kokkos/3.7.01-cuda\n   cmake/3.23.2                                                      kokkos/4.2.00/shared/PrgEnv-gnu/8.3.3/gnu/11.2.0/cuda_cudatoolkit_11.8.0 (D)\n   conda/2022-07-19                                                  llvm/release-15.0.0\n   conda/2022-09-08-hvd-nccl                                         llvm/release-16.0.0\n   conda/2022-09-08                                                  magma/2.6.2\n   conda/2023-01-10-unstable                                         magma/2.7.0                                                              (D)\n   conda/2023-10-04-openmpi                                          mpiwrappers/cray-mpich-oneapi                                            (D)\n   conda/2023-10-04                                           (D)    oneapi/release/2023.2.1\n   cudatoolkit-standalone/11.2.2                                     oneapi/release/2024.0\n   cudatoolkit-standalone/11.4.4                                     oneapi/upstream\n   cudatoolkit-standalone/11.6.2                                     paraview/paraview-5.11.1-mesa\n   cudatoolkit-standalone/11.7.1                                     paraview/paraview-5.11.2-EGL-test\n   cudatoolkit-standalone/12.0.0                                     paraview/paraview-5.11.2-mesa\n   e4s/22.05/mvapich2                                                paraview/paraview-5.12.0-RC1-mesa\n   e4s/22.05/PrgEnv-gnu                                       (D)    paraview/paraview-5.12.0-mesa                                            (D)\n   e4s/22.08/mvapich2                                                singularity/3.8.7\n   e4s/22.08/PrgEnv-gnu                                       (D)    tau/2.31.1\n   ffmpeg/ffmpeg-6.0                                                 tau/2.32\n   forge/23.0.4                                                      tau/2.33.1                                                               (D)\n   ginkgo/mpi/20230314/ginkgo                                        visit/visit\n   ginkgo/20230314/ginkgo                                     (D)    vmd/vmd-1.9.4a55\n   gnu-parallel/2021-09-22                                           xalt/3.0.1-202308151751\n   gsl/2.7                                                           xalt/3.0.1-202308261842                                                  (D)\n</code></pre>"},{"location":"polaris/system-updates/#modules-newly-installed","title":"Modules Newly Installed","text":"<p>The following modules have been newly installed:</p> <pre><code>   cabana/dev-9a1ad605/kokv/4.2.01/PrgEnv-gnu/8.5.0/gnu/12.3/cuda_cudatoolkit_12.2.91\n   cuda-PrgEnv-nvidia/12.2.91\n   cudatoolkit-standalone/12.2.2                                                      (D)\n   cudatoolkit-standalone/12.3.2\n   cudatoolkit-standalone/12.4.0\n   cudnn/9.0.0\n   forge/23.1.2\n   kokkos/4.2.01/shared/PrgEnv-gnu/8.5.0/gnu/12.3/cuda_cudatoolkit_12.2.91\n   spack-pe-base/0.6.1\n   spack-pe-gnu/0.6.1\n</code></pre> <p>Note that <code>spack-pe-base</code> and <code>spack-pe-gnu</code> are metamodules which contain further software offerings. See the Spack section below for details.</p>"},{"location":"polaris/system-updates/#spack","title":"Spack","text":"<p>We have newly installed Spack deployments in <code>/soft</code>. Spack is an HPC-oriented package manager which ALCF uses to install software for the user environment. However, no knowledge of Spack is necessary to use these software offerings. All ALCF-managed software is accessible to users via modules.</p> <p>The base suite of software tools and libraries can be accessed by loading the <code>spack-pe-base</code> module. This adds a path to <code>$MODULEPATH</code> which contains numerous modules.</p> <p>For example, to load <code>cmake</code> starting from the default environment, a user should run the following commands: <pre><code>module use /soft/modulefiles\nmodule load spack-pe-base\nmodule load cmake\n</code></pre> Other modules in <code>spack-pe-base</code> can be browsed by running <code>module avail</code> or <code>module --show-hidden avail</code>. The latter shows hidden modules which are installed as dependencies of the un-hidden modules.</p> <p>In addition to the base stack, a suite of higher-level libraries are installed in the <code>spack-pe-gnu</code> module. These are built with and are dependent on <code>PrgEnv-gnu</code>. A <code>PrgEnv-nvidia</code>-compatible stack will be available in the future.</p> <p>Note that not all software is installed through Spack; many applications and libraries are installed as standalone packages in <code>/soft</code>. Users are encouraged to browse the available modules with <code>module avail</code> to see what software is installed on the system.</p>"},{"location":"polaris/system-updates/#paraview-and-visit","title":"ParaView and Visit","text":"<p>The ParaView module has been updated. For more information, see ParaView Documentation and ParaView Manual Launch.</p> <p>The Visit module is in the process of being updated.</p>"},{"location":"polaris/system-updates/#changes-to-memory-limits-on-login-nodes","title":"Changes to Memory Limits on Login Nodes","text":"<p>Memory limits were lowered on the login nodes due to resource contention to 8GB of memory and 8 cores per user. This might result in error messages indicating abnormal process termination for user processes run on logins.</p> <p>Examples of the error messages people might see are:</p> <ul> <li><code>nvcc error   : 'cudafe++' died due to signal 9 (Kill signal)</code></li> <li><code>g++-12: fatal error: Killed signal terminated program cc1plus</code></li> </ul> <p>These errors are likely due to exhausting the per-user resources on a login node as each user is allocated 8 cores and 8GB memory. To avoid this, you can either:</p> <ul> <li>Reduce the parallelism of your compile, such as using <code>-j</code> or <code>-j4</code> flags</li> <li>Request a debug node and run your compile there where you will have the full resources of the node at your disposal</li> </ul>"},{"location":"polaris/applications-and-libraries/applications/QMCPACK/","title":"QMCPACK","text":""},{"location":"polaris/applications-and-libraries/applications/QMCPACK/#qmcpack-on-polaris","title":"QMCPACK on Polaris","text":"<p>QMCPACK is a modern high-performance open-source Quantum Monte Carlo (QMC) simulation code. Its main applications are electronic structure calculations of molecular, quasi-2D, and solid-state systems. Variational Monte Carlo (VMC), diffusion Monte Carlo (DMC), and a number of other advanced QMC algorithms are implemented. Orbital space auxiliary field QMC (AFQMC) has recently been added. By directly solving the Schr\u00f6dinger equation, QMC methods offer greater accuracy than methods such as density functional theory, but at the trade-off of much greater computational expense.</p> <p>Prebuilt executables are provided at <code>/soft/applications/qmcpack</code>. The directory of each installation also includes a job submission script example <code>qmcpack-polaris.job</code>. An updated build recipe is provided on GitHub.</p>"},{"location":"polaris/applications-and-libraries/applications/QuantumESPRESSO/","title":"Quantum ESPRESSO","text":""},{"location":"polaris/applications-and-libraries/applications/QuantumESPRESSO/#quantum-espresso-on-polaris","title":"Quantum ESPRESSO on Polaris","text":"<p>Quantum ESPRESSO is an integrated suite of open-source computer codes for electronic-structure calculations and materials modeling at the nanoscale. It is based on density-functional theory, plane waves, and pseudopotentials.</p> <p>Prebuilt executables are provided at <code>/soft/applications/quantum_espresso</code>. The directory of each installation also includes a job submission script example <code>job.sub</code> and a <code>README</code> file documenting the actual build recipe. We only support building QE using CMake.</p>"},{"location":"polaris/applications-and-libraries/applications/amber/","title":"Amber on Polaris","text":""},{"location":"polaris/applications-and-libraries/applications/amber/#what-is-amber","title":"What is Amber?","text":"<p>Amber is a suite of biomolecular simulation programs. Amber is distributed in two parts: AmberTools24 and Amber24. Please visit the Amber website for additional information on capabilities and licensing.</p>"},{"location":"polaris/applications-and-libraries/applications/amber/#using-amber-at-alcf","title":"Using Amber at ALCF","text":"<p>ALCF offers assistance with building binaries and compiling instructions for Amber. For questions, contact us at support@alcf.anl.gov.</p>"},{"location":"polaris/applications-and-libraries/applications/amber/#building-amber","title":"Building Amber","text":"<p>The following build instructions can be applied to both free and licensed versions of Amber.</p> <ol> <li>Download AmberTools24 and Amber24 from the Amber website. Note that they are two separate downloads. Copy the tarballs <code>AmberTools24.tar.bz2</code> and <code>Amber24.tar.bz2</code> into a home or project directory (e.g., <code>$HOME/Amber</code>).</li> </ol> <pre><code>$ cd Amber\n$ tar -xf AmberTools24.tar.bz2\n$ tar -xf Amber24.tar.bz2\n$ cd amber24_src\n</code></pre> <ol> <li> <p>Download and install the bzip2 library. Insert <code>-fPIC</code> into the <code>CFLAGS</code> variable in the Makefile and build with installation to a local directory (e.g., <code>make install PREFIX=$HOME/bzip2</code>).</p> </li> <li> <p>Download and install the FFTW3 library, extract the tarball, and rename it as <code>fftw</code>. Proceed to build with installation to a local directory (e.g., <code>./configure --prefix=$HOME/fftw ; make ; make install</code>).</p> </li> <li> <p>Update the user environment to include the newly installed <code>bzip2</code> and <code>fftw</code> and use the GNU programming environment.</p> </li> </ol> <pre><code>export PATH=\"/PATH-TO/bzip2/lib:$PATH\"\nexport PATH=\"/PATH-TO/bzip2:$PATH\"\nexport PATH=\"/PATH-TO/bzip2/include:$PATH\"\nexport PATH=\"/PATH-TO/fftw:$PATH\"\nexport PATH=\"/PATH-TO/fftw/lib:$PATH\"\nexport PATH=\"/PATH-TO/fftw/include:$PATH\"\n\nmodule use /soft/modulefiles\nmodule load PrgEnv-gnu/8.5.0\nmodule load cudatoolkit-standalone/12.4.0\n</code></pre> <ol> <li>Proceed with building Amber binaries by first modifying <code>run_cmake</code> and setting the following:</li> <li><code>-DMPI=TRUE</code></li> <li><code>-DCUDA=TRUE</code></li> <li><code>-DCOMPILER=MANUAL</code></li> <li><code>-DDISABLE_TOOLS=FEW</code></li> <li><code>-DCMAKE_C_COMPILER=gcc-12</code></li> <li><code>-DCMAKE_CXX_COMPILER=g++-12</code></li> <li><code>-DCMAKE_Fortran_COMPILER=gfortran-12</code></li> </ol> <pre><code>cd build\n./run_cmake\nmake install -j 8\n</code></pre> <p>All Amber binaries will be installed to the <code>amber24</code> folder.</p>"},{"location":"polaris/applications-and-libraries/applications/gromacs/","title":"Gromacs on Polaris","text":""},{"location":"polaris/applications-and-libraries/applications/gromacs/#what-is-gromacs","title":"What is Gromacs?","text":"<p>GROMACS is a versatile package to perform molecular dynamics, i.e., simulate the Newtonian equations of motion for systems with hundreds to millions of particles. It is primarily designed for biochemical molecules like proteins, lipids, and nucleic acids that have a lot of complicated bonded interactions. However, since GROMACS is extremely fast at calculating the nonbonded interactions (which usually dominate simulations), many groups are also using it for research on non-biological systems, e.g., polymers.</p>"},{"location":"polaris/applications-and-libraries/applications/gromacs/#using-gromacs-at-alcf","title":"Using GROMACS at ALCF","text":"<p>ALCF offers assistance with building binaries and compiling instructions for GROMACS. For questions, contact us at support@alcf.anl.gov.</p>"},{"location":"polaris/applications-and-libraries/applications/gromacs/#building-gromacs","title":"Building Gromacs","text":"<ol> <li>Download the latest source code: GROMACS Download</li> <li>Extract the tarball: <code>tar -xzf gromacs-2022.1.tar.gz</code></li> <li>Swap programming environments: <code>module swap PrgEnv-nvhpc PrgEnv-gnu</code></li> <li>Load the CUDA toolkit: <code>module load cudatoolkit-standalone/11.2.2</code></li> <li>Load GCC: <code>module load gcc/10.3.0</code></li> <li>Load CMake: <code>module load cmake</code></li> <li>Change directory: <code>cd gromacs-2022.1</code></li> <li>Create a build directory: <code>mkdir build</code></li> <li>Configure the build:    <pre><code>cmake -DCMAKE_C_COMPILER=cc -DCMAKE_CXX_COMPILER=CC \\\n      -DBUILD_SHARED_LIBS=OFF -DGMX_BUILD_OWN_FFTW=ON \\\n      -DCMAKE_INSTALL_PREFIX=/path-to/gromacs-2022.1/build \\\n      -DGMX_MPI=ON -DGMX_OPENMP=ON -DGMX_GPU=CUDA \\\n      -DCUDA_TOOLKIT_ROOT_DIR=/soft/compilers/cudatoolkit/cuda-11.2.2\n</code></pre></li> <li>Compile the code: <code>make -j 8</code></li> <li>Install the binaries: <code>make install</code></li> <li>The installed binary is <code>build/bin/gmx_mpi</code>.</li> </ol>"},{"location":"polaris/applications-and-libraries/applications/gromacs/#running-gromacs-on-polaris","title":"Running Gromacs on Polaris","text":"<p>Prebuilt GROMACS binaries can be found in the directory <code>/soft/applications/Gromacs/gromacs-2022.1</code>.</p> <p>A sample PBS script follows that will run GROMACS on two nodes, using 4 MPI ranks per node, and each rank with four OpenMP threads. The PME kernel owns one MPI rank and one GPU per node, while the nonbonded kernel uses 3 MPI ranks and 3 GPUs per node.</p> <pre><code>#!/bin/sh\n#PBS -l select=2:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -q debug\n#PBS -A PROJECT\n#PBS -l filesystems=home:eagle\n\ncd ${PBS_O_WORKDIR}\n\nmodule swap PrgEnv-nvhpc PrgEnv-gnu\nmodule load cudatoolkit-standalone/11.2.2\n\nexport OMP_NUM_THREADS=4\n\nmpirun --np 8 /soft/applications/Gromacs/gromacs-2022.1/gmx_mpi \\\n      mdrun -gputasks 0123 -nb gpu -pme gpu -npme 1 -ntomp 4 \\\n      -dlb yes -resethway -pin on -v deffnm step5_1 -g test.log\n</code></pre> <p>We strongly suggest that users try combinations of different numbers of nodes, MPI ranks per node, number of GPU tasks/devices, GPU task decomposition between nonbonded and PME kernels, and OMP threads per rank to find the optimal throughput for their particular workload.</p>"},{"location":"polaris/applications-and-libraries/applications/lammps/","title":"LAMMPS","text":""},{"location":"polaris/applications-and-libraries/applications/lammps/#overview","title":"Overview","text":"<p>LAMMPS is a general-purpose molecular dynamics software package for massively parallel computers. It is written in an exceptionally clean style that makes it one of the more popular codes for users to extend, and it currently has dozens of user-developed extensions.</p> <p>For details about the code and its usage, see the LAMMPS home page. This page provides information specific to running on Polaris at the ALCF.</p>"},{"location":"polaris/applications-and-libraries/applications/lammps/#using-lammps-at-alcf","title":"Using LAMMPS at ALCF","text":"<p>ALCF provides assistance with build instructions, compiling executables, submitting jobs, and providing prebuilt binaries (upon request). A collection of Makefiles and submission scripts are available in the ALCF GettingStarted repo here. For questions, contact us at support@alcf.anl.gov.</p>"},{"location":"polaris/applications-and-libraries/applications/lammps/#how-to-obtain-the-code","title":"How to Obtain the Code","text":"<p>LAMMPS is an open-source code, which can be downloaded from the LAMMPS website.</p>"},{"location":"polaris/applications-and-libraries/applications/lammps/#building-on-polaris","title":"Building on Polaris","text":"<p>After LAMMPS has been downloaded and unpacked on an ALCF filesystem, users should see a directory whose name is of the form <code>lammps-&lt;version&gt;</code>. One should then see the Makefile <code>lammps-&lt;version&gt;/src/MAKE/MACHINES/Makefile.polaris</code> in recent versions that can be used as a starting point for compilation on Polaris. Copies of Makefiles for building with the GPU/KOKKOS package using CUDA for GPU support with the GNU/NVHPC compiler are available in the ALCF GettingStarted repo here. For older versions of LAMMPS, you may need to take an existing Makefile (e.g., Makefile.mpi) for your specific version of LAMMPS and edit the top portion appropriately to create a new Makefile.polaris.</p>"},{"location":"polaris/applications-and-libraries/applications/lammps/#kokkos-package-and-gnu-compilers","title":"KOKKOS package and GNU compilers","text":"<p>The following modules are useful for this particular build. Note, the <code>cmake</code> module is not required if using the GNU Makefile procedure to build LAMMPS. The initial <code>module restore</code> is just setting the default environment as the starting point. Users may find it useful to copy these module commands into a small helper script to assist with compiling and running LAMMPS (e.g., <code>setup_lammps_gnu.sh</code>). </p> <pre><code>module restore\nmodule load craype-accel-nvidia80\nmodule swap PrgEnv-nvhpc PrgEnv-gnu\n\nmodule use /soft/modulefiles\nmodule load cudatoolkit-standalone\n\nmodule load spack-pe-base cmake\n</code></pre> <p>The top portion of <code>Makefile.polaris_gnu_kokkos</code> used to build LAMMPS with the KOKKOS package using GNU as the host-side compiler is shown as an example.</p> <pre><code># polaris_nvhpc_kokkos = Flags for NVIDIA A100, GNU Compiler, MPICH, CUDA\n# module load craype-accel-nvidia80\n# module swap PrgEnv-nvhpc PrgEnv-gnu\n# module use /soft/modulefiles\n# module load cudatoolkit-standalone\n# make polaris_gnu_kokkos -j 32\n\nSHELL = /bin/sh\n\n# ---------------------------------------------------------------------\n# compiler/linker settings\n# specify flags and libraries needed for your compiler\n\nKOKKOS_DEVICES = Cuda,OpenMP\nKOKKOS_ARCH = Ampere80\nKOKKOS_ABSOLUTE_PATH = $(shell cd $(KOKKOS_PATH); pwd)\nKOKKOS_CUDA_OPTIONS = \"enable_lambda,disable_malloc_async\"\nexport NVCC_WRAPPER_DEFAULT_COMPILER = nvc++\n\nCC =        $(KOKKOS_ABSOLUTE_PATH)/bin/nvcc_wrapper\nCCFLAGS =  -g -O3 -mp -DLAMMPS_MEMALIGN=64\n#CCFLAGS += -DLAMMPS_BIGBIG\nSHFLAGS =   -fPIC\nDEPFLAGS =  -M\n\nLINK =      $(CC)\nLINKFLAGS = $(CCFLAGS)\nLIB =\nSIZE =      size\n</code></pre> <p>With the appropriate LAMMPS Makefile in place, an executable can be compiled as in the following example. Note, per-user limits on the login nodes will reduce the maximum parallelism for compilation. Users are encouraged to compile on a compute node in an interactive session if necessary.</p> <pre><code>cd lammps-&lt;version&gt;/src\nmake yes-KOKKOS\nmake polaris_gnu_kokkos -j 32\n</code></pre>"},{"location":"polaris/applications-and-libraries/applications/lammps/#kokkos-package-and-nvhpc-compilers","title":"KOKKOS package and NVHPC compilers","text":"<p>The following modules are useful for this particular build. Note, the <code>cmake</code> module is not required if using the GNU Makefile procedure to build LAMMPS. The initial <code>module restore</code> is just setting the default environment as the starting point. Users may find it useful to copy these module commands into a small helper script to assist with compiling and running LAMMPS (e.g., <code>setup_lammps_nvhpc.sh</code>). </p> <pre><code>module restore\nmodule load craype-accel-nvidia80\n\nmodule use /soft/modulefiles\nmodule load spack-pe-base cmake\n</code></pre> <p>The top portion of <code>Makefile.polaris_nvhpc_kokkos</code> used to build LAMMPS with the KOKKOS package using the NVHPC compilers is shown as an example.</p> <pre><code># polaris_nvhpc_kokkos = Flags for NVIDIA A100, NVIDIA Compiler, MPICH, CUDA\n# make polaris_nvhpc_kokkos -j 16\n\nSHELL = /bin/sh\n\n# ---------------------------------------------------------------------\n# compiler/linker settings\n# specify flags and libraries needed for your compiler\n\nKOKKOS_DEVICES = Cuda,OpenMP\nKOKKOS_ARCH = Ampere80\nKOKKOS_ABSOLUTE_PATH = $(shell cd $(KOKKOS_PATH); pwd)\nKOKKOS_CUDA_OPTIONS = \"enable_lambda,disable_malloc_async\"\nexport NVCC_WRAPPER_DEFAULT_COMPILER = nvc++\n\nCRAY_INC = $(shell CC --cray-print-opts=cflags)\nCRAY_LIB = $(shell CC --cray-print-opts=libs)\n\n#$(info CRAY_INC = ${CRAY_INC})\n#$(info CRAY_LIB = ${CRAY_LIB})\n\nCC =        $(KOKKOS_ABSOLUTE_PATH)/bin/nvcc_wrapper\nCCFLAGS =  -g -O3 -mp -DLAMMPS_MEMALIGN=64\nCCFLAGS += $(CRAY_INC)\nSHFLAGS =   -fPIC\nDEPFLAGS =  -M\n\nLINK =      $(CC)\nLIB =\nLIB += $(CRAY_LIB)\nSIZE =      size\n</code></pre> <p>With the appropriate LAMMPS Makefile in place, an executable can be compiled as in the following example. Note, per-user limits on the login nodes will reduce the maximum parallelism for compilation. Users are encouraged to compile on a compute node in an interactive session if necessary.</p> <pre><code>cd lammps-&lt;version&gt;/src\nmake yes-KOKKOS\nmake polaris_nvhpc_kokkos -j 32\n</code></pre>"},{"location":"polaris/applications-and-libraries/applications/lammps/#running-jobs-on-polaris","title":"Running Jobs on Polaris","text":"<p>An example submission script for running a 64-node KOKKOS-enabled LAMMPS executable is below as an example. Additional information on LAMMPS application flags and options is described on the LAMMPS website.</p> <pre><code>#!/bin/sh\n#PBS -l select=64:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:15:00\n#PBS -l filesystems=home:eagle\n#PBS -q prod\n#PBS -A Catalyst\n\nexport MPICH_GPU_SUPPORT_ENABLED=1\n\nNNODES=`wc -l &lt; $PBS_NODEFILE`\n\n# per-node settings\nNRANKS=4\nNDEPTH=8\nNTHREADS=1\nNGPUS=4\n\nNTOTRANKS=$(( NNODES * NRANKS ))\n\n. ./setup_lammps_gnu.sh\n\nEXE=/home/knight/bin/lmp_polaris_gnu_kokkos\nEXE_ARG=\"-in in.reaxc.hns -k on g ${NGPUS} -sf kk -pk kokkos neigh half neigh/qeq full newton on \"\n\n# OMP settings mostly to quiet Kokkos messages\n\nMPI_ARG=\" -n ${NTOTRANKS} --ppn ${NRANKS} --depth=${NDEPTH} --cpu-bind depth \"\nOMP_ARG=\" --env OMP_NUM_THREADS=${NTHREADS} --env OMP_PROC_BIND=spread --env OMP_PLACES=cores \"\n\nCOMMAND=\"mpiexec ${MPI_ARG} ${OMP_ARG} ${EXE} ${EXE_ARG}\"\necho \"COMMAND= ${COMMAND}\"\n${COMMAND}\n</code></pre>"},{"location":"polaris/applications-and-libraries/applications/lammps/#gpu-package","title":"GPU package","text":"<p>The module environments above can be used to build LAMMPS with the GPU package as well. Copies of Makefiles for building with the GPU package using CUDA for GPU support with the GNU and NVHPC compilers are available in the ALCF GettingStarted repo here. </p>"},{"location":"polaris/applications-and-libraries/applications/lammps/#performance-notes","title":"Performance Notes","text":"<p>Some useful information on accelerator packages and expectations can be found on the LAMMPS website here.</p>"},{"location":"polaris/applications-and-libraries/applications/namd/","title":"NAMD on Polaris","text":""},{"location":"polaris/applications-and-libraries/applications/namd/#what-is-namd","title":"What is NAMD?","text":"<p>NAMD, recipient of a 2002 Gordon Bell Award, a 2012 Sidney Fernbach Award, and a 2020 Gordon Bell Prize, is a parallel molecular dynamics code designed for high-performance simulation of large biomolecular systems. Based on Charm++ parallel objects, NAMD scales to hundreds of cores for typical simulations and beyond 50,000 cores for the largest simulations. NAMD uses the popular molecular graphics program VMD for simulation setup and trajectory analysis, but is also file-compatible with AMBER, CHARMM, and X-PLOR.</p>"},{"location":"polaris/applications-and-libraries/applications/namd/#using-namd-at-alcf","title":"Using NAMD at ALCF","text":"<p>ALCF offers assistance with building binaries and compiling instructions for NAMD. For questions, contact us at support@alcf.anl.gov.</p>"},{"location":"polaris/applications-and-libraries/applications/namd/#running-namd-on-polaris","title":"Running NAMD on Polaris","text":"<p>Prebuilt releases of NAMD binaries can be found in the directory <code>/soft/applications/namd</code>.  * <code>Linux-x86_64-netlrts-smp-CUDA</code> supports GPU-resident runs.  * <code>Linux-x86_64-ofi-smp-CUDA</code> supports general GPU-offload runs. * <code>Linux-x86_64-ofi-smp-CUDA-memopt</code> supports memory-optimized input files and parallel I/O for the largest simulations (~100 million atoms or more).</p> <p>NAMD supports two types of parallelized simulations: single instance strong-scaling and multiple-copy weak-scaling (i.e., replica exchange). For more functionality details, please visit the NAMD website.</p>"},{"location":"polaris/applications-and-libraries/applications/namd/#gpu-resident-runs","title":"GPU-resident runs","text":"<p>A sample PBS script follows for GPU-resident runs on Polaris.</p> <pre><code>#!/bin/sh -l\n#PBS -l select=1:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -q debug\n#PBS -A PROJECT\n#PBS -l filesystems=home:eagle\n\nEXE=/PATH-TO/namd3\n\ncd ${PBS_O_WORKDIR}\n\nmpiexec -n 1 --ppn 1 --depth=16 --cpu-bind=depth $EXE +p 15 +devices 3,2,1,0 stmv.namd &gt; stmv.output\n</code></pre> <p>Measured performance for a ~1,000,000 atom system generated with the above submission script run under NPT conditions and a timestep of 2 fs was <code>16 CPUs 0.00381933 s/step 45.2435 ns/day</code>.</p> <p>Note, the GPU-resident version only runs on a single node currently, and some important functions remain to be implemented with it. A user is strongly encouraged to ensure the updated GPU-resident version fully supports the planned simulation in advance.</p>"},{"location":"polaris/applications-and-libraries/applications/namd/#multiple-copy-gpu-resident-runs","title":"Multiple-copy GPU-resident runs","text":"<p>A sample PBS script for multiple-copy GPU-resident runs follows.</p> <pre><code>#!/bin/sh -l\n#PBS -l select=4:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -q debug-scaling\n#PBS -A PROJECT\n#PBS -l filesystems=home:eagle\n\nEXE=/PATH-TO/namd3\nCHARMRUN=/PATH-TO/charmrum\n\ncd ${PBS_O_WORKDIR}\n\n$CHARMRUN ++mpiexec ++np 16 ++ppn 8 $EXE +replicas 16 init.conf --source rest2_remd.namd +pemap 0-31 +setcpuaffinity +devices 0,1,2,3 +stdout rest2_output/%d/job0.%d.log +devicesperreplica 1\n</code></pre> <p>This sample script launches a solute-tempering replica-exchange simulation with 16 replicas on 4 Polaris nodes. Each node accommodates 4 replicas, and each replica uses 8 CPU cores and binds to 1 GPU device.</p>"},{"location":"polaris/applications-and-libraries/applications/namd/#gpu-offload-run-on-multiple-nodes","title":"GPU-offload run on multiple nodes","text":"<p>A sample PBS script for a GPU-offload run follows.</p> <pre><code>#!/bin/sh -l\n#PBS -l select=64:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -q prod\n#PBS -A PROJECT\n#PBS -l filesystems=home:eagle\n\nEXE=/PATH-TO/namd3\n\ncd ${PBS_O_WORKDIR}\n\naprun -N 4 -n 256 --cc core --cpus-per-pe 8 $EXE +p 6 +setcpuaffinity +devices 3,2,1,0 stmv.namd &gt; stmv_64nodes.output\n</code></pre> <p>Measured performance for a ~1,000,000 atom system generated with the above submission script run under NPT conditions and a timestep of 2 fs was <code>1536 CPUs 0.00151797 s/step 113.724 ns/day</code>.</p>"},{"location":"polaris/applications-and-libraries/applications/namd/#multiple-copy-gpu-offload-run","title":"Multiple-copy GPU-offload run","text":"<p>A sample PBS script for multiple-copy GPU-offload runs follows.</p> <pre><code>#!/bin/sh -l\n#PBS -l select=4:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -q debug-scaling\n#PBS -A PROJECT\n#PBS -l filesystems=home:eagle\n\nEXE=/PATH-TO/namd3\n\ncd ${PBS_O_WORKDIR}\n\naprun -N 4 -n 16 --cc=core --cpus-per-pe 8 $EXE +replicas 16 init.conf --source rest2_remd.namd +setcpuaffinity +stdout rest2_output/%d/job0.%d.log +devicesperreplica 1\n</code></pre>"},{"location":"polaris/applications-and-libraries/applications/namd/#building-namd","title":"Building NAMD","text":"<p>We recommend using the provided NAMD binaries.</p> <p>The following instructions are for the GPU-offload version build on top of Slingshot-11 generic Charm++.</p> <ol> <li><code>module swap PrgEnv-nvhpc PrgEnv-gnu</code></li> <li>Download NAMD source code build_namd.sh<pre><code>tar -xzf NAMD_3.0_Source.tar.gz\ncd NAMD_3.0_Source\ntar xvf charm-8.0.0.tar\ncd charm-8.0.0\n./buildold charm++ ofi-crayshasta cxi slurmpmi2cray smp --with-production -j8 -DCMK_OBJIC_COLLECTION_BITS=8 -DCMK_OBJID_HOME_BITS=20\ncd ..\nwget http://www.ks.uiuc.edu/Research/namd/libraries/fftw-linux-x86_64.tar.gz\nwget http://www.ks.uiuc.edu/Research/namd/libraries/tcl8.6.13-linux-x86_64.tar.gz\nwget http://www.ks.uiuc.edu/Research/namd/libraries/tcl8.6.13-linux-x86_64-threaded.tar.gz\ntar xzf fftw-linux-x86_64.tar.gz\ntar xzf tcl8.6.13-linux-x86_64.tar.gz\ntar xzf tcl8.6.13-linux-x86_64-threaded.tar.gz\nmv linux-x86_64 fftw\nmv tcl8.6.13-linux-x86_64 tcl\nmv tcl8.6.13-linux-x86_64-threaded tcl-threaded \n./config Linux-x86_64-g++  --charm-base ./charm-8.0.0 --charm-arch ofi-crayshasta-cxi-slurmpmi2cray-smp --with-cuda --cuda-prefix /soft/compilers/cudatoolkit/cuda-12.2.2\ncd Linux-x86_64-g++\nmake -j8\n</code></pre></li> </ol> <p>The NAMD binary is <code>namd3</code>. To build a memory-optimized version, the flag <code>--with-memopt</code> needs to be inserted as a config argument.</p> <p>The configure steps above can be replaced with the following to build the GPU-resident version of NAMD.</p> <pre><code>./build charm++ netlrts-linux-x86_64 gcc smp -j8  --with-production\n./config Linux-x86_64-g++  --charm-base ./charm-8.0.0 --charm-arch netlrts-linux-x86_64-smp-gcc --with-cuda --with-single-node-cuda --cuda-prefix /soft/compilers/cudatoolkit/cuda-12.2.2\n</code></pre>"},{"location":"polaris/applications-and-libraries/applications/nekrs/","title":"nekRS","text":""},{"location":"polaris/applications-and-libraries/applications/nekrs/#overview","title":"Overview","text":"<p>nekRS is a fast and scalable computational fluid dynamics (CFD) software package targeting massively parallel computers. It is based on the high-order spectral element method and is capable of solving incompressible and low Mach-number fluid flow problems.  nekRS uses the OCCA portability layer for offloading compute kernels to GPU devices.</p> <p>For details about the code and its usage, see the nekRS home page. This page provides information specific to running on Polaris at the ALCF.</p>"},{"location":"polaris/applications-and-libraries/applications/nekrs/#using-nekrs-at-alcf","title":"Using nekRS at ALCF","text":"<p>ALCF provides assistance with build instructions, compiling executables, submitting jobs, and providing prebuilt binaries (upon request). For questions, contact us at support@alcf.anl.gov.</p>"},{"location":"polaris/applications-and-libraries/applications/nekrs/#how-to-obtain-the-code","title":"How to Obtain the Code","text":"<p>nekRS is an open-source code and can be downloaded from the website. Alternatively, the user can clone from the nekRS GitHub repository. We recommend using the next branch since it is the most updated branch with some of the latest features, including the in-situ visualization capability. </p> <p><pre><code>git clone https://github.com/Nek5000/nekRS.git\ngit checkout next\n</code></pre> The rest of this documentation is based on building and running using the <code>next</code> branch. Users who are interested in running the default <code>master</code> branch can contact support@alcf.anl.gov for additional support. </p>"},{"location":"polaris/applications-and-libraries/applications/nekrs/#building-on-polaris","title":"Building on Polaris","text":"<p>nekRS uses CMake to build and install the software package. After nekRS has been downloaded or cloned on an ALCF filesystem, users should see a directory with the name <code>nekRS</code>. Inside this directory, the user will find a file named <code>nrsconfig</code> that can be used to configure and customize the CMake build options. The user should remove previous build and installation directories whenever there is an update.</p>"},{"location":"polaris/applications-and-libraries/applications/nekrs/#building-using-gnu-compilers","title":"Building using GNU compilers","text":"<p>The following modules are to be loaded for this particular build. <pre><code>module restore\nmodule load craype-accel-nvidia80\nmodule swap PrgEnv-nvhpc PrgEnv-gnu\n\nmodule use /soft/modulefiles\nmodule load cudatoolkit-standalone/12.4.0\n\nmodule load spack-pe-base cmake\n</code></pre></p> <p>To build and install the code, run: <pre><code>CC=cc CXX=CC FC=ftn ./build.sh -DCMAKE_INSTALL_PREFIX=/path/to/installation/directory\n</code></pre> During the installation process, you will be prompted to verify the configuration options. If everything was done correctly, you should see the correct compilers and the <code>Default backend : CUDA</code> in the <code>Summary</code> section of the output. If you see this, press <code>Enter</code> to continue with the build and installation process.</p> <p>After installation, execute the following commands to set up the environment.  <pre><code>export NEKRS_HOME=/path/to/installation/directory\nexport PATH=$NEKRS_HOME/bin:$PATH\n</code></pre></p> <p>Alternatively, you may add the above lines to your <code>$HOME/.bashrc</code> and type <code>source $HOME/.bashrc</code> in the current terminal window.</p>"},{"location":"polaris/applications-and-libraries/applications/nekrs/#building-using-nvhpc-compilers","title":"Building using NVHPC compilers","text":"<p>The following modules are to be loaded for this particular build. The initial <code>module restore</code> is just setting the default environment as the starting point. <pre><code>module restore\nmodule load craype-accel-nvidia80\n\nmodule use /soft/modulefiles\nmodule load spack-pe-base cmake\n</code></pre></p>"},{"location":"polaris/applications-and-libraries/applications/nekrs/#running-jobs-on-polaris","title":"Running Jobs on Polaris","text":"<p>An example submission script for running a 2-node nekRS job is shown below as an example. Additional information on nekRS input files and application setup options is described here. The correct options to execute the script are as follows:</p> <p>NEKRS_HOME=  PROJ_ID= QUEUE= ./run.sh  <p>Users can copy the script below into a file <code>run.sh</code> and execute it using the command above. <pre><code>#!/bin/bash\n: ${PROJ_ID:=\"\"}\n\n: ${QUEUE:=\"prod\"} # debug, debug-scaling, prod https://docs.alcf.anl.gov/running-jobs/job-and-queue-scheduling/\n: ${NEKRS_HOME:=\"$HOME/.local/nekrs\"}\n: ${NEKRS_CACHE_BCAST:=1}\n: ${NEKRS_SKIP_BUILD_ONLY:=0}\n\nif [ $# -ne 3 ]; then\n  echo \"usage: [PROJ_ID] [QUEUE] $0 &lt;casename&gt; &lt;number of compute nodes&gt; &lt;hh:mm:ss&gt;\"\n  exit 0\nfi\n\nif [ -z \"$PROJ_ID\" ]; then\n  echo \"ERROR: PROJ_ID is empty\"\n  exit 1\nfi\n\nif [ -z \"$QUEUE\" ]; then\n  echo \"ERROR: QUEUE is empty\"\n  exit 1\nfi\n\nbin=${NEKRS_HOME}/bin/nekrs\ncase=$1\nnodes=$2\ngpu_per_node=4\ncores_per_numa=8\nlet nn=$nodes*$gpu_per_node\nlet ntasks=nn\ntime=$3\n\nbackend=CUDA\nNEKRS_GPU_MPI=1\n\nif [ ! -f $bin ]; then\n  echo \"Cannot find\" $bin\n  exit 1\nfi\n\nif [ ! -f $case.par ]; then\n  echo \"Cannot find\" $case.par\n  exit 1\nfi\n\nif [ ! -f $case.udf ]; then\n  echo \"Cannot find\" $case.udf\n  exit 1\nfi\n\nif [ ! -f $case.re2 ]; then\n  echo \"Cannot find\" $case.re2\n  exit 1\nfi\n\nstriping_unit=16777216\nmax_striping_factor=128\nlet striping_factor=$nodes/2\nif [ $striping_factor -gt $max_striping_factor ]; then\n  striping_factor=$max_striping_factor\nfi\nif [ $striping_factor -lt 1 ]; then\n  striping_factor=1\nfi\n\nMPICH_MPIIO_HINTS=\"*:striping_unit=${striping_unit}:striping_factor=${striping_factor}:romio_cb_write=enable:romio_ds_write=disable:romio_no_indep_rw=true\"\n\n# sbatch\nSFILE=s.bin\necho \"#!/bin/bash\" &gt; $SFILE\necho \"#PBS -A $PROJ_ID\" &gt;&gt;$SFILE\necho \"#PBS -N nekRS_$case\" &gt;&gt;$SFILE\necho \"#PBS -q $QUEUE\" &gt;&gt;$SFILE\necho \"#PBS -l walltime=$time\" &gt;&gt;$SFILE\necho \"#PBS -l filesystems=home:eagle\" &gt;&gt;$SFILE\necho \"#PBS -l select=$nodes:system=polaris\" &gt;&gt;$SFILE\necho \"#PBS -l place=scatter\" &gt;&gt;$SFILE\necho \"#PBS -k doe\" &gt;&gt;$SFILE #write directly to the destination, doe=direct, output, error\necho \"#PBS -j eo\" &gt;&gt;$SFILE  #oe=merge stdout/stderr to stdout\n\n# job to \u201crun\u201d from your submission directory\necho \"cd \\$PBS_O_WORKDIR\" &gt;&gt; $SFILE\n\necho \"module use /soft/modulefiles\" &gt;&gt; $SFILE\necho \"module use /opt/cray/pe/lmod/modulefiles/mix_compilers\" &gt;&gt; $SFILE\necho \"module load libfabric\" &gt;&gt; $SFILE\necho \"module load cpe-cuda\" &gt;&gt; $SFILE\necho \"module load PrgEnv-gnu\" &gt;&gt; $SFILE\necho \"module load nvhpc-mixed\" &gt;&gt; $SFILE\necho \"module load cmake\" &gt;&gt; $SFILE\necho \"module list\" &gt;&gt; $SFILE\n\necho \"nvidia-smi\" &gt;&gt; $SFILE\necho \"ulimit -s unlimited \" &gt;&gt;$SFILE\n\necho \"export NEKRS_HOME=$NEKRS_HOME\" &gt;&gt;$SFILE\necho \"export NEKRS_GPU_MPI=$NEKRS_GPU_MPI\" &gt;&gt;$SFILE\n\necho \"export MPICH_MPIIO_HINTS=$MPICH_MPIIO_HINTS\" &gt;&gt;$SFILE\necho \"export MPICH_MPIIO_STATS=1\" &gt;&gt;$SFILE\n\necho \"export NEKRS_CACHE_BCAST=$NEKRS_CACHE_BCAST\" &gt;&gt;$SFILE\necho \"export NEKRS_LOCAL_TMP_DIR=/local/scratch\" &gt;&gt;$SFILE\n\necho \"export MPICH_GPU_SUPPORT_ENABLED=1\" &gt;&gt; $SFILE\necho \"export MPICH_OFI_NIC_POLICY=NUMA\" &gt;&gt; $SFILE\n\n# https://github.com/Nek5000/Nek5000/issues/759\necho \"export FI_OFI_RXM_RX_SIZE=32768\" &gt;&gt; $SFILE # &gt;=lelt, large mpi-messsage for restart\n\nif [ $NEKRS_SKIP_BUILD_ONLY -eq 0 ]; then\necho \"mpiexec -n 1 $bin --setup ${case} --backend ${backend} --device-id 0 --build-only $nn\" &gt;&gt;$SFILE\nfi\n\nCMD=.lhelper\necho \"#!/bin/bash\" &gt;$CMD\necho \"gpu_id=\\$((${gpu_per_node} - 1 - \\${PMI_LOCAL_RANK} % ${gpu_per_node}))\" &gt;&gt;$CMD\necho \"export CUDA_VISIBLE_DEVICES=\\$gpu_id\" &gt;&gt;$CMD\necho \"$bin --setup ${case} --backend ${backend} --device-id 0\" &gt;&gt;$CMD\nchmod 755 $CMD\n\necho \"mpiexec -n $nn -ppn $gpu_per_node -d $cores_per_numa --cpu-bind depth ./$CMD\" &gt;&gt;$SFILE\n\nqsub -q $QUEUE $SFILE\n\n# clean-up\n#rm -rf $SFILE $ROMIO_HINTS .lhelper\n</code></pre></p>"},{"location":"polaris/applications-and-libraries/applications/nekrs/#just-in-time-jit-compilation","title":"Just-in-time (JIT) compilation","text":"<p>nekRS uses the OCCA library to translate, compile, and run GPU-targeted functions and kernels. Some useful notes on the cached object files can be found here.</p>"},{"location":"polaris/applications-and-libraries/applications/nekrs/#discussion-group","title":"Discussion Group","text":"<p>Users can visit the GitHub Discussions page to seek help, find solutions, share ideas, and follow discussions on several application-specific topics.</p>"},{"location":"polaris/applications-and-libraries/applications/openmm/","title":"OpenMM on Polaris","text":""},{"location":"polaris/applications-and-libraries/applications/openmm/#what-is-openmm","title":"What is OpenMM?","text":"<p>OpenMM is a high-performance toolkit for molecular simulations that can be used as a stand-alone application or as a library. It provides a combination of flexibility (through custom forces and integrators), openness, and high performance (especially on recent GPUs).</p>"},{"location":"polaris/applications-and-libraries/applications/openmm/#using-openmm-at-alcf","title":"Using OpenMM at ALCF","text":"<p>ALCF offers assistance with building binaries and compiling instructions for OpenMM. For questions, contact us at support@alcf.anl.gov.</p>"},{"location":"polaris/applications-and-libraries/applications/openmm/#building-openmm-using-conda-module","title":"Building OpenMM using Conda module","text":"<ol> <li>Update environment     <pre><code>$ module load conda/2022-07-19\n</code></pre></li> <li>Install OpenMM     <pre><code>$ mkdir conda\n$ conda create --prefix /path-to/conda/openmm_env\n$ conda activate /path-to/conda/openmm_env\n$ conda install -c conda-forge openmm cudatoolkit=11.4\n$ conda deactivate /path-to/conda/openmm_env\n</code></pre></li> <li> <p>Validate installation: If successful, information on code version, platform types, CUDA initialization, and force error tolerance will be shown.</p> <pre><code>$ cd /path-to/conda/openmm_env/share/openmm/examples\n$ python -m openmm.testInstallation\n</code></pre> </li> <li> <p>Benchmark testing using the PBS job script below.</p> <pre><code>$ cd /path-to/conda/openmm_env/share/openmm/examples\n$ qsub ./submit.sh\n</code></pre> </li> </ol>"},{"location":"polaris/applications-and-libraries/applications/openmm/#running-openmm-benchmark-on-polaris","title":"Running OpenMM Benchmark on Polaris","text":"<p>A sample PBS script follows that will run the OpenMM benchmark on one node.</p> <pre><code>#!/bin/sh\n#PBS -l select=1:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -q debug\n#PBS -A PROJECT\n#PBS -l filesystems=home:eagle\n\ncd ${PBS_O_WORKDIR}\n\nmodule load cudatoolkit-standalone/11.4.4\n\npython benchmark.py --platform=CUDA --test=pme --precision=mixed --seconds=30 --heavy-hydrogens &gt; test.output\n</code></pre>"},{"location":"polaris/applications-and-libraries/applications/openmm/#building-openmm-from-source","title":"Building OpenMM from Source","text":"<ol> <li>Update environment     <pre><code>$ module load cudatoolkit-standalone/11.4.4\n$ module load cray-python/3.9.12.1\n</code></pre></li> <li>Download OpenMM     <pre><code>$ git clone https://github.com/openmm/openmm.git\n$ cd openmm ; mkdir build\n</code></pre></li> <li>Download and build Doxygen     <pre><code>$ git clone https://github.com/doxygen/doxygen.git\n$ cd doxygen ; cmake . ; make ; make install ; cd ../\n</code></pre></li> <li>Download and install SWIG in the OpenMM directory.     <pre><code>$ tar xzf swig-4.0.2.tar.gz\n$ cd swig-4.0.2\n$ ./configure --prefix=/path-to/openmm/swig-4.0.2 ; make -j 8 ; make install\n</code></pre></li> <li>Build OpenMM     <pre><code>$ cmake -DDOXYGEN_EXECUTABLE=/path-to/openmm/doxygen/bin/doxygen \\\n        -DSWIG_EXECUTABLE=/path-to/openmm/swig-4.0.2/bin/swig \\\n        -DCMAKE_INSTALL_PREFIX=/path-to/openmm/build \\\n        -DCUDA_HOME=/soft/compilers/cudatoolkit/cuda-11.4.4 \\\n        -DCUDA_INCLUDE_DIR=/soft/compilers/cudatoolkit/cuda-11.4.4/include \\\n        -DCUDA_LIB_DIR=/soft/compilers/cudatoolkit/cuda-11.4.4/lib64\n$ make -j 8\n$ make install\n</code></pre></li> <li> <p>Validate installation: If successful, information on code version, platform types, CUDA initialization, and force error tolerance will be shown.</p> <pre><code>$ cd /path-to/openmm/examples\n$ python -m openmm.testInstallation\n</code></pre> </li> <li> <p>Benchmark testing using the PBS job script above.</p> <pre><code>$ cd /path-to/openmm/examples\n$ qsub ./submit.sh\n</code></pre> </li> </ol>"},{"location":"polaris/applications-and-libraries/applications/vasp/","title":"VASP","text":""},{"location":"polaris/applications-and-libraries/applications/vasp/#what-is-vasp","title":"What is VASP?","text":"<p>The Vienna Ab initio Simulation Package (VASP) is a software package for performing electronic structure calculations with periodic boundary conditions. It is most commonly used to perform density functional theory (DFT) calculations in a plane wave basis using the projector augmented wave (PAW) method. A more complete description of VASP can be found here: https://www.vasp.at</p>"},{"location":"polaris/applications-and-libraries/applications/vasp/#using-vasp-at-alcf","title":"Using VASP at ALCF","text":"<p>VASP is commercial software. Access to binaries compiled by ALCF can only be granted after the user requesting access has been verified to be on the VASP license by an official VASP license distributor.</p> <p>To access the VASP binary at ALCF, please email the details listed directly below to support@alcf.anl.gov. It can take up to 5\u201310 business days to verify a VASP license.</p> <p>Information to provide: - User\u2019s full name: - User\u2019s ALCF username: - Name of the organization that purchased the VASP license: - Principal investigator who is the POC for the VASP license: - VASP license number: - Version of VASP requested (VASP5, VASP6):</p>"},{"location":"polaris/applications-and-libraries/applications/vasp/#vasp-support-policy","title":"VASP support policy","text":"<p>ALCF compiles the latest release of VASP on a per-request basis. We do not offer support for compiling customized versions of VASP with plugins. We are able to provide Makefiles and step-by-step build instructions to users with a verified VASP license. Support for scientific runs that encounter performance or numerical issues should be directed to the official VASP support mailing list or the VASP user forum. Limited support is available for fatal errors encountered at runtime.</p>"},{"location":"polaris/applications-and-libraries/applications/vasp/#how-to-obtain-the-code","title":"How to obtain the code","text":"<p>The VASP source can only be obtained from an official license reseller of VASP. This is either the University of Vienna or Material Designs, Inc.</p>"},{"location":"polaris/applications-and-libraries/applications/vasp/#vasp-6xx-in-polaris-nvhpcopenaccopenmpcuda-mathcraympi","title":"VASP 6.x.x in Polaris (NVHPC+OpenACC+OpenMP+CUDA math+CrayMPI)","text":""},{"location":"polaris/applications-and-libraries/applications/vasp/#general-compilinginstalling-instructions-provided-by-vasp-support","title":"General compiling/installing instructions provided by VASP support","text":"<p>Instructions and samples of <code>makefile.include</code> can be found on the <code>vasp.at</code> wiki page.</p> <p>The following <code>makefile.include</code> was tailored for Polaris, originally taken from here.</p> <pre><code># Precompiler options\nCPP_OPTIONS = -DHOST=\\\"LinuxNV\\\" \\\n              -DMPI -DMPI_BLOCK=8000 -Duse_collective \\\n              -DscaLAPACK \\\n              -DCACHE_SIZE=4000 \\\n              -Davoidalloc \\\n              -Dvasp6 \\\n              -Duse_bse_te \\\n              -Dtbdyn \\\n              -Dqd_emulate \\\n              -Dfock_dblbuf \\\n              -D_OPENMP \\\n              -D_OPENACC \\\n              -DUSENCCL -DUSENCCLP2P\\\n\nCPP        = nvfortran -Mpreprocess -Mfree -Mextend -E $(CPP_OPTIONS) $*$(FUFFIX)  &gt; $*$(SUFFIX)\n\nFC         = ftn -acc -gpu=cc80 -mp -target-accel=nvidia80\nFCL        = ftn -acc -gpu=cc80 -c++libs -target-accel=nvidia80\n\nFREE       = -Mfree\n\nFFLAGS     = -Mbackslash -Mlarge_arrays\n\nOFLAG      = -fast\n\nDEBUG      = -Mfree -O0 -traceback\n\n# Specify your NV HPC-SDK installation, try to set NVROOT automatically\nNVROOT     =$(shell which nvfortran | awk -F /compilers/bin/nvfortran '{ print $$1 }')\n# ...or set NVROOT manually\nNVHPC      ?= /opt/nvidia/hpc_sdk\nNVVERSION  = 23.9\nNVROOT     = $(NVHPC)/Linux_x86_64/$(NVVERSION)\n\n# Use NV HPC-SDK provided BLAS and LAPACK libraries\nLIBAOCL=/soft/libraries/aocl/3.2.0\nBLAS       = /soft/applications/vasp/aol-libs/3.2/amd-blis/lib/LP64/libblis-mt.a\nLAPACK     = /soft/applications/vasp/aol-libs/3.2/amd-libflame/lib/LP64/libflame.a\n\nBLACS      =\nSCALAPACK  =\n#SCALAPACK  = -Mscalapack\n#SCALAPACK  = ${LIBAOCL}/lib/libscalapack.a\n\nCUDA       = -cudalib=cublas,cusolver,cufft,nccl -cuda\n\nLLIBS      = $(SCALAPACK) $(LAPACK) $(BLAS) $(CUDA)\n\n# Software emulation of quadruple precision\nQD         ?= $(NVROOT)/compilers/extras/qd\nLLIBS      += -L$(QD)/lib -lqdmod -lqd\nINCS       += -I$(QD)/include/qd\n\n#INCS       += -I/usr/include/linux\n#INCS       += -I/usr/include/c++/7/tr1\n#INCS       += -I/usr/include/c++/7\n#INCS       += -I/usr/include/x86_64-linux-gnu/c++/7\n#INCS       += -I/lus/theta-fs0/software/spack/spack-dev/opt/spack/linux-sles15-x86_64/gcc-9.3.0/gcc-10.2.0-r7v3naxd5xgzzaqxoe73jj2ytwuddamr/lib/gcc/x86_64-pc-linux-gnu/10.2.0/include/\n\n# Use the FFTs from fftw\nFFTW       = /soft/applications/vasp/aol-libs/3.2/amd-fftw\nLLIBS      += -L$(FFTW)/lib -lfftw3 -lfftw3_omp -lomp\n#INCS       += -I/soft/libraries/aocl/3.2.0/include_LP64/\nINCS       += -I$(FFTW)/include\n\nOBJECTS    = fftmpiw.o fftmpi_map.o fftw3d.o fft3dlib.o\n\n# Redefine the standard list of O1 and O2 objects\nSOURCE_O1  := pade_fit.o\nSOURCE_O2  := pead.o\n\n# For what used to be vasp.5.lib\nCPP_LIB    = $(CPP)\nFC_LIB     = nvfortran\nCC_LIB     = cc\nCFLAGS_LIB = -O $(INCS) -c++libs -cuda\nFFLAGS_LIB = -O1 -Mfixed\nFREE_LIB   = $(FREE)\n\nOBJECTS_LIB= linpack_double.o getshmem.o\n\n# For the parser library\nCXX_PARS   = nvc++ --no_warnings\n\n# Normally no need to change this\nSRCDIR     = ../../src\nBINDIR     = ../../bin\n</code></pre>"},{"location":"polaris/applications-and-libraries/applications/vasp/#setting-up-compiler-and-libraries-with-module","title":"Setting up compiler and libraries with <code>module</code>","text":"<p>The following modules will update the include and library paths used by the Cray compiler wrapper <code>ftn</code> to load additional math libraries for the CPU.</p> <pre><code>module restore\nmodule load PrgEnv-nvhpc\nmodule load cray-libsci\nmodule load craype-accel-nvidia80\nexport NVROOT=${NVIDIA_PATH}\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$NVROOT/compilers/extras/qd/lib\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/soft/applications/vasp/aol-libs/3.2/amd-blis/lib/ILP64/\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/soft/applications/vasp/aol-libs/3.2/amd-libflame/lib/ILP64/\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/soft/applications/vasp/aol-libs/3.2/amd-fftw/lib\n</code></pre>"},{"location":"polaris/applications-and-libraries/applications/vasp/#compiling-vasp","title":"Compiling VASP","text":"<p>Once the <code>modules</code> are loaded and a <code>makefile.include</code> is in the <code>vasp</code> folder, compiling all the object files and binaries is done with:</p> <pre><code>make -j1\n</code></pre>"},{"location":"polaris/applications-and-libraries/applications/vasp/#running-vasp-in-polaris","title":"Running VASP in Polaris","text":"<p>An example of a submission script can be found here <code>/soft/applications/vasp/script.sh</code>, which would look something similar to:</p> <pre><code>#!/bin/sh\n#PBS -l select=1:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -l filesystems=home:eagle\n#PBS -q debug\n#PBS -A MYPROJECT\n\nmodule load PrgEnv-nvhpc\nmodule load cray-libsci\nmodule load craype-accel-nvidia80\n\nNVROOT=${NVIDIA_PATH}\n\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$NVROOT/compilers/extras/qd/lib\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/soft/applications/vasp/aol-libs/3.2/amd-blis/lib/ILP64/\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/soft/applications/vasp/aol-libs/3.2/amd-libflame/lib/ILP64/\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/soft/applications/vasp/aol-libs/3.2/amd-fftw/lib\n\nexport MPICH_GPU_SUPPORT_ENABLED=1\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS=2\nNDEPTH=4\nNTHREADS=4\nNGPUS=2\nNTOTRANKS=$(( NNODES * NRANKS ))\n# Provide full path to VASP binary\nbin=/soft/applications/vasp/vasp.6.4.3/bin/vasp_std\n\ncd $PBS_O_WORKDIR\n\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS} --depth ${NDEPTH} --cpu-bind depth --env OMP_NUM_THREADS=${NTHREADS} $bin\n</code></pre> <p>Submission scripts should have executable attributes to be used with <code>qsub</code> script mode.</p> <pre><code>chmod +x script.sh\nqsub script.sh\n</code></pre>"},{"location":"polaris/applications-and-libraries/applications/vasp/#known-issues-versions-64x-in-polaris-old","title":"Known issues versions: &gt;= 6.4.x in Polaris (OLD)","text":"<ul> <li>Undefined <code>MPIX_Query_cuda_support</code> function at linking binary: This function is called in <code>src/openacc.F</code>. The <code>MPIX_Query_cuda_support</code> is not included in <code>cray-mpich</code>. One workaround to this issue is to comment this function call. See the following suggested changes marked by <code>!!!!!CHANGE HERE</code> in the <code>file:src/openacc.F</code></li> </ul> <pre><code>+!!!!!CHANGE HERE \n-      INTERFACE\n-        INTEGER(c_int) FUNCTION MPIX_Query_cuda_support() BIND(C, name=\"MPIX_Query_cuda_support\")\n-        END FUNCTION\n-      END INTERFACE\n\n       CHARACTER(LEN=1) :: ENVVAR_VALUE\n       INTEGER :: ENVVAR_STAT\n\n       ! This should tell us if MPI is CUDA-aware\n+!!!!!CHANGE HERE \n-       CUDA_AWARE_SUPPORT = MPIX_Query_cuda_support() == 1\n+       CUDA_AWARE_SUPPORT = .TRUE.\n       ! However, for OpenMPI some env variables can still deactivate it even though the previous\n       ! check was positive\n       CALL GET_ENVIRONMENT_VARIABLE(\"OMPI_MCA_mpi_cuda_support\", ENVVAR_VALUE, STATUS=ENVVAR_STAT)\n       IF (ENVVAR_STAT==0 .AND. ENVVAR_VALUE=='0') CUDA_AWARE_SUPPORT = .FALSE.\n       CALL GET_ENVIRONMENT_VARIABLE(\"OMPI_MCA_opal_cuda_support\", ENVVAR_VALUE, STATUS=ENVVAR_STAT)\n       IF (ENVVAR_STAT==0 .AND. ENVVAR_VALUE=='0') CUDA_AWARE_SUPPORT = .FALSE.\n       ! Just in case we might be non-OpenMPI, and their MPIX_Query_cuda_support behaves similarly\n       CALL GET_ENVIRONMENT_VARIABLE(\"MV2_USE_CUDA\", ENVVAR_VALUE, STATUS=ENVVAR_STAT)\n       IF (ENVVAR_STAT==0 .AND. ENVVAR_VALUE=='0') CUDA_AWARE_SUPPORT = .FALSE.\n       CALL GET_ENVIRONMENT_VARIABLE(\"MPICH_RDMA_ENABLED_CUDA\", ENVVAR_VALUE, STATUS=ENVVAR_STAT)\n       IF (ENVVAR_STAT==0 .AND. ENVVAR_VALUE=='0') CUDA_AWARE_SUPPORT = .FALSE.\n       CALL GET_ENVIRONMENT_VARIABLE(\"PMPI_GPU_AWARE\", ENVVAR_VALUE, STATUS=ENVVAR_STAT)\n       IF (ENVVAR_STAT==0) CUDA_AWARE_SUPPORT =(ENVVAR_VALUE == '1')\n+!!!!!CHANGE HERE \n+       CALL GET_ENVIRONMENT_VARIABLE(\"MPICH_GPU_SUPPORT_ENABLED\", ENVVAR_VALUE, STATUS=ENVVAR_STAT)\n+       IF (ENVVAR_STAT==0) CUDA_AWARE_SUPPORT =(ENVVAR_VALUE == '1')\n</code></pre>"},{"location":"polaris/applications-and-libraries/libraries/cabana-polaris/","title":"Cabana","text":""},{"location":"polaris/applications-and-libraries/libraries/cabana-polaris/#cabana_1","title":"Cabana","text":"<p>Cabana is built atop Kokkos. It provides class templates useful for implementing particle codes.</p>"},{"location":"polaris/applications-and-libraries/libraries/cabana-polaris/#cabana-documentation","title":"Cabana Documentation","text":"<ul> <li>Cabana Wiki</li> <li>Cabana GitHub</li> </ul>"},{"location":"polaris/applications-and-libraries/libraries/cabana-polaris/#cabana-on-polaris","title":"Cabana on Polaris","text":"<p>Following the Polaris upgrade to HPCM 1.10, the module setup to use the prebuilt Kokkos has changed.</p> <p>Built against the prebuilt Kokkos on Polaris, the prebuilt Cabana includes three backends: Serial and OpenMP for CPU execution, and CUDA for GPU execution. To use it, run:</p> <pre><code>module load craype-x86-milan\nmodule load craype-accel-nvidia80\nmodule swap PrgEnv-nvhpc PrgEnv-gnu\nmodule use /soft/modulefiles\nmodule load cuda-PrgEnv-nvidia/12.2.91\nmodule load kokkos cabana\n</code></pre> <p>Cabana is a headers-only package; there are no actual libraries installed.</p>"},{"location":"polaris/applications-and-libraries/libraries/math-libraries/","title":"Math Libraries","text":""},{"location":"polaris/applications-and-libraries/libraries/math-libraries/#blas-lapack-and-scalapack-for-cpus","title":"BLAS, LAPACK, and ScaLAPACK for CPUs","text":"<p>Some math libraries targeting CPUs are made available as part of the <code>nvhpc</code> modules and are based on the OpenBLAS project. Additional documentation is available from NVIDIA.</p> <ul> <li>BLAS &amp; LAPACK can be found in the <code>$NVIDIA_PATH/compilers/lib</code> directory.</li> <li>ScaLAPACK can be found in the <code>$NVIDIA_PATH/comm_libs</code> directory.</li> <li>The GNU Scientific Library, GSL-2.7, is available as <code>module help math_libs/gsl</code>.</li> <li>AMD Optimizing CPU Libraries, AOCL v4.2, is available as <code>module help math_libs/aocl</code>.</li> <li>Other Cray-based math libs such as Libsci and FFTW are made available by <code>module load cray-libsci</code> &amp; <code>module load cray-fftw</code>.</li> </ul>"},{"location":"polaris/applications-and-libraries/libraries/math-libraries/#nvidia-math-libraries-for-gpus","title":"NVIDIA Math Libraries for GPUs","text":"<p>Math libraries from NVIDIA are made available via the <code>nvhpc</code> modules. Many of the libraries users typically use can be found in the <code>$NVIDIA_PATH/math_libs</code> directory. Some examples follow, and additional documentation is available from NVIDIA.</p> <ul> <li>libcublas</li> <li>libcufft</li> <li>libcurand</li> <li>libcusolver</li> <li>libcusparse</li> </ul>"},{"location":"polaris/applications-and-libraries/libraries/nccl/","title":"NCCL","text":"<p>NVIDIA NCCL (pronounced \"Nickel\") is a standalone library of standard communication routines for GPUs, implementing all-reduce, all-gather, reduce, broadcast, reduce-scatter, as well as any send/receive-based communication pattern. It has been optimized to achieve high bandwidth on platforms using PCIe, NVLink, NVswitch, as well as networking using InfiniBand Verbs or TCP/IP sockets. NCCL supports an arbitrary number of GPUs installed in a single node or across multiple nodes and can be used in either single- or multi-process (e.g., MPI) applications.</p> <p>NCCL is a key library for scaling AI applications on NVIDIA systems. The Anaconda modules on Polaris are built with NCCL as the communication backend for distributed training of deep learning models. However, HPC applications can also choose NCCL for communication over MPI. The library is available in the following folder: <code>/soft/libraries/nccl</code>.</p> <p>We have done extensive performance tests and identified the following best environment setup.</p> <p><pre><code>export NCCL_NET_GDR_LEVEL=PHB\nexport NCCL_CROSS_NIC=1\nexport NCCL_COLLNET_ENABLE=1\nexport NCCL_NET=\"AWS Libfabric\"\nexport LD_LIBRARY_PATH=/soft/libraries/aws-ofi-nccl/v1.9.1-aws/lib:$LD_LIBRARY_PATH\nexport LD_LIBRARY_PATH=/soft/libraries/hwloc/lib/:$LD_LIBRARY_PATH\nexport FI_CXI_DISABLE_HOST_REGISTER=1\nexport FI_MR_CACHE_MONITOR=userfaultfd\nexport FI_CXI_DEFAULT_CQ_SIZE=131072\n</code></pre> The key here is to enable the AWS plugin (https://github.com/aws/aws-ofi-nccl). AWS OFI NCCL is a plugin that enables EC2 developers to use libfabric as a network provider while running NVIDIA's NCCL-based applications.</p> <p>This setup can lead to a 2-3x performance improvement for some communication workloads. For details, please refer to: https://github.com/argonne-lcf/alcf-nccl-tests.</p> <p>Warning</p> <p>For some applications such as Megatron-DeepSpeed, enabling the AWS plugin will cause a hang or NCCL timeout issue. If so, please disable it by: <pre><code>unset NCCL_NET_GDR_LEVEL NCCL_CROSS_NIC NCCL_COLLNET_ENABLE NCCL_NET\n</code></pre></p>"},{"location":"polaris/applications-and-libraries/libraries/spack-pe/","title":"Spack PE","text":"<p>Spack is an HPC-oriented package manager that ALCF uses to install software for the user environment.</p> <p>Users should depend on libraries in the Spack PE over libraries on the system when possible, as system libraries may slightly differ between compute nodes and login nodes.</p> <p>ALCF's Spack PE is a Spack-managed software stack that provides various build tools, utilities, and libraries. It consists of a base stack (<code>spack-pe-base</code>) and PrgEnv-dependent stacks (currently <code>spack-pe-gnu</code>).</p> <p><code>spack-pe-base</code> contains commonplace software compiled for CPU with the system GCC compilers. Accordingly, the software in <code>spack-pe-base</code> can be used regardless of the programming environment.</p> <p><code>spack-pe-gnu</code> is based on the E4S Project and provides performant HPC libraries built with <code>PrgEnv-gnu</code> and the <code>nvcc</code> CUDA compiler driver for GPU code. <code>spack-pe-gnu</code> is dependent on both <code>spack-pe-base</code> and <code>PrgEnv-gnu</code>.</p>"},{"location":"polaris/applications-and-libraries/libraries/spack-pe/#using-software-from-the-spack-pe","title":"Using software from the Spack PE","text":"<p>The base suite of software tools and libraries can be accessed by loading the <code>spack-pe-base</code> module. This adds a path to <code>$MODULEPATH</code> which contains numerous modules.</p> <p>For example, to load <code>cmake</code> starting from the default environment, a user should run the following commands:</p> <pre><code>module use /soft/modulefiles\nmodule load spack-pe-base\nmodule load cmake\n</code></pre> <p>The <code>spack-pe-base</code> module adds paths to the user's <code>MODULEPATH</code>; individual packages are subsequently loaded through the newly available modules. The full list of available packages can be viewed by running <code>module avail</code> or <code>module --show-hidden avail</code> for a complete listing. Packages are loaded in the same way from <code>spack-pe-gnu</code>.</p>"},{"location":"polaris/applications-and-libraries/libraries/spack-pe/#inspecting-packages","title":"Inspecting packages","text":"<p>When a module in the Spack PE is loaded, several environment variables are updated to integrate the package into the user's environment. Additionally, the <code>PACKAGE_ROOT</code> variable is set to the path to the installation prefix of the package. For example, after loading <code>cmake</code> as above:</p> <pre><code>$ echo $CMAKE_ROOT\n/soft/spack/gcc/0.6.1/install/linux-sles15-x86_64/gcc-12.3.0/cmake-3.27.7-a435jtzvweeos2es6enirbxdjdqhqgdp/\n$ ls -a $CMAKE_ROOT\n.  ..  bin  doc  share  .spack\n</code></pre> <p>This variable can be used to inspect software installations and find header or library paths. Additionally, Spack packages have a <code>.spack</code> directory in the installation prefix which contains build information and logs.</p>"},{"location":"polaris/applications-and-libraries/libraries/spack-pe/#building-software-with-spack","title":"Building software with Spack","text":"<p>Spack is a powerful package manager designed for HPC. The Spack PE is installed and managed with Spack; users can also install Spack in their own home or project directory to manage their software builds. Spack has a steep learning curve, but it may benefit workflows involving frequent builds with complex dependencies.</p> <p>For users who wish to use Spack to install their own software, we provide configuration files corresponding to the Spack PE deployments. These configuration files can be found in <code>config</code> directories in <code>/soft/spack</code> within the respective Spack PE installation directories. For example, the <code>spack-pe-base/0.6.1</code> configurations are in <code>/soft/spack/gcc/0.6.1/config</code>. Not all of these settings will be useful for all builds, and it is not recommended to adopt these wholesale as global settings. The recommended method is to include these settings ad hoc in a Spack environment to control what information Spack uses for its builds.</p> <p>Support requests and feedback should be directed to support@alcf.anl.gov. For general Spack questions, users are encouraged to consult the following resources:</p> <ul> <li>Spack development website</li> <li>Spack documentation</li> <li>Spack tutorial</li> <li>Spack Slack channel</li> </ul>"},{"location":"polaris/applications-and-libraries/libraries/xalt/","title":"XALT","text":""},{"location":"polaris/applications-and-libraries/libraries/xalt/#what-is-xalt","title":"What is XALT?","text":"<p>XALT is a user build and execution tracking framework; it is installed at ALCF on Polaris to track library usage.</p> <p>When XALT is enabled during builds:</p> <ul> <li>An XALT watermark is added to the ELF binary of the executable(s).</li> <li>An XALT link record containing information about the build is created.</li> </ul> <p>When XALT is enabled during application executions:</p> <ul> <li>An XALT start run record containing information about the execution is created; some link data is also included if the executable was built with XALT.</li> <li>If the execution exits normally, an XALT end run record containing information about the end of the process is created; if the process exits abnormally, no end run record is created.</li> <li>For MPI jobs, XALT run records are produced only for rank 0.</li> </ul>"},{"location":"polaris/applications-and-libraries/libraries/xalt/#xalt-implementation-details","title":"XALT implementation details","text":"<ul> <li>XALT uses an <code>ld</code> wrapper script to add the watermark to executables.</li> <li>XALT interposes an <code>LD_PRELOAD</code> library into the execution of the user's application. XALT runs as the user, with the user's primary and supplementary groups.</li> </ul>"},{"location":"polaris/applications-and-libraries/libraries/xalt/#how-to-disable-xalt","title":"How to disable XALT","text":"<ul> <li>Execute the command <code>module unload xalt</code>.</li> <li>If you disable XALT, please send an email to support@alcf.anl.gov detailing the reason you are disabling it.</li> </ul>"},{"location":"polaris/build-tools/cmake-polaris/","title":"CMake","text":""},{"location":"polaris/build-tools/cmake-polaris/#cmake_1","title":"CMake","text":"<p>CMake is a build configuration system that uses higher-level description files to automatically generate Makefiles.</p>"},{"location":"polaris/build-tools/cmake-polaris/#cmake-documentation","title":"CMake Documentation","text":"<ul> <li>CMake website</li> </ul>"},{"location":"polaris/build-tools/cmake-polaris/#cmake-on-polaris","title":"CMake on Polaris","text":"<p>To use CMake on Polaris, run:</p> <pre><code>module use /soft/modulefiles\nmodule load spack-pe-base cmake\n</code></pre>"},{"location":"polaris/compiling-and-linking/cce-compilers-polaris/","title":"CCE Compilers on Polaris","text":"<p>The Cray Compiling Environment (CCE) compilers are available on Polaris via the <code>PrgEnv-cray</code> module.</p> <p>The CCE compilers currently on Polaris only support AMD GPU targets for HIP and are thus not usable with the A100 GPUs.</p> <p>The <code>nvhpc</code> and <code>llvm</code> compilers can be used for compiling GPU-enabled applications.</p>"},{"location":"polaris/compiling-and-linking/compiling-and-linking-overview/","title":"Compiling and Linking Overview on Polaris","text":""},{"location":"polaris/compiling-and-linking/compiling-and-linking-overview/#compiling-on-polaris-login-and-compute-nodes","title":"Compiling on Polaris Login and Compute Nodes","text":"<p>If your build system does not require GPUs for the build process, as is usually the case, the compilation of GPU-accelerated codes is generally expected to work well on the Polaris login nodes. If your build system does require GPUs, you cannot yet compile on the Polaris login nodes, as they do not currently have GPUs installed. In this case, you may compile your applications on the Polaris compute nodes. Do this by submitting an interactive single-node job or running your build system in a batch job.</p>"},{"location":"polaris/compiling-and-linking/compiling-and-linking-overview/#home-file-system","title":"Home File System","text":"<p>It is helpful to realize that there is a single <code>HOME</code> filesystem for users that can be accessed from the login and compute nodes of each production resource at ALCF. Thus, users should be mindful of modifications to their environments (e.g., <code>.bashrc</code>) that may cause issues to arise due to differences between the systems.</p>"},{"location":"polaris/compiling-and-linking/compiling-and-linking-overview/#cray-programming-environment","title":"Cray Programming Environment","text":"<p>The Cray Programming Environment (PE) uses three compiler wrappers for building software. These compiler wrappers should be used when building MPI-enabled applications.</p> <ul> <li><code>cc</code> - C compiler</li> <li><code>CC</code> - C++ compiler</li> <li><code>ftn</code> - Fortran compiler</li> </ul> <p>Each of these wrappers can select a specific vendor compiler based on the PrgEnv module loaded in the environment. The following are some helpful options to understand what the compiler wrapper is invoking.</p> <ul> <li><code>--craype-verbose</code>: Print the command which is forwarded to the compiler invocation</li> <li><code>--cray-print-opts=libs</code>: Print library information</li> <li><code>--cray-print-opts=cflags</code>: Print include information</li> </ul> <p>The output from these commands may be useful in build scripts where a compiler other than that invoked by a compiler wrapper is desired. Defining some variables as such may prove useful in those situations.</p> <pre><code>CRAY_CFLAGS=$(cc --cray-print-opts=cflags)\nCRAY_LIB=$(cc --cray-print-opts=libs)\n</code></pre> <p>Further documentation and options are available via <code>man cc</code> and similar.</p>"},{"location":"polaris/compiling-and-linking/compiling-and-linking-overview/#compilers-provided-by-cray-programming-environments","title":"Compilers Provided by Cray Programming Environments","text":"<p>The default programming environment on Polaris is currently <code>NVHPC</code>. The <code>GNU</code> compilers are available via another programming environment. The following sequence of <code>module</code> commands can be used to switch to the <code>GNU</code> programming environment (gcc, g++, gfortran) and also have <code>NVIDIA</code> compilers available in your path.</p> <pre><code>module swap PrgEnv-nvhpc PrgEnv-gnu\nmodule load nvhpc-mixed\n</code></pre> <p>The compilers invoked by the Cray MPI wrappers are listed for each programming environment in the following table.</p> Module C C++ Fortran MPI Compiler Wrapper cc CC ftn PrgEnv-nvhpc nvc nvc++ nvfortran PrgEnv-gnu gcc g++ gfortran <p>Note, while gcc and g++ may be available in the default environment, the <code>PrgEnv-gnu</code> module is needed to provide gfortran.</p>"},{"location":"polaris/compiling-and-linking/compiling-and-linking-overview/#additional-compilers-provided-by-alcf","title":"Additional Compilers Provided by ALCF","text":"<p>The ALCF additionally provides compilers to enable the OpenMP and SYCL programming models for GPUs via <code>LLVM</code> as documented here.</p> <p>Additional documentation for using compilers is available on the respective programming model pages: OpenMP and SYCL.</p>"},{"location":"polaris/compiling-and-linking/compiling-and-linking-overview/#linking","title":"Linking","text":"<p>Dynamic linking of libraries is currently the default on Polaris. The Cray MPI wrappers will handle this automatically.</p>"},{"location":"polaris/compiling-and-linking/compiling-and-linking-overview/#notes-on-default-modules","title":"Notes on Default Modules","text":"<ul> <li> <p><code>craype-accel-nvidia80</code>: This module adds compiler flags to enable GPU acceleration for <code>NVHPC</code> compilers along with GPU-enabled MPI libraries, as it is assumed that the majority of applications to be compiled on Polaris will target the GPUs for acceleration. Users building CPU-only applications may find it useful to unload this module to silence \"GPU code generation\" warnings.</p> </li> <li> <p><code>xalt</code>: This module enables library tracking; it helps ALCF identify software important to our users. More information can be found on the XALT page.</p> </li> </ul>"},{"location":"polaris/compiling-and-linking/compiling-and-linking-overview/#mixed-cc-fortran-applications","title":"Mixed C/C++ &amp; Fortran Applications","text":"<p>For applications consisting of a mix of C/C++ and Fortran that also use MPI, it is suggested that the programming environment chosen for Fortran be used to build the full application because of mpi.mod (and similar) incompatibilities.</p>"},{"location":"polaris/compiling-and-linking/compiling-and-linking-overview/#compiling-for-gpus","title":"Compiling for GPUs","text":"<p>It is assumed the majority of applications to be built on Polaris will make use of the GPUs. As such, the <code>craype-accel-nvidia80</code> module is in the default environment. This has the effect of the Cray compiler wrappers adding <code>-gpu</code> to the compiler invocation along with additional include paths and libraries. Additional compiler flags may be needed depending on the compiler and GPU programming model used (e.g., <code>-cuda</code>, <code>-acc</code>, or <code>-mp=gpu</code>).</p> <p>This module also adds GPU Transport Layer (GTL) libraries to the link-line to support GPU-aware MPI applications.</p>"},{"location":"polaris/compiling-and-linking/compiling-and-linking-overview/#man-pages","title":"Man Pages","text":"<p>For additional information on the Cray wrappers, please refer to the man pages.</p> <pre><code>man cc\nman CC\nman ftn\n</code></pre>"},{"location":"polaris/compiling-and-linking/gnu-compilers-polaris/","title":"GNU Compilers on Polaris","text":"<p>The GNU compilers are available on Polaris via the <code>PrgEnv-gnu</code> and <code>gcc-mixed</code> modules. The <code>gcc-mixed</code> module can be useful when, for example, the <code>PrgEnv-nvhpc</code> compilers are used to compile C/C++ MPI-enabled code and gfortran is needed.</p> <p>The GNU compilers currently on Polaris do not support GPU code generation and thus can only be used for compiling CPU codes.</p> <p>The <code>nvhpc</code> and <code>llvm</code> compilers can be used for compiling GPU-enabled applications.</p>"},{"location":"polaris/compiling-and-linking/llvm-compilers-polaris/","title":"LLVM Compilers on Polaris","text":"<p>This page is not about LLVM-based Cray Compiling Environment (CCE) compilers from <code>PrgEnv-cray</code> but about open-source LLVM compilers.</p> <p>If LLVM compilers are needed without MPI support, simply load the <code>llvm</code> module.</p> <p>The Cray Programming Environment does not offer LLVM compiler support. Thus, cc/CC/ftn compiler wrappers using LLVM compilers currently are not available. To use Clang with MPI, one can load the <code>mpiwrappers/cray-mpich-llvm</code> module, which loads the following modules:</p> <ul> <li><code>llvm</code>, upstream LLVM compilers</li> <li><code>cray-mpich</code>, MPI compiler wrappers mpicc/mpicxx/mpif90. mpif90 uses gfortran because flang is not ready for production use.</li> <li><code>cray-pals</code>, MPI launchers mpiexec/aprun/mpirun</li> </ul> <p>Limitation: There is no GPU-aware MPI library linking support by default. If needed, users should manually add the GTL (GPU Transport Layer) library to the application link line.</p> <p>Update 04/25/2024: To access LLVM modules, <code>module use /soft/modulefiles</code> is required.</p>"},{"location":"polaris/compiling-and-linking/llvm-compilers-polaris/#openmp-offload","title":"OpenMP offload","text":"<p>When targeting the OpenMP or CUDA programming models for GPUs, the <code>cudatoolkit-standalone</code> module should also be loaded.</p>"},{"location":"polaris/compiling-and-linking/nvidia-compiler-polaris/","title":"NVIDIA Compilers on Polaris","text":"<p>The NVIDIA compilers (<code>nvc</code>, <code>nvc++</code>, <code>nvcc</code>, and <code>nvfortran</code>) are available on Polaris via the <code>PrgEnv-nvhpc</code> and <code>nvhpc</code> modules. There is currently a <code>PrgEnv-nvidia</code> module available, but that will soon be deprecated in Cray's PE, thus it is not recommended for use.</p> <p>The Cray compiler wrappers map to NVIDIA compilers as follows:</p> <pre><code>cc -&gt; nvc\nCC -&gt; nvc++\nftn -&gt; nvfortran\n</code></pre> <p>Users are encouraged to look through NVIDIA's documentation for the NVHPC SDK and specific information on the compilers, tools, and libraries.</p>"},{"location":"polaris/compiling-and-linking/nvidia-compiler-polaris/#notes-on-nvidia-compilers","title":"Notes on NVIDIA Compilers","text":""},{"location":"polaris/compiling-and-linking/nvidia-compiler-polaris/#pgi-compilers","title":"PGI compilers","text":"<p>The NVIDIA programming environments make available compilers from the NVIDIA HPC SDK. While the PGI compilers are available in this programming environment, it should be noted they are actually symlinks to the corresponding NVIDIA compilers.</p> <pre><code>pgcc -&gt; nvc\npgc++ -&gt; nvc++\npgf90 -&gt; nvfortran\npgfortran -&gt; nvfortran\n</code></pre> <p>While <code>nvcc</code> is the traditional CUDA C and CUDA C++ compiler for NVIDIA GPUs, the <code>nvc</code>, <code>nvc++</code>, and <code>nvfortran</code> compilers additionally target CPUs.</p>"},{"location":"polaris/compiling-and-linking/nvidia-compiler-polaris/#nvhpc-sdk-directory-structure","title":"NVHPC SDK Directory Structure","text":"<p>Users migrating from CUDA toolkits to the NVHPC SDK may find it beneficial to review the directory structure of the <code>hpc-sdk</code> directory to find the location of commonly used libraries (including math libraries for the CPU). With the <code>PrgEnv-nvhpc</code> module loaded, the <code>NVIDIA_PATH</code> environment variable can be used to locate the path to various NVIDIA tools, libraries, and examples.</p> <ul> <li><code>compiler/bin</code> - cuda-gdb, ncu, nsys, ...</li> <li><code>examples</code> - CUDA-Fortran, OpenMP, ...</li> <li><code>comm_libs</code> - nccl, nvshmem, ...</li> <li><code>compiler/libs</code> - blas, lapack, ...</li> <li><code>cuda/lib64</code> - cudart, OpenCL, ...</li> <li><code>math_libs/lib64</code> - cublas, cufft, ...</li> </ul>"},{"location":"polaris/compiling-and-linking/nvidia-compiler-polaris/#differences-between-nvcc-and-nvcnvc","title":"Differences between nvcc and nvc/nvc++","text":"<p>For users that want to continue using <code>nvcc</code>, it is important to be mindful of differences with the newer <code>nvc</code> and <code>nvc++</code> compilers. For example, the <code>-cuda</code> flag instructs <code>nvcc</code> to compile <code>.cu</code> input files to <code>.cu.cpp.ii</code> output files, which are to be separately compiled, whereas the same <code>-cuda</code> flag instructs <code>nvc</code>, <code>nvc++</code>, and <code>nvfortran</code> to enable CUDA C/C++ or CUDA Fortran code generation. The resulting output file in each case is different (text vs. object), and one may see an <code>unrecognized format error</code> when <code>-cuda</code> is incorrectly passed to <code>nvcc</code>.</p>"},{"location":"polaris/compiling-and-linking/nvidia-compiler-polaris/#known-issues-and-workarounds","title":"Known Issues and Workarounds","text":"<p>If you are using <code>nvcc</code> to invoke <code>nvc++</code> and compiling C++17 code, and are seeing the following warning and unable to compile C++17 constructs:</p> <pre><code>polaris-login-01(~)&gt; nvcc --std=c++17 -ccbin nvc++ ~/smalltests/bool_constant.cpp\nnvcc warning : The -std=c++17 flag is not supported with the configured host compiler. Flag will be ignored.\n\"/home/zippy/smalltests/bool_constant.cpp\", line 10: error: namespace \"std\" has no member class \"bool_constant\"\n      : std::bool_constant&lt;(UnaryPred&lt;Ts&gt;::value || ...)&gt; {};\n             ^\n\n\"/home/zippy/smalltests/bool_constant.cpp\", line 10: error: class or struct definition is missing\n      : std::bool_constant&lt;(UnaryPred&lt;Ts&gt;::value || ...)&gt; {};\n                          ^\n\n2 errors detected in the compilation of \"/home/zippy/smalltests/bool_constant.cpp\".\npolaris-login-01(~)&gt;\n</code></pre> <p>you will need to work around it by loading the latest cudatoolkit module atop PrgEnv-nvhpc:</p> <pre><code>module load cudatoolkit-standalone/11.6.2\n</code></pre>"},{"location":"polaris/compiling-and-linking/oneapi-compiler/","title":"oneAPI Compilers and Support","text":"<p>The Intel oneAPI compiler and Codeplay plugins for Nvidia GPUs are available on Polaris. The oneAPI compilers are not enabled under the Cray Programming Environment system but can be used separately. Two oneAPI variants are provided, the first being a \"release\" version based on Intel's officially released oneAPI toolkit. Intel Release Notes</p> <p>Note</p> <p>The 2023.2.1 release of the oneAPI Toolkit does not yet support oneDPL on Nvidia devices, though oneMKL is now added from the 2023.2.1 release onwards.</p>"},{"location":"polaris/compiling-and-linking/oneapi-compiler/#components","title":"Components","text":"<ul> <li>The following is a list of components associated with this module:</li> </ul> User Application Component Compilers DPC++ oneMKL Interfaces oneMKL <p>The other variant is a build from the open-source. This variant will be more up-to-date at the risk of bugs and breakages based on code that has not undergone a full release cycle. The documentation is located on the SYCL page. The most notable differences are that <code>icx/icpx</code> are the names of C/C++ compilers, respectively, when using the release version of the module, whereas <code>clang/clang++</code> are for the open-source variant.</p>"},{"location":"polaris/compiling-and-linking/oneapi-compiler/#compile-and-link","title":"Compile and Link","text":"<p>oneAPI uses the clang (or icx/icpx wrapper) for compiling and linking for the Nvidia A100 SM80 architecture.</p> <pre><code>module load oneapi/release\nicpx -std=c++17 -fsycl -fsycl-targets=nvptx64-nvidia-cuda -Xsycl-target-backend --cuda-gpu-arch=sm_80 test.cpp\n</code></pre> <pre><code>harms@polaris-login-04:~/working/polaris/oneapi&gt; icpx -v\nIntel(R) oneAPI DPC++/C++ Compiler 2023.2.0 (2023.2.0.20230721)\nTarget: x86_64-unknown-linux-gnu\nThread model: posix\nInstalledDir: /soft/compilers/oneapi/release/2023.2/compiler/2023.2.1/linux/bin-llvm\nConfiguration file: /soft/compilers/oneapi/release/2023.2/compiler/2023.2.1/linux/bin-llvm/../bin/icpx.cfg\nFound candidate GCC installation: /usr/lib64/gcc/x86_64-suse-linux/7\nSelected GCC installation: /usr/lib64/gcc/x86_64-suse-linux/7\nCandidate multilib: .;@m64\nSelected multilib: .;@m64\nFound CUDA installation: /opt/nvidia/hpc_sdk/Linux_x86_64/21.9/cuda/11.4, version 11.4\n</code></pre>"},{"location":"polaris/compiling-and-linking/oneapi-compiler/#running","title":"Running","text":"<p>The library should select the GPU by default, but the selection of the GPUs can be forced via the ONEAPI_DEVICE_SELECTOR.</p> <pre><code>$ ONEAPI_DEVICE_SELECTOR=cuda:gpu ./a.out\n</code></pre> <p>or a specific GPU.</p> <pre><code>$ ONEAPI_DEVICE_SELECTOR=cuda:gpu:3 ./a.out\n</code></pre>"},{"location":"polaris/compiling-and-linking/oneapi-compiler/#sycl-ls","title":"sycl-ls","text":"<p>Expected output of sycl-ls and which platforms are available.</p> <pre><code>harms@x3004c0s7b0n0:~&gt; which sycl-ls\n/soft/compilers/oneapi/release/2023.2/compiler/2023.2.1/linux/bin/sycl-ls\n\nharms@x3004c0s7b0n0:~&gt; sycl-ls\n[opencl:acc:0] Intel(R) FPGA Emulation Platform for OpenCL(TM), Intel(R) FPGA Emulation Device 1.2 [2023.16.7.0.21_160000]\n[opencl:cpu:1] Intel(R) OpenCL, AMD EPYC 7543P 32-Core Processor                3.0 [2023.16.7.0.21_160000]\n[ext_oneapi_cuda:gpu:0] NVIDIA CUDA BACKEND, NVIDIA A100-SXM4-40GB 8.8 [CUDA 11.4]\n[ext_oneapi_cuda:gpu:1] NVIDIA CUDA BACKEND, NVIDIA A100-SXM4-40GB 8.8 [CUDA 11.4]\n[ext_oneapi_cuda:gpu:2] NVIDIA CUDA BACKEND, NVIDIA A100-SXM4-40GB 8.8 [CUDA 11.4]\n[ext_oneapi_cuda:gpu:3] NVIDIA CUDA BACKEND, NVIDIA A100-SXM4-40GB 8.8 [CUDA 11.4]\n</code></pre>"},{"location":"polaris/compiling-and-linking/polaris-example-program-makefile/","title":"Example Programs and Makefiles for Polaris","text":"<p>Several simple examples of building CPU and GPU-enabled codes on Polaris are available in the ALCF GettingStarted repo for several programming models. If building your application is problematic for some reason (e.g., absence of a GPU), then users are encouraged to build and test applications directly on one of the Polaris compute nodes via an interactive job. The discussion below makes use of the <code>NVHPC</code> compilers in the default environment as illustrative examples. Similar examples for other compilers on Polaris are available in the ALCF GettingStarted repo.</p>"},{"location":"polaris/compiling-and-linking/polaris-example-program-makefile/#cpu-mpiopenmp-example","title":"CPU MPI+OpenMP Example","text":"<p>One of the first useful tasks with any new machine, scheduler, and job launcher is to ensure one is binding MPI ranks and OpenMP threads to the host CPU as intended. A simple HelloWorld MPI+OpenMP example is available here to get started with.</p> <p>The application can be straightforwardly compiled using the Cray compiler wrappers. <pre><code>CC -fopenmp main.cpp -o hello_affinity\n</code></pre></p> <p>The executable <code>hello_affinity</code> can then be launched in a job script (or directly in the shell of an interactive job) using <code>mpiexec</code> as discussed here.</p> <pre><code>#!/bin/sh\n#PBS -l select=1:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -l filesystems=home\n\n# MPI example w/ 16 MPI ranks per node spread evenly across cores\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS_PER_NODE=16\nNDEPTH=4\nNTHREADS=1\n\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}\"\n\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth ./hello_affinity\n</code></pre>"},{"location":"polaris/compiling-and-linking/polaris-example-program-makefile/#cuda","title":"CUDA","text":"<p>Several variants of C/C++ and Fortran CUDA examples are available here that include MPI and multi-GPU examples.</p> <p>One can use the Cray compiler wrappers to compile GPU-enabled applications as well. This example of simple vector addition uses the NVIDIA compilers.</p> <pre><code>CC -g -O3 -std=c++0x -cuda main.cpp -o vecadd\n</code></pre> <p>The <code>craype-accel-nvidia80</code> module in the default environment will add the <code>-gpu</code> compiler flag for <code>nvhpc</code> compilers along with appropriate include directories and libraries. It is left to the user to provide an additional flag to the <code>nvhpc</code> compilers to select the target GPU programming model. In this case, <code>-cuda</code> is used to indicate compilation of CUDA code. The application can then be launched within a batch job submission script or as follows on one of the compute nodes.</p> <pre><code>$ ./vecadd \n# of devices= 4\n  [0] Platform[ Nvidia ] Type[ GPU ] Device[ NVIDIA A100-SXM4-40GB ]\n  [1] Platform[ Nvidia ] Type[ GPU ] Device[ NVIDIA A100-SXM4-40GB ]\n  [2] Platform[ Nvidia ] Type[ GPU ] Device[ NVIDIA A100-SXM4-40GB ]\n  [3] Platform[ Nvidia ] Type[ GPU ] Device[ NVIDIA A100-SXM4-40GB ]\nRunning on GPU 0!\nUsing single-precision\n\n  Name= NVIDIA A100-SXM4-40GB\n  Locally unique identifier= \n  Clock Frequency(KHz)= 1410000\n  Compute Mode= 0\n  Major compute capability= 8\n  Minor compute capability= 0\n  Number of multiprocessors on device= 108\n  Warp size in threads= 32\n  Single precision performance ratio= 2\n\nResult is CORRECT!! :)\n</code></pre>"},{"location":"polaris/compiling-and-linking/polaris-example-program-makefile/#gpu-openacc","title":"GPU OpenACC","text":"<p>A simple MPI-parallel OpenACC example is available here. Compilation proceeds similar to the above CUDA example except for the use of the <code>-acc=gpu</code> compiler flag to indicate compilation of OpenACC code for GPUs. <pre><code>CC -g -O3 -std=c++0x -acc=gpu -gpu=cc80,cuda11.0 main.cpp -o vecadd\n</code></pre> In this example, each MPI rank sees all four GPUs on a Polaris node and GPUs are bound to MPI ranks round-robin within the application.</p> <p><pre><code>$ mpiexec -n 4 ./vecadd\n# of devices= 4\nUsing single-precision\n\nRank 0 running on GPU 0!\nRank 1 running on GPU 1!\nRank 2 running on GPU 2!\nRank 3 running on GPU 3!\n\nResult is CORRECT!! :)\n</code></pre> If the application instead relies on the job launcher to bind MPI ranks to available GPUs, then a small helper script can be used to explicitly set <code>CUDA_VISIBLE_DEVICES</code> appropriately for each MPI rank. One example is available here where each MPI rank is similarly bound to a single GPU with round-robin assignment. The binding of MPI ranks to GPUs is discussed in more detail here.</p>"},{"location":"polaris/compiling-and-linking/polaris-example-program-makefile/#gpu-opencl","title":"GPU OpenCL","text":"<p>A simple OpenCL example is available here. The OpenCL headers and library are available in the NVHPC SDK and CUDA toolkits. The environment variable <code>NVIDIA_PATH</code> is defined for the <code>PrgEnv-nvhpc</code> programming environment.  <pre><code>CC -o vecadd -g -O3 -std=c++0x  -I${NVIDIA_PATH}/cuda/include main.o -L${NVIDIA_PATH}/cuda/lib64 -lOpenCL\n</code></pre></p> <p>This simple example can be run on a Polaris compute node as follows. <pre><code>$ ./vecadd\nRunning on GPU!\nUsing single-precision\n\n    CL_DEVICE_NAME: NVIDIA A100-SXM4-40GB\n    CL_DEVICE_VERSION: OpenCL 3.0 CUDA\n    CL_DEVICE_OPENCL_C_VERSION: OpenCL C 1.2 \n    CL_DEVICE_MAX_COMPUTE_UNITS: 108\n    CL_DEVICE_MAX_CLOCK_FREQUENCY: 1410\n    CL_DEVICE_MAX_WORK_GROUP_SIZE: 1024\n\nResult is CORRECT!! :)\n</code></pre></p>"},{"location":"polaris/compiling-and-linking/polaris-example-program-makefile/#gpu-openmp","title":"GPU OpenMP","text":"<p>A simple MPI-parallel OpenMP example is available here. Compilation proceeds similar to the above examples except for the use of the <code>-mp=gpu</code> compiler flag to indicate compilation of OpenMP code for GPUs.</p> <pre><code>CC -g -O3 -std=c++0x -mp=gpu -gpu=cc80,cuda11.0 -c main.cpp -o vecadd\n</code></pre> <p>Similar to the OpenACC example above, this code binds MPI ranks to GPUs in a round-robin fashion.  <pre><code>$ mpiexec -n 4 ./vecadd\n# of devices= 4\nRank 0 running on GPU 0!\nRank 1 running on GPU 1!\nRank 2 running on GPU 2!\nRank 3 running on GPU 3!\n\nResult is CORRECT!! :)\n</code></pre></p>"},{"location":"polaris/compiling-and-linking/polaris-programming-models/","title":"Programming Models on Polaris","text":"<p>The software environment on Polaris supports several parallel programming models targeting the CPUs and GPUs.</p>"},{"location":"polaris/compiling-and-linking/polaris-programming-models/#cpu-parallel-programming-models","title":"CPU Parallel Programming Models","text":"<p>The Cray compiler wrappers <code>cc</code>, <code>CC</code>, and <code>ftn</code> are recommended for MPI applications as they provide the needed include paths and libraries for each programming environment. A summary of available CPU parallel programming models and relevant compiler flags is shown below. Users are encouraged to review the corresponding man pages and documentation.</p> Programming Model GNU NVHPC LLVM OpenMP -fopenmp -mp -fopenmp OpenACC -- -acc=multicore -- <p>Higher-level programming models such as Kokkos and Raja may also be used for CPU programming.</p>"},{"location":"polaris/compiling-and-linking/polaris-programming-models/#gpu-programming-models","title":"GPU Programming Models","text":"<p>A summary of available GPU programming models and relevant compiler flags is shown below for compilers that generate offloadable code. Users are encouraged to review the corresponding man pages and documentation.</p> Programming Model GNU NVHPC LLVM ONEAPI CUDA -- -cuda [-gpu=cuda8.0,cc11.0] -- -- HIP* -- -- -- -- OpenACC -- -acc -- -- OpenCL* -- -- -- -- OpenMP -- -mp=gpu --offload-arch=sm_80 -- SYCL -- -- -- -fsycl -fsycl-targets=nvptx64-nvidia-cuda -Xsycl-target-backend --cuda-gpu-arch=sm_80 <p>Note, the <code>llvm</code> and <code>oneapi</code> modules are provided by ALCF to complement the compilers provided by the Cray PE on Polaris.</p> <p>Higher-level programming models such as Kokkos and Raja may also be used for GPU programming.</p> <p>OpenCL is supported, but does not require specific compiler flags per se as the offloaded kernels are just-in-time compiled. Abstraction programming models, such as Kokkos, can be built on top of some of these programming models (see below).</p> <p>A HIP compiler supporting the A100 GPUs is still to be installed on Polaris.</p>"},{"location":"polaris/compiling-and-linking/polaris-programming-models/#mapping-programming-models-to-polaris-modules","title":"Mapping Programming Models to Polaris Modules","text":"<p>The table below offers some suggestions for how to get started setting up your environment on Polaris depending on the programming language and model. Note, mixed C/C++ and Fortran applications should choose the programming environment for the Fortran compiler because of mpi.mod and similar incompatibilities between Fortran-generated files from different compilers. Several simple examples for testing the software environment on Polaris for different programming models are available in the ALCF GettingStarted repo.</p> <p>Note, users are encouraged to use <code>PrgEnv-nvhpc</code> instead of <code>PrgEnv-nvidia</code> as the latter will soon be deprecated in Cray's PE. They are otherwise identical, pointing to compilers from the same NVIDIA SDK version.</p> Programming Language GPU Programming Model Likely used Modules/Compilers Notes C/C++ CUDA PrgEnv-nvhpc, PrgEnv-gnu, llvm NVIDIA (nvcc, nvc, nvc++) and clang compilers do GPU code generation C/C++ HIP N/A need to install with support for A100 C/C++ Kokkos See CUDA HIP, OpenMP, and SYCL/DPC++ also candidates C/C++ OpenACC PrgEnv-nvhpc C/C++ OpenCL PrgEnv-nvhpc, PrgEnv-gnu, llvm JIT GPU code generation C/C++ OpenMP PrgEnv-nvhpc, llvm C/C++ RAJA See CUDA HIP, OpenMP, and SYCL/DPC++ also candidates C/C++ SYCL/DPC++ llvm-sycl Fortran CUDA PrgEnv-nvhpc NVIDIA compiler (nvfortran) does GPU code generation; <code>gfortran</code> can be loaded via <code>gcc-mixed</code> Fortran HIP N/A need to install with support for A100 Fortran OpenACC PrgEnv-nvhpc Fortran OpenCL PrgEnv-nvhpc, PrgEnv-gnu JIT GPU code generation Fortran OpenMP PrgEnv-nvhpc"},{"location":"polaris/containers/containers/","title":"Containers on Polaris","text":"<p>Polaris, powered by NVIDIA A100 GPUs, benefits from container-based workloads for seamless compatibility across NVIDIA systems. This guide details the use of containers on Polaris, including custom container creation, large-scale execution, and common pitfalls.</p>"},{"location":"polaris/containers/containers/#apptainer-setup","title":"Apptainer Setup","text":"<p>Polaris employs Apptainer (formerly known as Singularity) for container management. To set up Apptainer, run:</p> <pre><code>ml use /soft/modulefiles\nml load spack-pe-base/0.8.1\nml load apptainer\nml load e2fsprogs\napptainer version #1.2.2\n</code></pre> <p>The Apptainer version on Polaris is 1.2.2. Detailed user documentation is available here.</p>"},{"location":"polaris/containers/containers/#building-from-docker-or-argonne-github-container-registry","title":"Building from Docker or Argonne GitHub Container Registry","text":"<p>Containers on Polaris can be built by writing Dockerfiles on a local machine and then publishing the container to DockerHub, or by directly building them on an ALCF compute node by writing an Apptainer recipe file. If you prefer to use existing containers, you can pull them from various registries like DockerHub and run them on Polaris.</p> <p>Since Docker requires root privileges, which users do not have on Polaris, existing Docker containers must be converted to Apptainer. To build a Docker-based container on Polaris, use the following as an example:</p> <pre><code>qsub -I -A &lt;Project&gt; -q debug -l select=1 -l walltime=01:00:00 -l filesystems=home:eagle -l singularity_fakeroot=true\nexport HTTP_PROXY=http://proxy.alcf.anl.gov:3128\nexport HTTPS_PROXY=http://proxy.alcf.anl.gov:3128\nexport http_proxy=http://proxy.alcf.anl.gov:3128\nexport https_proxy=http://proxy.alcf.anl.gov:3128\nml use /soft/modulefiles\nml load spack-pe-base/0.8.1\nml load apptainer\nml load e2fsprogs\napptainer build --fakeroot pytorch:22.06-py3.sing docker://nvcr.io/nvidia/pytorch:22.06-py3\n</code></pre> <p>You can find the latest prebuilt Nvidia PyTorch containers here. The TensorFlow containers are here (though note that LCF doesn't typically prebuild the TF-1 containers). You can search the full container registry here. For custom containers tailored for Polaris, visit ALCF's GitHub container registry.</p> <p>Note: Currently, container build and executions are only supported on the Polaris compute nodes.</p>"},{"location":"polaris/containers/containers/#running-containers-on-polaris","title":"Running Containers on Polaris","text":"<p>To run a container on Polaris, you can use the submission script described here. Below is an explanation of the job submission script.</p> <pre><code>#!/bin/sh\n#PBS -l select=2:system=polaris\n#PBS -q debug\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -l filesystems=home:eagle\n#PBS -A &lt;project_name&gt;\ncd ${PBS_O_WORKDIR}\necho $CONTAINER\n</code></pre> <p>We move to the current working directory and enable network access at runtime by setting the proxy. We also load Apptainer.</p> <pre><code># SET proxy for internet access\nml use /soft/modulefiles\nml load spack-pe-base/0.8.1\nml load apptainer\nml load e2fsprogs\nexport HTTP_PROXY=http://proxy.alcf.anl.gov:3128\nexport HTTPS_PROXY=http://proxy.alcf.anl.gov:3128\nexport http_proxy=http://proxy.alcf.anl.gov:3128\nexport https_proxy=http://proxy.alcf.anl.gov:3128\n</code></pre> <p>This is important for the system (Polaris - Cray) mpich to bind to the container's mpich. Set the following environment variables:</p> <pre><code>ADDITIONAL_PATH=/opt/cray/pe/pals/1.2.12/lib\nmodule load cray-mpich-abi\nexport APPTAINERENV_LD_LIBRARY_PATH=\"$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH:$ADDITIONAL_PATH\"\n</code></pre> <p>Set the number of ranks per node spread as per your scaling requirements:</p> <pre><code># MPI example w/ 16 MPI ranks per node spread evenly across cores\nNODES=`wc -l &lt; $PBS_NODEFILE`\nPPN=16\nPROCS=$((NODES * PPN))\necho \"NUM_OF_NODES= ${NODES} TOTAL_NUM_RANKS= ${PROCS} RANKS_PER_NODE= ${PPN}\"\n</code></pre> <p>Finally, launch your script:</p> <pre><code>echo C++ MPI\nmpiexec -hostfile $PBS_NODEFILE -n $PROCS -ppn $PPN apptainer exec -B /opt -B /var/run/palsd/ $CONTAINER /usr/source/mpi_hello_world\n\necho Python MPI\nmpiexec -hostfile $PBS_NODEFILE -n $PROCS -ppn $PPN apptainer exec -B /opt -B /var/run/palsd/ $CONTAINER python3 /usr/source/mpi_hello_world.py\n</code></pre> <p>The job can be submitted using:</p> <pre><code>qsub -v CONTAINER=mpich-4_latest.sif job_submission.sh\n</code></pre>"},{"location":"polaris/containers/containers/#recipe-based-container-building","title":"Recipe-Based Container Building","text":"<p>As mentioned earlier, you can build Apptainer containers from recipe files. Instructions are available here. See available containers for more recipes.</p> <p>Note: You can also build custom recipes by bootstrapping from prebuilt images. For example, the first two lines in a recipe to use our custom TensorFlow implementation would be <code>Bootstrap: oras</code> followed by <code>From: ghcr.io/argonne-lcf/tf2-mpich-nvidia-gpu:latest</code>.</p>"},{"location":"polaris/containers/containers/#available-containers","title":"Available containers","text":"<p>If you just want to know what containers are available, here you go:</p> <ul> <li> <p>Examples for running MPICH containers can be found here.</p> </li> <li> <p>Examples for running databases can be found here.</p> </li> <li> <p>For using shpc - that allows for running containers as modules. It can be found here.</p> </li> </ul> <p>The latest containers are updated periodically. If you have trouble using containers or request a newer or a different container, please contact ALCF support at <code>support@alcf.anl.gov</code>.</p>"},{"location":"polaris/containers/containers/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":"<p>Permission Denied Error: If you encounter permission errors during the build:</p> <ul> <li> <p>Check your quota and delete any unnecessary files.</p> </li> <li> <p>Clean up the Apptainer cache, <code>~/.apptainer/cache</code>, and set the Apptainer tmp and cache directories as below. If your home directory is full and if you are building your container on a compute node, then set the tmpdir and cachedir to local scratch:</p> </li> </ul> <pre><code>export BASE_SCRATCH_DIR=/local/scratch/ # FOR POLARIS\n#export BASE_SCRATCH_DIR=/raid/scratch/ # FOR SOPHIA\nexport APPTAINER_TMPDIR=$BASE_SCRATCH_DIR/apptainer-tmpdir\nmkdir $APPTAINER_TMPDIR\nexport APPTAINER_CACHEDIR=$BASE_SCRATCH_DIR/apptainer-cachedir/\nmkdir $APPTAINER_CACHEDIR\n</code></pre> <ul> <li> <p>Make sure you are not in a directory accessed with a symbolic link, i.e., check if <code>pwd</code> and <code>pwd -P</code> return the same path.</p> </li> <li> <p>If any of the above doesn't work, try running the build in your home directory.</p> </li> </ul> <p>Mapping to rank 0 on all nodes: Ensure that the container's MPI aligns with the system MPI. For example, follow the additional steps outlined in the container registry documentation for MPI on Polaris.</p> <p>libmpi.so.40 not found: This can happen if the container's application has an OpenMPI dependency, which is not currently supported on Polaris. It can also spring up if the container's base environment is not a Debian-based architecture such as Ubuntu. Ensure the application has an MPICH implementation as well. Also, try removing <code>.conda/</code>, <code>.cache/</code>, and <code>.local/</code> folders from your home directory and rebuilding the container.</p> <p>Disabled Port mapping, user namespace, and [network virtualization] Network virtualization is disabled for the container due to security constraints. See issue #2533.</p> <p>Apptainer instance errors with version 1.3.2</p> <p>Use <code>nohup</code> and <code>&amp;</code> as an alternative if you want to run Apptainer as a background process. See below for an example of running Postgres as a background process: <pre><code> nohup apptainer run \n -B pgrun:/var/run/postgresql \\\n -B pgdata:/var/lib/postgresql/data \\\n --env-file pg.env \\\n postgres.sing postgres &amp;\n\n # 3) Capture its PID so we can kill it later\n echo $! &gt; postgres_pid.txt\n echo \"Started Postgres in the background with PID $(cat postgres_pid.txt)\"\n\n# 4) Perform whatever work you need while Postgres is running\n#    In this demo, we just sleep for 30 minutes (1800 seconds).\nsleep 1800\n\n# 5) Kill the background process at the end of the job\nkill \"$(cat postgres_pid.txt)\"\nrm postgres_pid.txt\n</code></pre></p>"},{"location":"polaris/data-science/julia/","title":"Julia","text":"<p>Julia is a high-level, high-performance dynamic programming language for technical computing. It has a syntax familiar to users of many other technical computing environments. Designed at MIT to tackle large-scale partial-differential equation simulation and distributed linear algebra, Julia features a robust ecosystem of tools for optimization, statistics, parallel programming, and data visualization. Julia is actively developed by the Julia Labs team at MIT and in industry, along with hundreds of domain-expert scientists and programmers worldwide.</p>"},{"location":"polaris/data-science/julia/#contributing","title":"Contributing","text":"<p>This guide is a first draft of the Julia documentation for Polaris. If you have any suggestions or contributions, please open a pull request or contact us by opening a ticket at the ALCF Helpdesk.</p>"},{"location":"polaris/data-science/julia/#julia-installation","title":"Julia Installation","text":"<p>We encourage users interested in using Julia on Polaris to install in their home or project directories at this time. Using the official Julia 1.9 binaries from the Julia webpage is recommended. Juliaup provides a convenient way to install Julia and manage the various Julia versions. The default installation will install <code>julia</code>, <code>juliaup</code>, and other commands in a <code>${HOME}/.julia</code> directory and update profile files like <code>.bashrc</code> to update <code>PATH</code> to include that directory. One can customize the installation to change these defaults.</p> <pre><code>module load craype-accel-nvidia80\ncurl -fsSL https://install.julialang.org | sh\n</code></pre> <p>If you chose a custom installation, then be sure to update the <code>PATH</code> environment variable appropriately.</p> <pre><code>export PATH=${HOME}/.juliaup/bin:${PATH}\n</code></pre> <p>You may then list the available Julia versions with <code>juliaup list</code> and install a specific version with <code>juliaup install &lt;version&gt;</code>. You can then activate a specific version with <code>juliaup use &lt;version&gt;</code> and set the default version with <code>juliaup default &lt;version&gt;</code>. <code>juliaup update</code> will update the installed Julia versions. In general, the latest stable release of Julia should be used.</p> <pre><code>juliaup add release\n</code></pre>"},{"location":"polaris/data-science/julia/#julia-project-environment","title":"Julia Project Environment","text":"<p>The Julia built-in package manager allows you to create a project and enable project-specific dependencies. Julia manages packages in the Julia depot located by default in <code>~/.julia</code>. However, that NFS filesystem is not meant for high-speed access. Therefore, this Julia depot folder should be located on a fast filesystem of your choice. The Julia depot directory is set via the environment variable <code>JULIA_DEPOT_PATH</code>. For example, you can set the Julia depot to a directory on Polaris's Eagle filesystem by adding the following line to your <code>~/.bashrc</code> file:</p> <pre><code>export JULIA_DEPOT_PATH=/eagle/$PROJECT/$USER/julia_depot\n</code></pre>"},{"location":"polaris/data-science/julia/#programming-julia-on-polaris","title":"Programming Julia on Polaris","text":"<p>There are three key components to using Julia for large-scale computations:</p> <ol> <li>MPI support through MPI.jl</li> <li>GPU support through CUDA.jl</li> <li>HDF5 support through HDF5.jl</li> </ol> <p>In addition, we recommend VSCode with the Julia extension for a modern IDE experience, together with the ssh-remote extension for remote interactive development.</p>"},{"location":"polaris/data-science/julia/#mpi-support","title":"MPI Support","text":"<p>MPI support is provided through the MPI.jl. <pre><code>$ julia --project -e 'using Pkg; Pkg.add(\"MPI\")'\n</code></pre> This will install the MPI.jl package and default MPI prebuilt binaries provided by an artifact. For on-node debugging purposes the default artifact is sufficient. However, for large-scale computations, it is important to use the Cray MPICH installed on Polaris. As of MPI.jl v0.20 this is handled through MPIPrefences.jl. <pre><code>$ julia --project -e 'using Pkg; Pkg.add(\"MPIPreferences\")'\n$ julia --project -e 'using MPIPreferences; MPIPreferences.use_system_binary(vendor=\"cray\")'\n</code></pre></p> <p>The <code>vendor=\"cray\"</code> option is important if you intend to use gpu-aware MPI in your applications. </p> <p>Check that the correct MPI library is targeted with Julia. <pre><code>julia --project -e 'using MPI; MPI.versioninfo()'\nMPIPreferences:\n  binary:  system\n  abi:     MPICH\n  libmpi:  libmpi_nvidia.so\n  mpiexec: mpiexec\n\nPackage versions\n  MPI.jl:             0.20.19\n  MPIPreferences.jl:  0.1.11\n\nLibrary information:\n  libmpi:  libmpi_nvidia.so\n  libmpi dlpath:  /opt/cray/pe/lib64/libmpi_nvidia.so\n  MPI version:  3.1.0\n  Library version:  \n    MPI VERSION    : CRAY MPICH version 8.1.28.2 (ANL base 3.4a2)\n    MPI BUILD INFO : Wed Nov 15 21:59 2023 (git hash 1cde46f)\n</code></pre> When running on the login node, switch back to the default provided MPI binaries in <code>MPI_jll.jl</code> by removing the <code>LocalPreferences.toml</code> file.</p>"},{"location":"polaris/data-science/julia/#gpu-support","title":"GPU Support","text":"<p>NVIDIA GPU support is provided through the CUDA.jl package. The default in Julia is to download artifacts (e.g. CUDA toolkit) based on the runtime detected. While that should generally work, it is recommended to use the local CUDA installation provided on Polaris especially if using gpu-aware MPI in your workloads (important to use supported versions of CUDA with Cray MPICH provided). </p> <p>To use the local CUDA installation provided by the modules on Polaris, the <code>LocalPreferences.toml</code> file can be modified as follows.</p> <pre><code>$ head $JULIA_DEPOT_PATH/environments/v1.10/LocalPreferences.toml\n[CUDA_Runtime_jll]\nlocal = true\n</code></pre> <p>If using the default <code>PrgEnv-nvhpc</code> module on Polaris, then it will be necessary to correct a path to the CUPTI library to successfully install <code>CUDA.jl</code>.</p> <pre><code>$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CRAY_NVIDIA_PREFIX/cuda/12.2/extras/CUPTI/lib64/\n$ julia --project -e 'using Pkg; Pkg.add(\"CUDA\")'\n</code></pre> <p>The GPUs are not currently usable on the Polaris login nodes, so one can confirm the version of CUDA being used by testing in a batch or interactive job on a compute node.</p> <pre><code>$ qsub -I -l select=1,walltime=1:00:00,filesystems=home:eagle -A [PROJECT] -q debug\n\n$ julia --project -e \"using CUDA; CUDA.versioninfo()\"\nCUDA runtime 12.4, artifact installation\nCUDA driver 12.4\nNVIDIA driver 535.154.5, originally for CUDA 12.2\n\nCUDA libraries: \n- CUBLAS: 12.2.1\n- CURAND: 10.3.5\n- CUFFT: 11.2.1\n- CUSOLVER: 11.6.1\n- CUSPARSE: 12.3.1\n- CUPTI: 22.0.0\n- NVML: 12.0.0+535.154.5\n\nJulia packages: \n- CUDA: 5.3.3\n- CUDA_Driver_jll: 0.8.1+0\n- CUDA_Runtime_jll: 0.12.1+0\n\nToolchain:\n- Julia: 1.10.3\n- LLVM: 15.0.7\n\n4 devices:\n  0: NVIDIA A100-SXM4-40GB (sm_80, 39.390 GiB / 40.000 GiB available)\n  1: NVIDIA A100-SXM4-40GB (sm_80, 39.390 GiB / 40.000 GiB available)\n  2: NVIDIA A100-SXM4-40GB (sm_80, 39.390 GiB / 40.000 GiB available)\n  3: NVIDIA A100-SXM4-40GB (sm_80, 39.390 GiB / 40.000 GiB available)\n</code></pre> <p>One can then switch between versions of CUDA as needed. Note, the following commands were executed in an interactive job on a compute node.</p> <pre><code>$ julia --project -e \"using CUDA; CUDA.set_runtime_version!(local_toolkit=true)\"\n[ Info: Configure the active project to use the default CUDA from the local system; please re-start Julia for this to take effect.\n</code></pre> <pre><code>$ julia --project -e \"using CUDA; CUDA.versioninfo()\"\nCUDA runtime 12.2, local installation\nCUDA driver 12.4\nNVIDIA driver 535.154.5, originally for CUDA 12.2\n\nCUDA libraries: \n- CUBLAS: 12.2.1\n- CURAND: 10.3.3\n- CUFFT: 11.0.8\n- CUSOLVER: 11.5.0\n- CUSPARSE: 12.1.1\n- CUPTI: 20.0.0\n- NVML: 12.0.0+535.154.5\n\nJulia packages: \n- CUDA: 5.3.3\n- CUDA_Driver_jll: 0.8.1+0\n- CUDA_Runtime_jll: 0.12.1+0\n- CUDA_Runtime_Discovery: 0.2.4\n\nToolchain:\n- Julia: 1.10.3\n- LLVM: 15.0.7\n\nPreferences:\n- CUDA_Runtime_jll.local: true\n\n4 devices:\n  0: NVIDIA A100-SXM4-40GB (sm_80, 39.390 GiB / 40.000 GiB available)\n  1: NVIDIA A100-SXM4-40GB (sm_80, 39.390 GiB / 40.000 GiB available)\n  2: NVIDIA A100-SXM4-40GB (sm_80, 39.390 GiB / 40.000 GiB available)\n  3: NVIDIA A100-SXM4-40GB (sm_80, 39.390 GiB / 40.000 GiB available)```\n</code></pre> <p>Warning messages from the presence of CUDA in <code>LD_LIBRARY_PATH</code> were ommitted in output of the first two commands. In this case, the artifact and local installation are similar. If there was a difference, then the local installation should be preferred.</p> <p>In case you want write portable GPU kernels we highly recommend the KernelAbstractions.jl package. It provides a high-level abstraction for writing GPU kernels that can be compiled for different GPU backends.</p> <pre><code>julia --project -e 'using Pkg; Pkg.add(\"KernelAbstractions\")'\n</code></pre> <p>By loading either oneAPI.jl, AMDGPU.jl, or CUDA.jl (see quickstart guide below).</p>"},{"location":"polaris/data-science/julia/#cuda-aware-mpi","title":"CUDA-aware MPI","text":"<p>MPI.jl supports CUDA-aware MPI. This is enabled by setting the following environment variables.</p> <pre><code>export JULIA_CUDA_MEMORY_POOL=none\nexport MPICH_GPU_SUPPORT_ENABLED=1\nexport JULIA_MPI_PATH=${CRAY_MPICH_DIR}\nexport JULIA_MPI_HAS_CUDA=1\n</code></pre> <p>Note that <code>MPI.jl</code> needs to be rebuilt for the changes to take effect.</p> <pre><code>julia --project -e 'using Pkg; Pkg.build(\"MPI\")'\n</code></pre>"},{"location":"polaris/data-science/julia/#hdf5-support","title":"HDF5 Support","text":"<p>Parallel HDF5 support is provided by <pre><code>module load cray-hdf5-parallel\n</code></pre> After setting <code>export JULIA_HDF5_PATH=$HDF5_DIR</code> we can install the HDF5.jl package.</p> <pre><code>julia --project -e 'using Pkg; Pkg.add(\"HDF5\")'\n</code></pre> <p>To remove warning messages indicating that use of <code>JULIA_HDF5_PATH</code> has been deprecated, one can use the following command to set the HDF5 libraries.</p> <pre><code>$ echo $CRAY_HDF5_PARALLEL_PREFIX/\n/opt/cray/pe/hdf5-parallel/1.12.2.9/nvidia/23.3\n\n$ julia --project -e 'using HDF5; HDF5.API.set_libraries!(\"/opt/cray/pe/hdf5-parallel/1.12.2.9/nvidia/23.3/lib/libhdf5.so\", \"/opt/cray/pe/hdf5-parallel/1.12.2.9/nvidia/23.3/lib/libhdf5_hl.so\")'\n</code></pre>"},{"location":"polaris/data-science/julia/#quickstart-guide","title":"Quickstart Guide","text":"<p>The following example shows how to use MPI.jl, CUDA.jl, and HDF5.jl to write a parallel program that computes the sum of two vectors on the GPU and writes the result to an HDF5 file. A repository with an example code computing an approximation of pi can be found at Polaris.jl. In this repository, you will also find a <code>setup_polaris.sh</code> script that will build the HDF5.jl and MPI.jl package for the system libraries. The dependencies are installed with the following commands: <pre><code>julia --project\n</code></pre></p> <pre><code>julia&gt; ] up\n</code></pre> <pre><code>using CUDA\nusing HDF5\nusing MPI\nusing Printf\nusing Random\n\nfunction pi_kernel(x, y, d, n)\n    idx = (blockIdx().x-1) * blockDim().x + threadIdx().x\n    if idx &lt;= n\n        d[idx] = (x[idx] - 0.5)^2 + (y[idx] - 0.5)^2 &lt;= 0.25 ? 1 : 0\n    end\n    return nothing\nend\n\nfunction approximate_pi_gpu(n::Integer)\n    x = CUDA.rand(Float64, n)\n    y = CUDA.rand(Float64, n)\n    d = CUDA.zeros(Float64, n)\n\n    nblocks = ceil(Int64, n/32)\n\n    @cuda threads=32 blocks=nblocks pi_kernel(x,y,d,n)\n\n    return sum(d)\nend\n\nfunction main()\n    n = 100000  # Number of points to generate per rank\n    Random.seed!(1234)  # Set a fixed random seed for reproducibility\n\n    dsum = MPI.Allreduce(approximate_pi_gpu(n), MPI.SUM, MPI.COMM_WORLD)\n\n    pi_approx = (4 * dsum) / (n * MPI.Comm_size(MPI.COMM_WORLD))\n\n    if MPI.Comm_rank(MPI.COMM_WORLD) == 0\n        @printf \"Approximation of \u03c0 using Monte Carlo method: %.10f\\n\" pi_approx\n        @printf \"Error: %.10f\\n\" abs(pi_approx - \u03c0)\n    end\n    return pi_approx\nend\n\nMPI.Init()\nif !isinteractive()\n    pi_approx = main()\n    h5open(\"pi.h5\", \"w\") do file\n        write(file, \"pi\", pi_approx)\n    end\nend\n</code></pre>"},{"location":"polaris/data-science/julia/#job-submission-script","title":"Job submission script","text":"<p>This example can be run on Polaris with the following job submission script:</p> <pre><code>#!/bin/bash -l\n#PBS -l select=1:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -l filesystems=home:eagle\n#PBS -q debug\n#PBS -A PROJECT\n\ncd ${PBS_O_WORKDIR}\n\n# MPI example w/ 4 MPI ranks per node spread evenly across cores\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS_PER_NODE=4\nNDEPTH=8\nNTHREADS=1\n\n# Setup Julia environment\n. ./setup_env.sh\n\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE=${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}\"\n\nEXE=/home/knight/.julia/juliaup/julia-1.10.3+0.x64.linux.gnu/bin/julia\n\nMPI_ARGS=\"-n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth\"\n\nmpiexec ${MPI_ARGS} ${EXE} --check-bounds=no --project pi.jl\n</code></pre> <p>The <code>setup_env.sh</code> script updates the environment as indicated above.</p> <pre><code>$ cat ./setup_env.sh\nmodule restore\nmodule load craype-accel-nvidia80\nmodule load cray-hdf5-parallel\n\nexport PATH=/home/knight/.juliaup/bin:${PATH}\nexport JULIA_DEPOT_PATH=/eagle/catalyst/proj-shared/knight/polaris/julia/depot\n\nexport JULIA_HDF5_PATH=$HDF5_DIR\n\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CRAY_NVIDIA_PREFIX/cuda/12.2/extras/CUPTI/lib64/\n\nexport JULIA_CUDA_MEMORY_POOL=none\nexport MPICH_GPU_SUPPORT_ENABLED=1\nexport JULIA_MPI_PATH=${CRAY_MPICH_DIR}\nexport JULIA_MPI_HAS_CUDA=1\n\nexport TMPDIR=/local/scratch\n\n# Temporary workaround\nexport LD_PRELOAD=libmpi_gtl_cuda.so\n</code></pre> <p>Verify that <code>JULIA_DEPOT_PATH</code> is set to the correct path and <code>JULIA_PATH</code> points to the Julia executable. When using <code>juliaup</code>, the Julia executable is located in the <code>juliaup</code> folder of your <code>JULIA_DEPOT_PATH</code>.</p>"},{"location":"polaris/data-science/julia/#large-scale-parallelism","title":"Large-scale parallelism","text":"<p><code>CUDA.jl</code> uses the <code>nvcc</code> compiler to compile GPU kernels. This will create object files in the <code>TEMP</code> filesystem. The default <code>TMPDIR</code> in a job on Polaris is set to a temp directory that only exists on the head node of a job. We recommend setting <code>TEMPDIR</code> to a local directory on each compute node. <pre><code>export TMPDIR=/local/scratch\n</code></pre></p> <p>A simple example to test gpu-aware MPI on multiple nodes is available here.</p>"},{"location":"polaris/data-science/profiling_dl/","title":"Profiling Deep Learning Applications","text":"<p>We can use both a framework-specific (for example, PyTorch-specific) native profiler and the vendor-specific NVIDIA Nsys profiler to get high-level profiling information and a timeline of execution for an application. For kernel-level information, we may use Nsight Compute profiler. Refer to the respective documentation for more details:</p> <ul> <li>Nsight System User Guide</li> <li>Nsight Compute Documentation</li> <li>Nsight Compute CLI</li> <li>PyTorch Profiler</li> </ul>"},{"location":"polaris/data-science/profiling_dl/#example-usage","title":"Example Usage","text":"<p>Both the <code>nsys</code> and <code>ncu</code> profiler commands take the following generic structure:</p> <pre><code>nsys profile -o profile python application.py\n</code></pre> <p>If we want to launch the profiled application with MPI, then <code>mpiexec</code> must be used:</p> <pre><code>mpiexec ... nsys profile ... python application.py ... \n</code></pre> <p>These two commands show the basic command-line structure of deploying the profilers. Below we discuss important use cases that are relevant in large-scale distributed profiling.</p> <p>We can use <code>nsys</code> to trace an application running on multiple ranks and multiple nodes. A simple example, where we use a wrapper script to trace the rank 0 on each node of a 2-node job running a PyTorch application, is below:</p> nsys_wrapper.sh<pre><code>#!/bin/bash\n## This wrapper should be used with nsys profiler to trace in any number of nodes\n## The script is set up to trace rank 0 of the first 2 Nodes in the case of\n## profiling a job running on more than 2 nodes.\nFNAME_EXT=$(basename \"$2\")\nFNAME=\"${FNAME_EXT%%.*}\"\n\nNNODES=`wc -l &lt; $PBS_NODEFILE`\n\nWORK_DIR=/path/to/the/Python/application\nDTAG=$(date +%F_%H%M%S)\nPROFILER_OUTDIR=${WORK_DIR}/profiles/choice_of_name_nsys_n${NNODES}_${DTAG}/${FNAME}_n${NNODES}_${DTAG}\nRUN_ID=choice_of_name_nsys_n${NNODES}_${DTAG}\n\nmkdir -p ${PROFILER_OUTDIR}\nNSYS_OPTS=\" -o ${PROFILER_OUTDIR}/${RUN_ID}_%q{PMI_RANK} --stats=true --show-output=true \"\n\nPROFRANK=0\nRANKCUTOFF=8\n\nif [[ $PALS_LOCAL_RANKID -eq $PROFRANK ]] &amp;&amp; [[ $PMI_RANK -lt $RANKCUTOFF ]]; then\n  echo \"On rank ${PMI_RANK}, collecting traces \"\n  nsys profile $NSYS_OPTS \"$@\"\nelse\n  \"$@\"\nfi\n</code></pre> <p>There are a few important things to notice in the wrapper.</p> <ul> <li> <p><code>NSYS_OPTS</code>: These are the options that <code>nsys</code> uses to trace data at different levels. An exhaustive list of options can be found in the nsys user guide. Note that <code>%q{PMI_RANK}</code> is essential to get a per-rank profile.</p> </li> <li> <p><code>PROFRANK</code>: As implemented, this variable is set by the user to trace the rank of choice. For example, this wrapper will trace the rank 0 on each node.</p> </li> <li> <p><code>RANKCUTOFF</code>: This variable is Polaris specific. As we can run as many as 4 ranks per node (without using MPS), the first 2 nodes of a job will have 8 ranks running. This provides the upper cutoff of the label (in number) of ranks, beyond which <code>nsys</code> will not trace any rank. A user can change the number according to the number of maximum ranks running per node to set up how many ranks to be traced. <code>nsys</code> will produce a profile (<code>nsys-rep</code> file, by default) per traced rank.</p> </li> </ul> <p>To view the produced trace files, we need to use NVIDIA's Nsight Systems on the local machine.</p> <p>Getting Started, Download Nsys</p>"},{"location":"polaris/data-science/profiling_dl/#deployment","title":"Deployment","text":"<p>The wrapper above can be deployed using the following PBS job script:</p> pbs_jobscript_nsys.sh<pre><code>#!/bin/bash -l\n#PBS -l select=2:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:05:00\n#PBS -q debug-scaling\n#PBS -l filesystems=home:eagle\n#PBS -A YOUR ALLOCATION\n\n# What's the working directory for the benchmark?\nWORK_DIR=/path/to/the/Python/program\nTEMPORARY_DIR=/path/to/a/temporary/directory/for/`nsys`/to/use\nNSYS_WRAPPER=${WORK_DIR}/nsys_wrapper.sh\n\n# MPI and OpenMP settings\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS_PER_NODE=4\n\nlet NRANKS=${NNODES}*${NRANKS_PER_NODE}\n\nmodule use /soft/modulefiles/\nmodule load conda/2024-04-29\nconda activate\n\nmpiexec -n ${NRANKS} -ppn ${NRANKS_PER_NODE} --env TMPDIR=${TEMPORARY_DIR} -l --line-buffer \\\n${NSYS_WRAPPER} python ${WORK_DIR}/application.py\n</code></pre> <p>Note that <code>--env TMPDIR=${TEMPORARY_DIR}</code> is essential for <code>nsys</code> to function correctly.</p> <p>We can get kernel-level information (for example, roofline, Tensor Core usage) using NVIDIA's Nsight Compute profiler. Below is a simple wrapper script to show the usage.</p> ncu_wrapper.sh<pre><code>#!/bin/bash\nFNAME_EXT=$(basename \"$2\")\nFNAME=\"${FNAME_EXT%%.*}\"\n\nNNODES=`wc -l &lt; $PBS_NODEFILE`\n\nWORK_DIR=/path/to/the/Python/program\nDTAG=$(date +%F_%H%M%S)\nPROFILER_OUTDIR=${WORK_DIR}/profiles/choice_of_name_ncu_n${NNODES}_${DTAG}/${FNAME}_n${NNODES}_${DTAG}\nRUN_ID=choice_of_name_ncu_n${NNODES}_${DTAG}\n\nmkdir -p ${PROFILER_OUTDIR}\n#KERNEL_NAME=ampere_sgemm_128x128_tn\nKERNEL_NAME=ampere_bf16_s16816gemm_bf16_128x256_ldg8_f2f_stages_64x3_tn\n#NCU_OPTS_DETAILED=\" --set detailed -k ${KERNEL_NAME} -o ${PROFILER_OUTDIR}/${RUN_ID}_%q{PMI_RANK} \"\nNCU_OPTS_ROOFLINE=\" --set roofline -k ${KERNEL_NAME} -o ${PROFILER_OUTDIR}/${RUN_ID}_%q{PMI_RANK} \"\n#NCU_OPTS_FULL=\" --set full -k ${KERNEL_NAME} -o ${PROFILER_OUTDIR}/${RUN_ID}_%q{PMI_RANK} \"\n\nPROFRANK=0\nRANKCUTOFF=8\n\nif [[ $PALS_LOCAL_RANKID -eq $PROFRANK ]] &amp;&amp; [[ $PMI_RANK -lt $RANKCUTOFF ]]; then\n  echo \"On rank ${PMI_RANK}, collecting traces \"\n  ncu $NCU_OPTS_DETAILED \"$@\"\nelse\n  \"$@\"\nfi\n</code></pre> <p>This wrapper can be deployed as the <code>nsys</code> example above. In the <code>ncu</code> wrapper, we explicitly set the name of the kernel that we want to analyze (a GEMM kernel in this case). The exhaustive list of options to set the amount of data collection can be found in the command line section of the documentation. Here we only show standard options; either of the three could be chosen. Note that invoking each option will lead to varying amounts of time the profiler needs to run. This will be important in setting the requested walltime for your batch job.</p> <p><code>ncu</code> will generate <code>ncu-rep</code> files for each traced rank, and we will need NVIDIA's Nsight Compute system on the local machine.</p> <p>Download Nsight Compute</p> <p>The next step is to load the <code>nsys-rep</code> files in the Nsight Systems GUI, and the <code>ncu-rep</code> files to the Nsight Compute GUI.</p>"},{"location":"polaris/data-science/profiling_dl/#single-rank-run","title":"Single Rank Run","text":""},{"location":"polaris/data-science/profiling_dl/#nsys-profiles","title":"<code>nsys</code> profiles","text":"<p>In the single rank case, we go to the top left, go <code>file</code> --&gt; <code>open</code> and select the file that we want to look at. For this particular example, we have focused on the GPU activities. This activity is shown on the second column from the left, named as <code>CUDA HW ...</code>. If we expand the <code>CUDA HW ...</code> tab, we find an <code>NCCL</code> tab. This tab shows the communication library calls.</p>"},{"location":"polaris/data-science/profiling_dl/#ncu-profiles","title":"<code>ncu</code> profiles","text":"<p>The primary qualitative distinction between the <code>nsys-rep</code> files and the <code>ncu-rep</code> files is that the <code>nsys-rep</code> file presents data for the overall execution of the application, whereas the <code>ncu-rep</code> file presents data for the execution of one particular kernel. Our setup here traces only one kernel, but multiple kernels could be traced at a time, but that can become a time-consuming process.</p> <p>We use the <code>--stats=true --show-output=true</code> (see <code>nsys_wrapper.sh</code>) options while collecting the <code>nsys</code> data. As a result, we get a system-wide summary in our <code>.OU</code> files (if run with a job submission script, otherwise on the terminal), and find the names of the kernels that have been called/used for compute and communication. Often we would start with investigating the kernels that have been called the most times or the ones where we spent the most time executing them. In this particular instance, we chose to analyze the <code>gemm</code> kernels, which are related to the matrix multiplication. The full name of this kernel is passed to the <code>ncu</code> profiler with the option <code>-k</code> (see <code>ncu_wrapper.sh</code>).</p> <p>Loading the <code>ncu-rep</code> files works similarly as the <code>nsys-rep</code> files. Here, the important tab is the <code>Details</code> tab. We find that at the 3rd row from the top. Under that tab, we have the <code>GPU Speed of Light Throughput</code> section. In this section, we can find plots showing GPU compute and memory usage. On the right-hand side of the tab, there is a menu bar which gives us the option to select which plot to display, either the roofline plot or the compute-memory throughput chart.</p>"},{"location":"polaris/data-science/profiling_dl/#for-a-multi-rank-run","title":"For a Multi-Rank Run","text":""},{"location":"polaris/data-science/profiling_dl/#nsys-profiles_1","title":"<code>nsys</code> profiles","text":"<p>In the case where we have traced multiple ranks, whether from a single node or multiple nodes, <code>nsys</code> GUI allows us to view the reports in a combined fashion on a single timeline (same time-axis for both reports). This is done through the \"multi-report view\", <code>file</code> --&gt; <code>New multi-report view</code> or <code>file</code> --&gt; <code>Open</code> and selecting however many reports we would like to see in a combined timeline, <code>nsys</code> prompts the user to allow for a \"multi-report view\". These can also be viewed separately.</p>"},{"location":"polaris/data-science/profiling_dl/#profiler-options","title":"Profiler Options","text":"<p>In both cases, <code>nsys</code> and <code>ncu</code>, we have used the standard option sets to generate the profiles. The exhaustive list could be found in the respective documentation pages:</p> <ul> <li>Nsight System User Guide</li> <li>Nsight Compute Documentation</li> <li>Nsight Compute CLI</li> </ul> <p>There is much other information provided through these reports. Here we have discussed the way to view the high-level information.</p>"},{"location":"polaris/data-science/profiling_dl/#pytorch-profiler","title":"PyTorch Profiler","text":"<p>Using the PyTorch profiler requires changes in the application source code. A simple example is the following:</p> pytorch_profiler_example.py<pre><code>from torch.profiler import profile, record_function, ProfilerActivity\n\n# A tracer decorator for a function to be traced\ndef trace_func(func):\n   def wrapper(*args, **kwargs):\n      try:\n         function_name = func.__func__.__qualname__\n      except:\n         function_name = func.__qualname__\n      with record_function(function_name):\n         return func(*args, **kwargs)\n   return wrapper\n\n@trace_func\ndef trace_this_function(a, b, c):\n    ...\n    ...\n    return x, y, z\n\nactivities = [ProfilerActivity.CPU, ProfilerActivity.CUDA]\n\nwith profile(activities=activities, record_shapes=True) as prof:\n    result = trace_this_function(a, b, c)\nprof.export_chrome_trace(f\"{/path/to/the/trace/dir}/{name/of/the/trace}-{rank}-of-{world_size}.json\")\n</code></pre> <p>The procedure described above works for both single and multi-rank deployments.</p>"},{"location":"polaris/data-science/python/","title":"Python","text":"<p>We provide prebuilt <code>conda</code> environments containing GPU-supported builds of <code>torch</code>, <code>tensorflow</code> (both with <code>horovod</code> support for multi-node calculations), <code>jax</code>, and many other commonly-used Python modules.</p> <p>Users can activate this environment by first loading the <code>conda</code> module and then activating the base environment.</p> <p>Explicitly (either from an interactive job or inside a job script):</p> <pre><code>module use /soft/modulefiles; module load conda; conda activate base\n</code></pre> <p>This will load and activate the base environment.</p> <p>Tip</p> <p>We encourage users to use the pre-installed conda environment. Any custom environments are supported on a best-effort basis only.</p> <p>For Python issues or questions, please see the Contacting Support page.</p>"},{"location":"polaris/data-science/python/#virtual-environments-via-venv","title":"Virtual environments via <code>venv</code>","text":"<p>To install additional packages that are missing from the <code>base</code> environment, we can build a <code>venv</code> on top of it.</p> <p>Conda <code>base</code> environment + <code>venv</code></p> <p>If you need a package that is not already installed in the <code>base</code> environment, this is generally the recommended approach.</p> <p>We can create a <code>venv</code> on top of the base Anaconda environment (with <code>--system-site-packages</code> to inherit the <code>base</code> packages):</p> <pre><code>module use /soft/modulefiles; module load conda; conda activate base\nCONDA_NAME=$(echo ${CONDA_PREFIX} | tr '\\/' '\\t' | sed -E 's/mconda3|\\/base//g' | awk '{print $NF}')\nVENV_DIR=\"$(pwd)/venvs/${CONDA_NAME}\"\nmkdir -p \"${VENV_DIR}\"\npython -m venv \"${VENV_DIR}\" --system-site-packages\nsource \"${VENV_DIR}/bin/activate\"\n</code></pre> <p>You can always retroactively change the <code>--system-site-packages</code> flag state for this virtual environment by editing <code>${VENV_DIR}/pyvenv.cfg</code> and changing the value of the line <code>include-system-site-packages=false</code>.</p> <p>To install a different version of a package that is already installed in the base environment, you can use:</p> <pre><code>python3 -m pip install --ignore-installed &lt;package&gt; # or -I\n</code></pre> <p>The shared base environment is not writable, so it is impossible to remove or uninstall packages from it. The packages installed with the above <code>pip</code> command should shadow those installed in the base environment.</p>"},{"location":"polaris/data-science/python/#cloning-the-base-anaconda-environment","title":"Cloning the base Anaconda environment","text":"<p>Warning</p> <p>This approach is generally not recommended as it can be quite slow and can use significant storage space.</p> <p>If you need more flexibility, you can clone the conda environment into a custom path, which would then allow for root-like installations via <code>conda install &lt;module&gt;</code> or <code>pip install &lt;module&gt;</code>.</p> <p>Unlike the <code>venv</code> approach, using a cloned Anaconda environment requires you to copy the entirety of the base environment, which can use significant storage space.</p> <p>To clone the <code>base</code> environment:</p> <pre><code>module load conda; conda activate base\nconda create --clone base --prefix /path/to/envs/base-clone\nconda activate /path/to/envs/base-clone\n</code></pre> <p>where <code>/path/to/envs/base-clone</code> should be replaced by a suitable path. The cloning process can be quite slow.</p>"},{"location":"polaris/data-science/python/#using-pip-install-user","title":"Using <code>pip install --user</code>","text":"<p>Danger</p> <p>This is typically not recommended.</p> <p>With the conda environment setup, one can install common Python modules using <code>python3 -m pip install --user '&lt;module-name&gt;'</code>, which will install packages in <code>$PYTHONUSERBASE/lib/pythonX.Y/site-packages</code>.</p> <p>The <code>$PYTHONUSERBASE</code> environment variable is automatically set when you load the base conda module and is equal to <code>/home/$USER/.local/polaris/conda/YYYY-MM-DD</code>.</p> <p>Note, Python modules installed this way that contain command line binaries will not have those binaries automatically added to the shell's <code>$PATH</code>. To manually add the path:</p> <pre><code>export PATH=\"$PYTHONUSERBASE/bin:$PATH\"\n</code></pre> <p>Be sure to remove this location from <code>$PATH</code> if you deactivate the base Anaconda environment or unload the module.</p> <p>Cloning the Anaconda environment or using <code>venv</code> are both more flexible and transparent when compared to <code>--user</code> installs.</p>"},{"location":"polaris/data-science/python/#existing-issue-and-solution","title":"Existing issue and solution","text":"<p>There is an issue with the current conda environment. One may encounter the following error message:</p> <pre><code>aborting job:\nMPIDI_CRAY_init: GPU_SUPPORT_ENABLED is requested, but GTL library is not linked\n</code></pre> <p>To address this, please add the following line at the very beginning of your Python script.</p> <pre><code>from mpi4py import MPI\n</code></pre>"},{"location":"polaris/data-science/python/#creating-a-jupyter-kernel","title":"Creating a Jupyter Kernel","text":"<p>If you need to use your Python <code>venv</code> on JupyterHub, you will need to create a custom Jupyter kernel for it.</p>"},{"location":"polaris/data-science/applications/gpt-neox/","title":"Instructions for <code>gpt-neox</code>:","text":"<p>We include below a set of instructions to get <code>EleutherAI/gpt-neox</code> running on Polaris.</p> <p>A batch submission script for the following example is available here.</p> <p>Warning</p> <p>The instructions below should be run directly from a compute node.</p> <p>Explicitly, to request an interactive job (from <code>polaris-login</code>): <pre><code>$ qsub -A &lt;project&gt; -q debug-scaling -l select=2 -l walltime=01:00:00\n</code></pre></p> <p>Refer to job scheduling and execution for additional information.</p> <ol> <li> <p>Load and activate the base <code>conda</code> environment:    <pre><code>module load conda\nconda activate base\n</code></pre></p> </li> <li> <p>We've installed the requirements for running <code>gpt-neox</code> into a virtual environment. To activate this environment:    <pre><code>source /soft/datascience/venvs/polaris/2022-09-08/bin/activate\n</code></pre></p> </li> <li> <p>Clone the <code>EleutherAI/gpt-neox</code> repository if it doesn't already exist:    <pre><code>git clone https://github.com/EleutherAI/gpt-neox\n</code></pre></p> </li> <li> <p>Navigate into the <code>gpt-neox</code> directory:    <pre><code>cd gpt-neox\n</code></pre> <p>Note</p> <p>The remaining instructions assume you're inside the <code>gpt-neox</code> directory.</p> </p> </li> <li> <p>Create a DeepSpeed compliant <code>hostfile</code> (each line is formatted as <code>hostname, slots=N</code>):    <pre><code>cat $PBS_NODEFILE &gt; hostfile\nsed -e 's/$/ slots=4/' -i hostfile\nexport DLTS_HOSTFILE=hostfile \n</code></pre></p> </li> <li> <p>Create a <code>.deepspeed_env</code> file to ensure a consistent environment across all workers:    <pre><code>echo \"PATH=${PATH}\" &gt; .deepspeed_env\necho \"LD_LIBRARY_PATH=${LD_LIBRARY_PATH}\" &gt;&gt; .deepspeed_env\necho \"http_proxy=${http_proxy}\" &gt;&gt; .deepspeed_env\necho \"https_proxy=${https_proxy}\" &gt;&gt; .deepspeed_env\n</code></pre></p> </li> <li> <p>Prepare data:    <pre><code>python3 prepare_data.py -d ./data\n</code></pre></p> </li> <li> <p>Train:    <pre><code>python3 ./deepy.py train.py -d configs small.yml local_setup.yml\n</code></pre></p> </li> </ol> <p>Danger</p> <p>If your training seems to be getting stuck at</p> <pre><code>Using /home/user/.cache/torch_extensions as PyTorch extensions root...\n</code></pre> <p>there may be a leftover <code>.lock</code> file from an aborted build. Cleaning either the whole <code>.cache</code> or the extensions' sub-directory should force a clean build on the next attempt.</p>"},{"location":"polaris/data-science/applications/megatron-deepspeed/","title":"Megatron-DeepSpeed","text":"<p>We describe below the instructions for launching distributed training with Microsoft's Megatron-DeepSpeed and briefly describe some parallelism strategies and various optimizations that are supported.</p> <p>Note</p> <p>We maintain a forked version at <code>argonne-lcf/Megatron-DeepSpeed</code> that has some helper scripts for launching and setting various training options.</p>"},{"location":"polaris/data-science/applications/megatron-deepspeed/#setup","title":"Setup","text":"<ol> <li> <p>Load <code>conda</code> and activate the base environment:</p> <pre><code># load conda + activate base env\nmodule load conda/2023-10-04 ; conda activate base\n</code></pre> </li> <li> <p>Clone    <code>argonne-lcf/Megatron-DeepSpeed</code>    and navigate into it:</p> <pre><code># clone + navigate into Megatron-DeepSpeed repo\ngit clone https://github.com/argonne-lcf/Megatron-DeepSpeed\ncd Megatron-DeepSpeed\n</code></pre> </li> <li> <p>Make a virtual environment (on top of base conda):</p> <pre><code># make virtual environment (on top of base conda)\nmkdir -p venvs/polaris/2023-10-04\npython3 -m venv venvs/polaris/2023-10-04 --system-site-packages\nsource venvs/polaris/2023-10-04/bin/activate\n</code></pre> </li> <li> <p>Install the missing dependency:</p> <pre><code># install missing dependency\npython3 -m pip install \"git+https://github.com/saforem2/ezpz\"\n</code></pre> </li> <li> <p>Launch training:</p> <pre><code># ---- launch training -----------------------\n# - MODEL_SIZE_KEY: defined in ALCF/model.sh\n# - other args: defined in ALCF/args.sh\n# ---------------------------------------------\nMODEL_SIZE_KEY=\"GPT25B\" \\\n    SEQ_LEN=4096 \\\n    USE_FLASH_ATTN_V2=1 \\\n    MICRO_BATCH=1 \\\n    GAS=1 \\\n    SP_TYPE=\"megatron\" \\\n    ZERO_STAGE=1 \\\n    ./ALCF/train-gpt3.sh\n</code></pre> </li> </ol>"},{"location":"polaris/data-science/applications/megatron-deepspeed/#helper-scripts","title":"Helper Scripts","text":"<p><code>ALCF/train-gpt3.sh</code></p> <p>:   Main entry point for training. This script will automatically source the rest of the required ALCF/*.sh scripts below.</p> <p><code>ALCF/model.sh</code></p> <p>:   Contains some example model architectures for GPT3-style models.</p> <p><code>ALCF/args.sh</code></p> <p>:   Logic for parsing and setting up runtime options for Megatron and DeepSpeed.</p> <p><code>ALCF/setup.sh</code></p> <p>:   Locate and activate the virtual environment to be used, ensuring MPI variables are set properly.</p> <p><code>ALCF/launch.sh</code></p> <p>:   Identify available resources and build the command to be run, i.e., figure out how many: <code>{nodes, GPUs per node, GPUs total}</code>, to pass to <code>mpi{run,exec}</code> then, use this to build <code>mpiexec &lt;mpiexec-args&gt; python3 pretrain_gpt.py</code>.</p>"},{"location":"polaris/data-science/frameworks/deepspeed/","title":"DeepSpeed","text":"<p>The base <code>conda</code> environment on Polaris comes with Microsoft's DeepSpeed pre-installed. Instructions for using/cloning the base environment can be found here.</p> <p>A batch submission script for the following example is available here.</p> <p>We describe below the steps needed to get started with DeepSpeed on Polaris.</p> <p>We focus on the <code>cifar</code> example provided in the DeepSpeedExamples repository, though this approach should be generally applicable for running any model with DeepSpeed support.</p>"},{"location":"polaris/data-science/frameworks/deepspeed/#running-deepspeed-on-polaris","title":"Running DeepSpeed on Polaris","text":"<p>Note</p> <p>The instructions below should be run directly from a compute node.</p> <p>Explicitly, to request an interactive job (from <code>polaris-login</code>): <pre><code>qsub -A &lt;project&gt; -q debug-scaling -l select=2 -l walltime=01:00:00 -I\n</code></pre></p> <p>Refer to job scheduling and execution for additional information.</p> <ol> <li> <p>Load <code>conda</code> module and activate base environment:</p> <pre><code>module load conda ; conda activate base\n</code></pre> </li> <li> <p>Clone microsoft/DeepSpeedExamples and navigate into the directory:</p> <pre><code>git clone https://github.com/microsoft/DeepSpeedExamples.git\ncd DeepSpeedExamples/cifar\n</code></pre> </li> </ol> <p>Launching DeepSpeed</p> Launching with MPICHLaunching with DeepSpeed <ol> <li> <p>Get the total number of available GPUs:</p> <ol> <li>Count the number of lines in <code>$PBS_NODEFILE</code> (1 host per line)</li> <li>Count the number of GPUs available on the current host</li> <li><code>NGPUS=\"$((${NHOSTS}*${NGPU_PER_HOST}))\"</code> <pre><code>NHOSTS=$(wc -l &lt; \"${PBS_NODEFILE}\")\nNGPU_PER_HOST=$(nvidia-smi -L | wc -l)\nNGPUS=\"$((${NHOSTS}*${NGPU_PER_HOST}))\"\n</code></pre></li> </ol> </li> <li> <p>Launch with <code>mpiexec</code>: <pre><code>mpiexec \\\n  --verbose \\\n  --envall \\\n  -n \"${NGPUS}\" \\\n  --ppn \"${NGPU_PER_HOST}\" \\\n  --hostfile=\"${PBS_NODEFILE}\" \\\n  python3 \\\n    cifar10_deepspeed.py \\\n    --deepspeed_config ds_config.json\n</code></pre></p> </li> </ol> <ol> <li> <p>Create a DeepSpeed compliant <code>hostfile</code>, specifying the <code>hostname</code> and number of GPUs (<code>slots</code>) for each of our available workers: <pre><code>cat $PBS_NODEFILE &gt; hostfile\nsed -e 's/$/ slots=4/' -i hostfile\n</code></pre></p> </li> <li> <p>Create a <code>.deepspeed_env</code> containing the environment variables our workers will need access to: <pre><code>echo \"PATH=${PATH}\" &gt;&gt; .deepspeed_env\necho \"LD_LIBRARY_PATH=${LD_LIBRARY_PATH}\" &gt;&gt; .deepspeed_env\necho \"http_proxy=${http_proxy}\" &gt;&gt; .deepspeed_env\necho \"https_proxy=${https_proxy}\" &gt;&gt; .deepspeed_env\n</code></pre></p> </li> </ol> <p>Warning</p> <p>The <code>.deepspeed_env</code> file expects each line to be of the form <code>KEY=VALUE</code>. Each of these will then be set as environment variables on each available worker specified in our <code>hostfile</code>.</p> <p>We can then run the <code>cifar10_deepspeed.py</code> module using DeepSpeed: <pre><code>deepspeed --hostfile=hostfile cifar10_deepspeed.py \\\n    --deepspeed \\\n    --deepspeed_config ds_config.json\n</code></pre></p> <code>AssertionError: Micro batch size per gpu: 0 has to be greater than 0</code> <p>Depending on the details of your specific job, it may be necessary to modify the provided <code>ds_config.json</code>.</p> <p>If you encounter an error: <pre><code>x3202c0s31b0n0: AssertionError: Micro batch size per gpu: 0 has to be greater than 0\n</code></pre> you can modify the <code>\"train_batch_size\": 16</code> variable in the provided <code>ds_config.json</code> to the (total) number of available GPUs, and explicitly set <code>\"gradient_accumulation_steps\": 1</code>, as shown below. <pre><code>$ export NHOSTS=$(wc -l &lt; \"${PBS_NODEFILE}\")\n$ export NGPU_PER_HOST=$(nvidia-smi -L | wc -l)\n$ export NGPUS=\"$((${NHOSTS}*${NGPU_PER_HOST}))\"\n$ echo $NHOSTS $NGPU_PER_HOST $NGPUS\n24 4 96\n$ # replace \"train_batch_size\" with $NGPUS in ds_config.json\n$ # and write to `ds_config-polaris.json`\n$ sed \\\n    \"s/$(cat ds_config.json| grep batch | cut -d ':' -f 2)/ ${NGPUS},/\" \\\n    ds_config.json \\\n    &gt; ds_config-polaris.json\n$ cat ds_config-polaris.json\n{\n    \"train_batch_size\": 96,\n    \"gradient_accumulation_steps\": 1,\n    ...\n}\n</code></pre></p>"},{"location":"polaris/data-science/frameworks/gpytorch/","title":"GPyTorch on Polaris","text":""},{"location":"polaris/data-science/frameworks/gpytorch/#1-login-and-queue-a-job","title":"1. Login and queue a job","text":"<p>Login to Polaris <pre><code>ssh alcfusername@polaris.alcf.anl.gov\n</code></pre></p> <p>Note</p> <p>The instructions below should be run directly from a compute node.</p> <p>Explicitly, to request an interactive job (from <code>polaris-login</code>): <pre><code>qsub -A &lt;project&gt; -q debug-scaling -l select=2 -l walltime=01:00:00 -I\n</code></pre></p> <p>Refer to job scheduling and execution for additional information.</p>"},{"location":"polaris/data-science/frameworks/gpytorch/#2-load-modules","title":"2. Load Modules","text":"<p>Load the Anaconda environment module, which contains a PyTorch installation, since GPyTorch has PyTorch as a dependency: <pre><code>module use /soft/modulefiles\nmodule load conda\nconda activate\n</code></pre> * Notice that we can check the available modules with \"module avail\" and check the loaded modules with \"module list.\"</p> <p>Create a virtual environment with Python and activate it: <pre><code>python -m venv --system-site-packages path_to_myenv\nsource path_to_myenv/bin/activate\n</code></pre> Now the bash prompt should show that we're in the environment we just created, and we're good to use pip install: <pre><code>pip install gpytorch==version\n</code></pre></p>"},{"location":"polaris/data-science/frameworks/gpytorch/#loading-environment-in-future-sessions","title":"Loading environment in future sessions","text":"<p>After the first time, to run the files, simply activate the Python virtual environment on a compute node with: <pre><code>module use /soft/modulefiles\nmodule load conda\nsource path_to_myenv/bin/activate\n</code></pre></p>"},{"location":"polaris/data-science/frameworks/gpytorch/#3-using-jupyter-notebook-to-run-gpytorch-on-polaris","title":"3. Using Jupyter Notebook to Run GPyTorch on Polaris","text":"<p>Here is the guide:</p>"},{"location":"polaris/data-science/frameworks/gpytorch/#approach-1-use-alcf-jupyterhub","title":"Approach 1 - Use ALCF JupyterHub","text":"<ol> <li>Go to Jupyter Hub of ALCF, click Login Polaris.</li> <li>Queue up on a debug node.</li> </ol> <p>For the first time only, one needs to set up the environment and kernel by following these extra steps:</p> <ol> <li>Once Jupyter Notebook is launched on a compute node, click \"New\" and open a terminal.</li> <li>Run: <pre><code>module use /soft/modulefiles\nmodule load conda\nconda activate\nsource &lt;path_to_previously_created_python_venv&gt;/bin/activate\npython -m ipykernel install --user --name python_venv\n</code></pre> Note: Depending on the system and environment, you might need to install the \"ipykernel\" package first. The <code>python_venv</code> that I just created has the <code>ipykernel</code> module.</li> </ol> <p>Go back to your <code>.ipynb</code> file, change the kernel to <code>python_venv</code> from the dropdown menu, and we'll be good to run GPyTorch!</p>"},{"location":"polaris/data-science/frameworks/gpytorch/#approach-2-use-ssh-tunnel","title":"Approach 2 - Use SSH Tunnel","text":"<p>To use an SSH tunnel, we first need to be in an interactive session on a compute node. See Part 1, \"Login and queue a job\" for more details on this.</p> <p>On a compute node, follow these steps: 1. On the compute node terminal, do: <pre><code>module use /soft/modulefiles\nmodule load conda\nconda activate\njupyter notebook\n</code></pre> You should see a line like <code>http://localhost:XXXX/</code>, where <code>XXXX</code> is the port number that Jupyter Notebook is launched on the compute node, usually the default 8888. If it is not 8888, replace 8888 in the following with your port number.</p> <ol> <li>Then, on a new, local terminal, do: <pre><code>export PORT_NUM=8889\nssh -L $PORT_NUM:localhost:8888 &lt;yourusername@polaris.alcf.anl.gov&gt;\nssh -L 8888:localhost:8888 your_compute_node\nnavigate to localhost:8889 in your browser\n</code></pre></li> </ol> <p>You should see a Jupyter Notebook. Notice that for the first time doing this, one might need to input some password or key. Just follow the directions on that page.</p> <p>(Essentially, the above steps, using SSH, set the local port 8889 to listen to the allocated compute node port 8888 where we initiated a Jupyter Notebook.)</p> <p>For the first time only, one needs to set up the environment and kernel by following these extra steps:</p> <p>Click \"New\" and open a terminal, and run: <pre><code>module use /soft/modulefiles\nmodule load conda\nconda activate\nsource &lt;path_to_previously_created_python_venv&gt;/bin/activate\npython -m ipykernel install --user --name python_venv\n</code></pre> Note: Depending on the system and environment, you might need to install the \"ipykernel\" package first. The <code>python_venv</code> that I just created has the <code>ipykernel</code> module.</p> <p>Go back to your <code>.ipynb</code> file, change the kernel to <code>python_venv</code> from the dropdown menu, and we'll be good to run GPyTorch!</p>"},{"location":"polaris/data-science/frameworks/jax/","title":"JAX","text":"<p>JAX is another popular Python package for accelerated computing. JAX is built on XLA (the same XLA TensorFlow uses) as well as AutoGrad, and additionally has acceleration tools that operate on functions such as <code>vmap</code>, <code>jit</code>, etc. JAX is not as widespread in machine learning as TensorFlow and PyTorch for traditional models (Computer Vision, Language Models), though it is quickly gaining prominence. JAX is very powerful when a program needs non-traditional autodifferentiation or vectorization, such as forward-mode AD, higher-order derivatives, Jacobians, Hessians, or any combination of the above. Users of JAX on Polaris are encouraged to read the user documentation in detail, particularly the details about pure-functional programming, no in-place operations, and the common mistakes in writing functions for the <code>@jit</code> decorator.</p>"},{"location":"polaris/data-science/frameworks/jax/#jax-on-polaris","title":"JAX on Polaris","text":"<p>JAX is installed on Polaris via the <code>jax</code> module, available with: <pre><code>module use /soft/modulefiles; module load jax\n</code></pre></p> <p>Then, you can load JAX in <code>python</code> as usual (below showing results from the <code>conda/2024-04-29</code> module):</p> <pre><code>&gt;&gt;&gt; import jax\n&gt;&gt;&gt; jax.__version__\n'0.4.26'\n&gt;&gt;&gt;\n</code></pre>"},{"location":"polaris/data-science/frameworks/jax/#notes-on-jax-0426","title":"Notes on JAX 0.4.26","text":"<p>On Polaris, due to a bug, an environment variable must be set to use JAX on GPUs. The following code will crash: <pre><code>import jax.numpy as numpy\na = numpy.zeros(1000)\n</code></pre> outputting an error that looks like: <pre><code>jaxlib.xla_extension.XlaRuntimeError: UNKNOWN: no kernel image is available for execution on the device\n</code></pre></p> <p>You can fix this by setting an environment variable: <pre><code>export XLA_FLAGS=\"--xla_gpu_force_compilation_parallelism=1\"\n</code></pre></p>"},{"location":"polaris/data-science/frameworks/jax/#scaling-jax-to-multiple-gpus-and-multiple-nodes","title":"Scaling JAX to multiple GPUs and multiple Nodes","text":"<p>JAX has intrinsic scaling tools to use multiple GPUs on a single node, via the <code>pmap</code> function. If this is sufficient for your needs, excellent. If not, another alternative is to use the newer package mpi4jax.</p> <p>mpi4jax is a relatively new project and requires setting some environment variables for good performance and usability: - Set <code>MPI4JAX_USE_CUDA_MPI=1</code> to use CUDA-Aware MPI, supported in the <code>conda</code> module, to do operations directly from the GPU. - Set <code>MPICH_GPU_SUPPORT_ENABLED=1</code> to use CUDA-Aware MPI.</p> <p>The following code, based on a test script from the mpi4jax repository, can help you verify you are using mpi4jax properly:</p> <pre><code>import os\nfrom mpi4py import MPI\nimport jax\nimport jax.numpy as jnp\nimport mpi4jax\n\ncomm = MPI.COMM_WORLD\nrank = comm.Get_rank()\nlocal_rank = int(os.environ[\"PMI_LOCAL_RANK\"])\n\navailable_devices = jax.devices(\"gpu\")\nif len(available_devices) &lt;= local_rank:\n    raise Exception(\"Could not find enough GPUs\")\n\ntarget_device = available_devices[local_rank]\n\n\n@jax.jit\ndef foo(arr):\n    arr = arr + rank\n    arr_sum, _ = mpi4jax.allreduce(arr, op=MPI.SUM, comm=comm)\n    return arr_sum\n\nwith jax.default_device(target_device):\n    a = jnp.zeros((3, 3))\n    print(f\"Rank {rank}, local rank {local_rank}, a.device is {a.device()}\")\n    result = foo(a)\n    print(f\"Rank {rank}, local rank {local_rank}, result.device is {result.device()}\")\n\n    import time\n    print(\"Sleeping for 5 seconds if you want to look at nvidia-smi ... \")\n    time.sleep(5)\n    print(\"Done sleeping\")\n\nif rank == 0:\n    print(result)\n</code></pre> <p>JAX and mpi4jax are both still somewhat early in their software lifecycles. Updates are frequent, and if you require assistance please contact support@alcf.anl.gov.</p>"},{"location":"polaris/data-science/frameworks/libtorch/","title":"LibTorch C++ Library","text":"<p>LibTorch is a C++ library for Torch, with many of the APIs that are available in PyTorch. Users can find more information in the PyTorch documentation. This is useful for integrating the Torch ML framework into traditional HPC simulation codes and therefore enables training and inference of ML models. During compilation, Intel optimizations will be activated automatically once the IPEX dynamic library is linked.</p>"},{"location":"polaris/data-science/frameworks/libtorch/#environment-setup","title":"Environment Setup","text":"<p>To use LibTorch on Polaris, load the ML frameworks module: <pre><code>module use /soft/modulefiles\nmodule load conda/2024-04-29\nconda activate\n</code></pre> This will also load <code>PrgEnv-gnu/8.5.0</code> and <code>cmake</code>.</p>"},{"location":"polaris/data-science/frameworks/libtorch/#torch-libraries","title":"Torch Libraries","text":"<p>With the ML frameworks module loaded as shown above, run: <pre><code>python -c 'import torch; print(torch.__path__[0])'\npython -c 'import torch; print(torch.utils.cmake_prefix_path)'\n</code></pre> to find the path to the Torch libraries, include files, and CMake files.</p>"},{"location":"polaris/data-science/frameworks/libtorch/#linking-the-torch-libraries","title":"Linking the Torch Libraries","text":"<p>When using the CMake build system, the LibTorch libraries can be linked to an example C++ application using the following <code>CMakeLists.txt</code> file: <pre><code>cmake_minimum_required(VERSION 3.5 FATAL_ERROR)\ncmake_policy(SET CMP0074 NEW)\nproject(project-name)\n\nfind_package(Torch REQUIRED)\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS} -Wl,--no-as-needed\")\nset(TORCH_LIBS ${TORCH_LIBRARIES})\n\nadd_executable(exe main.cpp)\ntarget_link_libraries(exe ${TORCH_LIBS})\n\nset_property(TARGET exe PROPERTY CXX_STANDARD 17)\n</code></pre></p> <p>and configuring the build with: <pre><code>cmake \\\n    -DCMAKE_PREFIX_PATH=`python -c 'import torch; print(torch.utils.cmake_prefix_path)'` \\\n    ./\nmake\n</code></pre></p>"},{"location":"polaris/data-science/frameworks/libtorch/#device-introspection","title":"Device Introspection","text":"<p>Similar to PyTorch, LibTorch provides APIs to perform introspection on the devices available on the system. The simple code below shows how to check if CUDA devices are available, how many are present, and how to loop through them to discover some properties.</p> <pre><code>#include &lt;torch/torch.h&gt;\n\nint main(int argc, const char* argv[])\n{\n  torch::DeviceType device;\n  int num_devices = 0;\n  if (torch::cuda::is_available()) {\n    std::cout &lt;&lt; \"CUDA devices detected\" &lt;&lt; std::endl;\n    device = torch::kCUDA;\n\n    num_devices = torch::cuda::device_count();\n    std::cout &lt;&lt; \"Number of CUDA devices: \" &lt;&lt; num_devices &lt;&lt; std::endl;\n  } else {\n    device = torch::kCPU;\n    std::cout &lt;&lt; \"No CUDA devices detected, setting device to CPU\" &lt;&lt; std::endl;\n  }\n\n  return 0;\n}\n</code></pre>"},{"location":"polaris/data-science/frameworks/libtorch/#model-inferencing-using-the-torch-api","title":"Model Inferencing Using the Torch API","text":"<p>This example shows how to perform inference with the ResNet50 model using LibTorch. First, get a JIT-traced version of the model by executing <code>python resnet50_trace.py</code> (shown below) on a compute node. <pre><code>import torch\nimport torchvision\nfrom time import perf_counter\n\ndevice = 'cuda'\n\nmodel = torchvision.models.resnet50()\nmodel.to(device)\nmodel.eval()\n\ndummy_input = torch.rand(1, 3, 224, 224).to(device)\n\nmodel_jit = torch.jit.trace(model, dummy_input)\ntic = perf_counter()\npredictions = model_jit(dummy_input)\ntoc = perf_counter()\nprint(f\"Inference time: {toc-tic}\")\n\ntorch.jit.save(model_jit, f\"resnet50_jit.pt\")\n</code></pre></p> <p>Then, build <code>inference-example.cpp</code> (shown below): <pre><code>#include &lt;torch/torch.h&gt;\n#include &lt;torch/script.h&gt;\n\nint main(int argc, const char* argv[]) {\n  torch::jit::script::Module model;\n  try {\n    model = torch::jit::load(argv[1]);\n    std::cout &lt;&lt; \"Loaded the model\\n\";\n  }\n  catch (const c10::Error&amp; e) {\n    std::cerr &lt;&lt; \"Error loading the model\\n\";\n    return -1;\n  }\n\n  model.to(torch::Device(torch::kCUDA));\n  std::cout &lt;&lt; \"Model offloaded to GPU\\n\\n\";\n\n  auto options = torch::TensorOptions()\n                      .dtype(torch::kFloat32)\n                      .device(torch::kCUDA);\n  torch::Tensor input_tensor = torch::rand({1,3,224,224}, options);\n  assert(input_tensor.dtype() == torch::kFloat32);\n  assert(input_tensor.device().type() == torch::kCUDA);\n  std::cout &lt;&lt; \"Created the input tensor on GPU\\n\";\n\n  torch::Tensor output = model.forward({input_tensor}).toTensor();\n  std::cout &lt;&lt; \"Performed inference\\n\\n\";\n\n  std::cout &lt;&lt; \"Slice of predicted tensor is : \\n\";\n  std::cout &lt;&lt; output.slice(/*dim=*/1, /*start=*/0, /*end=*/10) &lt;&lt; '\\n';\n\n  return 0;\n}\n</code></pre></p> <p>and execute it with <code>./inference-example ./resnet50_jit.pt</code>.</p>"},{"location":"polaris/data-science/frameworks/pytorch/","title":"PyTorch on Polaris","text":"<p>PyTorch is a popular, open-source deep learning framework developed and released by Facebook. The PyTorch home page has more information about PyTorch, which you can refer to. For troubleshooting on Polaris, please contact support@alcf.anl.gov.</p>"},{"location":"polaris/data-science/frameworks/pytorch/#installation-on-polaris","title":"Installation on Polaris","text":"<p>PyTorch is installed on Polaris already, available in the <code>conda</code> module. To use it from a compute node, please do:</p> <pre><code>module use /soft/modulefiles\nmodule load conda\nconda activate\n</code></pre> <p>Then, you can load PyTorch in <code>python</code> as usual (below showing results from the <code>conda/2024-04-29</code> module):</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; torch.__version__\n'2.3.0'\n&gt;&gt;&gt;\n</code></pre> <p>This installation of PyTorch was built from source, and the CUDA libraries it uses are found via the <code>CUDA_HOME</code> environment variable (below showing results from the <code>conda/2024-04-29</code> module):</p> <pre><code>$ echo $CUDA_HOME\n/soft/compilers/cudatoolkit/cuda-12.4.1/\n</code></pre> <p>If you need to build applications that use this version of PyTorch and CUDA, we recommend using these CUDA libraries to ensure compatibility. We periodically update the PyTorch release, though updates will come in the form of new versions of the <code>conda</code> module.</p> <p>PyTorch is also available through NVIDIA containers that have been translated to Apptainer containers. For more information about containers, please see the containers documentation page.</p>"},{"location":"polaris/data-science/frameworks/pytorch/#pytorch-best-practices-on-polaris","title":"PyTorch Best Practices on Polaris","text":""},{"location":"polaris/data-science/frameworks/pytorch/#single-node-performance","title":"Single Node Performance","text":"<p>When running PyTorch applications, we have found the following practices to be generally, if not universally, useful and encourage you to try some of these techniques to boost the performance of your own applications.</p> <ol> <li> <p>Use Reduced Precision. Reduced Precision is available on A100 via tensor cores and is supported with PyTorch operations. In general, the way to do this is via the PyTorch Automatic Mixed Precision package (AMP), as described in the mixed precision documentation. In PyTorch, users generally need to manage casting and loss scaling manually, though context managers and function decorators can provide easy tools to do this.</p> </li> <li> <p>PyTorch has a <code>JIT</code> module as well as backends to support op fusion, similar to TensorFlow's <code>tf.function</code> tools. However, PyTorch JIT capabilities are newer and may not yield performance improvements. Please see TorchScript for more information.</p> </li> </ol>"},{"location":"polaris/data-science/frameworks/pytorch/#multi-gpu-multi-node-scale-up","title":"Multi-GPU / Multi-Node Scale up","text":"<p>PyTorch is compatible with scaling up to multiple GPUs per node, and across multiple nodes. Good scaling performance has been seen up to the entire Polaris system, &gt; 2048 GPUs. Good performance with PyTorch has been seen with both DDP and Horovod. For details, please see the Horovod documentation or the Distributed Data Parallel documentation. Some Polaris-specific details that may be helpful to you:</p> <ol> <li> <p>CPU affinity can improve performance, particularly for data loading processes. In particular, we encourage users to try their scaling measurements by manually setting the CPU affinity via mpiexec, such as with <code>--cpu-bind verbose,list:0,8,16,24</code> or <code>--cpu-bind depth -d 16</code>.</p> </li> <li> <p>NCCL settings:  We have done extensive performance tests and identified the following best environment setup.</p> </li> </ol> <p><pre><code>export NCCL_NET_GDR_LEVEL=PHB\nexport NCCL_CROSS_NIC=1\nexport NCCL_COLLNET_ENABLE=1\nexport NCCL_NET=\"AWS Libfabric\"\nexport LD_LIBRARY_PATH=/soft/libraries/aws-ofi-nccl/v1.9.1-aws/lib:$LD_LIBRARY_PATH\nexport LD_LIBRARY_PATH=/soft/libraries/hwloc/lib/:$LD_LIBRARY_PATH\nexport FI_CXI_DISABLE_HOST_REGISTER=1\nexport FI_MR_CACHE_MONITOR=userfaultfd\nexport FI_CXI_DEFAULT_CQ_SIZE=131072\n</code></pre> The key here is to enable the AWS plugin (https://github.com/aws/aws-ofi-nccl). AWS OFI NCCL is a plugin that enables EC2 developers to use libfabric as a network provider while running NVIDIA's NCCL-based applications.</p> <p>This setup can lead to a 2-3x performance improvement for some communication workloads. For details, please refer to: https://github.com/argonne-lcf/alcf-nccl-tests.</p> <p>Warning</p> <p>For some applications such as Megatron-DeepSpeed, enabling the AWS plugin will cause a hang or NCCL timeout issue. If so, please disable it by: <pre><code>unset NCCL_NET_GDR_LEVEL NCCL_CROSS_NIC NCCL_COLLNET_ENABLE NCCL_NET\n</code></pre></p> <ol> <li>CUDA device setting: it works best when you limit the visible devices to only one GPU. Note that if you import <code>mpi4py</code> or <code>horovod</code>, and then do something like <code>os.environ[\"CUDA_VISIBLE_DEVICES\"] = hvd.local_rank()</code>, it may not actually work! You must set the <code>CUDA_VISIBLE_DEVICES</code> environment variable prior to doing <code>MPI.COMM_WORLD.init()</code>, which is done in <code>horovod.init()</code> as well as implicitly in <code>from mpi4py import MPI</code>. On Polaris specifically, you can use the environment variable <code>PMI_LOCAL_RANK</code> (as well as <code>PMI_LOCAL_SIZE</code>) to learn information about the node-local MPI ranks.  </li> </ol>"},{"location":"polaris/data-science/frameworks/pytorch/#deepspeed","title":"DeepSpeed","text":"<p>DeepSpeed is also available and usable on Polaris. For more information, please see the DeepSpeed documentation directly.</p>"},{"location":"polaris/data-science/frameworks/pytorch/#pytorch-dataloader-and-multi-node-horovod","title":"PyTorch <code>DataLoader</code> and multi-node Horovod","text":"<p>For best performance, it is crucial to enable multiple workers in the data loader to avoid compute and I/O overlap and concurrent loading of the dataset. This can be set by tuning the \"num_workers\" parameter in <code>DataLoader</code> (see https://pytorch.org/docs/stable/data.html). According to our experience, generally, one can set 4 or 8 for best performance. Due to the total number of CPU cores available on a node, the maximum number of workers one can choose is 16. It is always best to tune this value and find the optimal setup for your own application.</p> <p>Aside from this, one also has to make sure that the worker threads spread over different CPU cores. To do this, one has to specify the CPU binding to be <code>depth</code> and choose a depth value larger than <code>num_workers</code> through the following flag in the <code>mpiexec</code> command:</p> <pre><code>mpiexec -np $NUM_GPUS -ppn 4 --cpu-bind depth -d 16 python3 ...\n</code></pre> <p>Before 2024, enabling multiple workers would cause a fatal hang, but this has been addressed after an OS upgrade on Polaris.</p>"},{"location":"polaris/data-science/frameworks/tensorflow/","title":"TensorFlow on Polaris","text":"<p>TensorFlow is a popular, open-source deep learning framework developed and released by Google. The TensorFlow home page has more information about TensorFlow, which you can refer to. For troubleshooting on Polaris, please contact support@alcf.anl.gov.</p>"},{"location":"polaris/data-science/frameworks/tensorflow/#installation-on-polaris","title":"Installation on Polaris","text":"<p>TensorFlow is already pre-installed on Polaris, available in the <code>conda</code> module. To use it from a compute node, please do:</p> <pre><code>module load conda\nconda activate\n</code></pre> <p>Then, you can load TensorFlow in <code>python</code> as usual (below showing results from the <code>conda/2024-04-29</code> module):</p> <pre><code>&gt;&gt;&gt; import tensorflow as tf\n&gt;&gt;&gt; tf.__version__\n'2.16.1'\n&gt;&gt;&gt;\n</code></pre> <p>This installation of TensorFlow was built from source, and the CUDA libraries it uses are found via the <code>CUDA_HOME</code> environment variable (below showing results from the <code>conda/2024-04-29</code> module):</p> <pre><code>$ echo $CUDA_HOME\n/soft/compilers/cudatoolkit/cuda-12.4.1/\n</code></pre> <p>If you need to build applications that use this version of TensorFlow and CUDA, we recommend using these CUDA libraries to ensure compatibility. We periodically update the TensorFlow release, though updates will come in the form of new versions of the <code>conda</code> module.</p> <p>TensorFlow is also available through NVIDIA containers that have been translated to Apptainer containers. For more information about containers, please see the Containers documentation page.</p>"},{"location":"polaris/data-science/frameworks/tensorflow/#tensorflow-best-practices-on-polaris","title":"TensorFlow Best Practices on Polaris","text":""},{"location":"polaris/data-science/frameworks/tensorflow/#single-node-performance","title":"Single Node Performance","text":"<p>When running TensorFlow applications, we have found the following practices to be generally, if not universally, useful and encourage you to try some of these techniques to boost the performance of your own applications.</p> <ol> <li> <p>Use Reduced Precision. Reduced Precision is available on A100 via tensor cores and is supported with TensorFlow operations. In general, the way to do this is via the <code>tf.keras.mixed_precision</code> Policy, as described in the mixed precision documentation. If you use a custom training loop (and not <code>keras.Model.fit</code>), you will also need to apply loss scaling.</p> </li> <li> <p>Use TensorFlow's graph API to improve the efficiency of operations. TensorFlow is, in general, an imperative language, but with function decorators like <code>@tf.function</code>, you can trace functions in your code. Tracing replaces your Python function with a lower-level, semi-compiled TensorFlow Graph. More information about the <code>tf.function</code> interface is available here. When possible, use jit_compile, but be aware of sharp bits when using <code>tf.function</code>: Python expressions that aren't tensors are often replaced as constants in the graph, which may or may not be your intention.</p> </li> <li> <p>Use XLA compilation on your code. XLA is the Accelerated Linear Algebra library that is available in TensorFlow and critical in software like JAX. XLA will compile a <code>tf.Graph</code> object, generated with <code>tf.function</code> or similar, and perform optimizations like operation-fusion. XLA can give impressive performance boosts with almost no user changes except to set an environment variable <code>TF_XLA_FLAGS=--tf_xla_auto_jit=2</code>. If your code is complex or has dynamically sized tensors (tensors where the shape changes every iteration), XLA can be detrimental: the overhead for compiling functions can be large enough to mitigate performance improvements. XLA is particularly powerful when combined with reduced precision, yielding speedups &gt; 100% in some models.</p> </li> </ol>"},{"location":"polaris/data-science/frameworks/tensorflow/#multi-gpu-multi-node-scale-up","title":"Multi-GPU / Multi-Node Scale up","text":"<p>TensorFlow is compatible with scaling up to multiple GPUs per node and across multiple nodes. Good scaling performance has been seen up to the entire Polaris system, &gt; 2048 GPUs. Good performance with TensorFlow has been seen with Horovod in particular. For details, please see the Horovod documentation. Some Polaris-specific details that may be helpful to you:</p> <ol> <li> <p>CPU affinity can improve performance, particularly for data loading processes. In particular, we encourage users to try their scaling measurements by manually setting the CPU affinity via mpiexec, such as with <code>--cpu-bind verbose,list:0,8,16,24</code> or <code>--cpu-bind depth -d 16</code>.</p> </li> <li> <p>NCCL settings:  We have done extensive performance tests and identified the following best environment setup.</p> </li> </ol> <p><pre><code>export NCCL_NET_GDR_LEVEL=PHB\nexport NCCL_CROSS_NIC=1\nexport NCCL_COLLNET_ENABLE=1\nexport NCCL_NET=\"AWS Libfabric\"\nexport LD_LIBRARY_PATH=/soft/libraries/aws-ofi-nccl/v1.9.1-aws/lib:$LD_LIBRARY_PATH\nexport LD_LIBRARY_PATH=/soft/libraries/hwloc/lib/:$LD_LIBRARY_PATH\nexport FI_CXI_DISABLE_HOST_REGISTER=1\nexport FI_MR_CACHE_MONITOR=userfaultfd\nexport FI_CXI_DEFAULT_CQ_SIZE=131072\n</code></pre> The key here is to enable the AWS plugin (https://github.com/aws/aws-ofi-nccl). AWS OFI NCCL is a plugin that enables EC2 developers to use libfabric as a network provider while running NVIDIA's NCCL-based applications.</p> <p>This setup can lead to a 2-3x performance improvement for some communication workloads. For details, please refer to: https://github.com/argonne-lcf/alcf-nccl-tests.</p> <p>Warning</p> <p>For some applications such as Megatron-DeepSpeed, enabling the AWS plugin will cause a hang or NCCL timeout issue. If so, please disable it by: <pre><code>unset NCCL_NET_GDR_LEVEL NCCL_CROSS_NIC NCCL_COLLNET_ENABLE NCCL_NET\n</code></pre></p> <ol> <li>CUDA device setting: it works best when you limit the visible devices to only one GPU. Note that if you import <code>mpi4py</code> or <code>horovod</code>, and then do something like <code>os.environ[\"CUDA_VISIBLE_DEVICES\"] = hvd.local_rank()</code>, it may not actually work! You must set the <code>CUDA_VISIBLE_DEVICES</code> environment variable prior to doing <code>MPI.COMM_WORLD.init()</code>, which is done in <code>horovod.init()</code> as well as implicitly in <code>from mpi4py import MPI</code>. On Polaris specifically, you can use the environment variable <code>PMI_LOCAL_RANK</code> (as well as <code>PMI_LOCAL_SIZE</code>) to learn information about the node-local MPI ranks.  </li> </ol>"},{"location":"polaris/data-science/frameworks/tensorflow/#tensorflow-dataloaders","title":"TensorFlow Dataloaders","text":"<p>It is crucial to enable multiple workers in the data pipeline for best performance. For details, please refer to TensorFlow Data Performance Guide.</p>"},{"location":"polaris/debugging-tools/CUDA-GDB/","title":"CUDA-GDB","text":""},{"location":"polaris/debugging-tools/CUDA-GDB/#references","title":"References","text":"<p>NVIDIA CUDA-GDB Documentation </p>"},{"location":"polaris/debugging-tools/CUDA-GDB/#introduction","title":"Introduction","text":"<p>CUDA-GDB is the NVIDIA tool for debugging CUDA applications running on Polaris. CUDA-GDB is an extension to GDB, the GNU Project debugger. The tool provides developers with a mechanism for debugging CUDA applications running on actual hardware. This enables developers to debug applications without the potential variations introduced by simulation and emulation environments.</p>"},{"location":"polaris/debugging-tools/CUDA-GDB/#step-by-step-guide","title":"Step-by-step guide","text":""},{"location":"polaris/debugging-tools/CUDA-GDB/#debug-compilation","title":"Debug Compilation","text":"<p>NVCC, the NVIDIA CUDA compiler driver, provides a mechanism for generating the debugging information necessary for CUDA-GDB to work properly. The <code>-g -G</code> option pair must be passed to NVCC when an application is compiled for ease of debugging with CUDA-GDB; for example, <pre><code>nvcc -g -G foo.cu -o foo\n</code></pre> Using this line to compile the CUDA application <code>foo.cu</code>: * Forces <code>-O0</code> compilation, with the exception of very limited dead-code eliminations and register-spilling optimizations. * Makes the compiler include debug information in the executable.</p>"},{"location":"polaris/debugging-tools/CUDA-GDB/#running-cuda-gdb-on-polaris-compute-nodes","title":"Running CUDA-GDB on Polaris compute nodes","text":"<p>Start an interactive job mode on Polaris as follows: <pre><code>$ qsub -I -l select=1 -l walltime=1:00:00\n\n$ cuda-gdb --version\nNVIDIA (R) CUDA Debugger\n11.4 release\nPortions Copyright (C) 2007-2021 NVIDIA Corporation\nGNU gdb (GDB) 10.1\nCopyright (C) 2020 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.\n\n$ cuda-gdb foo\n</code></pre></p>"},{"location":"polaris/debugging-tools/CUDA-GDB/#a-quick-example-with-a-stream-benchmark-on-a-polaris-compute-node","title":"A quick example with a stream benchmark on a Polaris compute node","text":"<pre><code>jkwack@polaris-login-02:~&gt; qsub -I -l select=1 -l walltime=1:00:00\nqsub: waiting for job 308834.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov to start\nqsub: job 308834.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov ready\n\nCurrently Loaded Modules:\n  1) craype-x86-rome          4) perftools-base/22.05.0   7) cray-dsmml/0.2.2   10) cray-pmi-lib/6.0.17  13) PrgEnv-nvhpc/8.3.3\n  2) libfabric/1.11.0.4.125   5) nvhpc/21.9               8) cray-mpich/8.1.16  11) cray-pals/1.1.7      14) craype-accel-nvidia80\n  3) craype-network-ofi       6) craype/2.7.15            9) cray-pmi/6.1.2     12) cray-libpals/1.1.7\n\njkwack@x3008c0s13b1n0:~/BabelStream/build_polaris_debug&gt; nvcc -g -G -c ../src/cuda/CUDAStream.cu  -I ../src/\n\njkwack@x3008c0s13b1n0:~/BabelStream/build_polaris_debug&gt; nvcc -g -G -c ../src/main.cpp -DCUDA -I ../src/cuda/ -I ../src/\n\njkwack@x3008c0s13b1n0:~/BabelStream/build_polaris_debug&gt; nvcc -g -G main.o CUDAStream.o -o cuda-stream-debug\n\njkwack@x3008c0s13b1n0:~/BabelStream/build_polaris_debug&gt; ./cuda-stream-debug \nBabelStream\nVersion: 4.0\nImplementation: CUDA\nRunning kernels 100 times\nPrecision: double\nArray size: 268.4 MB (=0.3 GB)\nTotal size: 805.3 MB (=0.8 GB)\nUsing CUDA device NVIDIA A100-SXM4-40GB\nDriver: 11040\nFunction    MBytes/sec  Min (sec)   Max         Average     \nCopy        1313940.694 0.00041     0.00047     0.00047     \nMul         1302000.791 0.00041     0.00048     0.00047     \nAdd         1296217.720 0.00062     0.00070     0.00069     \nTriad       1296027.887 0.00062     0.00070     0.00069     \nDot         823405.227  0.00065     0.00076     0.00075     \n\njkwack@x3008c0s13b1n0:~/BabelStream/build_polaris_debug&gt; cuda-gdb ./cuda-stream-debug \nNVIDIA (R) CUDA Debugger\n11.4 release\nPortions Copyright (C) 2007-2021 NVIDIA Corporation\nGNU gdb (GDB) 10.1\nCopyright (C) 2020 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.\nType \"show copying\" and \"show warranty\" for details.\nThis GDB was configured as \"x86_64-pc-linux-gnu\".\nType \"show configuration\" for configuration details.\nFor bug reporting instructions, please see:\n&lt;https://www.gnu.org/software/gdb/bugs/&gt;.\nFind the GDB manual and other documentation resources online at:\n    &lt;http://www.gnu.org/software/gdb/documentation/&gt;.\n\nFor help, type \"help\".\nType \"apropos word\" to search for commands related to \"word\"...\nReading symbols from ./cuda-stream-debug...\n(cuda-gdb) b CUDAStream.cu:203\nBreakpoint 1 at 0x412598: CUDAStream.cu:203. (2 locations)\n(cuda-gdb) r      \nStarting program: /home/jkwack/BabelStream/build_polaris_debug/cuda-stream-debug \n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib64/libthread_db.so.1\".\nBabelStream\nVersion: 4.0\nImplementation: CUDA\nRunning kernels 100 times\nPrecision: double\nArray size: 268.4 MB (=0.3 GB)\nTotal size: 805.3 MB (=0.8 GB)\n[Detaching after fork from child process 58459]\n[New Thread 0x15554c6bb000 (LWP 58475)]\nUsing CUDA device NVIDIA A100-SXM4-40GB\nDriver: 11040\n[New Thread 0x15554c4ba000 (LWP 58476)]\n[Switching focus to CUDA kernel 0, grid 5, block (0,0,0), thread (0,0,0), device 0, sm 0, warp 3, lane 0]\n\nThread 1 \"cuda-stream-deb\" hit Breakpoint 1, triad_kernel&lt;double&gt;&lt;&lt;&lt;(32768,1,1),(1024,1,1)&gt;&gt;&gt; (a=0x155506000000, b=0x1554f6000000, c=0x1554e6000000)\n    at ../src/cuda/CUDAStream.cu:203\n203   a[i] = b[i] + scalar * c[i];\n(cuda-gdb) c\nContinuing.\n[Switching focus to CUDA kernel 0, grid 5, block (1,0,0), thread (0,0,0), device 0, sm 0, warp 32, lane 0]\n\nThread 1 \"cuda-stream-deb\" hit Breakpoint 1, triad_kernel&lt;double&gt;&lt;&lt;&lt;(32768,1,1),(1024,1,1)&gt;&gt;&gt; (a=0x155506000000, b=0x1554f6000000, c=0x1554e6000000)\n    at ../src/cuda/CUDAStream.cu:203\n203   a[i] = b[i] + scalar * c[i];\n(cuda-gdb) info locals\ni = 1024\n(cuda-gdb) p b[i]\n$1 = 0.040000000000000008\n(cuda-gdb) p scalar\n$2 = 0.40000000000000002\n(cuda-gdb) p c[i]\n$3 = 0.14000000000000001\n(cuda-gdb) d 1\n(cuda-gdb) c\nContinuing.\nFunction    MBytes/sec  Min (sec)   Max         Average     \nCopy        1314941.553 0.00041     0.00041     0.00041     \nMul         1301022.680 0.00041     0.00042     0.00041     \nAdd         1293858.147 0.00062     0.00063     0.00063     \nTriad       1297681.929 0.00062     0.00063     0.00062     \nDot         828446.963  0.00065     0.00066     0.00065     \n[Thread 0x15554c4ba000 (LWP 58476) exited]\n[Thread 0x15554c6bb000 (LWP 58475) exited]\n[Inferior 1 (process 58454) exited normally]\n(cuda-gdb) q\n\njkwack@x3008c0s13b1n0:~/BabelStream/build_polaris_debug&gt; \n</code></pre>"},{"location":"polaris/hardware-overview/machine-overview/","title":"Polaris Machine Overview","text":"<p>Polaris is a 560-node HPE Apollo 6500 Gen 10+ based system. Each node has a single 2.8 GHz AMD EPYC Milan 7543P 32-core CPU with 512 GB of DDR4 RAM, four NVIDIA A100 GPUs connected via NVLink, a pair of local 1.6TB SSDs in RAID0 for user use, and a pair of Slingshot 11 network adapters. There are two nodes per chassis, seven chassis per rack, and 40 racks for a total of 560 nodes. More detailed specifications are as follows:</p>"},{"location":"polaris/hardware-overview/machine-overview/#polaris-compute-nodes","title":"Polaris Compute Nodes","text":"POLARIS COMPUTE DESCRIPTION PER NODE AGGREGATE Processor (Note 1) 2.8 GHz 7543P 1 560 Cores/Threads AMD Zen 3 (Milan) 32/64 17,920/35,840 RAM (Note 2) DDR4 512 GiB 280 TiB GPUs NVIDIA A100 4 2,240 Local SSD 1.6 TB 2/3.2 TB 1,120/1.8 PB <p>Note 1: 256 MB shared L3 cache, 512 KB L2 cache per core, 32 KB L1 cache per core Note 2: 8 memory channels rated at 204.8 GiB/s</p>"},{"location":"polaris/hardware-overview/machine-overview/#polaris-a100-gpu-information","title":"Polaris A100 GPU Information","text":"DESCRIPTION A100 PCIe A100 HGX (Polaris) GPU Memory 40 GiB HBM2 160 GiB HBM2 GPU Memory BW 1.6 TB/s 6.4 TB/s Interconnect PCIe Gen4 64 GB/s NVLink 600 GB/s FP 64 9.7 TF 38.8 TF FP64 Tensor Core 19.5 TF 78 TF FP 32 19.5 TF 78 TF BF16 Tensor Core 312 TF 1.3 PF FP16 Tensor Core 312 TF 1.3 PF INT8 Tensor Core 624 TOPS 2,496 TOPS Max TDP Power 250 W 400 W"},{"location":"polaris/hardware-overview/machine-overview/#polaris-device-affinity-information","title":"Polaris Device Affinity Information","text":"CPU Affinity NUMA Affinity GPU0 GPU1 GPU2 GPU3 mlx5_0 mlx5_1 24-31,56-63 3 GPU0 X NV4 NV4 NV4 SYS SYS 16-23,48-55 2 GPU1 NV4 X NV4 NV4 SYS PHB 8-15,40-47 1 GPU2 NV4 NV4 X NV4 SYS SYS 0-7,32-39 0 GPU3 NV4 NV4 NV4 X PHB SYS mlx5_0 SYS SYS SYS PHB X SYS mlx5_1 SYS PHB SYS SYS SYS X"},{"location":"polaris/hardware-overview/machine-overview/#legend","title":"Legend:","text":"<p>X = Self SYS = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI) NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node PHB = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU) PXB = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge) PIX = Connection traversing at most a single PCIe bridge NV# = Connection traversing a bonded set of # NVLinks  </p> <p>Links to detailed NVIDIA A100 documentation: - NVIDIA A100 Tensor Core GPU Architecture - NVIDIA Ampere Architecture In-Depth</p>"},{"location":"polaris/hardware-overview/machine-overview/#login-nodes","title":"Login Nodes","text":"<p>There are four login nodes available to users for editing code, building code, submitting/monitoring jobs, checking usage (<code>sbank</code>), etc. Their full hostnames are <code>polaris-login-N.hsn.cm.polaris.alcf.anl.gov</code> for <code>N</code> equal to <code>01</code> through <code>04</code>; there are an additional two login nodes that are not user-accessible, which are used for running services such as JupyterHub. The various compilers and libraries are present on the logins, so most users should be able to build their code. However, if your build requires the physical presence of the GPU, you will need to build on a compute node.</p> <p>All users share the same login nodes, so please be courteous and respectful of your fellow users. For example, please do not run computationally or I/O intensive pre- or post-processing on the logins and keep the parallelism of your builds to a reasonable level.</p> POLARIS LOGIN DESCRIPTION PER NODE AGGREGATE Processor (Note 1) 2.0 GHz 7713 2 12 Cores/Threads AMD Zen 3 (Milan) 128/256 768/1,536 RAM (Note 2) DDR4 512 GiB 3 TiB GPUs (Note 3) No GPUs 0 0 Local SSD None 0 0 <p>Note 1: 256 MB shared L3 cache, 512 KB L2 cache per core, 32 KB L1 cache per core Note 2: 8 memory channels rated at 204.8 GiB/s per socket Note 3: If your build requires the physical presence of a GPU, you will need to build on a compute node.</p>"},{"location":"polaris/hardware-overview/machine-overview/#gateway-nodes","title":"Gateway Nodes","text":"<p>There are 50 gateway nodes. These nodes are not user-accessible but are used transparently for access to the storage systems. Each node has a single 200 Gbps HDR IB card for access to the storage area network. This gives a theoretical peak bandwidth of 1,250 GB/s, which is approximately the aggregate bandwidth of the global file systems (1,300 GB/s).</p>"},{"location":"polaris/hardware-overview/machine-overview/#storage","title":"Storage","text":"<p>Polaris has access to the ALCF global file systems. Details on storage can be found here.</p>"},{"location":"polaris/performance-tools/NVIDIA-Nsight/","title":"NVIDIA Nsight tools","text":""},{"location":"polaris/performance-tools/NVIDIA-Nsight/#references","title":"References","text":"<p>NVIDIA Nsight Systems Documentation NVIDIA Nsight Compute Documentation</p>"},{"location":"polaris/performance-tools/NVIDIA-Nsight/#introduction","title":"Introduction","text":"<p>NVIDIA\u00ae Nsight\u2122 Systems provides developers a system-wide visualization of an applications performance. Developers can optimize bottlenecks to scale efficiently across any number or size of CPUs and GPUs on Polaris. For further optimizations to compute kernels developers should use Nsight Compute.</p> <p>The NVIDIA Nsight Compute is an interactive kernel profiler for CUDA applications. It provides detailed performance metrics and API debugging via a user interface and command line tool. </p> <p>In addition, the baseline feature of this tool allows users to compare results within the tool. NVIDIA Nsight Compute provides a customizable and data-driven user interface,  metric collection, and can be extended with analysis scripts for post-processing results.</p>"},{"location":"polaris/performance-tools/NVIDIA-Nsight/#step-by-step-guide","title":"Step-by-step guide","text":""},{"location":"polaris/performance-tools/NVIDIA-Nsight/#common-part-on-polaris","title":"Common part on Polaris","text":"<p>Build your application for Polaris, and then submit your job script to Polaris or start an interactive job mode on Polaris as follows: <pre><code>$ qsub -I -l select=1 -l walltime=1:00:00 -l filesystems=home:eagle -q debug -A &lt;project-name&gt;\n\n$ module li\n\nCurrently Loaded Modules:\n  1) nvhpc/23.9          5) cray-pmi/6.1.13      9) PrgEnv-nvhpc/8.5.0      13) darshan/3.4.4\n  2) craype/2.7.30       6) cray-pals/1.3.4     10) libfabric/1.15.2.0\n  3) cray-dsmml/0.2.2    7) cray-libpals/1.3.4  11) craype-network-ofi\n  4) cray-mpich/8.1.28   8) craype-x86-milan    12) perftools-base/23.12.0\n\n$ nsys --version\nNVIDIA Nsight Systems version 2023.3.1.92-233133147223v0\n\n$ ncu --version\nNVIDIA (R) Nsight Compute Command Line Profiler\nCopyright (c) 2018-2023 NVIDIA Corporation\nVersion 2023.2.1.0 (build 33050884) (public-release)\nNVIDIA Nsight Systems version 2022.4.2.1-df9881f\n</code></pre></p>"},{"location":"polaris/performance-tools/NVIDIA-Nsight/#nsight-systems","title":"Nsight Systems","text":"<p>Run your application with Nsight Systems as follows: <pre><code>$ nsys profile -o {output_filename} --stats=true ./{your_application}\n</code></pre></p> <p>Run your application on multiple nodes (e.g., 2 nodes) with Nsight Systems as follows: <pre><code>$ mpirun -n 8 --ppn 4 --env TMPDIR=/home/{user ID}/ nsys profile -o {output_filename}_%q{PMI_RANK} --stats=true ./{your_application}\n</code></pre></p>"},{"location":"polaris/performance-tools/NVIDIA-Nsight/#nsight-compute","title":"Nsight Compute","text":"<p>Run your application with Nsight Compute. <pre><code>$ ncu --set detailed -k {kernel_name} -o {output_filename} ./{your_application}\n</code></pre></p> <p>Remark: Without -o option, Nsight Compute provides performance data as a standard output</p>"},{"location":"polaris/performance-tools/NVIDIA-Nsight/#post-processing-the-profiled-data","title":"Post-processing the profiled data","text":""},{"location":"polaris/performance-tools/NVIDIA-Nsight/#post-processing-via-cli","title":"Post-processing via CLI","text":"<pre><code>$ nsys stats {output_filename}.qdrep\n$ ncu -i {output_filename}.ncu-rep  \n</code></pre>"},{"location":"polaris/performance-tools/NVIDIA-Nsight/#post-processing-on-your-local-system-via-gui","title":"Post-processing on your local system via GUI","text":"<ul> <li>Install NVIDIA Nsight Systems and NVIDIA Nsight Compute after downloading both of them from the  NVIDIA Developer Zone.  Remark: Local client version should be the same as or newer than NVIDIA Nsight tools on Polaris. </li> <li>Download nsys output files (i.e., ending with .qdrep and . sqlite) to your local system, and then open them with NVIDIA Nsight Systems on your local system.  </li> <li>Download ncu output files (i.e., ending with .ncu-rep) to your local system, and then open them with NVIDIA Nsight Compute on your local system.  </li> </ul>"},{"location":"polaris/performance-tools/NVIDIA-Nsight/#more-options-for-performance-analysis-with-nsight-systems-and-nsight-compute","title":"More options for performance analysis with Nsight Systems and Nsight Compute","text":"<pre><code>$ nsys --help\n$ ncu --help\n</code></pre>"},{"location":"polaris/performance-tools/NVIDIA-Nsight/#a-quick-example","title":"A quick example","text":""},{"location":"polaris/performance-tools/NVIDIA-Nsight/#nsight-systems_1","title":"Nsight Systems","text":""},{"location":"polaris/performance-tools/NVIDIA-Nsight/#running-a-stream-benchmark-with-nsight-systems","title":"Running a stream benchmark with Nsight Systems","text":"<pre><code>jkwack@x3008c0s13b1n0:~/BabelStream/build_polaris&gt; nsys profile -o JKreport-nsys-BableStream --stats=true ./cuda-stream\nWarning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\nCollecting data...\nBabelStream\nVersion: 4.0\nImplementation: CUDA\nRunning kernels 100 times\nPrecision: double\nArray size: 268.4 MB (=0.3 GB)\nTotal size: 805.3 MB (=0.8 GB)\nUsing CUDA device NVIDIA A100-SXM4-40GB\nDriver: 11040\nFunction    MBytes/sec  Min (sec)   Max         Average     \nCopy        1368294.603 0.00039     0.00044     0.00039     \nMul         1334324.779 0.00040     0.00051     0.00041     \nAdd         1358476.737 0.00059     0.00060     0.00059     \nTriad       1366095.332 0.00059     0.00059     0.00059     \nDot         1190200.569 0.00045     0.00047     0.00046     \nProcessing events...\nSaving temporary \"/var/tmp/pbs.308834.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov/nsys-report-f594-c524-6b4c-300a.qdstrm\" file to disk...\n\nCreating final output files...\nProcessing [===============================================================100%]\nSaved report file to \"/var/tmp/pbs.308834.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov/nsys-report-f594-c524-6b4c-300a.qdrep\"\nExporting 7675 events: [===================================================100%]\n\nExported successfully to\n/var/tmp/pbs.308834.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov/nsys-report-f594-c524-6b4c-300a.sqlite\n\n\nCUDA API Statistics:\n\n Time(%)  Total Time (ns)  Num Calls  Average (ns)  Minimum (ns)  Maximum (ns)  StdDev (ns)           Name         \n -------  ---------------  ---------  ------------  ------------  ------------  ------------  ---------------------\n    41.5      197,225,738        401     491,834.8       386,695       592,751      96,647.5  cudaDeviceSynchronize\n    35.4      168,294,004          4  42,073,501.0       144,211   167,547,885  83,649,622.0  cudaMalloc           \n    22.5      106,822,589        103   1,037,112.5       446,617    20,588,840   3,380,727.4  cudaMemcpy           \n     0.4        1,823,597        501       3,639.9         3,166        24,125       1,228.9  cudaLaunchKernel     \n     0.2        1,166,186          4     291,546.5       130,595       431,599     123,479.8  cudaFree             \n\n\n\nCUDA Kernel Statistics:\n\n Time(%)  Total Time (ns)  Instances  Average (ns)  Minimum (ns)  Maximum (ns)  StdDev (ns)                             Name                           \n -------  ---------------  ---------  ------------  ------------  ------------  -----------  ----------------------------------------------------------\n    24.5       58,415,138        100     584,151.4       582,522       585,817        543.0  void add_kernel&lt;double&gt;(const T1 *, const T1 *, T1 *)     \n    24.4       58,080,329        100     580,803.3       579,802       582,586        520.5  void triad_kernel&lt;double&gt;(T1 *, const T1 *, const T1 *)   \n    18.3       43,602,345        100     436,023.5       430,555       445,979      2,619.5  void dot_kernel&lt;double&gt;(const T1 *, const T1 *, T1 *, int)\n    16.5       39,402,677        100     394,026.8       392,444       395,708        611.5  void mul_kernel&lt;double&gt;(T1 *, const T1 *)                 \n    16.1       38,393,119        100     383,931.2       382,556       396,892      1,434.1  void copy_kernel&lt;double&gt;(const T1 *, T1 *)                \n     0.2          523,355          1     523,355.0       523,355       523,355          0.0  void init_kernel&lt;double&gt;(T1 *, T1 *, T1 *, T1, T1, T1)    \n\n\n\nCUDA Memory Operation Statistics (by time):\n\n Time(%)  Total Time (ns)  Count  Average (ns)  Minimum (ns)  Maximum (ns)  StdDev (ns)      Operation     \n -------  ---------------  -----  ------------  ------------  ------------  -----------  ------------------\n   100.0       61,323,171    103     595,370.6         2,399    20,470,146  3,439,982.0  [CUDA memcpy DtoH]\n\n\n\nCUDA Memory Operation Statistics (by size):\n\n Total (MB)  Count  Average (MB)  Minimum (MB)  Maximum (MB)  StdDev (MB)      Operation     \n ----------  -----  ------------  ------------  ------------  -----------  ------------------\n    805.511    103         7.820         0.002       268.435       45.361  [CUDA memcpy DtoH]\n\n\n\nOperating System Runtime API Statistics:\n\n Time(%)  Total Time (ns)  Num Calls  Average (ns)  Minimum (ns)  Maximum (ns)  StdDev (ns)        Name     \n -------  ---------------  ---------  ------------  ------------  ------------  ------------  --------------\n    85.9      600,896,697         20  30,044,834.9         3,477   100,141,768  42,475,064.1  poll          \n    13.5       94,610,402      1,201      78,776.4         1,002    11,348,375     402,562.6  ioctl         \n     0.2        1,374,312         79      17,396.4         3,486       434,715      48,015.2  mmap64        \n     0.1          877,705         51      17,209.9         1,031       748,723     104,491.6  fopen         \n     0.1          741,969         12      61,830.8        17,272       256,852      64,706.5  sem_timedwait \n     0.1          529,563        120       4,413.0         1,292        20,579       2,134.3  open64        \n     0.0          251,602          4      62,900.5        57,337        72,126       6,412.6  pthread_create\n     0.0           93,461         18       5,192.3         1,011        19,386       4,401.0  mmap          \n     0.0           37,621         11       3,420.1         1,302        11,672       2,867.6  munmap        \n     0.0           35,735          9       3,970.6         1,723         6,251       1,477.2  fgetc         \n     0.0           33,533          1      33,533.0        33,533        33,533           0.0  fgets         \n     0.0           26,832         13       2,064.0         1,452         3,366         542.6  write         \n     0.0           21,341          5       4,268.2         1,213         9,738       3,378.3  putc          \n     0.0           20,838          6       3,473.0         1,763         6,853       1,801.1  open          \n     0.0           17,016         10       1,701.6         1,523         1,834          96.9  read          \n     0.0           11,430          8       1,428.8         1,082         1,583         151.9  fclose        \n     0.0            6,202          1       6,202.0         6,202         6,202           0.0  pipe2         \n     0.0            5,961          2       2,980.5         2,254         3,707       1,027.4  socket        \n     0.0            5,670          2       2,835.0         2,795         2,875          56.6  fwrite        \n     0.0            5,481          1       5,481.0         5,481         5,481           0.0  connect       \n     0.0            5,279          2       2,639.5         1,743         3,536       1,267.8  fread         \n     0.0            1,082          1       1,082.0         1,082         1,082           0.0  bind          \n\nReport file moved to \"/home/jkwack/BabelStream/build_polaris/JKreport-nsys-BableStream.qdrep\"\nReport file moved to \"/home/jkwack/BabelStream/build_polaris/JKreport-nsys-BableStream.sqlite\"\n</code></pre>"},{"location":"polaris/performance-tools/NVIDIA-Nsight/#reviewing-the-nsight-systems-data-via-gui","title":"Reviewing the Nsight Systems data via GUI","text":""},{"location":"polaris/performance-tools/NVIDIA-Nsight/#nsight-compute_1","title":"Nsight Compute","text":""},{"location":"polaris/performance-tools/NVIDIA-Nsight/#running-a-stream-benchmark-with-nsight-compute-for-triad_kernel","title":"Running a stream benchmark with Nsight Compute for triad_kernel","text":"<pre><code>jkwack@x3008c0s13b1n0:~/BabelStream/build_polaris&gt; ncu --set detailed -k triad_kernel -o JKreport-ncu_detailed-triad_kernel-BableStream ./cuda-stream\nBabelStream\nVersion: 4.0\nImplementation: CUDA\nRunning kernels 100 times\nPrecision: double\nArray size: 268.4 MB (=0.3 GB)\nTotal size: 805.3 MB (=0.8 GB)\n==PROF== Connected to process 56600 (/home/jkwack/BabelStream/build_polaris/cuda-stream)\nUsing CUDA device NVIDIA A100-SXM4-40GB\nDriver: 11040\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\nFunction    MBytes/sec  Min (sec)   Max         Average     \nCopy        1331076.105 0.00040     0.00042     0.00041     \nMul         1304696.608 0.00041     0.00043     0.00042     \nAdd         1322600.587 0.00061     0.00062     0.00061     \nTriad       1327.700    0.60654     0.62352     0.61106     \nDot         850376.762  0.00063     0.00070     0.00065     \n==PROF== Disconnected from process 56600\n==PROF== Report: /home/jkwack/BabelStream/build_polaris/JKreport-ncu_detailed-triad_kernel-BableStream.ncu-rep\n</code></pre>"},{"location":"polaris/performance-tools/NVIDIA-Nsight/#reviewing-the-nsight-compute-data-via-gui","title":"Reviewing the Nsight Compute data via GUI","text":""},{"location":"polaris/programming-models/kokkos-polaris/","title":"Kokkos","text":""},{"location":"polaris/programming-models/kokkos-polaris/#kokkos_1","title":"Kokkos","text":"<p>Kokkos Core implements a programming model in C++ for writing performance-portable applications targeting all major HPC platforms. For that purpose, it provides abstractions for both parallel execution of code and data management. Kokkos is designed to target complex node architectures with N-level memory hierarchies and multiple types of execution resources. It currently can use Serial and OpenMP (threads) for CPU execution spaces (\"backends\") and CUDA, HIP, SYCL, and OpenMPTarget for GPU execution spaces. By convention, Kokkos only allows one GPU backend at a time.</p>"},{"location":"polaris/programming-models/kokkos-polaris/#kokkos-documentation","title":"Kokkos Documentation","text":"<ul> <li>Kokkos-core Wiki</li> <li>Kokkos GitHub</li> </ul>"},{"location":"polaris/programming-models/kokkos-polaris/#kokkos-on-polaris","title":"Kokkos on Polaris","text":"<p>Following the Polaris upgrade to HPCM 1.10, the module setup to use the prebuilt Kokkos changed.</p> <p>The prebuilt Kokkos on Polaris includes three backends: Serial and OpenMP for CPU execution and CUDA for GPU execution. To use it, run:</p> <pre><code>module load craype-x86-milan\nmodule load craype-accel-nvidia80\nmodule swap PrgEnv-nvhpc PrgEnv-gnu\nmodule use /soft/modulefiles\nmodule load cuda-PrgEnv-nvidia/12.2.91\nmodule load kokkos\n</code></pre> <p>This sets the following environment variables, some of which are used by <code>cmake</code>:</p> <ul> <li><code>KOKKOS_HOME</code> - path to the <code>lib64/</code>, <code>include/</code> files installed</li> <li><code>LIBRARY_PATH</code> - prepends <code>$KOKKOS_HOME/lib64</code> to this variable used by <code>cmake</code></li> <li><code>CPATH</code> - prepends <code>$KOKKOS_HOME/include</code> to this variable used by <code>cmake</code></li> <li><code>LD_LIBRARY_PATH</code> - prepends <code>$KOKKOS_HOME/lib64</code> to this variable</li> </ul>"},{"location":"polaris/programming-models/kokkos-polaris/#building-a-kokkos-application-using-cmake","title":"Building a Kokkos Application Using <code>cmake</code>","text":"<p>Add these lines to <code>CMakeLists.txt</code>:</p> <pre><code>find_package(Kokkos REQUIRED)\ntarget_link_libraries(myTarget Kokkos::kokkoscore)\n</code></pre> <p>Here is a simple example <code>CMakeLists.txt</code> to compile an example program:</p> <pre><code>cmake_minimum_required(VERSION 3.22)\nproject(buildExample)\nfind_package(Kokkos REQUIRED)\n\nset(buildExample_SOURCE_DIR \".\")\n\nset(top_SRCS\n  ${buildExample_SOURCE_DIR}/example1.cpp)\n\nset(SOURCE_FILES ${top_SRCS})\n\nadd_executable(example1_sycl_aot ${SOURCE_FILES})\ntarget_link_libraries(example1_sycl_aot Kokkos::kokkoscore)\ntarget_include_directories(example1_sycl_aot PUBLIC ${buildExample_SOURCE_DIR})\n</code></pre> <p>Configure and build it like this:</p> <pre><code>mkdir build\ncd build\ncmake -DCMAKE_CXX_COMPILER=CC -DCMAKE_C_COMPILER=cc ..\nmake\n</code></pre>"},{"location":"polaris/programming-models/kokkos-polaris/#building-a-kokkos-application-using-make","title":"Building a Kokkos Application Using <code>make</code>","text":"<p>Here's an example <code>Makefile</code>:</p> <pre><code># KOKKOS_HOME set via:\n#   module load kokkos\n\n# You can look at the first lines of $KOKKOS_HOME/KokkosConfigCommon.cmake to\n# see the flags used in cmake configuration of the kokkos library build. The\n# default Kokkos module on Polaris was built with PrgEnv-nvhpc and includes\n# Serial, OpenMP (threads), and CUDA backends. So you should have that\n# environment module loaded and include compiler flags for CUDA and OpenMP:\n\n# Cray MPI wrapper for C++ and C compilers:\nCXX=CC\nCC=cc\n\nCPPFLAGS=-cuda -fopenmp\nLDFLAGS=\n\nLDFLAGS=$(CPPFLAGS) $(LDFLAGS)\nLDLIBS=-L$(KOKKOS_HOME)/lib64 -lkokkoscore -lkokkossimd -lpthread\n\nSRCS=example1.cpp\nOBJS=$(subst .cpp,.o,$(SRCS))\n\nall: example1_polaris\n\nexample1_polaris: $(OBJS)\n        $(CXX) $(LDFLAGS) -o example1_polaris $(OBJS) $(LDLIBS)\n\nexample1.o: example1.cpp\n\nclean:\n        rm -f $(OBJS)\n\ndistclean: clean\n        rm -f example1_polaris\n</code></pre>"},{"location":"polaris/programming-models/kokkos-polaris/#configuring-your-own-kokkos-build-on-polaris","title":"Configuring Your Own Kokkos Build on Polaris","text":"<p>Here are recommended environment settings and configuration to build your own Kokkos libraries on Polaris:</p>"},{"location":"polaris/programming-models/kokkos-polaris/#environment","title":"Environment","text":"<p>To match what was done in the centrally-built Kokkos associated with the modules discussed above, use the programming environment <code>PrgEnv-gnu</code>, and use the Cray wrapper <code>CC</code> as the C++ compiler. You'll also need to explicitly load the CUDA toolkit version 12.2.91 as shown:</p> <pre><code>module restore\nmodule load craype-x86-milan\nmodule load craype-accel-nvidia80\nmodule swap PrgEnv-nvhpc PrgEnv-gnu\nmodule use /soft/modulefiles\nmodule load cuda-PrgEnv-nvidia/12.2.91\nmodule load spack-pe-base cmake\n</code></pre>"},{"location":"polaris/programming-models/kokkos-polaris/#cmake-configuration","title":"CMake Configuration","text":"<p>This example builds three backends: OpenMP, Serial, and CUDA.</p> <pre><code>git clone git@github.com:kokkos/kokkos.git\ncd kokkos\nmkdir build\ncd build\n\ncmake\\\n -DCMAKE_BUILD_TYPE=RelWithDebInfo\\\n -DCMAKE_INSTALL_PREFIX=\"./install\"\\\n -DCMAKE_CXX_COMPILER=CC\\\n -DKokkos_ENABLE_OPENMP=ON\\\n -DKokkos_ENABLE_SERIAL=ON\\\n -DKokkos_ARCH_ZEN3=ON\\\n -DKokkos_ARCH_AMPERE80=ON\\\n -DKokkos_ENABLE_CUDA=ON\\\n -DKokkos_ENABLE_AGGRESSIVE_VECTORIZATION=ON\\\n -DKokkos_ENABLE_TESTS=OFF\\\n -DBUILD_TESTING=OFF\\\n -DKokkos_ENABLE_CUDA_LAMBDA=ON\\\n -DKokkos_ENABLE_IMPL_DESUL_ATOMICS=OFF\\\n -DCMAKE_CXX_STANDARD=17\\\n -DCMAKE_EXE_LINKER_FLAGS=-no-gcc-rpath\\\n ..\n\nmake -j8 install\n</code></pre>"},{"location":"polaris/programming-models/openmp-polaris/","title":"OpenMP","text":""},{"location":"polaris/programming-models/openmp-polaris/#overview","title":"Overview","text":"<p>The OpenMP API is an open standard for parallel programming. The specification document can be found here: OpenMP Specification. The specification describes directives, runtime routines, and environment variables that allow an application developer to express parallelism (e.g., shared memory multiprocessing and device offloading). Many compiler vendors provide implementations of the OpenMP specification (OpenMP Specifications).</p>"},{"location":"polaris/programming-models/openmp-polaris/#setting-the-environment-to-use-openmp-on-polaris","title":"Setting the environment to use OpenMP on Polaris","text":"<p>Many of the programming environments available on Polaris have OpenMP support.</p> Module OpenMP CPU Support? OpenMP GPU Support? PrgEnv-nvhpc Yes Yes llvm Yes Yes PrgEnv-gnu Yes No PrgEnv-cray Yes Yes* <p>*Currently, PrgEnv-cray is not recommended for OpenMP offload.</p> <p>By default, the PrgEnv-nvhpc module is loaded. To switch to other modules, you can use <code>module switch</code>.</p>"},{"location":"polaris/programming-models/openmp-polaris/#using-prgenv-nvhpc","title":"Using PrgEnv-nvhpc","text":"<p>This is loaded by default, so there's no need to load additional modules. You can confirm that it is loaded by running <code>module list</code> to check that PrgEnv-nvhpc is in the list.</p>"},{"location":"polaris/programming-models/openmp-polaris/#using-llvm","title":"Using LLVM","text":"<p>To use the LLVM module, load the following:</p> <pre><code>module use /soft/modulefiles\nmodule load mpiwrappers/cray-mpich-llvm\nmodule load cudatoolkit-standalone\n</code></pre> <p>See the LLVM compiling page here for more information.</p>"},{"location":"polaris/programming-models/openmp-polaris/#using-prgenv-gnu","title":"Using PrgEnv-gnu","text":"<p>To switch from PrgEnv-nvhpc to PrgEnv-gnu, you can run:</p> <pre><code>module switch PrgEnv-nvhpc PrgEnv-gnu\n</code></pre> <p>The gcc/gfortran on Polaris was not built with GPU support.</p>"},{"location":"polaris/programming-models/openmp-polaris/#using-prgenv-cray","title":"Using PrgEnv-cray","text":"<p>To switch from PrgEnv-nvhpc to PrgEnv-cray, you can run:</p> <pre><code>module switch PrgEnv-nvhpc PrgEnv-cray\n</code></pre> <p>To use OpenMP on the GPU, load <code>cudatoolkit-standalone</code>, although this is not recommended at the moment.</p> <pre><code>module load cudatoolkit-standalone\n</code></pre>"},{"location":"polaris/programming-models/openmp-polaris/#building-on-polaris","title":"Building on Polaris","text":"<p>The following table shows what compiler and flags to use with which PrgEnv:</p> Module Compiler Flags PrgEnv-nvhpc cc/CC/ftn (nvc/nvc++/nvfortran) -mp=gpu -gpu=cc80 llvm mpicc/mpicxx (clang/clang++) -fopenmp --offload-arch=sm_80 PrgEnv-gnu cc/CC/ftn (gcc/g++/gfortran) -fopenmp PrgEnv-cray cc/CC/ftn -fopenmp <p>For example, to compile a simple code hello.cpp:</p>"},{"location":"polaris/programming-models/openmp-polaris/#for-prgenv-nvhpc-after-loading-the-modules-as-discussed-above-we-would-use","title":"For PrgEnv-nvhpc, after loading the modules as discussed above, we would use:","text":"<pre><code>CC -mp=gpu -gpu=cc80 hello.cpp\nftn -mp=gpu -gpu=cc80 hello.F90\n</code></pre>"},{"location":"polaris/programming-models/openmp-polaris/#for-llvm-after-loading-the-modules-as-discussed-above","title":"For LLVM, after loading the modules as discussed above:","text":"<pre><code>mpicxx -fopenmp --offload-arch=sm_80 hello.cpp \n</code></pre> <p>Note that if you want to force the code to error out if it cannot run on the GPU (instead of falling back to run on the host, which is the default), you can additionally compile with <code>-fopenmp-offload-mandatory</code>.</p>"},{"location":"polaris/programming-models/openmp-polaris/#for-prgenv-gnu-after-loading-the-modules-as-discussed-above-we-would-use","title":"For PrgEnv-gnu, after loading the modules as discussed above, we would use:","text":"<pre><code>CC -fopenmp hello.cpp\nftn -fopenmp hello.F90\n</code></pre>"},{"location":"polaris/programming-models/openmp-polaris/#for-prgenv-cray-after-loading-the-modules-as-discussed-above-we-would-use","title":"For PrgEnv-cray, after loading the modules as discussed above, we would use:","text":"<pre><code>CC -fopenmp hello.cpp\nftn -fopenmp hello.F90\n</code></pre>"},{"location":"polaris/programming-models/openmp-polaris/#running-on-polaris","title":"Running on Polaris","text":"<p>To run, you can execute the produced executable or use mpiexec in a job script, and then submit the script to the Polaris queue, like:</p> <pre><code>$ cat submit.sh\n#!/bin/sh\n#PBS -l select=1:system=polaris\n#PBS -l walltime=0:30:00\n#PBS -q debug \n#PBS -A Catalyst\n#PBS -l filesystems=home:eagle\n\ncd ${PBS_O_WORKDIR}\nmpiexec -n 1 ./executable\n$ # submit to the queue:\n$ qsub -l select=1:system=polaris -l walltime=0:30:00 -l filesystems=home:eagle -q debug -A Catalyst ./submit.sh\n</code></pre> <p>In the above, having the PBS options in the script and on the command line is redundant, but we put it there to show both ways of launching. This submits the script to one node in the debug queue on Polaris, requesting 30 minutes and the eagle and home filesystems. It will charge project Catalyst for the time.</p> <p>More details for setting up the job script are in the Job Scheduling and Execution section.</p>"},{"location":"polaris/programming-models/openmp-polaris/#example","title":"Example","text":"<pre><code>$ cat hello.cpp\n#include &lt;stdio.h&gt;\n#include &lt;omp.h&gt;\n\nint main(int argc, char** argv) {\n\n  printf(\"Number of devices: %d\\n\", omp_get_num_devices());\n\n  #pragma omp target\n  {\n    if (!omp_is_initial_device())\n      printf(\"Hello world from accelerator.\\n\");\n    else\n      printf(\"Hello world from host.\\n\");\n  }\n  return 0;\n}\n\n$ cat hello.F90\nprogram main\n  use omp_lib\n  implicit none\n  integer flag\n\n  write(*,*) \"Number of devices:\", omp_get_num_devices()\n\n  !$omp target map(from:flag)\n    if (.not. omp_is_initial_device()) then\n      flag = 1\n    else\n      flag = 0\n    endif\n  !$omp end target\n\n  if (flag == 1) then\n    print *, \"Hello world from accelerator\"\n  else\n    print *, \"Hello world from host\"\n  endif\n\nend program main\n\n$ # To compile\n$ CC -mp=gpu -gpu=cc80 hello.cpp -o c_test\n$ ftn -mp=gpu -gpu=cc80 hello.F90 -o f_test\n\n$ # To run \n$ mpiexec -n 1 ./c_test\nNumber of devices: 4\nHello world from accelerator.\n$ mpiexec -n 1 ./f_test\n Number of devices:            4\n Hello world from accelerator\n</code></pre>"},{"location":"polaris/programming-models/sycl-polaris/","title":"SYCL","text":"<p>SYCL (pronounced \u2018sickle\u2019) is a royalty-free, cross-platform abstraction layer that enables code for heterogeneous processors to be written using standard ISO C++ with the host and kernel code for an application contained in the same source file.</p> <ul> <li>Specification: https://www.khronos.org/sycl/</li> <li>Source code of the compiler: https://github.com/intel/llvm</li> <li>ALCF Tutorial: https://github.com/argonne-lcf/sycltrain</li> </ul> <pre><code>module load oneapi/upstream\n</code></pre> <p>Note</p> <p>This module (compilers, libraries) is built periodically from the latest open-source rather than releases. For more details on the release version of the compiler, please find the details here. As such, these compilers will get new features and updates quickly that may break on occasion. Please submit any issues at the respective GitHub repositories for the compilers and libraries.</p>"},{"location":"polaris/programming-models/sycl-polaris/#components","title":"Components","text":"<ul> <li>These are the components associated with this module:</li> </ul> User Application Component Compilers DPC++ oneMKL Interfaces oneMKL oneDPL oneDPL SYCLomatic/DPCT dpct"},{"location":"polaris/programming-models/sycl-polaris/#dependencies","title":"Dependencies","text":"<ul> <li>The SYCL programming model is supported through <code>oneapi</code> compilers that were built from source code.</li> <li>Loading this module switches the default programming environment to GNU with the following dependencies:</li> <li>PrgEnv-gnu</li> <li>cuda-PrgEnv-nvidia</li> <li>An environment variable is set when loading the module: <code>ONEAPI_DEVICE_SELECTOR=cuda:gpu</code></li> </ul>"},{"location":"polaris/programming-models/sycl-polaris/#example-how-to-use-sycl-with-mpi-and-openmp","title":"Example: How to use SYCL with MPI and OpenMP","text":"Toggle for SYCL example with OpenMP &amp; MPI for CPU-side <pre><code>#include &lt;stdlib.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;iostream&gt;\n#include &lt;iomanip&gt;\n#include &lt;string.h&gt;\n#include &lt;mpi.h&gt;\n#include &lt;sched.h&gt;\n#include &lt;sycl/sycl.hpp&gt;\n#include &lt;omp.h&gt;\n\n// SYCL port of https://code.ornl.gov/olcf/hello_jobstep\n// To compile: mpicxx -fsycl -fopenmp -fsycl-targets=nvptx64-nvidia-cuda -Xsycl-target-backend --cuda-gpu-arch=sm_80 hello_jobstep.cpp -o hello_jobstep.out\n\nint main(int argc, char *argv[]){\n\n  MPI_Init(&amp;argc, &amp;argv);\n\n  int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &amp;size);\n\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);\n\n  char name[MPI_MAX_PROCESSOR_NAME];\n  int resultlength;\n  MPI_Get_processor_name(name, &amp;resultlength);\n\n  // If CUDA_VISIBLE_DEVICES is set, capture visible GPUs\n  const char* gpu_id_list;\n  const char* cuda_visible_devices = getenv(\"CUDA_VISIBLE_DEVICES\");\n  if(cuda_visible_devices == NULL){\n    gpu_id_list = \"N/A\";\n  }\n  else{\n    gpu_id_list = cuda_visible_devices;\n  }\n\n  // Find how many GPUs L0 runtime says are available\n  int num_devices = 0;\n  std::vector&lt;sycl::device&gt; sycl_all_devs = sycl::device::get_devices(sycl::info::device_type::gpu);\n  num_devices = sycl_all_devs.size();\n\n  int hwthread;\n  int thread_id = 0;\n\n  if(num_devices == 0){\n#pragma omp parallel default(shared) private(hwthread, thread_id)\n    {\n      thread_id = omp_get_thread_num();\n      hwthread = sched_getcpu();\n\n      printf(\"MPI %03d - OMP %03d - HWT %03d - Node %s\\n\",\n             rank, thread_id, hwthread, name);\n\n    }\n  }\n  else{\n\n    std::string busid = \"\";\n\n    std::string busid_list = \"\";\n    std::string rt_gpu_id_list = \"\";\n\n    // Loop over the GPUs available to each MPI rank\n    for(int i=0; i&lt;num_devices; i++){\n\n      // // Get the PCIBusId for each GPU and use it to query for UUID\n      busid = sycl_all_devs[i].get_info&lt;sycl::ext::intel::info::device::pci_address&gt;();\n      busid_list.append(busid);\n\n      // Concatenate per-MPIrank GPU info into strings for print\n      if(i &gt; 0) rt_gpu_id_list.append(\",\");\n      rt_gpu_id_list.append(std::to_string(i));\n    }\n\n#pragma omp parallel default(shared) private(hwthread, thread_id)\n    {\n#pragma omp critical\n      {\n        thread_id = omp_get_thread_num();\n        hwthread = sched_getcpu();\n\n        printf(\"MPI %03d - OMP %03d - HWT %03d - Node %s - RT_GPU_ID %s - GPU_ID %s - Bus_ID %s\\n\",\n               rank, thread_id, hwthread, name, rt_gpu_id_list.c_str(), gpu_id_list, busid_list.c_str());\n      }\n    }\n  }\n\n  MPI_Finalize();\n\n  return 0;\n}\n</code></pre> <p>Compile and Run <pre><code>$ mpiexec -n 4 --ppn 4 --env OMP_NUM_THREADS=1 ./set_affinity_gpu_polaris.sh ./hello_jobstep.out\n\nMPI 000 - OMP 000 - HWT 000 - Node x3200c0s37b0n0 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID 0000:C7:00.0\nMPI 001 - OMP 000 - HWT 001 - Node x3200c0s37b0n0 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID 0000:85:00.0\nMPI 003 - OMP 000 - HWT 003 - Node x3200c0s37b0n0 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID 0000:07:00.0\nMPI 002 - OMP 000 - HWT 002 - Node x3200c0s37b0n0 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID 0000:46:00.0\n$ ./a.out\n</code></pre></p>"},{"location":"polaris/programming-models/sycl-polaris/#example-using-gpu-aware-mpi","title":"Example (using GPU-aware MPI)","text":"<pre><code>#include &lt;stdlib.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;mpi.h&gt;\n\n#include &lt;sycl/sycl.hpp&gt;\n\n// Modified from NERSC website:\n// https://docs.nersc.gov/development/programming-models/mpi\nint main(int argc, char *argv[]) {\n\n    int myrank, num_ranks;\n    double *val_device;\n    double *val_host;\n    char machine_name[MPI_MAX_PROCESSOR_NAME];\n    int name_len=0;\n\n    MPI_Init(&amp;argc, &amp;argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &amp;myrank);\n    MPI_Comm_size(MPI_COMM_WORLD, &amp;num_ranks);\n    MPI_Get_processor_name(machine_name, &amp;name_len);\n\n    sycl::queue q{sycl::gpu_selector_v};\n\n    std::cout &lt;&lt; \"Rank #\" &lt;&lt; myrank &lt;&lt; \" runs on: \" &lt;&lt; machine_name\n              &lt;&lt; \", uses device: \"\n              &lt;&lt; q.get_device().get_info&lt;sycl::info::device::name&gt;() &lt;&lt; \"\\n\";\n\n    MPI_Barrier(MPI_COMM_WORLD);\n    int one=1;\n    val_host = (double *)malloc(one*sizeof(double));\n    val_device = sycl::malloc_device&lt;double&gt;(one,q);\n\n    const size_t size_of_double = sizeof(double);\n    *val_host = -1.0;\n    if (myrank != 0) {\n        std::cout &lt;&lt; \"I am rank \" &lt;&lt; myrank\n                  &lt;&lt; \" and my initial value is: \" &lt;&lt; *val_host &lt;&lt; \"\\n\";\n    }\n\n    if (myrank == 0) {\n        *val_host = 42.0;\n        q.memcpy(val_device,val_host,size_of_double).wait();\n        std::cout &lt;&lt; \"I am rank \" &lt;&lt; myrank\n                  &lt;&lt; \" and will broadcast value: \" &lt;&lt; *val_host &lt;&lt; \"\\n\";\n    }\n\n    MPI_Bcast(val_device, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    double check = 42.0;\n    if (myrank != 0) {\n        //Device to Host\n        q.memcpy(val_host,val_device,size_of_double).wait();\n        assert(*val_host == check);\n        std::cout &lt;&lt; \"I am rank \" &lt;&lt; myrank\n                  &lt;&lt; \" and received broadcast value: \" &lt;&lt; *val_host &lt;&lt; \"\\n\";\n    }\n\n    sycl::free(val_device,q);\n    free(val_host);\n\n    MPI_Finalize();\n\n    return 0;\n}\n</code></pre> <p>Load Modules</p> <pre><code>module load oneapi/upstream\nmodule load mpiwrappers/cray-mpich-oneapi-upstream\nmodule load craype-accel-nvidia80\nexport MPICH_GPU_SUPPORT_ENABLED=1\n</code></pre> <p>Compile and Run</p> <p><pre><code>$ mpicxx -L/opt/cray/pe/mpich/8.1.28/gtl/lib -lmpi_gtl_cuda -std=c++17 -fsycl -fsycl-targets=nvptx64-nvidia-cuda -Xsycl-target-backend --cuda-gpu-arch=sm_80 main.cpp\n$ mpiexec -n 2 --ppn 2 --depth=1 --cpu-bind depth ./set_affinity_gpu_polaris.sh ./a.out\n</code></pre> For further details regarding the arguments passed to the <code>mpiexec</code> command shown above, please visit the Job Scheduling and Execution section. A simple example describing the details and execution of the <code>set_affinity_gpu_polaris.sh</code> file can be found here.</p> <p>Note: By default, there is no GPU-aware MPI library linking support. The example above shows how the user can enable the linking by specifying the path to the GTL (GPU Transport Layer) library (<code>libmpi_gtl_cuda</code>) to the link line.</p>"},{"location":"polaris/programming-models/sycl-polaris/#oneapi-math-kernel-library-onemkl-interfaces","title":"oneAPI Math Kernel Library (oneMKL) Interfaces","text":"<p>oneMKL Interfaces is an open-source implementation of the oneMKL Data Parallel C++ (DPC++) interface according to the oneMKL specification. It works with multiple devices (backends) using device-specific libraries underneath.</p> <p>oneMKL is part of oneAPI. Various backends supported are shown below. More information here.</p> User Application Third-Party Library cuBLAS oneMKL interface cuSOLVER cuRAND"},{"location":"polaris/programming-models/sycl-polaris/#example-using-onemklgemm","title":"Example (using onemkl::gemm)","text":"<p>The following snippet shows how to compile and run a SYCL code with the oneMKL library. For instance, a GPU-based GEMM is performed using the <code>mkl::gemm</code> API and the results are compared to a CPU-based GEMM performed using the traditional BLAS (e.g., AOCL-BLIS) library. <pre><code>#include &lt;limits&gt;\n#include &lt;random&gt;\n\n#include &lt;sycl/sycl.hpp&gt;\n\n#include &lt;oneapi/mkl.hpp&gt;  // ONEMKL GPU header\n#include &lt;cblas.h&gt;         // BLIS   CPU header\n\n// Matrix size constants\n#define SIZE 4800 // Must be a multiple of 8.\n#define M SIZE / 8\n#define N SIZE / 4\n#define P SIZE / 2\n\n//////////////////////////////////////////////////////////////////////////////////////////\n\nbool ValueSame(double a, double b) { return std::fabs(a - b) &lt; 1.0e-08; }\nint VerifyResult(double *c_A, double *c_B) {\n  bool MismatchFound = false;\n\n  for (size_t i = 0; i &lt; M; i++) {\n    for (size_t j = 0; j &lt; P; j++) {\n      if (!ValueSame(c_A[i * P + j], c_B[i * P + j])) {\n        std::cout &lt;&lt; \"fail - The result is incorrect for element: [\" &lt;&lt; i &lt;&lt; \", \" &lt;&lt; j\n                  &lt;&lt; \"], expected: \" &lt;&lt; c_A[i * P + j] &lt;&lt; \" , but got: \" &lt;&lt; c_B[i * P + j]\n                  &lt;&lt; std::endl;\n        MismatchFound = true;\n      }\n    }\n  }\n\n  if (!MismatchFound) {\n    std::cout &lt;&lt; \"SUCCESS - The results are correct!\" &lt;&lt; std::endl;\n    return 0;\n  } else {\n    std::cout &lt;&lt; \"FAIL - The results mis-match!\" &lt;&lt; std::endl;\n    return -1;\n  }\n}\n\n//////////////////////////////////////////////////////////////////////////////////////////\n\nint main() {\n  std::random_device rd;  // Will be used to obtain a seed for the random number engine\n  std::mt19937 gen(rd()); // Standard mersenne_twister_engine seeded with rd()\n  std::uniform_real_distribution&lt;&gt; dis(1.0, 2.0);\n\n  // C = alpha * op(A) * op(B)  + beta * C\n  oneapi::mkl::transpose transA = oneapi::mkl::transpose::nontrans;\n  oneapi::mkl::transpose transB = oneapi::mkl::transpose::nontrans;\n\n  // matrix data sizes\n  int m = M;\n  int n = P;\n  int k = N;\n\n  // leading dimensions of data\n  int ldA = k;\n  int ldB = n;\n  int ldC = n;\n\n  // set scalar fp values\n  double alpha = 1.0;\n  double beta = 0.0;\n\n  // 1D arrays on host side\n  double *A;\n  double *B;\n  double *C_host_onemkl, *C_cblas;\n\n  A = new double[M * N]{};\n  B = new double[N * P]{};\n  C_cblas = new double[M * P]{};\n  C_host_onemkl = new double[M * P]{};\n\n  // prepare matrix data with ROW-major style\n  // A(M, N)\n  for (size_t i = 0; i &lt; M; i++)\n    for (size_t j = 0; j &lt; N; j++)\n      A[i * N + j] = dis(gen);\n  // B(N, P)\n  for (size_t i = 0; i &lt; N; i++)\n    for (size_t j = 0; j &lt; P; j++)\n      B[i * P + j] = dis(gen);\n\n  std::cout &lt;&lt; \"Problem size: c(\" &lt;&lt; M &lt;&lt; \",\" &lt;&lt; P &lt;&lt; \") = a(\" &lt;&lt; M &lt;&lt; \",\" &lt;&lt; N &lt;&lt; \") * b(\" &lt;&lt; N\n            &lt;&lt; \",\" &lt;&lt; P &lt;&lt; \")\" &lt;&lt; std::endl;\n\n  // Resultant matrix: C_cblas\n  cblas_dgemm(CblasRowMajor, CblasNoTrans, CblasNoTrans, m, n, k, alpha, A, ldA, B, ldB, beta,\n              C_cblas, ldC);\n\n  // Resultant matrix: C_onemkl\n  sycl::queue q(sycl::property_list{sycl::property::queue::in_order{}});\n  std::cout &lt;&lt; \"Device: \" &lt;&lt; q.get_device().get_info&lt;sycl::info::device::name&gt;() &lt;&lt; std::endl &lt;&lt; std::endl;\n\n  double* A_dev        = sycl::malloc_device&lt;double&gt;(M*N, q);\n  double* B_dev        = sycl::malloc_device&lt;double&gt;(N*P, q);\n  double* C_dev_onemkl = sycl::malloc_device&lt;double&gt;(M*P, q);\n\n  q.memcpy(A_dev, A, (M*N) * sizeof(double));\n  q.memcpy(B_dev, B, (N*P) * sizeof(double));\n\n  auto gemm_event = oneapi::mkl::blas::column_major::gemm(q, transB, transA, n, m, k, alpha, B_dev, ldB, A_dev, ldA, beta, C_dev_onemkl, ldC);\n\n  q.memcpy(C_host_onemkl, C_dev_onemkl, (M*P) * sizeof(double));\n\n  q.wait();\n  std::cout &lt;&lt; \"Verify results between OneMKL &amp; CBLAS: \";\n  int result_cblas = VerifyResult(C_cblas, C_host_onemkl);\n\n  delete[] A;\n  delete[] B;\n  delete[] C_cblas;\n  delete[] C_host_onemkl;\n  sycl::free(A_dev, q);\n  sycl::free(B_dev, q);\n  sycl::free(C_dev_onemkl, q);\n  return result_cblas;\n}\n</code></pre></p> <p>Compile and Run</p> <p>The user would need to provide paths to the math libraries as shown below. Also, please provide the AOCL library for CPU GEMM by <code>module load aocl</code>. Environment variables <code>MKLROOT</code> is defined with the <code>oneapi</code> module &amp; <code>AOCL_ROOT</code> is defined with the <code>aocl</code> module. Note: Please pay attention to the linker options for AOCL &amp; oneMKL libraries. <pre><code>$ clang++ -std=c++17 -sycl-std=2020 -O3 -fsycl -fsycl-targets=nvptx64-nvidia-cuda -Xsycl-target-backend --cuda-gpu-arch=sm_80 -L$AOCL_ROOT/lib -lblis -L$MKLROOT/lib -lonemkl sycl_onemkl_gemm.cpp -o sycl_onemkl_gemm.out\n</code></pre></p>"},{"location":"polaris/visualization/ffmpeg/","title":"FFmpeg on Polaris","text":""},{"location":"polaris/visualization/ffmpeg/#note-the-ffmpeg-module-is-currently-missing-on-polaris-after-a-recent-upgrade-a-spack-build-of-ffmpeg-will-be-available-soon","title":"NOTE: The FFmpeg module is currently missing on Polaris after a recent upgrade. A Spack build of FFmpeg will be available soon.","text":"<p>To use FFmpeg on Polaris, first load the corresponding module:</p> <pre><code>module load ffmpeg\n</code></pre> <p>This is a typical command line to create a movie from a series of snapshots in PNG format:</p> <pre><code>ffmpeg -r 15 -i frames.%03d.png -r 25 -pix_fmt yuv420p movie.mp4\n</code></pre> <p>where:</p> <ul> <li><code>-r 15</code> is the input frame rate. Experiment with values smaller than the output frame rate for longer movies.</li> <li><code>-r 25</code> is the output frame rate (use this value for standard 25 frames per second).</li> <li><code>-i frames.%03d.png</code> reads the input frames in sequence.</li> <li><code>-pix_fmt yuv420p</code> is needed for movies to play in browsers.</li> <li><code>movie.mp4</code> is the resulting movie.</li> </ul>"},{"location":"polaris/visualization/imagemagick/","title":"ImageMagick on Polaris","text":"<p>To use ImageMagick on Polaris, first load the corresponding module:</p> <pre><code>module use /soft/modulefiles\nmodule load spack-pe-base imagemagick\n</code></pre>"},{"location":"polaris/visualization/paraview-manual-launch/","title":"Manually launching a ParaView server on Polaris","text":"<p>Sometimes it is convenient to manually launch an instance of the ParaView server. In this section, we will explain an alternative method to run the ParaView server on Polaris using an interactive job, where the user can launch the ParaView server from the command line interface.</p> <p>Note: this method is better suited for experienced users. If you are just starting with ParaView, we recommend the client/server mode as your primary method for using this tool.</p>"},{"location":"polaris/visualization/paraview-manual-launch/#setting-up-paraview","title":"Setting up ParaView","text":"<p>From your local client, select Connect, either from the File menu or by clicking on the icon circled below:</p> <p> </p> <p>A new window will open where you can configure a server. Click on Add Server:</p> <p></p> <p>Give your server a name, select Client/Server, localhost, and a TCP port (8000 in this example).</p> <p></p> <p>Click \"Configure\". In the next window, there is an option to set up how the ParaView server will be launched, and the default is \"Manual\". Leave it on \"Manual\" and click \"Save\".</p> <p>You will use these settings when establishing the connection.</p>"},{"location":"polaris/visualization/paraview-manual-launch/#launching-the-paraview-server-on-polaris","title":"Launching the ParaView server on Polaris","text":"<p>You can launch an interactive session on Polaris compute nodes with the following command (adjust parameters as needed to match your allocation, desired number of nodes, queue, walltime, and filesystems):</p> <pre><code>qsub -l walltime=01:00:00 -l select=2 -A yourallocation -q debug -I -l filesystems=home:eagle\n</code></pre> <p>When the job starts, you will receive a prompt on your head node like this:</p> <pre><code>username@x3005c0s7b0n0:~&gt;\n</code></pre> <p>Make a note of the node hostname (<code>x3005c0s7b0n0</code> in the example above). You can also get this information from <code>qstat -fx jobID</code>.</p> <p>Now load the ParaView module:</p> <pre><code>username@x3005c0s7b0n0:~&gt; module use /soft/modulefiles \nusername@x3005c0s7b0n0:~&gt; module load visualization/paraview/paraview-5.12.0-EGL\n</code></pre> <p>and launch the ParaView server with:</p> <pre><code>username@x3005c0s7b0n0:~&gt; mpirun -n 8 pvserver --server-port=8000\nWaiting for client...\nConnection URL: cs://x3005c0s7b0n0:8000\nAccepting connection(s): x3005c0s7b0n0:8000\n</code></pre> <p>In this case, <code>pvserver</code> will be listening on TCP port 8000 of your head node. You can change this port if you want.</p>"},{"location":"polaris/visualization/paraview-manual-launch/#creating-a-tunnel-over-ssh","title":"Creating a tunnel over ssh","text":"<p>We need to establish an ssh tunnel to connect the client to the server. On your local machine, open a new terminal and type:</p> <pre><code>ssh -v -N -L 8000:x3005c0s7b0n0:8000 polaris.alcf.anl.gov\n</code></pre> <p>where 8000 is a TCP port and <code>x3005c0s7b0n0</code> is the name of your head node. Adjust these values accordingly.</p> <p>Among multiple lines with debug information, you should see something like:</p> <pre><code>debug1: Local connections to LOCALHOST:8000 forwarded to remote address x3005c0s7b0n0:8000\n</code></pre> <p>Keep this terminal open for the duration of your session to keep the ssh tunnel active.</p> <p>Now you are ready to launch your ParaView client locally. Keep in mind that client and server versions must match. The ParaView version currently deployed on Polaris is 5.12.0.</p>"},{"location":"polaris/visualization/paraview-manual-launch/#connecting-to-paraview-server","title":"Connecting to ParaView server","text":"<p>Connect your ParaView client to the server configuration you created above. You can select Connect, either from the File menu or the icon circled in the figure:</p> <p> </p> <p>and select the configuration you created in a previous step.</p> <p>The connection should point to:</p> <pre><code>localhost:8000\n</code></pre> <p>In the terminal where you launched the server, you will see now that the connection is established. Note that ParaView may take a few seconds to connect. This is normal behavior.</p> <pre><code>username@x3005c0s7b0n0:~&gt; mpirun -n 8 pvserver --server-port=8000\nWaiting for client...\nConnection URL: cs://x3005c0s7b0n0:8000\nAccepting connection(s): x3005c0s7b0n0:8000\nClient connected.\n</code></pre> <p>At this point, you can use ParaView normally.</p>"},{"location":"polaris/visualization/paraview-tutorial/","title":"ParaView Tutorial","text":""},{"location":"polaris/visualization/paraview-tutorial/#overview","title":"Overview","text":"<p>This tutorial is intended to be a hands-on resource for users interested in learning the basic concepts of ParaView. The examples can easily be run on a laptop, using the example data set provided.</p> <ul> <li>Tour of ParaView</li> <li>Show range of visualization methods</li> <li>Walk through various visualization techniques, hopefully illustrating how these can apply to your own data</li> <li>Feel for ParaView \"way\"</li> <li>Terminology and step-by-step process peculiar to ParaView, which may differ from other packages, e.g., VisIt</li> </ul> <p> </p> Bloodflow Visualization by Joe Insley, ALCF"},{"location":"polaris/visualization/paraview-tutorial/#data","title":"Data","text":"<p>The data used for this tutorial is:</p> <ul> <li>Blood flow simulation data</li> <li>Multiple data types</li> <li>Continuum data field (unstructured mesh, tetrahedral): fluid field, plasma</li> <li>Particle data (unstructured points): individual particles moving in the flow</li> <li>Red Blood Cells (RBC, unstructured mesh, triangle): mesh of the surface of an RBC<ul> <li>Healthy</li> <li>Diseased</li> </ul> </li> <li>Generated using an integrated Nektar/LAMMPS simulation code</li> <li>Courtesy of George Karniadakis and Leopold Grinberg of Brown University</li> </ul> <p>The data is available for download here (~27MB compressed, ~39MB uncompressed): Data set for ParaView Red Blood Cell Tutorial</p>"},{"location":"polaris/visualization/paraview-tutorial/#1-load-multi-component-dataset","title":"1. Load Multi-component Dataset","text":"<ul> <li>From the File menu, (you can also click the file folder icon, shown above) open each of the following data sets (select then click \"OK\").</li> <li>The files will then appear in the Pipeline Browser.</li> <li>Click Apply in the Object Inspector.</li> <li>You will need to do this one at a time:</li> <li>continuum...vtu</li> <li>particles...vtu</li> <li>rbc_...vtu</li> <li>bad_rbc...vtu</li> </ul> <p>Note: The \"...\" in the name, and the arrow in the file browser, indicates that there are multiple time steps for each of these files.</p> <p> </p> With all of the default settings, you should see something like this"},{"location":"polaris/visualization/paraview-tutorial/#2-select-which-data-to-view","title":"2. Select which data to view","text":"<p>Let's start by looking at the continuum.000* data. This is an unstructured mesh that has velocity and count (density) values.</p> <ul> <li>Hide other data sets using the Eyeball icon next to their names in the Pipeline Browser.</li> <li>Black = visible, Grey = hidden</li> <li>Select continuum.000* (name is highlighted) in the Pipeline Browser.</li> <li>Click on the name to highlight it.</li> <li>When manipulating appearance or applying filters, these always affect the selected data set.</li> <li>Switch to the Display tab in the Object Inspector.</li> <li>Under Color by, select Velocity from the dropdown.</li> <li>There is also a shortcut to Color by in the menu bar near the top of the GUI.</li> </ul> <p> </p> Select which data to view"},{"location":"polaris/visualization/paraview-tutorial/#3-manipulating-the-color-map","title":"3. Manipulating the Color Map","text":"<p>To change the colors used to represent the Velocity:</p> <ul> <li>Under Color by, click the Edit Color Map... button.</li> <li>On the Color Scale Editor window, click the Choose Preset button.</li> <li>On the Preset Color Scales window, select: Blue to Red Rainbow, and click OK. Then click Close on the Color Scale Editor window.</li> <li>You can also create and save your own color maps.</li> </ul> <p> </p> Manipulating the Color Map"},{"location":"polaris/visualization/paraview-tutorial/#4-data-representation","title":"4. Data Representation","text":"<p>In order to be able to see the particles and red blood cells inside the cylinder, we need to be able to see through it. If we scroll down a bit in the Object Inspector view:</p> <ul> <li>Group of controls labeled Style</li> <li>In the Representation dropdown, select Wireframe</li> </ul> <p> </p> Data Representation"},{"location":"polaris/visualization/paraview-tutorial/#5-generate-streamlines","title":"5. Generate Streamlines","text":"<ul> <li>ParaView enables the generation of different types of data from existing data sets in the Pipeline.</li> <li>Streamlines: Generated from vectors of the flow field. These curves show the direction a fluid element will travel in at any point in time.</li> <li>Make sure that the continuum.000* data is selected in the Pipeline Browser.</li> <li>From the main menu select: Filters-&gt;Alphabetical-&gt;Stream Tracer, or click on the Stream Tracer icon from the menu bar.</li> <li>In the Object Inspector make sure the Properties tab is selected.</li> <li>Scroll down to seeds, and change Seed Type to Line Source.</li> <li>Click the Y Axis button to set the seed line to run along the Y axis.</li> <li>The default Resolution is set to 100. This will make things a bit cluttered, especially when we start adding in the other data, so let's reduce this to 25.</li> <li>Click the Apply button.</li> </ul> Generate Streamlines"},{"location":"polaris/visualization/paraview-tutorial/#6-streamlines-as-tubes","title":"6. Streamlines as Tubes","text":"<p>The streamlines are just that, lines. We can use the Tubes filter to represent them as 3D objects, rather than just lines.</p> <ul> <li>With StreamTracer1 selected in the Pipeline Browser, from the main menu select: Filters-&gt;Alphabetical-&gt;Tube.</li> <li>In the Object Inspector make sure the Properties tab is selected.</li> <li>The default value for the Radius is a bit too large for this data, let's set that value to 0.</li> <li>Click the Apply button.</li> <li>Notice that the StreamLine1 object has automatically been hidden.</li> <li>There are many different ways to color these tubes.</li> <li>With Tubes1 selected, switch to the Display tab in the Object Inspector.</li> <li>The Color by dropdown lets you choose from a handful of different variables.</li> </ul> <p> </p> Streamlines as Tubes"},{"location":"polaris/visualization/paraview-tutorial/#7-cutting-planes-slices","title":"7. Cutting Planes (Slices)","text":"<p>Now let's add some cutting planes, or slices, to see what the cross-section of the continuum data looks like.</p> <ul> <li>Again, be sure that the <code>continuum.000* data</code> is selected in the Pipeline Browser.</li> <li>Filters-&gt;Alphabetical-&gt;Slice or Click on the Slice icon from the menu bar.</li> <li>In the Object Inspector make sure the Properties tab is selected.</li> <li>At the bottom of the Object Inspector is a section titled Slice Offset Values. Here we can generate values for multiple slices to be made.</li> <li>First click the Delete All button to remove initial values.</li> <li>Next, click the New Range button. This will bring up an Add Range dialog box.</li> <li>Set the number of Steps to 7. Click OK.</li> <li>Click the Apply button.</li> <li>With Slice1 selected in the Object Inspector, switch to the Display tab.</li> <li>Set Color by value to Velocity.</li> </ul> <p> </p> Cutting Planes (Slices)"},{"location":"polaris/visualization/paraview-tutorial/#8-data-representation-opacity","title":"8. Data Representation: Opacity","text":"<p>Even with the continuum data represented as wireframe, there is still considerable occlusion of the interior structures. In order to further reduce this occlusion by the wireframe, we can make it more transparent.</p> <ul> <li>Again, be sure that the <code>continuum.000* data</code> is selected in the Pipeline Browser.</li> <li>In the Object Inspector make sure the Display tab is selected.</li> <li>In the Object Inspector there is a section titled Style.</li> <li>Set Opacity to 0.2.</li> </ul> <p> </p> Data Representation: Opacity"},{"location":"polaris/visualization/paraview-tutorial/#9-animating-simulation-data","title":"9. Animating Simulation Data","text":"<p>Since our data has multiple time steps, we can easily animate through them to see how the data changes over time.</p> <ul> <li>Simply click the Play button on the animation bar at the top of the GUI.</li> <li>Pause to make it stop.</li> <li>Loop: With this button toggled on, animation will repeat until stopped.</li> </ul> <p> </p> Animating Simulation Data"},{"location":"polaris/visualization/paraview-tutorial/#10-animations","title":"10. Animations","text":"<p>Animations can be saved to disk as a movie file, to be played back later.</p> <ul> <li>From the main menu: File-&gt;Save Animation.</li> <li>Animation Settings Dialog: Save Animation.</li> <li>Files of type: AVI files (*.avi).</li> <li>Enter a name in File name:</li> <li>Click OK.</li> <li>Movie can be played back with standard media players (Windows Media Player, QuickTime, VLC, etc.).</li> </ul> <p> </p> Animations"},{"location":"polaris/visualization/paraview-tutorial/#11-particles-as-glyphs","title":"11. Particles as Glyphs","text":"<p>Glyphs are another way of visually representing data where the attributes of a graphical element are dictated by attributes of the data.</p> <p>All of the particles are displayed as red points in the graphics window. There are ~39K particles in this particular data set, which makes the display a bit cluttered. In order to both filter some of these out, and create 3D representations for them, let's apply a glyph filter to this data.</p> <p>Now let's add some of our other data back into the scene. Let's start with the particle data.</p> <p>All of the particles are displayed as red points in the graphics window. There are ~39K particles in this particular data set, which makes the display rather cluttered. In order to both filter some of these out, and create 3D representations for them, we will apply the glyph filter to this data.</p> <p>Note: that the particles.000* is still visible.</p> <ul> <li>Unhide the <code>particles.000* data</code>: click Eye icon.</li> <li>Select <code>particles.000* data</code>: click on name.</li> <li>Filters-&gt;Alphabetical-&gt;Glyph or click on the Glyph icon from the menu bar.</li> <li>Glyph Type: Sphere.</li> <li>Radius: 0.15.</li> <li>Orient: Unchecked.</li> <li>Scale Mode: off.</li> <li>Set Scale Factor: 1 - Edit: Checked.</li> <li>Maximum Number of Points: 3000.</li> <li>Mask Points: Checked.</li> <li>Random Mode: Unchecked.</li> <li>Click the Apply button.</li> <li>Since our goal was to unclutter the display, let's hide the particles.000* by toggling them off, by clicking on the Eye icon next to it in the Pipeline Browser.</li> <li>Let's also switch to the Display tab in the Object Inspector, with Glyph1 selected, and change the Color by value to GlyphVector. Since the GlyphVector value is based on the velocity, we can Edit Color Map... and choose the same Blue to Red Rainbow preset that we previously chose for velocity.</li> </ul> <p> </p> Particles as Glyphs"},{"location":"polaris/visualization/paraview-tutorial/#12-enter-red-blood-cells","title":"12. Enter: Red Blood Cells","text":"<p>Now let's add in both of the other data sets, which are polygonal meshes that make up Red Blood Cells (RBCs).</p> <p>These two data sets are essentially the same kind of data, so we can apply the same filters and make the same types of representation changes to each of them. However, some of the RBCs are marked by the simulation that generated them as healthy (rbc.000) and some of them are marked as diseased (bad_rbc.000).</p> <ul> <li>Unhide the rbc.000 and bad_rbc.000 data sets by clicking the Eye icon next to each of them to make them visible.</li> </ul> <p> </p> Enter: Red Blood Cells"},{"location":"polaris/visualization/paraview-tutorial/#13-using-color-to-differentiate-data","title":"13. Using Color to Differentiate Data","text":"<p>To enable us to distinguish these two types of data from one another, we can vary their representations.</p> <p>One way to do this is by setting the color of the two data sets to different colors. Repeat this process for each of rbc.000 and bad_rbc.000, picking different colors.</p> <ul> <li>Select one of the rbc data sets in the Pipeline Browser.</li> <li>Go to the Display tab in the Object Inspector.</li> <li>In the Color by: dropdown select Solid Color.</li> <li>Click on the Set Solid Color... button.</li> <li>Select a color from the Select Color dialog that appears.</li> <li>Repeat for the other RBC data set, choosing a different color.</li> </ul> <p> </p> Using Color to Differentiate Data"},{"location":"polaris/visualization/paraview-tutorial/#14-further-exploration-highlight-the-mesh","title":"14. Further Exploration: Highlight the Mesh","text":"<p>Change the representation of one of the RBC data sets.</p> <p>In this example, the continuum.000* data is also hidden to reduce confusion with showing multiple overlapping meshes.</p> <ul> <li>Select one of the RBC data sets.</li> <li>Go to the Display tab in the Object Inspector.</li> <li>For the Representation select Surface With Edges.</li> <li>In the Edge Style section click on the Set Edge Color... button to select a different color from the Select Color dialog.</li> </ul> <p> </p> Further Exploration: Highlight the Mesh"},{"location":"polaris/visualization/paraview-tutorial/#15-further-exploration-highlight-the-vertices","title":"15. Further Exploration: Highlight the Vertices","text":"<p>Add glyphs to illustrate the position of the vertices of one of the RBC data sets.</p> <ul> <li>Select one of the RBC data sets.</li> <li>Select the Glyph filter.</li> <li>Since this filter was used recently, it can also be found under: Filters-&gt;Recent-&gt;Glyph.</li> <li>As in the earlier example, set the various configuration options for the glyph attributes.</li> <li>Note: that this time, we want to show all of the vertices of the RBC, so we should uncheck the Mask Points option.</li> </ul> <p> </p> Further Exploration: Highlight the Vertices"},{"location":"polaris/visualization/paraview-tutorial/#16-further-exploration-color-by-variable","title":"16. Further Exploration: Color by Variable","text":"<p>Try playing around with the viewing options and representations of the other data objects.</p> <p>Change the:</p> <ul> <li>Color by values</li> <li>Opacity</li> <li>Representation</li> <li>Etc.</li> </ul> <p> </p> Further Exploration: Color by Variable"},{"location":"polaris/visualization/paraview-tutorial/#17-background-color","title":"17. Background Color","text":"<ul> <li>Background color is an important part of final visualization.</li> <li>From the main menu choose: Edit-&gt;View Settings...</li> <li>Under General in the View Settings dialog box, select Choose Color.</li> <li>Select Color: OK.</li> <li>Apply, then OK.</li> </ul> Background Color <p>This tutorial was developed with support from National Science Foundation Grant OCI-0904190, and from the Argonne Leadership Computing Facility at Argonne National Laboratory, which is supported by the Office of Science of the U.S. Department of Energy under contract DE-AC02-06CH11357.</p>"},{"location":"polaris/visualization/paraview/","title":"ParaView on Polaris","text":"<p>The recommended way of running ParaView on Polaris is in client/server mode. This consists of running the ParaView client on your local resource and the ParaView server on the Polaris compute nodes. The ParaView client needs to first be installed on your local resource and must match the version that you run on Polaris.</p> <p>There are multiple versions of ParaView installed on Polaris. To find the versions of ParaView currently available on Polaris, run the following command on a login node:  <pre><code>module use /soft/modulefiles\nmodule avail paraview\n</code></pre></p> <p>Binary and source packages of the ParaView client for Linux, macOS, and Windows are available from the ParaView Download Page. </p>"},{"location":"polaris/visualization/paraview/#connecting-to-the-paraview-server-on-polaris","title":"Connecting to the ParaView server on Polaris","text":"<p>This section describes how to launch the ParaView server on Polaris from a local ParaView client.</p>"},{"location":"polaris/visualization/paraview/#start-paraview-client","title":"Start ParaView Client","text":"<p>First, launch the ParaView client on your local resource. You will need to configure some server settings in the client. This initial setup should only need to be done once and can be reused each time you want to run ParaView on Polaris.</p>"},{"location":"polaris/visualization/paraview/#server-configuration","title":"Server Configuration","text":""},{"location":"polaris/visualization/paraview/#1-select-connect","title":"1. Select Connect","text":"<p>From the ParaView client, choose to connect to a server by either clicking on the \"Connect\" icon in the menu bar</p> <p> </p> <p>or selecting File-&gt;Connect from the main menu.</p> <p></p>"},{"location":"polaris/visualization/paraview/#2-set-up-servers-first-time-only","title":"2. Set Up Servers (first time only)","text":"<p>The first time you want to run a server on Polaris and have it connect to your local ParaView client, you will need to set up a server. Once this server is set up, you can reuse it each time you run the ParaView client with the ParaView server on Polaris.</p> <p>Kitware, the developers of ParaView, maintain a database of server configurations that you can retrieve through the ParaView client. In the File-&gt;Connect menu, press the button named \"Fetch Servers\" and select POLARIS@ANL. Windows users should select \"windows to POLARIS@ANL\". Press \"Import Selected\".</p> <p></p>"},{"location":"polaris/visualization/paraview/#3-use-paraview","title":"3. Use ParaView","text":"<p>After the previous step, you can now select POLARIS@ANL in the File-&gt;Connect menu and press Connect.</p> <p></p> <p>At this point, a new window will pop up.</p> <p></p> <p>There are a number of parameters that you must enter manually here:</p> <p>Xterm executable: the path of a terminal on your system. The figure shows the case of a Mac with XQuartz. You may need to change these values for Windows or Linux.</p> <p>SSH executable: the name of your SSH command. It may be different on Windows depending on the SSH client installed (e.g., PuTTY).</p> <p>Remote machine: leave this value at polaris.alcf.anl.gov.</p> <p>Username: your ALCF username.</p> <p>ParaView version: the version of ParaView that you want to use. Verify first that this version is installed on the system (as described at the top of this document). You will also need to add a <code>-EGL</code> suffix.</p> <p>Example: <pre><code>5.12.0-EGL\n</code></pre></p> <p>Client port: it is safe to use the default value.</p> <p>Server port: it is safe to use the default value.</p> <p>Number of nodes to reserve: enter the number of Polaris compute nodes you want to use for your job.</p> <p>Number of ranks per node: enter the number of ranks per node.</p> <p>Number of minutes to reserve: the duration of your job in minutes.</p> <p>Account: enter here the name of your ALCF allocation.</p> <p>Queue: the name of the Polaris queue you would like to use (e.g., <code>debug</code> for small, quick jobs, <code>prod</code>, <code>preemptable</code>).</p> <p>File Systems: enter here the file systems you need for your job, separated with colons, no spaces. Keep in mind that your job may not run if one of these file systems is not available at that time, so enter these values carefully.</p> <p>Job name: safe to use the default value. The PBS scheduler will assign this name to your job.</p> <p>Now you can press OK to establish the connection with a ParaView server on Polaris.</p> <p>An SSH connection will be established with a Polaris login node, and a password will be requested in a terminal, similar to the process you normally use to connect and work on the system.</p> <p>After you enter your password, a job will be queued, and you will see a window like this:</p> <p></p> <p>When the job is launched on the compute nodes, the previous window will go away, and ParaView will show it is connected to Polaris in its Pipeline Browser:</p> <p></p> <p>At this point, you can open datasets stored on the ALCF file systems and use ParaView normally.</p>"},{"location":"polaris/visualization/paraview/#additional-information","title":"Additional Information","text":"<ul> <li>ParaView Documentation</li> <li>ParaView Community Support</li> </ul>"},{"location":"polaris/visualization/visit/","title":"VisIt on Polaris","text":""},{"location":"polaris/visualization/visit/#getting-started","title":"Getting Started","text":"<p>The latest VisIt versions installed on Polaris are 3.3.3 and 3.4.0.</p> <p>Please note that at the time of this writing, VisIt version 3.4.0 does not yet have a client for Mac available.</p> <p>Follow these steps to install VisIt on your local machine:</p> <ul> <li>Download and install VisIt for your local platform (macOS, Windows, Linux). The version you download must match the server version installed on Polaris. Use this page</li> <li>Download the Polaris host profile for VisIt (you may need to right-click and choose \"Save link as...\" or \"Save target as...\")</li> <li>Copy this file to a file called <code>~/.visit/hosts/host_anl_polaris.xml</code> on Mac or Linux. For Windows, specify the equivalent path.</li> </ul> <p>Note: VisIt allows the user to download host profiles for ANL, but all these settings are outdated. We are working with the VisIt developers to update the ANL host list.</p> <p>Additional information for using VisIt in client/server mode is available here.</p>"},{"location":"polaris/visualization/visit/#running-visit","title":"Running VisIt","text":"<ul> <li>Start up VisIt on your local machine.</li> <li>Click File -&gt; Open File and choose \"ANL Polaris\" from the \"Host\" dropdown.</li> </ul> <ul> <li>You'll be prompted for your password; enter your ALCF authenticator app response.</li> <li>When you open a selected file, it will launch a job on Polaris.</li> <li>You will need to specify the \"Bank\" (Project) to use when VisIt submits jobs to the queue on Polaris. Specify a project in the Options box.</li> <li>If your environment doesn't get sourced correctly with non-interactive SSH, you can set the default project to use under Options -&gt; Host profiles.</li> </ul> <ul> <li>Note: Don't change the contents of the \"Machine file\" field (it should be <code>$PBS_NODEFILE</code>).</li> <li>Note: The default Launch Profile is set to serial. We recommend leaving this setting in its default value, but using the parallel method to launch jobs on Polaris.</li> <li>Note: Don't change the contents of \"launchMethod\". It must be <code>qsub/aprun</code> even though Polaris does not use <code>aprun</code>.</li> <li>If you'd like to change other job parameters (like the number of processes, nodes, and walltime), you can do so. Please enter time in the format required by the PBS scheduler (i.e., 1:00:00 for one hour).</li> <li>If you'd like these changes to be used as your default, be sure to save them using Save Settings under the Options menu.</li> </ul>"},{"location":"polaris/visualization/visit/#additional-information","title":"Additional Information","text":"<ul> <li>VisIt user manual</li> <li>VisIt wiki</li> </ul>"},{"location":"polaris/visualization/visualization/","title":"Visualization on Polaris","text":"<p>Starting in January 2024, Polaris will serve as the primary production resource for visualization and analysis.</p> <p>Below is a list of the available visualization tools along with links to their corresponding documentation.</p> <p>ParaView: ParaView is an open-source visualization engine that seamlessly integrates with your existing tools and workflows. It allows you to construct visualization pipelines for quick data analysis. Whether interactively exploring large datasets in 3D or performing batch processing programmatically, ParaView provides versatile capabilities. For additional information, visit the Kitware website.</p> <p>VisIt: VisIt is an open-source, interactive, and scalable visualization, animation, and analysis tool. Users can rapidly generate visualizations, animate them over time, apply various operators and mathematical expressions, and save resulting images and animations for presentations. VisIt supports a diverse range of visualization features, enabling users to view data, including scalar and vector fields, on 2D and 3D structured, adaptive, and unstructured meshes. Thanks to its customizable plugin design, VisIt can visualize data from over 120 different scientific data formats. For more information, check the VisIt project GitHub page.</p> <p>FFmpeg: FFmpeg is a complete solution to record, convert, and stream audio and video. For more information, visit the FFmpeg webpage.</p> <p>ImageMagick: ImageMagick is a free, open-source software suite used for editing and manipulating digital images. It can be used to create, edit, compose, or convert bitmap images, and supports a wide range of file formats, including JPEG, PNG, GIF, TIFF, and PDF. More information is available on the ImageMagick webpage.</p>"},{"location":"polaris/visualization/vnc/","title":"VNC on Polaris","text":""},{"location":"polaris/visualization/vnc/#overview","title":"Overview","text":"<p>VNC (Virtual Network Computing) is available on Polaris to run graphics applications. To avoid slowing down login nodes, please run VNC on compute nodes.</p>"},{"location":"polaris/visualization/vnc/#password-for-vnc","title":"Password for VNC","text":"<p>Create a unique password for VNC and store it in a secure directory.</p> <p>Setting the password:</p> <pre><code>vncpasswd [file]\n</code></pre> <p>Omitting <code>[file]</code> will save the password to <code>~/.vnc/passwd</code>.</p>"},{"location":"polaris/visualization/vnc/#vnc-launch-script","title":"VNC Launch Script","text":"<p>Create a script for your preferred shell (e.g., Bash) with the following commands:</p> <pre><code>#!/bin/bash\n\nXvnc -PasswordFile ~/.vnc/passwd -geometry 1920x1080 :0 &amp;\nsleep 2\nicewm --display=:0 &amp;\nxterm -display :0 &amp;\n</code></pre>"},{"location":"polaris/visualization/vnc/#launching-the-script","title":"Launching the Script","text":"<ol> <li>Start an interactive session using <code>qsub</code> with the <code>-I</code> option.</li> <li>Run the VNC launch script on the compute node.</li> </ol>"},{"location":"polaris/visualization/vnc/#establishing-an-ssh-tunnel","title":"Establishing an SSH Tunnel","text":"<p>On another terminal, type:</p> <pre><code>ssh -v -N -L 5900:x3005c0s7b0n0:5900 yourusername@polaris.alcf.anl.gov\n</code></pre> <p>where <code>x3005c0s7b0n0</code> is the name of your Polaris compute node. The default TCP port for VNC is 5900. Edit as needed.</p>"},{"location":"polaris/visualization/vnc/#connecting-to-your-vnc-server","title":"Connecting to Your VNC Server","text":"<ol> <li>Install a VNC client on your local computer.</li> <li>Run the client and connect to <code>localhost:5900</code>.</li> <li>Enter your VNC password.</li> <li>Launch your graphics application from the xterm.</li> </ol>"},{"location":"polaris/visualization/vnc/#tips","title":"Tips","text":"<ul> <li>Keep the SSH tunnel terminal open during your session.</li> <li>Adjust the SSH tunnel command as needed for your setup.</li> <li>If for any reason you need to restart the VNC server and get error messages, try:</li> </ul> <pre><code>killall Xvnc icewm xterm\n</code></pre>"},{"location":"polaris/workflows/balsam/","title":"Balsam","text":"<p>Balsam is a Python-based workflow manager that helps users execute large numbers of jobs, potentially with inter-job dependencies, track job outcomes, and manage post-processing analysis. A Balsam Site runs on a node with access to the job scheduler, where it can submit and monitor jobs. Overall job state is aggregated on the Balsam Server, making job data from all Sites accessible from any individual site (or the user's laptop) via the command-line interface or the Python API. To get information on how to use the command-line tool, you can type <code>balsam --help</code> in your shell.</p> <p>Full documentation for Balsam is available online.</p> <p>Balsam requires Python 3.7+. To install Balsam on Polaris, first set up a virtual Python environment:</p> <pre><code>module use /soft/modulefiles\nmodule load conda\nconda activate base\npython -m venv env\nsource env/bin/activate\npip install --upgrade pip\npip install --pre balsam\n</code></pre> <p>To use Balsam, users need an account on the Balsam server. Users can get an account by contacting the ALCF Help Desk. Once a user has an account, they can log in and create a new site. A Balsam site is a project space for your workflow. You will be prompted to select which machine (Polaris) you are working on when creating a new site:</p> <pre><code>balsam login\nbalsam site init -n new-site new-site\ncd new-site\nbalsam site start\n</code></pre> <p>See the Balsam documentation for full details.</p>"},{"location":"polaris/workflows/libensemble/","title":"libEnsemble","text":"<p>libEnsemble is a Python toolkit for running dynamic ensembles of calculations.</p> <p>Users provide generator and simulator functions to express their ensembles, where the generator can steer the ensemble based on previous results. These functions can portably submit external executables at any scale.</p> <p>System details are detected, and dynamic resource management is provided. This includes automatically detecting, assigning, and reassigning GPUs for ensemble members.</p> <p>libEnsemble can be used in a consistent manner on laptops, clusters, and supercomputers with minimal required dependencies.</p>"},{"location":"polaris/workflows/libensemble/#getting-libensemble-on-polaris","title":"Getting libEnsemble on Polaris","text":"<p>libEnsemble is provided on Polaris in the conda module:</p> <pre><code>module use /soft/modulefiles\nmodule load conda\nconda activate base\n</code></pre> <p>See the docs for more details on using Python on Polaris.</p> Example: creating a virtual environment and updating libEnsemble      E.g., to create a virtual environment that allows installation of     further packages with pip:      <pre><code>python -m venv /path/to-venv --system-site-packages\n. /path/to-venv/bin/activate\n</code></pre>      Where ``/path/to-venv`` can be anywhere you have write access.     For future uses, just load the conda module and run the activate line.      You can also ensure you are using the latest version of libEnsemble:      <pre><code>pip install libensemble\n</code></pre>"},{"location":"polaris/workflows/libensemble/#libensemble-examples","title":"libEnsemble examples","text":"<p>For a very simple example of using libEnsemble, see the Simple Introduction tutorial.</p> <p>For an example that runs a small ensemble using a C application (offloading work to the GPU), see the GPU app tutorial. The required files for this tutorial can be found in this directory. A video demo is also available.</p>"},{"location":"polaris/workflows/libensemble/#job-submission","title":"Job Submission","text":"<p>libEnsemble runs on the compute nodes on Polaris using either Python's <code>multiprocessing</code> or <code>mpi4py</code>. The user can set the number of workers for maximum concurrency. libEnsemble will detect the nodes available from the PBS environment and use these for running simulations. Polaris supports running multiple concurrent simulations on each node if desired.</p> <p>A simple example batch script for a libEnsemble use case that runs five workers on one node:</p> <pre><code>    #!/bin/bash -l\n    #PBS -l select=1:system=polaris\n    #PBS -l walltime=00:15:00\n    #PBS -l filesystems=home:eagle\n    #PBS -q debug\n    #PBS -A &lt;myproject&gt;\n\n    export MPICH_GPU_SUPPORT_ENABLED=1\n    cd $PBS_O_WORKDIR\n    python run_libe_forces.py --comms local --nworkers 5\n</code></pre> <p>The script can be run with:</p> <pre><code>qsub submit_libe.sh\n</code></pre> <p>Or you can run an interactive session with:</p> <pre><code>qsub -A &lt;myproject&gt; -l select=1 -l walltime=15:00 -lfilesystems=home:eagle -qdebug -I\n</code></pre>"},{"location":"polaris/workflows/libensemble/#further-links","title":"Further links","text":"<p>Docs: https://libensemble.readthedocs.io  GitHub: https://github.com/Libensemble/libensemble</p>"},{"location":"polaris/workflows/mig-compute/","title":"Multi-Instance GPU (MIG) mode","text":"<p>MIG mode can be enabled and configured on Polaris by passing a valid configuration file to <code>qsub</code>:</p> <p>qsub ... -l mig_config=/home/ME/path/to/mig_config.json ...</p> <p>You can find a concise explanation of MIG concepts and terms at NVIDIA MIG User Guide.</p>"},{"location":"polaris/workflows/mig-compute/#configuration","title":"Configuration","text":"<p>Please study the following example of a valid configuration file:</p> <pre><code>{\n  \"group1\": {\n    \"gpus\": [0,1],\n    \"mig_enabled\": true,\n    \"instances\": {\"7g.40gb\": [\"4c.7g.40gb\", \"3c.7g.40gb\"] }\n  },\n  \"group2\": {\n    \"gpus\": [2,3],\n    \"mig_enabled\": true,\n    \"instances\": {\"3g.20gb\": [\"2c.3g.20gb\", \"1c.3g.20gb\"], \"2g.10gb\": [\"2g.10gb\"], \"1g.5gb\": [\"1g.5gb\"]}\n  }\n}\n</code></pre>"},{"location":"polaris/workflows/mig-compute/#notes","title":"Notes","text":"<ul> <li>Group names are arbitrary but must be unique.</li> <li><code>\"gpus\"</code> must be an array of integers. If only one physical GPU is being configured in a group, it must still be contained within an array (e.g., <code>\"gpus\": [0],</code>).</li> <li>Only groups with <code>mig_enabled</code> set to <code>true</code> will be configured.</li> <li><code>instances</code> denote the MIG GPU instances and the nested compute instances you wish to be configured.</li> <li>Syntax is <code>{\"gpu instance 1\": [\"cpu instance 1\", \"cpu instance 2\"], ...}</code>.</li> <li>Valid GPU instances are <code>1g.5gb</code>, <code>1g.10gb</code>, <code>2g.10gb</code>, <code>3g.20gb</code>, <code>4g.20gb</code>, and <code>7g.40gb</code>. The first number denotes the number of slots used out of 7 total, and the second number denotes memory in GB.</li> <li>The default CPU instance for any GPU instance has the same identifier as the GPU instance (in which case it will be the only one configurable).</li> <li>Other CPU instances can be configured with the identifier syntax <code>Xc.Y</code>, where <code>X</code> is the number of slots available in that GPU instance, and <code>Y</code> is the GPU instance identifier string.</li> <li>Some GPU instances cannot be configured adjacently, despite there being sufficient slots/memory remaining (e.g., <code>3g.20gb</code> and <code>4g.20gb</code>). Please see NVIDIA MIG documentation for further details.</li> <li>Currently, MIG configuration is only available in the debug, debug-scaling, and preemptable queues. Submissions to other queues will result in any MIG config files passed being silently ignored.</li> <li>Files that do not match the above syntax will be silently rejected, and any invalid configurations in properly formatted files will be silently ignored. Please test any changes to your configuration in an interactive job session before use.</li> <li>A basic validator script is available at <code>/soft/pbs/mig_conf_validate.sh</code>. It will check for simple errors in your config and print the expected configuration. For example:</li> </ul> <pre><code>ascovel@polaris-login-02:~&gt; /soft/pbs/mig_conf_validate.sh -h\nusage: mig_conf_validate.sh -c CONFIG_FILE\nascovel@polaris-login-02:~&gt; /soft/pbs/mig_conf_validate.sh -c ./polaris-mig/mig_config.json\nexpected MIG configuration:\nGPU     GPU_INST   COMPUTE_INST\n-------------------------------\n0       7g.40gb    4c.7g.40gb\n0       7g.40gb    3c.7g.40gb\n1       7g.40gb    4c.7g.40gb\n1       7g.40gb    3c.7g.40gb\n2       2g.10gb    2g.10gb\n2       4g.20gb    2c.4g.20gb\n2       4g.20gb    2c.4g.20gb\n3       2g.10gb    2g.10gb\n3       4g.20gb    2c.4g.20gb\n3       4g.20gb    2c.4g.20gb\nascovel@polaris-login-02:~&gt;\n</code></pre>"},{"location":"polaris/workflows/mig-compute/#example-use-of-mig-compute-instances","title":"Example use of MIG compute instances","text":"<p>The following example demonstrates the use of MIG compute instances via the <code>CUDA_VISIBLE_DEVICES</code> environment variable:</p> <pre><code>ascovel@polaris-login-02:~/polaris-mig&gt; qsub -l mig_config=/home/ascovel/polaris-mig/mig_config.json -l select=1 -l walltime=60:00 -l filesystems=home:eagle -A Operations -q R639752 -k doe -I\nqsub: waiting for job 640002.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov to start\nqsub: job 640002.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov ready\n\nascovel@x3209c0s19b0n0:~&gt; cat ./polaris-mig/mig_config.json\n{\n  \"group1\": {\n    \"gpus\": [0,1],\n    \"mig_enabled\": true,\n    \"instances\": {\"7g.40gb\": [\"4c.7g.40gb\", \"3c.7g.40gb\"] }\n  },\n  \"group2\": {\n    \"gpus\": [2,3],\n    \"mig_enabled\": true,\n    \"instances\": {\"4g.20gb\": [\"2c.4g.20gb\", \"2c.4g.20gb\"], \"2g.10gb\": [\"2g.10gb\"] }\n  }\n}\nascovel@x3209c0s19b0n0:~&gt; nvidia-smi -L | grep -Po -e \"MIG[0-9a-f\\-]+\"\nMIG-63aa1884-acb8-5880-a586-173f6506966c\nMIG-b86283ae-9953-514f-81df-99be7e0553a5\nMIG-79065f64-bdbb-53ff-89e3-9d35f270b208\nMIG-6dd56a9d-e362-567e-95b1-108afbcfc674\nMIG-76459138-79df-5d00-a11f-b0a2a747bd9e\nMIG-4d5c9fb3-b0e3-50e8-a60c-233104222611\nMIG-bdfeeb2d-7a50-5e39-b3c5-767838a0b7a3\nMIG-87a2c2f3-d008-51be-b64b-6adb56deb679\nMIG-3d4cdd8c-fc36-5ce9-9676-a6e46d4a6c86\nMIG-773e8e18-f62a-5250-af1e-9343c9286ce1\nascovel@x3209c0s19b0n0:~&gt; for mig in $( nvidia-smi -L | grep -Po -e \"MIG[0-9a-f\\-]+\" ) ; do CUDA_VISIBLE_DEVICES=${mig} ./saxpy &amp; done 2&gt;/dev/null\nascovel@x3209c0s19b0n0:~&gt; nvidia-smi | tail -n 16\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0    0    0      17480      C   ./saxpy                          8413MiB |\n|    0    0    1      17481      C   ./saxpy                          8363MiB |\n|    1    0    0      17482      C   ./saxpy                          8413MiB |\n|    1    0    1      17483      C   ./saxpy                          8363MiB |\n|    2    1    0      17484      C   ./saxpy                          8313MiB |\n|    2    1    1      17485      C   ./saxpy                          8313MiB |\n|    2    5    0      17486      C   ./saxpy                          8313MiB |\n|    3    1    0      17487      C   ./saxpy                          8313MiB |\n|    3    1    1      17488      C   ./saxpy                          8313MiB |\n|    3    5    0      17489      C   ./saxpy                          8313MiB |\n+-----------------------------------------------------------------------------+\nascovel@x3209c0s19b0n0:~&gt;\n</code></pre>"},{"location":"polaris/workflows/parsl/","title":"Parsl on Polaris","text":"<p>Parsl is a flexible and scalable parallel programming library for Python.</p> <p>-- Parsl Documentation</p> <p>For many applications, managing an ensemble of jobs into a workflow is a critical step that can easily become a performance bottleneck. Many tools exist to address this, of which <code>parsl</code> is just one. On this page, we'll highlight some of the key pieces of information about <code>parsl</code> that are relevant to Polaris. <code>Parsl</code> is also extensively documented, has a dedicated Slack channel, and a large community of users and developers beyond ALCF. We encourage you to engage with the <code>parsl</code> community for support with <code>parsl</code>-specific questions, and for Polaris-specific questions or problems, please contact support@alcf.anl.gov.</p>"},{"location":"polaris/workflows/parsl/#getting-parsl-on-polaris","title":"Getting Parsl on Polaris","text":"<p>You can install parsl by building off of the <code>conda</code> modules. You have some flexibility in how you want to extend the <code>conda</code> module to include parsl, but here is an example way to do it:</p> <pre><code># Load the Conda Module (needed every time you use parsl)\nmodule use /soft/modulefiles\nmodule load conda\nconda activate\n\n# Create a virtual env that uses the conda env as the system packages.\n# Only do the next line on initial setup:\npython -m venv --system-site-packages /path/to/your/virtualenv\n\n# Load the virtual env (every time):\nsource /path/to/your/virtualenv/bin/activate\n\n# Install parsl (only once)\npip install parsl\n</code></pre>"},{"location":"polaris/workflows/parsl/#using-parsl-on-polaris","title":"Using Parsl on Polaris","text":"<p>Parsl has a variety of possible configuration settings. As an example, we provide the configuration below that will run one task per GPU:</p> <pre><code>from parsl.config import Config\n\n# PBSPro is the right provider for Polaris:\nfrom parsl.providers import PBSProProvider\n# The high throughput executor is for scaling to HPC systems:\nfrom parsl.executors import HighThroughputExecutor\n# You can use the MPI launcher, but may want the Gnu Parallel launcher, see below\nfrom parsl.launchers import MpiExecLauncher, GnuParallelLauncher\n# address_by_interface is needed for the HighThroughputExecutor:\nfrom parsl.addresses import address_by_interface\n# For checkpointing:\nfrom parsl.utils import get_all_checkpoints\n\n# Adjust your user-specific options here:\nrun_dir=\"/lus/eagle/projects/yourproject/yourrundir/\"\n\nuser_opts = {\n    \"worker_init\":      f\"source /path/to/your/virtualenv/bin/activate; cd {run_dir}\", # load the environment where parsl is installed\n    \"scheduler_options\":\"#PBS -l filesystems=home:eagle\" , # specify any PBS options here, like filesystems\n    \"account\":          \"YOURPROJECT\",\n    \"queue\":            \"debug-scaling\",\n    \"walltime\":         \"1:00:00\",\n    \"nodes_per_block\":  3, # think of a block as one job on polaris, so to run on the main queues, set this &gt;= 10\n    \"cpus_per_node\":    32, # Up to 64 with multithreading\n    \"available_accelerators\": 4, # Each Polaris node has 4 GPUs, setting this ensures one worker per GPU\n    \"cores_per_worker\": 8, # this will set the number of cpu hardware threads per worker.  \n}\n\ncheckpoints = get_all_checkpoints(run_dir)\nprint(\"Found the following checkpoints: \", checkpoints)\n\nconfig = Config(\n        executors=[\n            HighThroughputExecutor(\n                label=\"htex\",\n                heartbeat_period=15,\n                heartbeat_threshold=120,\n                worker_debug=True,\n                available_accelerators=user_opts[\"available_accelerators\"], # if this is set, it will override other settings for max_workers if set\n                cores_per_worker=user_opts[\"cores_per_worker\"],\n                address=address_by_interface(\"bond0\"),\n                cpu_affinity=\"block-reverse\",\n                prefetch_capacity=0,\n                start_method=\"spawn\",  # Needed to avoid interactions between MPI and os.fork\n                provider=PBSProProvider(\n                    launcher=MpiExecLauncher(bind_cmd=\"--cpu-bind\", overrides=\"--depth=64 --ppn 1\"),\n                    # Which launcher to use?  Check out the note below for some details.  Try MPI first!\n                    # launcher=GnuParallelLauncher(),\n                    account=user_opts[\"account\"],\n                    queue=user_opts[\"queue\"],\n                    select_options=\"ngpus=4\",\n                    # PBS directives (header lines): for array jobs pass '-J' option\n                    scheduler_options=user_opts[\"scheduler_options\"],\n                    # Command to be run before starting a worker, such as:\n                    worker_init=user_opts[\"worker_init\"],\n                    # number of compute nodes allocated for each block\n                    nodes_per_block=user_opts[\"nodes_per_block\"],\n                    init_blocks=1,\n                    min_blocks=0,\n                    max_blocks=1, # Can increase more to have more parallel jobs\n                    cpus_per_node=user_opts[\"cpus_per_node\"],\n                    walltime=user_opts[\"walltime\"]\n                ),\n            ),\n        ],\n        checkpoint_files = checkpoints,\n        run_dir=run_dir,\n        checkpoint_mode = 'task_exit',\n        retries=2,\n        app_cache=True,\n)\n</code></pre>"},{"location":"polaris/workflows/parsl/#special-notes-for-polaris","title":"Special notes for Polaris","text":"<p>On Polaris, there is a known bug where Python applications launched with <code>mpi</code> and that use <code>fork</code> to spawn processes can sometimes have unexplained hangs. For this reason, it is recommended to use <code>start_method=\"spawn\"</code> on Polaris when using the <code>MpiExecLauncher</code> as shown in the example config above. Alternatively, another solution is to use the <code>GNUParallelLauncher</code> which uses <code>GNU Parallel</code> to spawn processes. <code>GNU Parallel</code> can be loaded in your environment with the command <code>module load gnu-parallel</code>. Both of these approaches will circumvent the hang issue from using <code>fork</code>.</p>"},{"location":"polaris/workflows/parsl/#updates","title":"Updates","text":"<p>For <code>parsl</code> versions after July 2023, the <code>address</code> passed in the <code>HighThroughputExecutor</code> needs to be set to <code>address = address_by_interface(\"bond0\")</code>. With <code>parsl</code> versions prior to July 2023, it was recommended to use <code>address = address_by_hostname()</code> on Polaris, but with later versions, this will not work on Polaris (or any other machine).</p>"},{"location":"polaris/workflows/smartsim/","title":"SmartSim and SmartRedis","text":"<p>SmartSim is an open-source tool developed by Hewlett Packard Enterprise (HPE) designed to facilitate the integration of traditional HPC simulation applications with machine learning workflows. There are two core components to SmartSim:</p> <ul> <li>Infrastructure library (IL)</li> <li>Provides an API to start, stop, and monitor HPC applications from Python</li> <li>Interfaces with the scheduler to launch jobs (PBSPro on Polaris and Cobalt on Theta/ThetaGPU)</li> <li>Deploys a distributed in-memory database called the Orchestrator</li> <li>SmartRedis client library</li> <li>Provides clients that connect to the Orchestrator from Fortran, C, C++, and Python code</li> <li>The client API library enables data transfer to/from the database and the ability to load and run JIT-traced Python and ML runtimes acting on stored data</li> </ul> <p>For more resources on SmartSim, follow the links below:</p> <ul> <li>Source code</li> <li>Documentation</li> <li>Zoo of examples</li> <li>Fall 2023 ALCF User Hands-On Workshop</li> <li>NekRS-ML</li> </ul>"},{"location":"polaris/workflows/smartsim/#installation-with-pytorch-gpu-backend","title":"Installation with PyTorch GPU Backend","text":"<p>SmartSim on Polaris can be installed by creating a virtual environment based on the ML conda module. From a compute node, execute: <pre><code>module use /soft/modulefiles\nmodule load conda/2024-04-29\nconda activate base\npython -m venv --clear /path/to/_ssim_env --system-site-packages\nsource /path/to/_ssim_env/bin/activate\n</code></pre> Note that <code>/path/to/</code> can either be a user's home or project directory.</p> <p>Then set up the environment variables: <pre><code>export CC=cc\nexport CXX=CC\nexport CUDNN_BASE=/soft/libraries/cudnn/cudnn-12-linux-x64-v9.1.0.70\nexport CUDNN_LIBRARY=$CUDNN_BASE/lib/\nexport CUDNN_INCLUDE_DIR=$CUDNN_BASE/include/\nexport LD_LIBRARY_PATH=$CUDNN_LIBRARY:$LD_LIBRARY_PATH\nexport TORCH_CMAKE_PATH=$( python -c 'import torch;print(torch.utils.cmake_prefix_path)' )\nexport TORCH_PATH=$( python -c 'import torch; print(torch.__path__[0])' )\nexport LD_LIBRARY_PATH=$TORCH_PATH/lib:$LD_LIBRARY_PATH\n</code></pre></p> <p>Now, install SmartSim and the PyTorch GPU backend: <pre><code>git clone https://github.com/rickybalin/SmartSim.git\ncd SmartSim\npip install -e .\nsmart build -v --device gpu --torch_dir $TORCH_CMAKE_PATH --no_tf\ncd ..\n</code></pre></p> <p>and validate the build with: <pre><code>smart validate\n</code></pre></p> <p>Finally, install the SmartRedis library: <pre><code>git clone https://github.com/rickybalin/SmartRedis.git\ncd SmartRedis\nmake lib DEP_CC=cc DEP_CXX=CC\npip install -e .\ncd ..\n</code></pre></p> <p>To use SmartSim in the future, simply source the following environment. Note that the Torch libraries need to be prepended to <code>LD_LIBRARY_PATH</code>: <pre><code>module use /soft/modulefiles\nmodule load conda/2024-04-29\nconda activate base\nsource /path/to/_ssim_env/bin/activate\nexport TORCH_PATH=$( python -c 'import torch; print(torch.__path__[0])' )\nexport LD_LIBRARY_PATH=$TORCH_PATH/lib:$LD_LIBRARY_PATH\n</code></pre></p>"},{"location":"polaris/workflows/smartsim/#installation-with-tensorflow-gpu-backend","title":"Installation with TensorFlow GPU Backend","text":"<p>To use the TensorFlow backend with the SmartSim Orchestrator, the installation steps are very similar but require downgrading the TensorFlow version to 2.13.1. Follow the same instructions outlined above for the PyTorch backend, with the following exceptions:</p> <ul> <li>After creating and sourcing the Python virtual environment, downgrade TensorFlow with <code>pip install tensorflow==2.13.1</code>. Note that this will also downgrade typing-extensions, which will cause compatibility issues with PyTorch in the conda module.</li> <li>No need to export <code>TORCH_CMAKE_PATH</code> and <code>TORCH_PATH</code>, or modify <code>LD_LIBRARY_PATH</code>.</li> <li>Build the SmartSim backend with <code>smart build -v --device gpu --no_pt</code>.</li> </ul>"},{"location":"polaris/workflows/smartsim/#examples","title":"Examples","text":"<p>You can find examples of in situ training and inference of ML models from an ongoing CFD simulation at the NekRS-ML repository. The <code>smartredis</code> branch has instructions on how to build and run the examples on Polaris.</p> <p>The Fall 2023 ALCF User Hands-On Workshop repository also contains information on how to use SmartSim and NekRS-ML on Polaris, but note that some of the instructions are specific to the Fall of 2023.</p>"},{"location":"polaris/workflows/smartsim/#notes","title":"Notes","text":"<ul> <li>SmartSim workflows, such as online training, often require launching multiple MPI applications on the same set of nodes. On Polaris, the <code>MPICH_OFI_CXI_PID_BASE=0</code> must be exported before the first call to <code>mpiexec</code>, and then incremented by 1 and re-exported before each successive call. This is done with the SmartSim API by adding <code>env_vars={'MPICH_OFI_CXI_PID_BASE':str(0)}</code> to the <code>PalsMpiexecSettings()</code> API.</li> </ul>"},{"location":"policies/alcf-acknowledgement-policy/","title":"ALCF Acknowledgement Policy","text":"<p>As a U.S. Department of Energy user facility dedicated to the advancement of scientific discoveries, the Argonne Leadership Computing Facility (ALCF) provides unique computing resources and expertise to a user community that is bound by certain policies designed to acknowledge and promote the work of others as well as the resources used to accomplish this work.</p> <p>The ALCF requests your continued compliance with the terms of your program or discretionary award, specifically with regard to acknowledgments in publications and presentations based on work done with ALCF resources. Also, please forward your accepted publication citations to pubs@alcf.anl.gov.</p>"},{"location":"policies/alcf-acknowledgement-policy/#ai-testbeds-publication-guidance","title":"AI Testbeds Publication Guidance","text":"<p>To publish technical reports and research papers using the ALCF AI testbeds, we request you to provide us with a draft of your paper prior to submission by emailing a copy to us at support@alcf.anl.gov. We will work closely with the AI testbed vendors to provide feedback in a timely manner. We strongly recommend you engage us and the vendors early and often in this process to help us facilitate your research objectives.</p> <p>For guidance on acknowledgements, please see the following sample policies:</p>"},{"location":"policies/alcf-acknowledgement-policy/#alcf-only-acknowledgement","title":"ALCF Only Acknowledgement","text":"<p>Users, and ALCF staff scientists without direct project funding, should acknowledge the ALCF in all publications and presentations that speak to work performed on ALCF resources.</p> <p>This research used resources of the Argonne Leadership Computing Facility, a U.S. Department of Energy (DOE) Office of Science user facility at Argonne National Laboratory and is based on research supported by the U.S. DOE Office of Science-Advanced Scientific Computing Research Program, under Contract No. DE-AC02-06CH11357.</p>"},{"location":"policies/alcf-acknowledgement-policy/#incitealcf-acknowledgement","title":"INCITE/ALCF Acknowledgement","text":"<p>Users should acknowledge the ALCF in all publications and presentations that speak to INCITE work performed on ALCF resources.</p> <p>An award for computer time was provided by the U.S. Department of Energy\u2019s (DOE) Innovative and Novel Computational Impact on Theory and Experiment (INCITE) Program. This research used resources from the Argonne Leadership Computing Facility, a U.S. DOE Office of Science user facility at Argonne National Laboratory, which is supported by the Office of Science of the U.S. DOE under Contract No. DE-AC02-06CH11357.</p>"},{"location":"policies/alcf-acknowledgement-policy/#incitealcfolcf-acknowledgement","title":"INCITE/ALCF/OLCF Acknowledgement","text":"<p>Users should acknowledge the ALCF and OLCF in all publications and presentations that speak to INCITE work performed on ALCF and OLCF resources.</p> <p>An award for computer time was provided by the U.S. Department of Energy\u2019s (DOE) Innovative and Novel Computational Impact on Theory and Experiment (INCITE) Program. This research used supporting resources at the Argonne and the Oak Ridge Leadership Computing Facilities. The Argonne Leadership Computing Facility at Argonne National Laboratory is supported by the Office of Science of the U.S. DOE under Contract No. DE-AC02-06CH11357. The Oak Ridge Leadership Computing Facility at the Oak Ridge National Laboratory is supported by the Office of Science of the U.S. DOE under Contract No. DE-AC05-00OR22725.</p>"},{"location":"policies/facility-policies/","title":"ALCF Facility Policies","text":"<p>Be sure to familiarize yourself with the various policies and procedures for ALCF users, categorized below.</p>"},{"location":"policies/facility-policies/#accounts","title":"Accounts","text":"<p>All holders of user accounts must comply with ALCF and Argonne National Laboratory computing usage policies, including meeting certain security requirements and executing specific science- or engineering-related computing jobs.</p> <ul> <li>Accounts Policy</li> <li>Account Sponsorship and Retention Policy</li> <li>User Authentication Policy</li> </ul>"},{"location":"policies/facility-policies/#alcf-acknowledgement-policy","title":"ALCF Acknowledgement Policy","text":"<p>As a U.S. Department of Energy Office of Science User Facility dedicated to the advancement of scientific discovery, the ALCF requests that its users acknowledge and promote the work of others and the resources with which this work was accomplished.</p> <ul> <li>ALCF Acknowledgement Policy</li> </ul>"},{"location":"policies/facility-policies/#data-and-allocation","title":"Data and Allocation","text":"<p>These policies detail data and software usage, as well as pullback and refunds of computing hours.</p> <ul> <li>Data Policy</li> <li>Pullback Policy</li> <li>Refund Policy</li> <li>Software Policy</li> </ul>"},{"location":"policies/facility-policies/#quarterly-reports","title":"Quarterly Reports","text":"<p>The ALCF is required to report the progress and accomplishments of its allocation projects. Policies are detailed by award type.</p> <ul> <li>Quarterly Report Policy</li> </ul>"},{"location":"policies/facility-policies/#queue-and-scheduling-policies","title":"Queue and Scheduling Policies","text":"<ul> <li>General Policies</li> </ul>"},{"location":"policies/accounts/account-sponsorship-retention-policy/","title":"Account Sponsorship &amp; Retention Policy","text":"<p>This page is designed to help you understand the different types of accounts that you will encounter at the ALCF. The policy outlined reviews the responsibilities of an account holder, an account sponsor, and those of a foreign national.</p>"},{"location":"policies/accounts/account-sponsorship-retention-policy/#alcf-account-types","title":"ALCF Account Types","text":"<p>Annual: This account applies to users who are not ALCF Regular Employees. The default renewal date (account deactivation date) for the account is a year from the day the account was requested. These accounts are renewed annually and must be approved by an ALCF Staff member or a Project PI (also known as the \u201capprover\u201d). Users are required to update their account information and agree to the Terms of Use each year. Users need to be a part of an active project for their account to be renewed.</p> <p>Permanent: This account applies to individuals who are Regular Employees within the ALCF and CPS Divisions. If you hold this type of account, periodic renewal is not necessary.</p> <p>Note: Foreign Nationals have a second date (apart from their account deactivation date) that controls their account access. Accounts held by foreign nationals require paperwork referred to as an ANL-593 (or just 593 for shorthand). This paperwork is also required for any on-site access and also applies to computer accounts. DOE requirements state that the ALCF is to disable any account with expired 593 paperwork.</p> <p>A notification system has been established that issues a warning notice to users when expiration approaches and requests action to ensure that accounts are not needlessly turned off. An approval from the project PI is required to renew ANL 593 for project members that are foreign nationals.</p>"},{"location":"policies/accounts/account-sponsorship-retention-policy/#your-responsibilities-as-an-account-approver","title":"Your responsibilities as an account approver","text":"<p>If you approve any accounts, please take note of the following roles and responsibilities:</p> <p>By approving someone for an account at the ALCF, you are accepting responsibility for the account applicant and confirming that this individual is who they claim to be and is thus entitled to work on our computers. Do not simply \"rubber stamp\" any account application that claims you as an account approver/project PI.</p> <p>You are also responsible for approving account renewal requests. When an account is about to expire, we send a warning notification to the account holder. Among other things, the account holder is asked to contact the approver (the PI of any of the active projects the account holder is associated with) if they wish to renew their account. We cannot and will not extend someone's account without an approval. An important aspect of this process to note is that inaction will result in the account becoming deactivated on the expiration date.</p> <p>You are also responsible for approving ANL 593 renewal requests. When an account\u2019s 593 is about to expire, we send a warning notification to the account holder. Among other things, the account holder is asked to contact the approver (the PI of any of the active projects the account holder is associated with) if they wish to renew their 593. We cannot and will not extend someone's 593 without an approval. An important aspect of this process to note is that inaction will result in the account becoming deactivated at the expiration date.</p>"},{"location":"policies/accounts/account-sponsorship-retention-policy/#account-retention-policy","title":"Account Retention Policy","text":"<p>Accounts can exist in one of three states:</p> <ul> <li>Active: The active state is normal for an account.</li> <li>Inactive: The inactive state occurs when an account expires, and the ability to use ALCF resources is removed by changing the active status of the account to inactive. All files continue to exist in the user's home directory. An account will remain in the inactive state for at least 90 days before moving to the next state.</li> <li>Deleted: After 90 days, an inactive account will be deleted. This removes all references to the account from the system (except the accounts database), including any files and home directories.</li> </ul> <p>Users with inactive or deleted accounts can request a reactivation by visiting https://my.alcf.anl.gov/accounts/#/accountReactivate and clicking on the \u201cReactivate An Account\u201d link.</p>"},{"location":"policies/accounts/accounts-policy/","title":"Accounts Policy","text":"<p>All holders of user accounts must abide by all appropriate Argonne Leadership Computing Facility and Argonne National Laboratory computing usage policies. The policy details are outlined in the following documents:</p> <ul> <li>ANL's Information Technology Access Agreement</li> <li>Addendum to ANL's Information Technology Access Agreement</li> </ul> <p>These are described at the time of the account request and include requirements such as using a sufficiently strong password, appropriate use of the system, and so on. Any user not following these requirements will have their account disabled.</p> <p>Furthermore, ALCF resources are intended to be used as a computing resource for specific computational science or engineering work, not as a general-purpose computing system.</p> <p>If someone is using the system extensively but not carrying out any computational activities, their account could be disabled.</p>"},{"location":"policies/accounts/user-authentication-policy/","title":"User Authentication Policy","text":"<p>Users of the ALCF systems are required to use a SafeNet token (physical or mobile) one-time password, multifactor authentication system.</p> <p>This document explains the policies users must follow regarding SafeNet tokens for accessing the ALCF systems.</p>"},{"location":"policies/accounts/user-authentication-policy/#multifactor-authentication","title":"MultiFactor Authentication","text":"<p>\"Authentication systems are frequently described by the authentication factors that they incorporate. The three factors often considered as the cornerstone of authentication are: Something you know (for example, a password); Something you have (for example, an ID badge or a cryptographic key); and Something you are (for example, a voice print or other biometric measurement).\" -- NIST ITL Bulletin, Aug 2004</p> <p>By the NIST guidelines for identification and authentication (NIST 800-53, Revision 3, Control IA-2), ALCF aims for a Moderate level of security controls. All production systems in ALCF require multifactor authentication for users with network and local (privileged and non-privileged accounts) using the SafeNet tokens.</p>"},{"location":"policies/accounts/user-authentication-policy/#mobile-and-physical-tokens","title":"Mobile and Physical Tokens","text":"<p>ALCF provides every user of the production resources a physical or mobile token called a SafeNet Token. This is named after the company that developed the key fob and mobile software (the organization is now called SafeNet). \"Both tokens use AES-256 bit encryption to generate OTPs [One-Time Passwords] comprised of digits, digits and letters or digits, letters and special characters...\"</p> <p>When you receive your physical token, it will be initialized, but it will have no access privileges until you have contacted us to verify your identity.</p> <p>At the end of your account or project lifecycle, please return the token to the ALCF help desk:</p> <p>ALCF Service Desk Argonne National Laboratory 9700 South Cass Avenue Building 240 Argonne, IL 60439</p>"},{"location":"policies/accounts/user-authentication-policy/#protect-your-passcode-token","title":"Protect Your Passcode Token","text":"<p>Your passcode token should be protected by you as carefully as your credit cards or house keys. If your token is lost, stolen, or damaged, please contact us immediately so that we can deactivate the token and prevent unauthorized access. Sharing of tokens is strictly forbidden. Please do not mark on the token or alter it in any way.</p>"},{"location":"policies/accounts/user-authentication-policy/#more-information","title":"More Information","text":"<p>New User Guide</p> <p>Using Passcode Tokens</p>"},{"location":"policies/accounts/user-authentication-policy/#references","title":"References","text":"<ul> <li>http://www.itl.nist.gov/lab/bulletns/bltnaug04.htm</li> <li>http://csrc.nist.gov/publications/nistpubs/800-53-Rev3/sp800-53-rev3-final_updated-errata_05-01-2010.pdf</li> <li>http://csrc.nist.gov/publications/nistpubs/800-63-1/SP-800-63-1.pdf</li> <li>https://safenet.gemalto.com/multi-factor-authentication/authenticators/one-time-password-otp/</li> </ul>"},{"location":"policies/data-and-software-policies/data-policy/","title":"Data Policy","text":""},{"location":"policies/data-and-software-policies/data-policy/#alcf-data-confidentiality","title":"ALCF Data Confidentiality","text":"<p>The Argonne Leadership Computing Facility (ALCF) network is an open-research network. Because our resources and networks are open to many users and cannot be protected at a partitioned level, we cannot guarantee complete security for any data that resides here. It is up to users to provide the security they need.</p> <p>Data is not encrypted at rest. Data transferred via SSH (i.e., scp) is encrypted in transmission using SSH\u2019s mechanisms (e.g., AES256, etc.). Data transferred via Globus (GridFTP) isn't normally fully encrypted. The GridFTP control channel is encrypted, but the data channel by default is not (though the authentication processes for both channels are encrypted). If you need full encryption of the data stream, you need to explicitly select \"encrypt transfer\" in the \"Transfer &amp; Timer Options\" in the Globus UI or use equivalent options in the CLI or transfer API if you're using those. More information here: https://docs.globus.org/faq/security.</p> <p>The basic level of protection provided is UNIX file level permissions; it is the user's responsibility to ensure that file permissions and umasks are set to match their needs.</p> <p>Warning</p> <p>The default permissions and umasks are group and world readable. For help determining or setting file permissions or umasks, or creating a UNIX group, contact support@alcf.anl.gov.</p>"},{"location":"policies/data-and-software-policies/data-policy/#alcf-staff-with-root-privileges","title":"ALCF Staff with Root Privileges","text":"<p>ALCF resource administrators with root privileges are not constrained by the file permissions, and they have the capability to open and/or copy all files on the system. They can also assume a user\u2019s identity on the system. There is no audit trail for access, touching, or moving data; however, ALCF staff does not view or modify project data unless directed by a PI or project member to help debug a problem. Data may be touched or accessed by the filesystem itself if data needs to be repaired or verified for integrity after a filesystem event (e.g., a fsck).</p> <p>The ALCF resources are Federal resources and are the property of the United States Government. Any or all uses of this system and all files on this system may be intercepted, monitored, recorded, copied, audited, inspected, and disclosed to authorized site, Department of Energy, and law enforcement personnel, as well as authorized officials of other agencies, both domestic and foreign.</p> <p>Administrators use elevated privileges for maintenance and system management. Following are instances where ALCF staff might look at your files:</p> <ul> <li>We maintain copies of all job submission error, output, and log files and may review them to determine if a job failure was due to user error or a system failure.</li> <li>If you request our assistance via any mechanism (for example, support ticket, direct personal email, in-person, etc.), be aware we may need to view your files using elevated privileges to aid us in resolving your issue.</li> </ul>"},{"location":"policies/data-and-software-policies/data-policy/#use-of-proprietarylicensed-software","title":"Use of Proprietary/Licensed Software","text":"<p>All software used on ALCF computers must be appropriately acquired and used according to the appropriate licensing. Possession or use of illegally copied software is prohibited. Likewise, users shall not copy copyrighted software, except as permitted by the owner of the copyright. Currently, the use of export-controlled codes is prohibited.</p>"},{"location":"policies/data-and-software-policies/data-policy/#prohibited-data","title":"Prohibited Data","text":"<p>The ALCF computer systems are operated as research systems and contain only data related to scientific research. Use of ALCF resources to store, manipulate, or remotely access any sensitive or national security information is prohibited unless documented and approved by the PI and ALCF leadership.</p> <p>This includes, but is not limited to, personally identifiable information (data that falls under the Privacy Act of 1974, 5 U.S.C. 552a), controlled unclassified information (CUI) to include unclassified controlled nuclear information (UCNI), naval nuclear propulsion information (NNPI), International Traffic in Arms Relations (ITAR), the design or development of nuclear, biological, or chemical weapons, or any weapons of mass destruction. The use of ALCF resources for personal or non-work-related activities is also prohibited.</p>"},{"location":"policies/data-and-software-policies/data-policy/#export-control","title":"Export Control","text":"<p>All principal investigators using ALCF resources and ALCF staff members working with project teams are responsible for knowing whether their project generates any of these prohibited data types or information that falls under Export Control. For questions, contact ALCF Support at support@alcf.anl.gov.</p>"},{"location":"policies/data-and-software-policies/data-policy/#data-storage-systems","title":"Data Storage Systems","text":"<p>Data stored for any length of time on ALCF resources should only be data directly related to work done on any of the ALCF leadership computing systems. Specific policies apply to the three types of data storage systems maintained at ALCF. Read these policies carefully and plan accordingly in terms of space, usage, and data protection.</p>"},{"location":"policies/data-and-software-policies/data-policy/#home-file-system-space-agile-home-gecko-home","title":"Home File System Space (agile-home, gecko-home)","text":"<p>The home file system (/home) is intended to hold your executable files, configuration files, etc. It is NOT meant to hold the output from your application runs (use the data/parallel file system for that purpose). The home file system space is generally moderate in size and is the best protected. Because of its size, backups are practical to accomplish. The system performs tape backups, enabling the recovery of files more than seven days old or recovery from a catastrophic disk failure. Users should email support@alcf.anl.gov if they need assistance. The table below indicates the capabilities and characteristics of each file system.</p> <p>AI Testbed home</p> <p><code>/home/</code> shared across the ALCF AI testbed systems, including the AI testbed's login and compute nodes, is different from mira-home. Default user quota on the AI testbed's home is 1 TB storage and 1,000,000 files. This space is backed up.</p>"},{"location":"policies/data-and-software-policies/data-policy/#team-project-or-campaign-file-system-eagle-flare","title":"Team Project or Campaign File System (Eagle, Flare)","text":"<p>The team project/campaign file system is intended primarily for results output from your computational runs on the ALCF computing systems. This space is accessible to the team members of your project that have an ALCF account. Default storage quota is 1 TB and the default period is 1 year. Consider this space intermediate-term storage. Once any active production and/or analysis is complete and you no longer need regular access to the data, archive it within the ALCF (explained below) or transfer it to your home institution or move it to Eagle to share it with the broader community (explained below).</p> <p>This space has redundancy in the servers and storage but is so large that replication, snapshots, and backups are not practical. Eagle is a Lustre global parallel file system. All new projects will be given storage allocations on Eagle. More information on Lustre File Striping Basics: Lustre File Striping Basics.</p> <p>Pullback Policy: Projects that do not use a minimum of 50% of their allocated space after 6 months will be subject to a quota limit reduction.</p> <p>AI Testbed projects file system</p> <p>The team project/campaign file system /projects mounted on AI Testbed's login and compute nodes is intended to facilitate project collaboration and is accessible to the team members of your project that have an ALCF account. Default group storage quota is 2 TB and 2,000,000 files. Please note that this space isn't backed up. Our policy is that data will be purged from disk 6 months after project completion.</p>"},{"location":"policies/data-and-software-policies/data-policy/#shared-community-project-or-campaign-file-system-eagle-flare","title":"Shared Community Project or Campaign File System (Eagle, Flare)","text":"<p>These Lustre global parallel file systems have community sharing abilities and are useful for sharing the project/campaign data with the broader research community via Globus. This space does not have redundancy in the servers or storage and is so large that replication, snapshots, and backups are not practical. The table below indicates the capabilities and characteristics of each file system. Default storage quota is 1 TB and the default period is 2 years. More information on Lustre file striping can be found in this presentation.</p> <p>Data Pullback Policy:  Projects that do not use a minimum of 50% of their allocated space after 6 months will be subject to a quota limit reduction.</p> <p>Access Termination Policy:  Project endpoints that have exhibited no activity* for a period of 6 months will be disabled and the storage space will be reclaimed. Notification will be sent to the PI and project members 30 days prior to and the day of the action.</p> <p>Activity is defined as, but not limited to:</p> <ul> <li>Creation of the Globus endpoint</li> <li>Globus transfers to and from the endpoint</li> <li>atime audits of data files indicating access</li> <li>Other factors may include DOIs and citations referring to the project</li> </ul>"},{"location":"policies/data-and-software-policies/data-policy/#archive-space","title":"Archive Space","text":"<p>The archive space is intended for offline storage of results you wish to retain but either have no immediate need to access or no room in your parallel file system space. Archiving capabilities are available via HPSS. The primary HPSS access is via HSI. HTAR is available, but its path length and file size limitations often cause it to fail. Globus Online and GridFTP are clients that can also be used with HPSS.  Due to the possibility of data corruption, users can request 2 copies for particularly critical data. Such requests will be handled on a case-by-case basis.</p>"},{"location":"policies/data-and-software-policies/data-policy/#data-storage-policies","title":"Data Storage Policies","text":""},{"location":"policies/data-and-software-policies/data-policy/#disk-capacity-and-retention-policies","title":"Disk Capacity and Retention Policies","text":"---- /home lus/eagle/projects, /eagle, /grand, lus/flare/projects or /flare Default Quota <sup>1</sup> 50 GB 1 TB / 1 million files Quota Enforcement <sup>2</sup> hard/soft hard/soft Disk Redundancy <sup>3</sup> dual parity dual parity File Server Snapshots <sup>6</sup> (frequency/retained) none none File Server Metadata Redundancy yes yes File Server Metadata Replication <sup>4</sup> yes yes File Server Data Replication <sup>5</sup> yes no Data Purged from Disk n/a After 6 months of inactivity (see Access termination policy listed in the section above) <sup>8</sup>"},{"location":"policies/data-and-software-policies/data-policy/#tape-capacity-and-retention-policies","title":"Tape Capacity and Retention Policies","text":"---- /home lus/eagle/projects, /eagle, /grand, lus/flare/projects or /flare Automatic Backup to Tape? <sup>6</sup> yes no Archived to Tape Before Deleted from Disk? <sup>8</sup> yes no <ol> <li>While quotas are subject to negotiation on a case-by-case basis, disk space is a finite resource and projects must exercise good data management practices for their own sake and the sake of other users of the facility. With Lustre, it has become necessary to enforce file quotas as well, which are also negotiable.</li> <li>\u201cHard quota enforcement\u201d means a job will fail when writing output if you exceed the hard quota limit. \"Soft quota enforcement\" means you may exceed the soft quota limit (but never the higher hard quota value) for up to seven days. If you do not drop back below the soft quota limit within seven days, writes will begin to fail.</li> <li>Hard drives are in redundancy groups of 10 disks (8 data + 2 parity). In other words, three out of 10 drives would have to fail before data loss occurred.</li> <li>Metadata (i.e., information listing which blocks are part of which files) is written twice to two different storage arrays. Thus, even if an entire array were lost, the metadata would be preserved.</li> <li>Refers to the fact that data (user output) is written twice with each block on two different storage arrays, so that even if an entire array were lost, the data would be preserved.</li> <li>\u201cYes\u201d denotes that ALCF does regular backups without intervention from the user. Currently gecko-home is unable to be backed up.  </li> <li>The project directory is available on disk for the stipulated period but project quotas are reduced immediately following project end date. Access to the directory will be removed after 180 days. Requests to restore/extend access or reset the quota are reviewed on a case-by-case basis. </li> <li>Users who wish to retain data must archive or transfer their data elsewhere at the end of the project. Users need an active ALCF account to access archived data on HPSS. See Account Retention Policy for more information. The user is responsible for archiving the data to HPSS or copying it to another facility as desired. Data will be retained on tape for 2 years, at which time it is eligible for removal (subject to change). </li> </ol>"},{"location":"policies/data-and-software-policies/software-policy/","title":"ALCF Resource Software Use","text":"<p>All software used on ALCF computers must be appropriately acquired and used according to the appropriate licensing. Possession or use of illegally copied software is prohibited. Likewise, users shall not copy copyrighted software, except as permitted by the owner of the copyright. Currently, the use of export-controlled codes is prohibited.</p>"},{"location":"policies/data-and-software-policies/software-policy/#community-software-policy","title":"Community Software Policy","text":"<p>ALCF supports the deployment of community software from active projects on production systems. A project may provide and support a code on ALCF systems for the ALCF user community as described in the [Community Software Service].</p> <p>User deployments are system-specific, and their maintenance is the sole responsibility of the project deploying it. There shall be no expectation of additional support from ALCF, other than for the provisioning of space and integration with the module system. Projects will be provided with an initial module file from a template, with the expectation that they will update and maintain the module, providing paths and instructions so that user communities can access the software.</p>"},{"location":"policies/queue-scheduling/pullback-policy/","title":"Pullback Policy","text":"<p>In an effort to ensure that valuable ALCF computing resources are used judiciously, a pullback policy has been instituted. Projects granted allocations under the INCITE and ALCC programs that have not used a significant amount of their allocation will be evaluated and adjusted during the year following the policies outlined on this page.</p> <p>The figures outlined below represent the maximum amount that will be pulled back from projects after specific dates during the allocation period. The decision to reduce allocations will be made on a case-by-case basis in discussion with the project's primary investigators (PIs).</p>"},{"location":"policies/queue-scheduling/pullback-policy/#incite-pullback-policy","title":"INCITE Pullback Policy","text":"<p>On May 1 of the current INCITE calendar year: - If usage is less than 15%, remove up to 15% of the unused balance. - If usage is less than 10%, remove up to 30% of the unused balance.</p> <p>On September 1 of the current INCITE calendar year: - If usage is less than 50%, remove up to 33% of the unused balance. - If usage is less than 33%, remove up to 50% of the unused balance. - If usage is less than 10%, remove up to 75% of the unused balance.</p>"},{"location":"policies/queue-scheduling/pullback-policy/#alcc-pullback-policy","title":"ALCC Pullback Policy","text":"<p>ALCC projects must use 50% of their allocation within the first seven months of the allocation cycle. Any unused time in excess of 50% will be deducted from the project allocation at the end of the seven-month period.</p>"},{"location":"policies/queue-scheduling/queue-and-scheduling-policy/","title":"Queue and Scheduling Policy","text":""},{"location":"policies/queue-scheduling/queue-and-scheduling-policy/#general-policy","title":"General Policy","text":"<p>We ask that all users follow good etiquette and be excellent to one another.</p>"},{"location":"policies/queue-scheduling/queue-and-scheduling-policy/#priority","title":"Priority","text":"<p>As with all Argonne Leadership Computing Facility production systems, job priority in the queue is based on several criteria:</p> <ul> <li>Positive balance of your project</li> <li>Size (in nodes) of the job; larger jobs receive higher priority</li> <li>The type of project (e.g., INCITE, ALCC, or discretionary)</li> <li>Job duration - shorter duration jobs will accumulate priority more quickly, so it is best to specify the job run time as accurately as possible</li> </ul>"},{"location":"policies/queue-scheduling/queue-and-scheduling-policy/#reservations-and-scheduling-policy","title":"Reservations and Scheduling Policy","text":"<p>Some work will require use of Polaris that necessitates deviation from regular policy. On such occasions, normal reservation policy applies. Please send the regular form no fewer than five (5) business days in advance.</p>"},{"location":"policies/queue-scheduling/queue-and-scheduling-policy/#big-run-mondays","title":"Big Run Mondays","text":"<p>As part of our regular maintenance procedures on Mondays, we will promote to the highest priority any jobs in the queued state requesting 802 nodes or more. Promotion is subject to operational discretion.</p> <p>We may also, at our discretion, take the opportunity to promote the priority of capability jobs (20% of the machine) if the system has been drained of jobs for any other reason.</p>"},{"location":"policies/queue-scheduling/queue-and-scheduling-policy/#monday-maintenance","title":"Monday Maintenance","text":"<p>On Mondays when the ALCF is on a regular business schedule, the system may be expected to undergo maintenance from 9:00 am until 5:00 pm US Central Time. The <code>showres</code> command may be used to view pending and active maintenance reservations.</p>"},{"location":"policies/queue-scheduling/queue-and-scheduling-policy/#incitealcc-overburn-policy","title":"INCITE/ALCC Overburn Policy","text":"<p>If an INCITE or ALCC project has exhausted its allocation in the first 11 months of its allocation year, it is eligible for overburn running. At this point, capability jobs (20% of the machine) submitted by INCITE and ALCC projects will run in the default queue (instead of backfill) for the first 11 months of the allocation year until 125% of the project allocation has been consumed.</p> <p>INCITE and ALCC projects needing additional overburn hours should email support@alcf.anl.gov with a short description of what they plan to do with the additional hours, highlighting specific goals or milestones and the time expected to accomplish them. This will be reviewed by the scheduling committee, allocations committee, and ALCF management. Requests should be submitted 15 days before the start of the next quarter of the allocation year for full consideration. Non-capability jobs (less than 20% of the machine) from projects that have exhausted their allocation will continue to run in backfill.</p> <p>To be clear, this policy does not constitute a guarantee of extra time, and we reserve the right to prioritize the scheduling of jobs submitted by projects that have not yet used 100% of their allocations, so the earlier that an INCITE or ALCC project exhausts its allocation, the more likely it is to be able to take full advantage of this policy.</p>"},{"location":"policies/queue-scheduling/refund-policy/","title":"Refund Policy","text":"<p>If a system problem affects your run, ALCF will consider a refund of node hours. The ALCF expects all applications to regularly checkpoint, so refunds are typically capped at four hours of runtime for the affected job, unless the problem in question prevented checkpoints.</p> <p>ALCF strongly advises against symlinking between filesystems or hard-coding paths to a different filesystem.</p> <p>To request a refund, send the following information to support@alcf.anl.gov: - Job ID - Machine - Reason for refund request</p> <p>For more information, contact support@alcf.anl.gov.</p>"},{"location":"running-jobs/example-job-scripts/","title":"Example Job Scripts","text":"<p>This page contains a small collection of example job scripts users may find useful for submitting their jobs on Polaris. Additional information on PBS and how to submit these job scripts is available here.</p> <p>A simple example using a similar script on Polaris is available in the Getting Started Repo.</p> <p>Comments in PBS scripts</p> <p>Since <code>#</code> is required prior to each PBS directive, comments should be added after the directives have been listed in your submission script. If you try to add comments within the directive list, you could experience submission issues due to PBS attempting to read your comment as an additional directive. This includes adding comments on the same line as a directive (i.e., <code>#PBS -q &lt;queue_name&gt;  #comment</code>).</p>"},{"location":"running-jobs/example-job-scripts/#cpu-mpi-openmp-examples","title":"CPU MPI-OpenMP Examples","text":"<p>The following <code>submit.sh</code> example submits a 1-node job to Polaris with 16 MPI ranks per node and 2 OpenMP threads per rank. See Queues for details on practical limits to node counts and job times for different sizes of jobs.</p> <p>The <code>hello_affinity</code> program is a compiled C++ code, which is built via <code>make -f Makefile.nvhpc</code> in the linked directory after cloning the Getting Started repository.</p> submit.sh<pre><code>#!/bin/bash -l\n#PBS -l select=1:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -l filesystems=home:eagle\n#PBS -q debug\n#PBS -A Catalyst\n\n# Change to working directory\ncd ${PBS_O_WORKDIR}  # (1)!\n\n# MPI and OpenMP settings\nNNODES=`wc -l &lt; $PBS_NODEFILE` # (2)!\nNRANKS_PER_NODE=16 # (3)!\nNDEPTH=2 # (4)!\nNTHREADS=2 # (5)!\n\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE )) # (6)!\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}\"\n\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth --env OMP_NUM_THREADS=${NTHREADS} -env OMP_PLACES=threads ./hello_affinity\n</code></pre> <ol> <li><code>cd ${PBS_O_WORKDIR}</code>: change into the working directory from where <code>qsub</code> was executed.</li> <li><code>NNODES= `wc -l &lt; $PBS_NODEFILE`</code>: one method for determining the total number of nodes allocated to a job.</li> <li><code>NRANKS_PER_NODE=16</code>: This is a helper variable to set the number of MPI ranks for each node to 16.</li> <li><code>NDEPTH=2</code>: This is a helper variable to space MPI ranks 2 \"slots\" from each other. In this example, individual threads correspond to a slot. This will be used together with the <code>--cpu-bind</code> option from <code>mpiexec</code> and additional binding options are available (e.g. <code>numa</code>, <code>socket</code>, <code>core</code>, etc.).</li> <li><code>NTHREADS=2</code>: This is a helper variable to set the number of OpenMP threads per MPI rank.</li> <li><code>NTOTRANKS=$(( NNODES * NRANKS_PER_NODE))</code>: This is a helper variable calculating the total number of MPI ranks spanning all nodes in the job.</li> </ol> <p>The following function in the <code>hello_affinity</code> source code is essential for uniquely identifying the CUDA device even when Multi-Instance GPU (MIG) is enabled, as each physical device will be partitioned into multiple virtual devices, each with unique UUIDs differentiated by the last few characters:</p> Identifying physical or virtual GPU by UUID <pre><code>//https://stackoverflow.com/questions/68823023/set-cuda-device-by-uuid\nvoid uuid_print(cudaUUID_t a){\n  std::cout &lt;&lt; \"GPU\";\n  std::vector&lt;std::tuple&lt;int, int&gt; &gt; r = {{0,4}, {4,6}, {6,8}, {8,10}, {10,16}};\n  for (auto t : r){\n    std::cout &lt;&lt; \"-\";\n    for (int i = std::get&lt;0&gt;(t); i &lt; std::get&lt;1&gt;(t); i++)\n      std::cout &lt;&lt; std::hex &lt;&lt; std::setfill('0') &lt;&lt; std::setw(2) &lt;&lt; (unsigned)(unsigned char)a.bytes[i];\n  }\n  std::cout &lt;&lt; std::endl;\n}\n</code></pre> <p>Zsh users</p> <p>If you are a <code>zsh</code> user, you will need to ensure all PBS job submission and shell scripts include the <code>-l</code> flag following <code>#!/bin/bash</code> as seen in the example above to ensure your environment is being instantiated properly. <code>zsh</code> is not officially supported by HPE and support from ALCF will be best effort only.*</p> <p>Each Polaris compute node has 1 Milan CPU with a total of 32 physical cores, with each core supporting 2 hardware threads (for a total of 64 logical cores).</p> <p>The process affinity in this example is set up to map each MPI rank to 2 physical cores. Each MPI rank spawns 2 OpenMP threads, so 1 thread per physical core. The OpenMP settings bind each OpenMP thread to a single hardware thread within a core, such that all 32 physical cores are utilized. CPU core IDs <code>32</code> to <code>63</code> are not mapped to any MPI rank, since they correspond to simultaneous multithreaded (SMT) sibling hardware threads that share the execution resources of the core ids <code>0</code> to <code>31</code>, respectively.</p> <ul> <li><code>cd ${PBS_O_WORKDIR}</code>: change into the working directory from where <code>qsub</code> was executed.</li> <li><code>NNODES= `wc -l &lt; $PBS_NODEFILE`</code>: one method for determining the total number of nodes allocated to a job.</li> <li><code>NRANKS_PER_NODE=16</code>: This is a helper variable to set the number of MPI ranks for each node to 16.</li> <li><code>NDEPTH=2</code>: This is a helper variable to space MPI ranks 2 \"slots\" from each other. In this example, individual threads correspond to a slot. This will be used together with the <code>--cpu-bind</code> option from <code>mpiexec</code> and additional binding options are available (e.g. <code>numa</code>, <code>socket</code>, <code>core</code>, etc.).</li> <li><code>NTHREADS=2</code>: This is a helper variable to set the number of OpenMP threads per MPI rank.</li> <li><code>NTOTRANKS=$(( NNODES * NRANKS_PER_NODE))</code>: This is a helper variable calculating the total number of MPI ranks spanning all nodes in the job.</li> </ul> <p>Information on the use of <code>mpiexec</code> is available via <code>man mpiexec</code>. Some notes on the specific options used in the above example follow.</p> <ul> <li><code>-n ${NTOTRANKS}</code>: This is specifying the total number of MPI ranks to start.</li> <li><code>--ppn ${NRANKS_PER_NODE}</code>: This is specifying the number of MPI ranks to start on each node.</li> <li><code>--depth=${NDEPTH}</code>: This is specifying how many cores/threads to space MPI ranks apart on each node.</li> <li><code>--cpu-bind depth</code>: This is indicating the number of cores/threads will be bound to MPI ranks based on the <code>depth</code> argument.</li> <li><code>--env OMP_NUM_THREADS=${NTHREADS}</code>: This is setting the environment variable <code>OMP_NUM_THREADS</code> to determine the number of OpenMP threads per MPI rank.</li> <li><code>--env OMP_PLACES=threads</code>: This is indicating how OpenMP should distribute threads across the resource, in this case across hardware threads.</li> </ul>"},{"location":"running-jobs/example-job-scripts/#hardware-threads","title":"Hardware threads","text":"<p>This example is similar to the previous, but it exhausts all 64 logical cores available on each compute node CPU. We double the number of MPI ranks to 32, one per each physical core. Using <code>--cpu-bind=core</code>, the <code>--depth</code> flag value becomes interpreted by Cray MPICH as spacing in number of physical cores, so <code>NDEPTH=1</code> ensures that rank 0 is bound to CPU core IDs <code>(0,32)</code>, the 2 SMT sibling hardware threads that share the first physical core.</p> submit_hw_threads.sh<pre><code>#!/bin/bash -l\n#PBS -l select=1:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -l filesystems=home:eagle\n#PBS -q debug\n#PBS -A Catalyst\n\n# Change to working directory\ncd ${PBS_O_WORKDIR}\n\n# MPI and OpenMP settings\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS_PER_NODE=32\nNDEPTH=1\nNTHREADS=2\n\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}\"\n\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind core --env OMP_NUM_THREADS=${NTHREADS} -env OMP_PLACES=threads ./hello_affinity\n</code></pre> <p>Many HPC applications do not benefit from utilizing the CPU's SMT2 capabilities, and such software may achieve better performance by using the previous script such that each of the 32 physical cores only runs a single OpenMP thread.</p>"},{"location":"running-jobs/example-job-scripts/#gpu-mpi-examples","title":"GPU MPI Examples","text":"<p>Using the CPU job submission examples above as a baseline, there are not many additional changes needed to enable an application to make use of the 4 NVIDIA A100 GPUs on each Polaris node. In the following 2-node example (because <code>#PBS -l select=2</code> indicates the number of nodes requested), 4 MPI ranks will be started on each node assigning 1 MPI rank to each GPU in a round-robin fashion. A simple example using a similar job submission script on Polaris is available in the Getting Started Repo.</p> submit_gpu.sh<pre><code>#!/bin/bash -l\n#PBS -l select=2:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -l filesystems=home:eagle\n#PBS -j oe\n#PBS -q debug\n#PBS -A Catalyst\n\n# Enable GPU-MPI (if supported by application)\nexport MPICH_GPU_SUPPORT_ENABLED=1\n\n# Change to working directory\ncd ${PBS_O_WORKDIR}\n\n# MPI and OpenMP settings\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS_PER_NODE=$(nvidia-smi -L | wc -l)\nNDEPTH=8\nNTHREADS=1\n\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}\"\n\n# For applications that internally handle binding MPI/OpenMP processes to GPUs\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth --env OMP_NUM_THREADS=${NTHREADS} -env OMP_PLACES=threads ./hello_affinity\n\n# For applications that need mpiexec to bind MPI ranks to GPUs\n#mpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth --env OMP_NUM_THREADS=${NTHREADS} -env OMP_PLACES=threads ./set_affinity_gpu_polaris.sh ./hello_affinity\n</code></pre> <p>The affinity options <code>NDEPTH=8;</code> and <code>--cpu-bind depth</code> or <code>core</code> are set to ensure that each MPI rank is bound to a separate NUMA node. If OpenMP threading is desired, set <code>NTHREADS=8</code> for each MPI rank to spawn 1 thread per physical core (all in the same NUMA domain that the rank is bound to). The OpenMP-related options are not needed if your application does not use OpenMP. Nothing additional is required on the <code>mpiexec</code> command for applications that internally manage GPU devices and handle the binding of MPI/OpenMP processes to GPUs. A small helper script is available for those with applications that rely on MPI to handle the binding of MPI ranks to GPUs. Some notes on this helper script and other key differences with the early CPU example follow.</p> <p><code>export MPICH_GPU_SUPPORT_ENABLED=1</code></p> <p>For applications that support GPU-enabled MPI (i.e. use MPI to communicate data directly between GPUs), this environment variable is required to enable GPU support in Cray's MPICH. Omitting this will result in a segfault. Support for this also requires that the application was linked against the GPU Transport Layer library (e.g. -lmpi_gtl_cuda), which is automatically included for users by the <code>craype-accel-nvidia80</code> module in the default environment on Polaris. If this gtl library is not properly linked, then users will see an error message indicating that upon executing the first MPI command that uses a device pointer.</p> <p><code>./set_affinity_gpu_polaris.sh</code></p> <p>This script is useful for those applications that rely on MPI to bind MPI ranks to GPUs on each node. Such a script is not necessary when the application handles process-gpu binding. This script simply sets the environment variable <code>CUDA_VISIBLE_DEVICES</code> to a restricted set of GPUs (e.g. each MPI rank sees only one GPU). Otherwise, users would find that all MPI ranks on a node will target the first GPU likely having a negative impact on performance. An example for this script is available in the Getting Started repo and copied below.</p>"},{"location":"running-jobs/example-job-scripts/#hardware-threads_1","title":"Hardware threads","text":"submit_gpu_hw_threads.sh<pre><code>#!/bin/bash -l\n#PBS -l select=2:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -l filesystems=home:eagle\n#PBS -q debug\n#PBS -A Catalyst\n\n# Enable GPU-MPI (if supported by application)\nexport MPICH_GPU_SUPPORT_ENABLED=1\n\n# Change to working directory\ncd ${PBS_O_WORKDIR}\n\n# MPI and OpenMP settings\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS_PER_NODE=$(nvidia-smi -L | wc -l)\nNDEPTH=16\nNTHREADS=16\n\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}\"\n\n# For applications that internally handle binding MPI/OpenMP processes to GPUs\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind numa --env OMP_NUM_THREADS=${NTHREADS} -env OMP_PLACES=threads ./hello_affinity\n\n# For applications that need mpiexec to bind MPI ranks to GPUs\n#mpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind numa --env OMP_NUM_THREADS=${NTHREADS} -env OMP_PLACES=threads ./set_affinity_gpu_polaris.sh ./hello_affinity\n</code></pre> <p>As in the previous hardware threads example, the MPI ranks are spaced apart assuming the user wants to utilize all 64 logical cores (achieved by setting <code>NTHREADS=$NDEPTH=16</code> and <code>--cpu-bind numa</code> here).</p> <p>In this script, we have added <code>-j oe</code> to the list of PBS options; <code>-j oe</code> combines stdout and stderr to the same file and uses the stdout filename provided (if provided). <code>-j eo</code> would do the same but use the stderr filename provided. Without these options, separate files containing stdout and stderr of the job are produced.</p> <p>Here we compare two bare-bones PBS submission scripts for a CUDA example with and without MPI:</p> No MPIWith MPI <pre><code>#!/bin/bash\n#PBS -l select=1\n#PBS -l walltime=00:10:00\n#PBS -q debug\n#PBS -l filesystems=home\n#PBS -A &lt;project-name&gt;\n#PBS -o logs/\n#PBS -e logs/\n\n\n\n$HOME/ALCFBeginnersGuide/polaris/examples/01_example_cu\n</code></pre> <pre><code>#!/bin/bash\n#PBS -l select=2\n#PBS -l walltime=00:10:00\n#PBS -q debug\n#PBS -l filesystems=home\n#PBS -A &lt;project-name&gt;\n#PBS -o logs/\n#PBS -e logs/\n\n# Count number of nodes assigned\nNNODES=`wc -l &lt; $PBS_NODEFILE`\n# set 1 MPI rank per GPU\nNRANKS_PER_NODE=4\n# calculate total ranks\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))\necho NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE}\n\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} $HOME/ALCFBeginnersGuide/polaris/examples/01_example_mpi\n</code></pre>"},{"location":"running-jobs/example-job-scripts/#setting-gpu-affinity-for-each-mpi-rank","title":"Setting GPU affinity for each MPI rank","text":"<p>The <code>CUDA_VISIBLE_DEVICES</code> environment variable is provided for users to set which GPUs on a node are accessible to an application or MPI ranks started on a node.</p> <p>A copy of the small helper script provided in the Getting Started repo is provided below for reference:</p> GPU affinity script <pre><code>#!/bin/bash -l\nnum_gpus=4\n# need to assign GPUs in reverse order due to topology\n# See Polaris Device Affinity Information https://www.alcf.anl.gov/support/user-guides/polaris/hardware-overview/machine-overview/index.html\ngpu=$((${num_gpus} - 1 - ${PMI_LOCAL_RANK} % ${num_gpus}))\nexport CUDA_VISIBLE_DEVICES=$gpu\necho \u201cRANK= ${PMI_RANK} LOCAL_RANK= ${PMI_LOCAL_RANK} gpu= ${gpu}\u201d\nexec \"$@\"\n</code></pre> <p>Note</p> <p>The <code>echo</code> command prints a helpful message for the user to confirm the desired mapping is achieved. Users are encouraged to edit this file as necessary for their particular needs.</p> <p>Warning</p> <p>If planning large-scale runs with many thousands of MPI ranks, it is advised to comment out the <code>echo</code> command above so as not to have thousands of lines of output written to <code>stdout</code>.</p>"},{"location":"running-jobs/example-job-scripts/#using-mps-on-the-gpus","title":"Using MPS on the GPUs","text":"<p>Documentation for the NVIDIA Multi-Process Service (MPS) can be found here</p> <p>In the script below, note that if you are going to run this as a multi-node job you will need to do this on every compute node, and you will need to ensure that the paths you specify for <code>CUDA_MPS_PIPE_DIRECTORY</code> and <code>CUDA_MPS_LOG_DIRECTORY</code> do not \"collide\" and end up with all the nodes writing to the same place.</p> <p>An example is available in the Getting Started Repo and discussed below. The local SSDs or <code>/dev/shm</code> or incorporation of the node name into the path would all be possible ways of dealing with that issue.</p> <pre><code>#!/bin/bash -l\nexport CUDA_MPS_PIPE_DIRECTORY=&lt;/path/writeable/by/you&gt;\nexport CUDA_MPS_LOG_DIRECTORY=&lt;/path/writeable/by/you&gt;\nCUDA_VISIBLE_DEVICES=0,1,2,3 nvidia-cuda-mps-control -d\necho \"start_server -uid $( id -u )\" | nvidia-cuda-mps-control\n</code></pre> <p>To verify the control service is running:</p> <pre><code>nvidia-smi | grep -B1 -A15 Processes\n</code></pre> <p>And the output should look similar to this:</p> <pre><code>+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A     58874      C   nvidia-cuda-mps-server             27MiB |\n|    1   N/A  N/A     58874      C   nvidia-cuda-mps-server             27MiB |\n|    2   N/A  N/A     58874      C   nvidia-cuda-mps-server             27MiB |\n|    3   N/A  N/A     58874      C   nvidia-cuda-mps-server             27MiB |\n+-----------------------------------------------------------------------------+\n</code></pre> <p>To shut down the service:</p> <p><code>echo \"quit\" | nvidia-cuda-mps-control</code></p> <p>To verify the service shut down properly:</p> <p><code>nvidia-smi | grep -B1 -A15 Processes</code></p> <p>And the output should look like this:</p> <pre><code>+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre>"},{"location":"running-jobs/example-job-scripts/#using-mps-in-multi-node-jobs","title":"Using MPS in Multi-node Jobs","text":"<p>As stated earlier, it is important to start the MPS control service on each node in a job that requires it. An example is available in the Getting Started Repo. The helper script <code>enable_mps_polaris.sh</code> can be used to start the MPS on a node.</p> <pre><code>#!/bin/bash -l\n\nexport CUDA_MPS_PIPE_DIRECTORY=/tmp/nvidia-mps\nexport CUDA_MPS_LOG_DIRECTORY=/tmp/nvidia-log\nCUDA_VISIBLE_DEVICES=0,1,2,3 nvidia-cuda-mps-control -d\necho \"start_server -uid $( id -u )\" | nvidia-cuda-mps-control\n</code></pre> <p>The helper script <code>disable_mps_polaris.sh</code> can be used to disable MPS at appropriate points during a job script, if needed.</p> <pre><code>#!/bin/bash -l\n\necho quit | nvidia-cuda-mps-control\n</code></pre> <p>In the example job script <code>submit.sh</code> below, MPS is first enabled on all nodes in the job using <code>mpiexec -n ${NNODES} --ppn 1</code> to launch the enablement script using a single MPI rank on each compute node. The application is then run as normally. If desired, a similar one-rank-per-node <code>mpiexec</code> command can be used to disable MPS on all the nodes in a job.</p> <pre><code>#!/bin/bash -l\n#PBS -l select=1:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -q debug\n#PBS -A Catalyst\n#PBS -l filesystems=home:eagle\n\ncd ${PBS_O_WORKDIR}\n\n# MPI example w/ 8 MPI ranks per node spread evenly across cores\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS_PER_NODE=8\nNDEPTH=8\nNTHREADS=1\n\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}\"\n\n# Enable MPS on each node allocated to job\nmpiexec -n ${NNODES} --ppn 1 ./enable_mps_polaris.sh\n\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth ./hello_affinity\n\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth ./set_affinity_gpu_polaris.sh ./hello_affinity\n\n# Disable MPS on each node allocated to job\nmpiexec -n ${NNODES} --ppn 1 ./disable_mps_polaris.sh\n</code></pre>"},{"location":"running-jobs/example-job-scripts/#single-node-ensemble-calculations-example","title":"Single-node Ensemble Calculations Example","text":"<p>In the script below, a set of four applications are launched simultaneously on a single node. Each application runs on 8 MPI ranks and targets a specific GPU using the <code>CUDA_VISIBLE_DEVICES</code> environment variable. In the first instance, MPI ranks 0-7 will spawn on CPUs 24-31, and GPU 0 is used. This pairing of CPUs and GPU is based on output of the <code>nvidia-smi topo-m</code> command showing which CPUs share a NUMA domain with each GPU. It is important to background processes using <code>&amp;</code> and to <code>wait</code> for all runs to complete before exiting the script or continuing on with additional work. Note, multiple applications can run on the same set of CPU resources, but it may not be optimal depending on the workload. An example is available in the Getting Started Repo.</p> <pre><code>#!/bin/bash -l\n#PBS -l select=1:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -q debug\n#PBS -A Catalyst\n#PBS -l filesystems=home:eagle\n\n#cd ${PBS_O_WORKDIR}\n\n# MPI example w/ 8 MPI ranks per node spread evenly across cores\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS_PER_NODE=8\nNTHREADS=1\n\nnvidia-smi topo -m\n\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}\"\n\nexport CUDA_VISIBLE_DEVICES=0\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --cpu-bind list:24:25:26:27:28:29:30:31 ./hello_affinity &amp;\n\nexport CUDA_VISIBLE_DEVICES=1\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --cpu-bind list:16:17:18:19:20:21:22:23 ./hello_affinity &amp;\n\nexport CUDA_VISIBLE_DEVICES=2\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --cpu-bind list:8:9:10:11:12:13:14:15 ./hello_affinity &amp;\n\nexport CUDA_VISIBLE_DEVICES=3\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --cpu-bind list:0:1:2:3:4:5:6:7 ./hello_affinity &amp;\n\nwait\n</code></pre>"},{"location":"running-jobs/example-job-scripts/#multi-node-ensemble-calculations-example","title":"Multi-node Ensemble Calculations Example","text":"<p>To run multiple concurrent applications on distinct sets of nodes, one simply needs to provide appropriate hostfiles to the <code>mpiexec</code> command. The <code>split</code> unix command is one convenient way to create several unique hostfiles, each containing a subset of nodes available to the job. In the 8-node example below, a total of four applications will be launched on separate sets of nodes. The <code>$PBS_NODEFILE</code> file will be split into several hostfiles, each containing two lines (nodes). These smaller hostfiles are then used as the argument to the <code>--hostfile</code> argument of <code>mpiexec</code> to the launch applications. It is important to background processes using <code>&amp;</code> and to <code>wait</code> for applications to finish running before leaving the script or continuing on with additional work. Note, multiple applications can run on the same set of CPU resources, but it may not be optimal depending on the workload. An example is available in the Getting Started Repo.</p> <pre><code>#!/bin/bash -l\n#PBS -l select=8:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -q debug-scaling\n#PBS -A Catalyst\n#PBS -l filesystems=home:eagle\n\ncd ${PBS_O_WORKDIR}\n\n# MPI example w/ multiple runs per batch job\nNNODES=`wc -l &lt; $PBS_NODEFILE`\n\n# Settings for each run: 2 nodes, 4 MPI ranks per node spread evenly across cores\n# User must ensure there are enough nodes in job to support all concurrent runs\nNUM_NODES_PER_MPI=2\nNRANKS_PER_NODE=4\nNDEPTH=8\nNTHREADS=1\n\nNTOTRANKS=$(( NUM_NODES_PER_MPI * NRANKS_PER_NODE ))\necho \"NUM_OF_NODES= ${NNODES} NUM_NODES_PER_MPI= ${NUM_NODES_PER_MPI} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}\"\n\n# Increase value of suffix-length if more than 99 jobs\nsplit --lines=${NUM_NODES_PER_MPI} --numeric-suffixes=1 --suffix-length=2 $PBS_NODEFILE local_hostfile.\n\nfor lh in local_hostfile*\ndo\n  echo \"Launching mpiexec w/ ${lh}\"\n  mpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --hostfile ${lh} --depth=${NDEPTH} --cpu-bind depth ./hello_affinity &amp;\n  sleep 1s\ndone\n\nwait\n</code></pre>"},{"location":"running-jobs/example-job-scripts/#job-array-example","title":"Job array example","text":"<p>In situations where you wish to repeat a job multiple times with a small change each time, such as in a parameter space study, a job array may be an option. Unlike the multi-node ensemble case above, each subjob in a job array is its own job and will have its own initialization and tear-down by PBS. Also, a job array will not block all nodes for the length of the longest running task, as is the case for an ensemble job. Jobs on Polaris cannot share nodes with other jobs, so job arrays on Polaris cannot be used to distribute work to different CPU cores or GPUs on a node. In that case, an ensemble job or using <code>mpiexec</code> as a parallel launcher can accomplish that goal.</p> <p>Both ensemble jobs and job arrays become unwieldy and inefficient for very large numbers of tasks. They either have limits to the number of tasks that can be created at once (job arrays) or are unable to refill idle nodes when tasks complete (ensemble jobs). In such cases, a workflow management tool that can manage the running of tasks is recommended.</p>"},{"location":"running-jobs/example-job-scripts/#job-array-submission-scripts","title":"Job array submission scripts","text":"<p>An example job array submission script:</p> submit_array.sh<pre><code>#!/bin/bash -l\n#PBS -l select=1:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:10:00\n#PBS -q preemptable\n#PBS -A datascience\n#PBS -l filesystems=home:eagle\n#PBS -j oe\n#PBS -r y\n#PBS -J 0-7:2\n\ncd ${PBS_O_WORKDIR}\n\n# Create a unique subdirectory for subjob with PBS_ARRAY_INDEX\nSUBDIRECTORY=\"${PBS_ARRAY_INDEX}\"\nmkdir -p ${SUBDIRECTORY}\ncd ${SUBDIRECTORY}\n\n# File name where stdout and stderr of application will be directed in subjob subdirectory\nOUT_FILE=\"subjob_${PBS_ARRAY_INDEX}.out\"\n\necho \"Running subjob ${SUBDIRECTORY}\"\necho \"Directing application output to ${SUBDIRECTORY}/${OUT_FILE}\"\n\n# MPI example w/ 16 MPI ranks per node spread evenly across cores\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS_PER_NODE=16\nNDEPTH=4\nNTHREADS=1\n\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}\"\n\nAPP_PATH=${PBS_O_WORKDIR}/hello_affinity\n\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth ${APP_PATH} &amp;&gt; ${OUT_FILE}\n</code></pre> <p>There are two required options for job arrays in PBS: <code>-r</code> and <code>-J</code>.</p> <p>The <code>-r</code> option must be set like this: <pre><code>#PBS -r y\n</code></pre></p> <p>The <code>-J</code> option sets the number of subjobs in the array and the value of their array indices. The example script above will run 4 subjobs and space their array indices in increments of 2, so the array indices will be 0, 2, 4, and 6.</p> <p>The form the <code>-J</code> option takes is <pre><code>#PBS -J &lt;start_index&gt;-&lt;end_index&gt;:&lt;skip_index&gt;%&lt;num_concurrent&gt;\n</code></pre> * <code>&lt;start_index&gt;</code> is the index of the first job in the array * <code>&lt;end_index&gt;</code> is the index of the last job in the array * <code>&lt;skip_index&gt;</code> is the number of index integers to skip between subjobs * <code>&lt;num_concurrent&gt;</code> is the maximum number of subjobs that will run concurrently at one time</p> <p>Within a subjob, the environment variable <code>PBS_ARRAY_INDEX</code> will contain the index of the subjob in the array. It can be used in the job script to set the value or paths of inputs or outputs.</p>"},{"location":"running-jobs/example-job-scripts/#interacting-with-job-arrays","title":"Interacting with job arrays","text":"<p>The status of job arrays can be queried with the command: <pre><code>qstat -t\n</code></pre></p> <p>When interacting with a job array with commands like <code>qdel</code> or <code>qalter</code>, include the brackets with the jobid, e.g.: <pre><code>qdel 1991684[]\n</code></pre></p>"},{"location":"running-jobs/example-job-scripts/#limits-on-job-arrays","title":"Limits on job arrays","text":"<p>The number of subjobs in a job array is limited by the number of jobs that can be submitted to the queue.</p> <p>On Polaris, for the debug queue, that is 1, for preemptable, that is 20, and for prod that is 10.</p> <p>The limit for prod on Polaris is 10 because 10 is the maximum number of jobs that can be routed by prod to one of the execution queues (small, medium, or large). One note, PBS will allow job array submissions of up to 100 subjobs in prod, however, these job arrays will not run because they will not route to an execution queue. This is a known issue on Polaris.</p>"},{"location":"running-jobs/job-and-queue-scheduling/","title":"Running Jobs using PBS","text":""},{"location":"running-jobs/job-and-queue-scheduling/#documentation-tools","title":"Documentation / Tools","text":"<ul> <li>The PBS \"BigBook\": This is really excellent. We highly suggest you download it and search through it when you have questions. However, it is big at about 2000 pages / 40MB and contains a bunch of stuff you don't really need, so you can also download the guides separately here:<ul> <li>The PBS Users Guide: This is the user's guide.</li> <li>The PBS Reference Guide: This is the Reference Guide. It shows every option and gives you details on how to format various elements on the command line.</li> </ul> </li> <li>Cobalt qsub options to PBS qsub options: Shows how to map Cobalt command line options to PBS command line options. Can be found at the link above.</li> </ul>"},{"location":"running-jobs/job-and-queue-scheduling/#introduction","title":"Introduction","text":"<p>At a high level, getting computational tasks run on an HPC system is a two-step process:</p> <ol> <li> <p>You request and get allocated resources (we allocate at the node level, but some facilities you request number of cores and RAM, etc.) on one or more of the systems. This is accomplished by interacting with the job scheduler / workload manager. In the ALCF, we use PBS Professional.</p> </li> <li> <p>You execute your tasks on those resources. This is accomplished in your job script by interacting with various system services (MPI, OpenMP, the HPE PALS task launch system, etc.)</p> </li> </ol> <p>Our documentation is organized in two sections aligned with the two steps described above.</p>"},{"location":"running-jobs/job-and-queue-scheduling/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Obtaining and managing compute resources at ALCF - General PBS information common to all systems<ul> <li>Definitions and Notes</li> <li>Quick Start</li> <li>qsub - submit a job to run</li> <li>qstat - query the status of jobs/queues</li> <li>qalter - alter a queued job</li> <li>qdel - delete a queued or running job</li> <li>qmove - move a job to a different queue</li> <li>qhold,qrls - place/release a hold on a job in a queue</li> <li>qselect - utility to select jobids that meet criteria</li> <li>qmsg - write a message into a job's output file</li> <li>qsig - send a signal to a job</li> <li>pbsnodes - Get information about the current state of nodes</li> <li>Using Fakeroot with Singularity</li> </ul> </li> </ul>"},{"location":"running-jobs/job-and-queue-scheduling/#obtaining-and-managing-compute-resources-at-alcf","title":"Obtaining and managing compute resources at ALCF","text":""},{"location":"running-jobs/job-and-queue-scheduling/#definitions-and-notes","title":"Definitions and Notes","text":"<p><code>chunk</code>: A set of resources allocated as a unit to a job. Specified inside a selection directive. All parts of a chunk come from the same host. In a typical MPI (Message-Passing Interface) job, there is one chunk per MPI process.</p> <p><code>vnode</code>: A virtual node, or vnode, is an abstract object representing a host or a set of resources which form a usable part of an execution host. This could be an entire host, or a nodeboard or a blade. A single host can be made up of multiple vnodes. Each vnode can be managed and scheduled independently. Each vnode in a complex must have a unique name. Vnodes on a host can share resources, such as node-locked licenses. PBS operates on vnodes. A vnode can, and in ALCF often will, represent an entire host, but it doesn't have to. For instance, there is a mode on Polaris where we could have each physical host look like four vnodes, each with 16 threads, 1/4 of the RAM and one A100.</p> <p><code>ncpus</code>: Number of resources available to execute a program. In ALCF, given the way we configure PBS, this equates to a hardware thread. For example, a single socket node with a 32 core CPU, each with two hardware threads would report that as ncpus=64.</p> <p><code>ngpus</code>: The number of allocable GPUs on the vnode. For an NVIDIA A100, this could be one, however, if we enable Multi Instance GPU (MIG) mode and use cgroups it could be as high as 7.</p> <p><code>job</code>: A job equates to a qsub. A set of resources allocated to you for a period of time. You will execute one or more <code>tasks</code> on those resources during your job.</p> <p><code>task</code>: A single execution on the resources of your job, often an <code>mpiexec</code> invocation launched by PALS or PMIx. You may run one task or many tasks during your job. You may run tasks sequentially or divide your resources up and run several tasks concurrently. Also sometimes referred to as job steps.</p>"},{"location":"running-jobs/job-and-queue-scheduling/#quick-start","title":"Quick Start","text":"<p>If you are an ALCF user and are familiar with Cobalt, you will find the PBS commands very similar though the options to qsub are quite different. Here are the \"Big Four\" commands you will use:</p> <ol> <li><code>qsub</code>: request resources (generally compute nodes) to run your job and start your script/executable on the head node. Here is the minimal qsub allowed at the ALCF:<ul> <li><code>qsub -A &lt;project&gt; -l select=&lt;# of nodes&gt;,walltime=HH:MM:SS,filesystems=fs1:fs2 &lt;your job script&gt;</code></li> <li>The <code>-A</code>, <code>walltime</code>, and <code>filesystems</code> are mandatory. You will receive errors if they are not specified.</li> <li>We automatically add <code>-k doe</code> for you. This streams your output back rather than spooling it and copying it back at the end of the job. It probably isn't a bad idea to specify it in your script, but we enforce that option, so if you try and change it, you will get an error.</li> <li>It is highly likely you will also want to add <code>-l place=scatter</code> so that each of your chunks (<code>&lt;# of nodes&gt;</code>) gets its own vnode.</li> <li>If you want to run an executable rather than a script replace <code>&lt;your jobs script&gt;</code> in the example above with <code>-- &lt;your executable&gt;</code> (that is dash dash)</li> <li>PBS Documentation: Users Guide, Chapter 2, page UG-11 and Reference Guide Chapter 2, section 2.57, page RG-216</li> </ul> </li> <li><code>qstat</code>: check on the status of your jobs or queues<ul> <li>Try these variations and see which you like best: <code>qstat</code>, <code>qstat -was</code>, <code>qstat -was1</code>, <code>qstat -wan</code>, <code>qstat -wan1</code>. Add <code>-x</code> to see jobs that have completed. We keep two weeks of history.</li> <li><code>qstat -Q</code> will list all the queues in case you forget.</li> <li>PBS Documentation: Users Guide Sec. 10.2, page UG-175; Reference Guide Sec. 2.55, page RG-200</li> </ul> </li> <li><code>qalter</code>: update your request for resources<ul> <li>Just like qsub, just add a jobid at the end. Only works before the job starts;</li> <li>If you want to change the walltime to 30 minutes: <code>qalter -l walltime=30:00:00 &lt;jobid&gt;</code></li> <li>PBS Documentation: Users Guide Sec. 9.2, page UG-168; Reference Guide Sec. 2.40, page RG-130</li> </ul> </li> <li><code>qdel</code>: cancel a job that you don't need. This will also kill a running job<ul> <li><code>qdel &lt;jobid&gt;</code></li> <li>Occasionally, the job will still show up in <code>qstat</code> after you try and <code>qdel</code> it. When this happens you can try <code>qdel -W force &lt;jobid&gt;</code>. If it still won't go away, please send mail to support@alcf.anl.gov and one of the administrators can remove it for you. DO NOT just default to using <code>-W force</code>. The force does not do all of the clean up and can cause problems of its own.</li> <li>PBS Documentation: Users Guide Sec. 9.3, page UG-170; Reference Guide Sec. 2.41, page RG-143</li> </ul> </li> </ol> <p>Note: The page numbers in the PBS guides are unique. If you search for the specified page number it will take you directly to the relevant page.</p>"},{"location":"running-jobs/job-and-queue-scheduling/#qsub-submit-a-job-to-run","title":"<code>qsub</code>: submit a job to run","text":"<p>Users Guide, Chapter 2, page UG-11 and Reference Guide Chapter 2, section 2.57, page RG-216</p> <p>At the ALCF, your qsub will likely use the following parameters:</p> <p><code>qsub -A &lt;project&gt; -k doe -l select=&lt;#&gt;:system=&lt;name&gt;, walltime=HH:MM:SS, filesystems=fs1:fs2, place=scatter &lt;your job script&gt;</code></p> <p>Where:</p> <ul> <li>project is the project name associated with your allocation. What you check the balance of with the <code>sbank</code> command. This is a mandatory option at the ALCF. If you don't include it you will get <code>qsub: Account_Name is required to be set.</code></li> <li>-k doe is telling PBS to stream your output rather than buffer it on the compute nodes and then scp it at the end of the job. Note we will automatically add this if you don't specify it. We enforce this option, so if you try and specify any other output handling you will get an error.</li> <li># of chunks (typically nodes). Each of our systems has a PBS \"resource\" called <code>system</code> defined and set to the system name (polaris, sunspot, etc)</li> <li><code>walltime=HH:MM:SS</code> specifying a wall time is mandatory at the ALCF. Valid wall times depend on the queue you are using. There is a table with the queues for each machine at the end of this section and in the machine specific documentation.</li> <li><code>filesystems=fs1:fs2:...</code> Specifying which filesystems your application uses is mandatory at ALCF. The reason for this is if a filesystem goes down, we have a way of making PBS aware of that and it won't run jobs that need that filesystem. If you don't specify filesystems you will receive the following error: <code>qsub: Resource: filesystems is required to be set.</code></li> <li><code>place=scatter</code> is telling PBS you want each of your chunks on a separate vnode. By default, PBS will pack your chunks to get maximum utilization. If you requested <code>ncpus=1</code> and <code>chunks=64</code> without <code>place=scatter</code> on a system with <code>ncpus=64</code>, all your chunks would end up on one node.</li> <li>Your job script:  See Example Job Scripts for more information about how to build your job script. For options that won't change, you do have the option of taking things off the command line and putting them in your job script. For instance the above command line could be simplified to <code>qsub -l select=&lt;#&gt; &lt;your job script&gt;</code> if you added the following to the top (the PBS directives have to be before any executable line) of your job script:</li> </ul> <pre><code>#PBS -A &lt;project&gt;\n#PBS -k doe\n#PBS -l walltime=HH:MM:SS\n#PBS -l filesystems=fs1:fs2\n</code></pre> <p>Also note that if you want to run an executable directly rather than a script you use two dashes and the executable name in place of your script name like this: <code>-- /usr/bin/sleep 600</code></p>"},{"location":"running-jobs/job-and-queue-scheduling/#more-detail","title":"More detail:","text":"<p>The single biggest difference between Cobalt and PBS is the way you select resources when submitting a job. In Cobalt, every system had its own Cobalt server and you just specified the number of nodes you wanted (-n). With PBS, we are planning on running a single \"PBS Complex\" which means there will be a single PBS server for all systems in the ALCF and you need to specify enough constraints to get your job to run on the resources you want/need. One advantage of this is that getting resources from two different systems or \"co-scheduling\" is trivially possible.</p>"},{"location":"running-jobs/job-and-queue-scheduling/#resource-selection-and-job-placement","title":"Resource Selection and Job Placement","text":"<p>Section 2.57.2.6 RG-219 Requesting Resources and Placing jobs in the Reference Guide.</p> <p>Resources come in two flavors:</p> <ul> <li>Job Wide: Walltime is the most common example of a job wide resource. You use the <code>-l</code> option to specify job wide resources, i.e. <code>-l walltime=06:00:00</code>. All the resources in the job have the same walltime.</li> <li><code>-l &lt;resource name&gt;=&lt;value&gt;[,&lt;resource name&gt;=&lt;value&gt; ...]</code></li> <li>Chunks: (see the definition above) This is how you describe what your needs are to run your job. You do this with the <code>-l select=</code> syntax. In the ALCF, we do whole node scheduling and every node has a resource called <code>system</code> which is set to the system name it belongs to (Polaris, Aurora, etc). This means you can typically get away with the very simple <code>-l select=128:system=foo</code> which will give you 128 complete nodes on system foo.</li> <li><code>-l select=[&lt;N&gt;:]&lt;chunk&gt;[+[&lt;N&gt;:]&lt;chunk&gt; ...]</code> where N specifies how many of that chunk and a chunk is of the form:</li> <li><code>&lt;resource name&gt;=&lt;value&gt;[:&lt;resource name&gt;=&lt;value&gt; ...]</code></li> <li>Here is a hypothetical example that would select resources with A100s and other resources with A40 GPUs. PBS takes care of co-scheduling the nodes on the two systems for you transparently. Note that in this case since we did not specify <code>system=</code> if there were multiple systems that could satisfy a chunk you wouldn't know ahead of time which system you would get.</li> </ul> <p><code>-l select=128:ncpus=64:ngpus=4:gputype=A100+32:ncpus=64:ngpus=2:gputype=A40</code></p> <p>You also have to tell PBS how you want the chunks distributed across the physical hardware. You do that via the <code>-l place</code> option:</p> <ul> <li><code>-l place=[&lt;arrangement&gt;][: &lt;sharing&gt; ][: &lt;grouping&gt;]</code> where</li> <li>arrangement is one of <code>free | pack | scatter | vscatter</code><ul> <li>unless you have a specific reason to do otherwise, you probably want to set this to <code>scatter</code>, otherwise you may not get what you expect. For instance on a host with ncpus=64, if you requested <code>-l select=8:ncpus=8</code> you could end up with all of our chunks on one node.</li> <li><code>free</code> means PBS can distribute them as it sees fit</li> <li><code>pack</code> means all chunks from one host. Note that this is not the minimum number of hosts, it is one host. If the chunks can't fit on one host, the qsub will fail.</li> <li><code>scatter</code> means take only one chunk from any given host.</li> <li><code>vscatter</code> means take only one chunk from any given vnode. If a host has multiple vnodes, you could end up with more than one chunk on the host.</li> </ul> </li> <li>sharing is one of <code>excl | shared | exclhost</code> where<ul> <li>NOTE: Node configuration can override your requested sharing mode. For instance, in most cases ALCF sets the nodes to <code>force_exclhost</code>, so normally you don't have to specify this.</li> <li><code>excl</code> means this job gets the entire vnode</li> <li><code>shared</code> means the vnode could be shared with another job from another user.</li> <li><code>exclhost</code> means this job gets the entire host, even if it has multiple vnodes.</li> </ul> </li> <li>group=<code>&lt;resource name&gt;</code><ul> <li>As an example, for machines that use a dragonfly network topology, we provide a PBS resource named <code>tier1</code> indicating which dragonfly group a node is in. If you wanted to ensure that all the chunks came from a single dragonfly group, you could specify <code>place=group=tier1</code> as part of your qsub. <code>tier0</code> is rack granularity, so <code>group=tier0</code> would ensure your nodes all came from one rack. Note that if you requested more nodes than were available in a rack your job would never run and you would see something like <code>Not Running: Insufficient amount of resource: tier0</code>.</li> </ul> </li> </ul> <p>We have defined placement sets for the tier0 and tier1 resources. As a result, if you don't specify a grouping PBS will preferentially group your nodes in a placement set, but it won't drain or delay your job start to do so. For example, if you request 10 nodes and don't specify a grouping, if 10 nodes are available in the same rack, all your nodes will be in one rack. If not, but there are 10 nodes in a single dragonfly group, all your nodes will be in one dragonfly group. If you wish to specify a specific rack or dragonfly group, that is accomplished via the select syntax. For instance, <code>qsub ... -l select=10:tier1=g0</code> would force your 10 nodes to be in dragonfly group 0.</p> <p>Here is a heavily commented sample PBS submission script that shows some more of the options, but remember that the PBS manuals referenced at the top of this page are the ultimate resource.</p> <pre><code>#!/bin/bash -l\n# UG Section 2.5, page UG-24 Job Submission Options\n# Add another # at the beginning of the line to comment out a line\n# NOTE: adding a switch to the command line will override values in this file.\n\n# These options are MANDATORY at ALCF; Your qsub will fail if you don't provide them.\n#PBS -A &lt;short project name&gt;\n#PBS -l walltime=HH:MM:SS\n#file systems used by the job\n#PBS -l filesystems=home:eagle\n\n\n# Highly recommended\n# The first 15 characters of the job name are displayed in the qstat output:\n#PBS -N &lt;name&gt;\n\n# If you need a queue other than the default, which is prod (uncomment to use)\n##PBS -q &lt;queue name&gt;\n\n# Controlling the output of your application\n# UG Sec 3.3 page UG-42 Managing Output and Error Files\n# By default, PBS spools your output on the compute node and then uses scp to move it the\n# destination directory after the job finishes.  Since we have globally mounted file systems\n# it is highly recommended that you use the -k option to write directly to the destination\n# the doe stands for direct, output, error\n#PBS -k doe\n#PBS -o &lt;path for stdout&gt;\n#PBS -e &lt;path for stderr&gt;\n\n# If you want to merge stdout and stderr, use the -j option\n# oe=merge stdout/stderr to stdout, eo=merge stderr/stdout to stderr, n=don't merge\n#PBS -j n\n\n# Controlling email notifications\n# UG Sec 2.5.1, page UG-25 Specifying Email Notification\n# When to send email b=job begin, e=job end, a=job abort, j=subjobs (job arrays), n=no mail\n#PBS -m be\n# Be default, mail goes to the submitter, use this option to add others (uncomment to use)\n#PBS -M &lt;email addresses&gt;\n\n# Setting job dependencies\n# UG Section 6.2, page UG-109 Using Job Dependencies\n# There are many options for how to set up dependencies;  afterok will give behavior similar\n# to Cobalt (uncomment to use)\n##PBS depend=afterok:&lt;jobid&gt;:&lt;jobid&gt;\n\n# Environment variables (uncomment to use)\n# UG Section 6.12, page UG-126 Using Environment Variables\n# RG Sect 2.57.7, page RG-233 Environment variables PBS puts in the job environment\n##PBS -v &lt;variable list&gt;\n## -v a=10, \"var2='A,B'\", c=20, HOME=/home/zzz\n##PBS -V exports all the environment variables in your environment to the compute node\n\n\n# The rest is an example of how an MPI job might be set up\necho Working directory is $PBS_O_WORKDIR\ncd $PBS_O_WORKDIR\n\necho Jobid: $PBS_JOBID\necho Running on host `hostname`\necho Running on nodes `cat $PBS_NODEFILE`\n\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS=1           # Number of MPI ranks per node\nNDEPTH=1           # Number of hardware threads per rank, spacing between MPI ranks on a node\nNTHREADS=1         # Number of OMP threads per rank, given to OMP_NUM_THREADS\n\nNTOTRANKS=$(( NNODES * NRANKS ))\n\necho \"NUM_OF_NODES=${NNODES}  TOTAL_NUM_RANKS=${NTOTRANKS}  RANKS_PER_NODE=${NRANKS}  THREADS_PER_RANK=${NTHREADS}\"\n\nmpiexec --np ${NTOTRANKS} -ppn ${NRANKS} -d ${NDEPTH} -env OMP_NUM_THREADS=${NTHREADS} ./hello_mpi\n</code></pre>"},{"location":"running-jobs/job-and-queue-scheduling/#email-notifications","title":"Email Notifications","text":"<p>Users should add <code>-M &lt;email address&gt;</code> if they want notifications as a best practice.</p> <p>Note: For users with '@alcf.anl.gov' email addresses, PBS will send out an email once the job has ended by default. If you do not want to receive these notifications, you will need to add <code>#PBS -m n</code> to your script."},{"location":"running-jobs/job-and-queue-scheduling/#specifying-filesystems","title":"Specifying Filesystems","text":"<p>Note: The <code>filesystems</code> attribute is mandatory. If you do not specify a filesystem(s) you will receive the following error message upon submission:</p> <p><code>qsub: Resource: filesystems is required to be set.</code></p> <p>Valid filesystems are <code>home</code> and <code>eagle</code>. For example, to request the home and Eagle filesystems for your job you would add <code>-l filesystems=home:eagle</code> to your <code>qsub</code> command.</p> <p>If a job is submitted while a filesystem it requested is marked down, the job will be queued but will not run, with a message in the comment field of the job as to why it is not running. Run <code>qstat -f &lt;jobid&gt;</code> to see the comment field. For example, if the job requested for eagle and if Eagle is unavailable, the comment field will have <code>Can Never Run: Insufficient amount of server resource: eagle_fs (True != False)</code>). Once the affected filesystem has been returned to normal operation, and the filesystem is marked as being available, the job will then be scheduled normally. The job cannot run until all filesystems requested by the job are available.</p> <p>If a job requesting a filesystem that is marked down is already in the queue, the job will be not run until all of its requested filesystems are available.</p> <p>An example of a job requesting filesystems:</p> <p><code>qsub -l select=10:ncpus=64,walltime=30:00,filesystems=eagle:home -A ProjectX -q prod my_job.sh</code></p> <p>To update the filesystems list for your job, use <code>qalter</code>.</p>"},{"location":"running-jobs/job-and-queue-scheduling/#qsub-examples","title":"qsub examples","text":"<ul> <li><code>qsub -A my_allocation -l select=4:system=polaris -l filesystems=home:eagle -l walltime=30:00 -q debug-scaling -- a.out</code><ul> <li>run a.out on 4 chunks on polaris with a walltime of 30 minutes in debug-scaling queue; charge my_allocation;</li> <li>Since we allocate full nodes on Polaris, 4 chunks will be 4 nodes. If we shared nodes, that would be 4 threads.</li> <li>use the -- (dash dash) syntax when directly running an executable.</li> </ul> </li> <li><code>qsub -A my_allocation -l place=scatter  -l filesystems=home:eagle -l select=32:ncpus=32 -q prod -l walltime=30:00 mpi_mm_64.sh</code><ul> <li>32 chunks on any system that meets the requirements. Each chunk must have 32 HW threads; <code>place=scatter</code> means use a different vnode for each chunk, even if you could fit more than one on a vnode. Use the queue named <code>prod</code>.</li> </ul> </li> </ul>"},{"location":"running-jobs/job-and-queue-scheduling/#qstat-query-the-status-of-jobsqueues","title":"<code>qstat</code>: Query the status of jobs/queues","text":"<p>Users Guide Sec. 10.2, page UG-175; Reference Guide Sec. 2.55, page RG-200</p>"},{"location":"running-jobs/job-and-queue-scheduling/#jobs","title":"Jobs","text":"<p>At its most basic, you just type <code>qstat</code> and it will list all the jobs currently running, queued, or held on the system. If you are interested in a specific job or jobs, you can provide a space-separated list on the command line: <code>qstat job1 job2...</code>.</p> <pre><code>Job id            Name             User              Time Use S Queue\n----------------  ---------------- ----------------  -------- - -----\n349726.polaris-p* PDE2             user1                    0 Q prod\n336987.polaris-p* inf_clDB         user2                    0 H large\n353205.polaris-p* 3d-2.sub         user3             2044:14* R large\n</code></pre> <p>One of the annoying things about <code>qstat</code> is that the output fields are fixed width and it will truncate the output. This is indicated by an asterisk as the last character. You can add <code>-w</code> for wide. It doesn't prevent truncation, but makes it less likely. A useful variant is <code>qstat -was1</code>. It shows the number of nodes, tasks, the requested walltime, and the comment, all on one line. <code>qstat -wan</code> will give you the node list you ran on, just remember that can be long. If you want an estimate of when the job will start, add the <code>-T</code> option. Note that start time is not available for all jobs, just the next N jobs that are expected to run. If you want to know everything there is to know about the job, add the <code>-f</code> flag.</p> <p><pre><code>                                                            Req'd  Req'd   Elap\nJob ID          Username Queue    Jobname    SessID NDS TSK Memory Time  S Time\n--------------- -------- -------- ---------- ------ --- --- ------ ----- - -----\n353201.polaris* user1    large    3d-1.sub    34449  60 38*    --  24:00 R 08:25    Job run at Tue Nov 15 at 16:44 on (x3006c0s13b1n0:ngpus=4:ncpus=64)+(x...\n353289.polaris* user2    medium   run_mae_l*    --   32 20*    --  12:00 Q   --     Not Running: Job would conflict with reservation or top job\n353411.polaris* user3    large    1310W60       --   64  64    --  06:00 Q   --     Not Running: Not enough free nodes available\n336990.polaris* user4    large    inf_clDB      --  464 29*    --  01:00 H   --     Job held by user4 on Mon Oct  3 20:16:26 2022\n</code></pre> The <code>comment</code> field is your friend. Wondering why your job isn't running? Check the comment. Wondering about the fate of a finished job? Add the <code>-x</code> option to see finished jobs (our history retention is currently set at two weeks) and check the comment. This cannot be stressed enough. Often, when a user ticket comes in about PBS, we answer it by looking at the comment.</p> <p>If you are familiar with <code>jq</code> or some other command line JSON tool, the <code>-F JSON</code> option can be quite handy. <code>grep</code> is great, but when you grep the <code>-f</code> output for something, you probably want to know which node the found lines belong to. With the JSON output that is trivial.</p> <pre><code>allcock@polaris-login-02:~/.ssh&gt;  qstat -fF JSON | jq '.Jobs | map_values(select(.job_state == \"R\") | {Job_Name, Account_Name, qtime, stime})'\n{\n  \"349710.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov\": {\n    \"Job_Name\": \"P38\",\n    \"Account_Name\": \"CompBioAffin\",\n    \"qtime\": \"Fri Nov  4 11:04:12 2022\",\n    \"stime\": \"Fri Nov 11 07:52:12 2022\"\n  },\n  \"352220.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov\": {\n    \"Job_Name\": \"mdsim_10000_run1.pbs\",\n    \"Account_Name\": \"RL-fold\",\n    \"qtime\": \"Thu Nov 10 22:41:55 2022\",\n    \"stime\": \"Fri Nov 11 09:00:12 2022\"\n  },\n</code></pre>"},{"location":"running-jobs/job-and-queue-scheduling/#queues","title":"Queues","text":"<p><code>qstat -Q</code> Will show you the names of all the queues and tell you their status. If they are enabled (Ena column), you can queue jobs into them. If they are started (Str column) then the scheduler will try and run jobs from it. There is a <code>-f</code> (full) option but that is mostly for admins, though you can find the min and max node count <code>(resources_[min|max].nodect)</code> and min and max walltime <code>(resources_[min|max]walltime)</code> from the output. Those values are also available in this documentation.</p>"},{"location":"running-jobs/job-and-queue-scheduling/#qalter-alter-a-queued-job","title":"<code>qalter</code>: Alter a queued job","text":"<p>Users Guide Sec. 9.2, page UG-168; Reference Guide Sec. 2.40, page RG-130</p> <p>Basically takes the same options as <code>qsub</code>;  Say you typoed and set the walltime to 300 minutes instead of 30 minutes. You could fix it (if the job had not started running) by doing <code>qalter -l walltime=30:00 &lt;jobid&gt; [&lt;jobid&gt; &lt;jobid&gt;...]</code>  The new value overwrites any previous value.</p>"},{"location":"running-jobs/job-and-queue-scheduling/#qdel-delete-a-queued-or-running-job","title":"<code>qdel</code>: Delete a queued or running job:","text":"<p>Users Guide Sec. 9.3, page UG-170; Reference Guide Sec. 2.41, page RG-143</p> <p><code>qdel &lt;jobid&gt; [&lt;jobid&gt; &lt;jobid&gt;...]</code></p> <p>Occasionally, the job will still show up in <code>qstat</code> after you try and <code>qdel</code> it. When this happens you can try <code>qdel -W force &lt;jobid&gt;</code>. If it still won't go away, please send mail to support@alcf.anl.gov  and one of the administrators can remove it for you. DO NOT just default to using <code>-W force</code>. The force does not do all of the clean up and can cause problems of its own.</p>"},{"location":"running-jobs/job-and-queue-scheduling/#qmove-move-a-job-to-a-different-queue","title":"<code>qmove</code>: Move a job to a different queue","text":"<p>Users Guide Sec. 9.7, page UG-173; Reference Guide Sec. 2.46, page RG-175</p> <ul> <li><code>qmove &lt;new queue&gt; &lt;jobid&gt; [&lt;jobid&gt; &lt;jobid&gt;...]</code></li> <li>Only works before a job starts running</li> </ul>"},{"location":"running-jobs/job-and-queue-scheduling/#qholdqrls-place-release-a-user-hold-on-a-job","title":"<code>qhold,qrls</code>: Place / release a user hold on a job","text":"<p>Reference Guide Sec 2.44, page RG-150 and Sec 2.50, page RG-183</p> <ul> <li><code>[qhold | qrls] &lt;jobid&gt; [&lt;jobid&gt; &lt;jobid&gt;...]</code></li> </ul>"},{"location":"running-jobs/job-and-queue-scheduling/#qselect-query-jobids-for-use-in-commands","title":"<code>qselect</code>: Query jobids for use in commands","text":"<p>Users Guide Sec. 10.1, page UG-175; Reference Guide Sec. 2.52, page RG-189</p> <ul> <li><code>qdel `qselect -N test1`</code> will delete all the jobs that had the job name set to <code>test1</code>.</li> </ul>"},{"location":"running-jobs/job-and-queue-scheduling/#qmsg-write-a-message-into-a-jobs-output-file","title":"<code>qmsg</code> Write a message into a job's output file","text":"<p>Users Guide Sec. 9.4, page UG-171; Reference Guide Sec. 2.47, page RG-177</p> <ul> <li><code>qmsg -E -O \"This is the message\" &lt;jobid&gt; [&lt;jobid&gt; &lt;jobid&gt;...]</code></li> <li><code>-E</code> writes it to standard error, <code>-O</code> writes it to standard out</li> </ul>"},{"location":"running-jobs/job-and-queue-scheduling/#qsig-send-a-signal-to-a-job","title":"<code>qsig</code> Send a signal to a job","text":"<p>Users Guide Sec. 9.5, page UG-172; Reference Guide Sec. 2.53, page RG-195</p> <ul> <li><code>qsig -s &lt;signal&gt; &lt;jobid&gt; [&lt;jobid&gt; &lt;jobid&gt;...]</code></li> <li>If you don't specify a signal, <code>SIGTERM</code> is sent.</li> </ul>"},{"location":"running-jobs/job-and-queue-scheduling/#pbsnodes-get-information-about-the-current-state-of-nodes","title":"<code>pbsnodes</code> Get information about the current state of nodes","text":"<p>Reference Guide Sec 2.7 page RG-36</p> <p>This is more for admins, but it can tell you what nodes are free (state), how many \"CPUs\" which is actually the number of threads (ncpus), how many GPUs (ngpus) which with some GPUs like NVIDIA A100s can change depending on the MIG mode, and if the node is shared or not (sharing).</p> <p><code>pbsnodes &lt;node name&gt;</code>: Everything there is to know about a node</p> <pre><code>&gt; pbsnodes x3002c0s7b1n0\nx3002c0s7b1n0\n     Mom = x3002c0s7b1n0.hsn.cm.polaris.alcf.anl.gov\n     Port = 15002\n     pbs_version = 2022.1.1.20220926110806\n     ntype = PBS\n     state = free\n     pcpus = 64\n     resources_available.arch = linux\n     resources_available.demand = False\n     resources_available.gputype = A100\n     resources_available.host = x3002c0s7b1n0\n     resources_available.mem = 527672492kb\n     resources_available.ncpus = 64\n     resources_available.ngpus = 4\n     resources_available.system = polaris\n     resources_available.tier0 = x3002-g0\n     resources_available.tier1 = g0\n     resources_available.vnode = x3002c0s7b1n0\n     resources_assigned.accelerator_memory = 0kb\n     resources_assigned.hbmem = 0kb\n     resources_assigned.mem = 0kb\n     resources_assigned.naccelerators = 0\n     resources_assigned.ncpus = 0\n     resources_assigned.ngpus = 0\n     resources_assigned.vmem = 0kb\n     resv_enable = True\n     sharing = force_exclhost\n     license = l\n     last_state_change_time = Tue Nov 15 19:26:39 2022\n     last_used_time = Tue Nov 15 19:26:39 2022\n     server_instance_id = polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov:15001\n</code></pre> <p><code>pbsnodes -avSj</code>: A nice table to see what is free and in use</p> <pre><code>&gt; pbsnodes -avSj\n                                                        mem       ncpus   nmics   ngpus\nvnode           state           njobs   run   susp      f/t        f/t     f/t     f/t   jobs\n--------------- --------------- ------ ----- ------ ------------ ------- ------- ------- -------\nx3014c0s19b0n0  job-exclusive        1     1      0  503gb/503gb   63/64     0/0     4/4 353394\nx3014c0s19b1n0  resv-exclusive       0     0      0  503gb/503gb    0/64     0/0     4/4 --\nx3014c0s1b0n0   offline              0     0      0  503gb/503gb   64/64     0/0     4/4 --\n</code></pre> <p><code>pbsnodes -avSj | grep free | wc -l</code>: A quick way to see how many nodes are free</p> <pre><code>[20220217-21:09:30]&gt; pbsnodes -avSj | grep free | wc -l\n38\n</code></pre> <p><code>pbsnodes -avSj | grep free | awk '{print $1}'</code>: Lists the free nodes</p> <pre><code>[20220217-21:09:30]&gt; pbsnodes -avSj | grep free | awk '{print $1}'\nx3201c0s25b0n0\nx3209c0s13b0n0\nx3209c0s19b0n0\nx3209c0s1b1n0\n</code></pre> <p><code>pbsnodes -l</code>: (lowercase  l) see which nodes are down. The comment often indicates why it is down</p> <pre><code>[20220217-21:10:31]&gt; pbsnodes -l\nx3014c0s19b0n0       offline,resv-exclusive Xid 74 -- GPUs need reseat\nx3014c0s25b0n0       offline,resv-exclusive Checking on ConnectX-5 firmware\n</code></pre>"},{"location":"running-jobs/job-and-queue-scheduling/#job-priority","title":"Job Priority","text":"<p>In PBS it is not easy to see a priority order for which jobs will run next. The best way is to use the <code>-T</code> option on qsub and look at the estimated start times. ALCF runs a custom scheduler algorithm, but in general, the job priority in the queue is based on several criteria:</p> <ol> <li>positive balance of your project</li> <li>size (in nodes) of the job, larger jobs receive higher priority</li> <li>the type of project (e.g. INCITE, ALCC, or discretionary)</li> <li>job duration: shorter duration jobs will accumulate priority more quickly, so it is best to specify the job run time as accurately as possible</li> </ol>"},{"location":"running-jobs/job-and-queue-scheduling/#troubleshooting-common-errors","title":"Troubleshooting / Common Errors","text":"<p>If you receive a <code>qsub: Job rejected by all possible destinations</code> error, then check your submission parameters. The issue is most likely that your walltime or node count do not fall within the ranges listed above for the production execution queues. Please see the table above for limits on production queue job sizes.</p> <p>Job missing from queue</p> <p>If you receive a job ID but you cannot find your job with <code>qstat</code>, then this may be a submission parameter issue. This can happen for batch submission because the job is being accepted into the routing (<code>prod</code>) queue. The routing/<code>prod</code> queue's parameters are more broad since it needs to accommodate for all three production queues (<code>small</code>, <code>medium</code>, &amp; <code>large</code>). The prod routing queue accepts the job, generating a job ID. Depending on what is going on with the system, the routing may or may not occur before the <code>qsub</code> returns (i.e., if the queues are backed-up the routing queue can't route the job before the <code>qsub</code> returns). If the routing is delayed then a job ID is returned, and routing is completed later. Since the <code>qsub</code> has ended then there isn't a way to inform the user that this has been rejected by all routing destinations. If you run a <code>qstat</code> on the <code>jobid</code>, it will return <code>qstat: Unknown Job Id &lt;jobid&gt;</code>.</p>"},{"location":"running-jobs/job-and-queue-scheduling/#using-fakeroot-with-singularity","title":"Using Fakeroot with Singularity","text":"<p>The fakeroot feature (commonly referred to as rootless mode) allows an unprivileged user to run a container as a \u201cfake root\u201d user by leveraging user namespace UID/GID mapping. To request this feature be enabled on your job add the following to your <code>qsub</code> command line:</p> <p><code>-l singularity_fakeroot=true</code></p>"},{"location":"running-jobs/machine-reservations/","title":"Machine Reservations","text":"<p>To get a reservation, you must first demonstrate a need to run outside of the normal queueing policies. Reservations are available only to projects with a positive allocation.</p> <p>A 5-business-day lead time is recommended to ensure timely approval. Scheduling is contingent on machine availability.</p> <p>Disclaimer</p> <p>Approval for reservation requests is subject to their appropriateness and machine availability. Not all requests will be approved. It is particularly difficult to accommodate reservation requests during busy times of the year, e.g., Supercomputing, end of the ALCC and INCITE allocation cycles.</p> <p>Kindness Policy</p> <p>We do monitor reservation utilization. The Scheduling Committee reserves the right to cancel reservations without notice if we decide a reservation is underutilized, not being properly utilized, or otherwise wasting resources. For instance, requesting a 12-hour reservation for interactive work, but then going to lunch leaving the reservation empty with no work.</p> <p>Early Completion Policy</p> <p>If you have finished running your jobs before your reservation has ended, please reach out to the support team (support@alcf.anl.gov) to release it for other users. At this time, there is no way for a user to release a reservation early.</p>"},{"location":"running-jobs/machine-reservations/#relevant-terms","title":"Relevant terms","text":"Reservation A number of nodes on a system set aside from the general pool of resources for a limited time. Only certain users or projects can submit to the queues assigned to the reservation. Score boost A job with a boosted score helps it move ahead in the queue, but it still allows the scheduler to more efficiently fill the machine."},{"location":"running-jobs/machine-reservations/#-fill-out-the-form-","title":"-&gt; Fill out the Form &lt;-","text":""},{"location":"running-jobs/machine-reservations/#querying-reservations-via-command-line","title":"Querying Reservations via Command Line","text":"<p>You can see reservations using the <code>pbs_rstat</code> command:</p>"},{"location":"running-jobs/machine-reservations/#submitting-to-a-reservation","title":"Submitting to a reservation","text":"<p>Use the <code>pbs_rstat</code> command on the login node to view the list of all reservations.</p> <pre><code>&gt; pbs_rstat\nResv ID      Queue     User     State               Start / Duration / End             \n---------------------------------------------------------------------------\nA123456.po   A123456   smith@   CO       Mon Aug 18 09:00 / 43200 / Tue Aug 19 11:00\n</code></pre> <p>For recurring reservations, the <code>reserve_start</code> and <code>reserve_end</code> are always the first instance. <code>reserve_index</code> and <code>reserve_count</code> tell you where you are in the recurrence.</p>"},{"location":"running-jobs/machine-reservations/#using-a-reservation","title":"Using a Reservation","text":"<p>Once the reservation is set up, jobs can be submitted to the reservation queue prior to the reservation start time. In the example above, the queue name is shown in the <code>Queue</code> column.</p> <pre><code>qsub -q A123456 walltime=60:00 -l select=1024:system=polaris -l filesystems=eagle myprog.exe\n</code></pre> <p>For jobs using 33 percent or more of a system, place your job in the queue at least 12 hours prior to the start of the reservation or your reservation may be canceled. The machine will start to drain for your reservation, and it is important that your job is ready to run.</p> <p>You can also move jobs from the regular queue to the reservation queue at any time using the <code>qmove</code> command. Keep in mind that a job won't start unless enough time is left in the reservation.</p> <p>Danger</p> <p>There is absolutely no time padding at the end of the reservation. When the reservation ends, all jobs are terminated, deleted, and the reservation queue is deleted. If a routing queue is used for the reservation, then jobs may be preserved, but any running job(s) are still terminated.</p>"},{"location":"running-jobs/pbs-qsub-options-table/","title":"PBS Pro <code>qsub</code> Options","text":"<p>Version 1.2 2021-04-28 </p> <p><code>-l select</code> and similar options use a lowercase \"L\", <code>-I</code> for interactive is an uppercase \"I\".</p> Cobalt CLI PBS CLI PBS Directive Function and Page Reference <code>-A &lt;account_string&gt;</code> <code>-A &lt;account_string&gt;</code> <code>#PBS Account_Name=&lt;accounting string&gt;</code> \"Specifying Accounting String\u201d UG-29 <code>-n NODES</code><code>--nodecount NODES</code> <code>-l select=NODES:system=&lt;hostname&gt;</code> One or more <code>#PBS -l &lt;resource name&gt;=&lt;value&gt;</code> directives \"Requesting Resources\u201d UG-51 <code>-t</code> <code>--walltime</code> <code>-l walltime=H:MM:SS</code> One or more <code>#PBS -l &lt;resource name&gt;=&lt;value&gt;</code> directives \"Requesting Resources\u201d UG-51 <code>--attrs</code> <code>filesystems=&lt;resource&gt;</code> <code>-l filesystems=&lt;resource&gt;</code> One or more <code>#PBS -l &lt;resource name&gt;=&lt;value&gt;</code> directives \"Requesting Resources\u201d UG-51 <code>-q</code> <code>-q &lt;destination&gt;</code> <code>#PBS -q &lt;queue name&gt;</code> <code>#PBS -q @&lt;server name&gt;</code> <code>#PBS -q &lt;queue name&gt;@&lt;server name&gt;</code> \"Specifying Server and/or Queue\u201d UG-29 <code>--env</code> <code>-v &lt;variable list&gt;</code> \"Exporting Specific Environment Variables\u201d UG-126 <code>--env</code> <code>-V</code> <code>#PBS -V</code> \"Exporting All Environment Variables\u201d UG-126 <code>--attrs</code> Done via custom resources and select statements \"Setting Job Attributes\u201d UG-16 <code>--dependencies=&lt;list&gt;</code> <code>-W depend=afterok:&lt;list&gt;</code> <code>#PBS depend=...</code> \"Using Job Dependencies\u201d UG-107 <code>-I</code><code>--interactive</code> <code>-I</code> Deprecated for use in a script \"Running Your Job Interactively\u201d UG-121 <code>--jobname</code> <code>-N &lt;name&gt;</code> <code>#PBS -N &lt;job name&gt;</code> <code>#PBS -WJob_Name=&lt;job name&gt;</code> \"Specifying Job Name\u201d UG-27 <code>-e</code><code>--error=</code> <code>-e &lt;path&gt;</code> <code>#PBS -e &lt;path&gt;</code><code>#PBS Error_Path=&lt;path&gt;</code> \"Paths for Output and Error Files\u201d UG-42 <code>-o</code>--output= <code>-o &lt;path&gt;</code> <code>#PBS -o &lt;path&gt;</code><code>#PBS Output_Path=&lt;path&gt;</code> \"Paths for Output and Error Files\u201d UG-42 <code>-M</code>--notify see note #1 <code>-M &lt;user list&gt;</code> <code>-m &lt;mail options&gt;</code> (<code>-m be</code> is suggested) <code>#PBS -M &lt;mail recipients&gt;</code> <code>#PBS -WMail_Users=&lt;mail recipients&gt;</code> <code>#PBS -m &lt;mail points&gt;</code> <code>#PBS -WMail_Points=&lt;mail points&gt;</code> \"Setting Email Recipient List\u201d UG-26 <code>-u</code><code>--umask</code> <code>-W umask=&lt;value&gt;</code> <code>#PBS umask=&lt;value&gt;</code> \"Changing Linux Job umask\u201d UG-45 <code>-h</code> <code>-h</code> <code>#PBS -h</code> \"Holding and Releasing Jobs\u201d UG-115 <code>--proccount</code> See Note #2 <code>-l mpiprocs</code>Not needed to get equivalent Cobalt functionality One or more <code>#PBS -l &lt;resource name&gt;=&lt;value&gt;</code> directives \"Requesting Resources\u201d UG-51"},{"location":"running-jobs/pbs-qsub-options-table/#pbs-options-that-provide-functionality-above-and-beyond-cobalt","title":"PBS options that provide functionality above and beyond Cobalt","text":"<p>Depending on policy decisions, not all of these options may be available.</p> Cobalt CLI PBS CLI PBS Directive Function and Page Reference N/A <code>-a &lt;date_time&gt;</code> <code>#PBS -a</code> \"Deferring Execution\u201d UG-119 N/A <code>-C &lt;directive prefix&gt;</code> \"Changing the Directive Prefix\u201d UG-16 N/A <code>-c &lt;interval&gt;</code> <code>#PBS -c</code> \"Using Checkpointing\u201d UG-113 N/A <code>-G</code> \"Submitting Interactive GUI Jobs on Windows\u201d UG-125 N/A <code>-J X-Y[:Z]</code> <code>#PBS -J</code> \"Submitting a Job Array\u201d UG-150 N/A <code>-j &lt;join&gt;</code> <code>#PBS Join_Path=&lt;joining option&gt;</code> \"Merging Output and Error Files\u201d UG-43 N/A <code>-k &lt;keep&gt;</code> <code>#PBS Keep_Files=&lt;keep option&gt;</code> \"Keeping Output and Error Files on Execution Host\u201d UG-44 N/A <code>-p &lt;priority&gt;</code> <code>#PBS -p</code> \"Setting Priority for Your Job\u201d UG-120 N/A <code>-P &lt;project&gt;</code> <code>#PBS project=&lt;project name&gt;</code> \"Specifying a Project for a Job\u201d UG-27 N/A <code>-r &lt;value&gt;</code> <code>#PBS -r</code> \"Allowing Your Job to be Re-run\u201d UG-118 N/A <code>-R &lt;remove options&gt;</code> \"Avoiding Creation of stdout and/or stderr\u201d UG-43 N/A <code>-S &lt;path list&gt;</code> \"Specifying the Top Shell for Your Job\u201d UG-19 N/A See Note #3 <code>-u &lt;user list&gt;</code> <code>#PBS User_List=&lt;username list&gt;</code> \"Specifying Job Username\u201d UG-28 N/A <code>-W block=true</code> <code>#PBS block=true</code> \"Making qsub Wait Until Job Ends\u201d UG-120 N/A <code>-W group_list=&lt;list&gt;</code> <code>#PBS group_list=&lt;group list&gt;</code> \"Specifying Job Group ID\u201d UG-28 N/A <code>-W release_nodes_on_stageout=&lt;value&gt;</code> \"Releasing Unneeded Vnodes from Your Job\u201d UG-127 N/A <code>-W run_count=&lt;value&gt;</code> \"Controlling Number of Times Job is Re-run\u201d UG-119 N/A <code>-W sandbox=&lt;value&gt;</code> \"Staging and Execution Directory: User Home vs. Job-specific\u201d UG-31 N/A <code>-W stagein=&lt;list&gt;</code> <code>#PBS -W stagein=&lt;execution path&gt;@&lt;input file storage host&gt;:&lt;input file storage path&gt;[,...]</code> \"Input/Output File Staging\u201d UG-31 N/A <code>-W stageout=&lt;list&gt;</code> <code>#PBS -W stageout=&lt;execution path&gt;@&lt;output file storage host&gt;:&lt;output file storage path&gt;[,...]</code> \"Input/Output File Staging\u201d UG-31 N/A <code>-X</code> \"Receiving X Output from Interactive Linux Jobs\u201d UG-124 N/A <code>-z</code> <code>#PBS -z</code> \"Suppressing Printing Job Identifier to stdout\u201d UG-30"},{"location":"running-jobs/pbs-qsub-options-table/#notes","title":"Notes","text":"<ol> <li>To get the equivalent mail notifications from PBS, it requires two parameters: the <code>-M</code> just like Cobalt, but also <code>-m be</code> (the <code>be</code> stands for \"beginning\" and \"end\") to specify when the mails should go out. This will give you the same behavior as Cobalt.</li> <li><code>--proccount</code>, while available, only changed behavior on the Blue Gene machines. To get equivalent functionality, just drop it from the CLI. In PBS, it does influence the <code>PBS_NODES</code> file. See Section 5.1.3 in the PBS Users Guide page UG-78.</li> <li>The following Cobalt options have no equivalent in PBS:<ul> <li><code>--cwd</code>: use a script and <code>cd</code> to the directory you want to run from.</li> <li><code>--user_list</code>: There is no way to do this. We will work on adding this functionality.</li> <li><code>--debuglog</code>: Are we going to try and generate the equivalent of a <code>.cobalt</code> file?</li> </ul> </li> <li>The following Cobalt options were Blue Gene specific and no longer apply:<ul> <li><code>--kernel</code></li> <li><code>-K KERNELOPTIONS</code></li> <li><code>--ion_kernel</code></li> <li><code>--ion_kerneloption</code></li> <li><code>--mode</code>: see notes on running scripts, Python, and other executables</li> <li><code>--geometry</code></li> <li><code>--disable_preboot</code></li> </ul> </li> </ol>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/","title":"PBS Admin Quick Start Guide","text":"<p>The single most important thing I can tell you is where to get the PBS BigBook.  It is very good and a search will usually get you what you need if it isn't in here.</p> <ul> <li>PBS Admin Quick Start Guide</li> <li>Checking Server Status</li> <li>Checking / Setting Node Status</li> <li>Troubleshooting</li> <li>Starting, stopping, restarting, status of the daemons:</li> <li>Starting, stopping scheduling across the entire complex</li> <li>Starting, stopping queues:</li> <li>\"Boosting\" jobs (running them sooner)</li> <li>Reservations</li> <li>MIG Mode</li> <li>Rack and Dragonfly group mappings</li> <li>Restricting a Reservation to Vnodes With Specific Resources</li> <li>Removing Blocking Resources</li> </ul>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#checking-server-status","title":"Checking Server Status","text":"<p>You can check overall server status and settings with: <code>qmgr -c \"list server\"</code> or <code>qstat -Bf</code> (add <code>-w</code> to <code>qstat</code> if you want to remove wrapping) This will show current server parameters.  If you have manager/operator permissions you will also see any hidden resources. You may also check parameters of the scheduler with <code>qmgr -c \"list sched\"</code>, and by checking <code>$PBS_HOME/sched_priv/sched_config</code>. Hook information can be checked with <code>qmgr -c \"list hook\"</code> and <code>qmgr -c \"list pbshook\"</code>.  Due to permissions all hook operations require root.</p>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#checking-setting-node-status","title":"Checking / Setting Node Status","text":"<p>The <code>pbsnodes</code> command is your friend.</p> <ul> <li>check status</li> <li><code>pbsnodes -av</code> gives you everything; grep will be useful here</li> <li><code>pbsnodes -v &lt;node&gt; &lt;node&gt; ...</code> will give you all information on the listed nodes</li> <li><code>pbsnodes -avSj</code> gives you a nice table summary</li> <li><code>pbsnodes -l</code> lists the nodes that are offline</li> <li>Taking nodes on and offline</li> <li><code>pbsnodes -C &lt;comment&gt; -o &lt;nodelist&gt;</code> will mark a node offline in PBS (unschedulable)<ul> <li>Adding the time and date and why you took it offline in the comment is helpful </li> <li><code>&lt;nodelist&gt;</code> is space separated </li> </ul> </li> <li><code>pbsnodes -r &lt;node list&gt;</code> will attempt to bring a node back online     This will only remove the \"offline\" state from a node, if the node would be down for other reasons, that will not change.         * Use -C \"\" to remove any comment that was set when the node was originally marked offline.  </li> </ul>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>PBS_EXEC (where all the executables are): <code>/opt/pbs/[bin|sbin]</code></li> <li>PBS_HOME (where all the data is): <code>/var/spool/pbs</code></li> <li>logs: <code>/var/spool/pbs/[server|mom|sched|comm]_logs</code></li> <li>config: <code>/var/spool/pbs/[server|mom|sched]_priv/</code></li> <li><code>/etc/pbs.conf</code> - Reference Guide Section 9.1, page RG-371 </li> <li><code>qstat -[x]f [jobid]</code></li> <li>the -x shows jobs that have already completed.  We are currently holding two weeks history.</li> <li>the <code>comment</code> field is particularly useful.  It will tell you why it failed, got held, couldn't run, etc..</li> <li>The jobid is optional.  Without it you get all jobs.</li> <li><code>tracejob &lt;jobid&gt;</code> </li> <li>This will pull all of the logs related to the jobid on that node.  Run on the pbs.server host to get most of the job information</li> <li>If this is run on a compute node involved in jobid then it will aggregate all logs from the mom on that job from that node.</li> <li>You may pass it the <code>-n #</code> option where # is number of days to look back to tell the command to search more days back in the logs.  This defaults to 1 day.</li> <li>This does a rudimentary aggregation and filter of the logs for you.</li> <li><code>qselect</code> -  Reference Guide Section 2.54 page RG-187.</li> <li>allows you to query and return jobids that meet criteria for instance the command below would delete all the jobs from Yankee Doodle Dandy, username yddandy:</li> <li><code>qdel `qselect -u yddandy`</code></li> <li>Error Code Table (Reference Guide Chapter 14, RG-391)</li> <li>If a CLI command (qmgr, qsub, whatever) spits out an error code at you, go look it up in the table, you may well save yourself a good bit of time.</li> <li>We are going to try and either get the error text to come with the code or write a utility to look it up and have that on all the systems.</li> </ul>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#starting-stopping-restarting-status-of-the-daemons","title":"Starting, stopping, restarting, status of the daemons:","text":"<ul> <li>Server: on pbs0 run <code>systemctl [start | stop |restart | status] pbs</code></li> <li>MoM:</li> <li>If you only want to restart a single MoM, ssh to the host and issue the same commands as above for ther server.</li> <li>If you want to restart the MoM on every compute node, <code>ssh admin.polaris</code> then do: <code>pdsh -g custom-compute \"systemctl [start | stop |restart | status] pbs\"</code> </li> </ul>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#starting-stopping-scheduling-across-the-entire-complex","title":"Starting, stopping scheduling across the entire complex","text":"<p><code>qmgr -c \"set server scheduling = [True | False]\"</code></p> <p>IMPORTANT NOTE: If we are running a single PBS complex for all our systems (same server is handling Polaris, Aurora, Cooley2, etc) this will stop scheduling on everything.</p> <p>To check the current status you may do: <code>qmgr -c \"list server scheduling\"</code></p>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#starting-stopping-queues","title":"Starting, stopping queues:","text":"<ul> <li>started: Can you queue a job or not</li> <li>enabled: Will the scheduler run jobs that are in the queue</li> </ul> <p>So if a queue is started, but not enabled, users can issue qsubs and the job will get queued, but nothing will run until we renable the queue.  Running jobs are unaffected.</p> <p><code>qmgr -c \"set queue &lt;queue name&gt; started = [True | False]\"</code> <code>qmgr -c \"set queue &lt;queue name&gt; enabled = [True | False]\"</code></p>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#boosting-jobs-running-them-sooner","title":"\"Boosting\" jobs (running them sooner)","text":"<p>There are two ways you can run a job sooner:</p> <ol> <li><code>qmove run_next &lt;jobid&gt;</code> <ol> <li>Because of the way policy is set for the acceptance testing period, any job in the <code>run_next</code> queue will run before jobs in the default <code>workq</code> with the exception of jobs that are backfilled.  So by moving the job into the <code>run_next</code> queue, you moved it to the front of the line.  There are no restrictions on this, so please do not abuse it.</li> </ol> </li> <li><code>qorder &lt;jobid&gt; &lt;jobid&gt;</code></li> <li>If you don't necessarily need it to run next, but just want to rearrange the order a bit, you can use <code>qorder</code> which swaps the positions of the specified jobids.  So, if one of them was 10th in line and one was 20th, they would switch positions. </li> <li><code>qalter -l score_boost=NNNNN &lt;jobid&gt; &lt;jobid&gt;</code>     If the <code>job_sort_function</code> is enabled and shows up when querying the server, you can add a numeric boost to the score of a job to push it further ahead in the queue. You have to be a manager or operator to alter this value.</li> </ol>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#reservations","title":"Reservations","text":"<p>Most of the reservation commands are similar to the job commands, but prefixed with <code>pbs_r</code> instead of <code>q</code>: <code>pbs_rsub, pbs_rstat, pbs_ralter, pbs_rdel</code>.  You get the picture.  In general, their behavior is reasonably similar to the equivalent jobs commands.  Note that by default, users can set their own reservations.  We have to use a hook, no_user_rsub, to prevent that.  The hook does allow anyone with manager or operator permissions to set reservations.</p> <ul> <li>There are three types of reservations:</li> <li>Advance and standing reservations - reservations for users;  Note that you typically don't specify the nodes.  You do a resource request like with qsub and PBS will find the nodes for you.</li> <li>job-specific now reservations - we have not used these.  Where they could come in handy is for debugging.  A user gets a job through, we convert it to a job-specific reservation, then if their job dies, they don't have to wait through the queue again, they can keep iterating until the wall time runs out.</li> <li>maintenance reservations. - You can explicitly set which hosts to include in the reservation.</li> <li>Also note that reservations occur in two steps.  The <code>pbs_rsub</code> will return with an ID but will say <code>unconfirmed</code>.  That means it was syntactically correct, but PBS hasn't figured out if the resources are available yet.  Once it has the resources, it will switch to confirmed.  This normally is done as fast as you can run <code>pbs_rstat</code>.  A reservation can only be confirmed if scheduling is enabled on the server.</li> <li>-R (start) -E (end) are in \"datetime\" format: [[[[CC]YY]MM]DD]hhmm[.SS] </li> <li>1315, 171315, 12171315, 2112171315 and 202112171315 would all be Dec 17th, 2021 @ 13:15<ul> <li>If that is in the future they are all equivalent and valid</li> <li>If it were Dec 17th, 2021 @ 1400, then 1315 would default to the next day @ 14:00, the rest would be errors because they are in the past. </li> <li>Be careful or this will bite you.  It will confirm the reservation and you will expect it to start in a few minutes, but it is actually for tomorrow.</li> </ul> </li> <li><code>pbs_rsub -N rsub_test -R 2023 -D 05:00 -l select=4</code></li> <li>probably not what you think: <code>resv_nodes = (edtb-03[0]:ncpus=1)+(edtb-03[0]:ncpus=1)+(edtb-03[0]:ncpus=1)+(edtb-03[0]:ncpus=1)</code>  It gave me 4 cores on the same node.</li> <li><code>pbs_rsub -N rsub_test -R 2023 -D 05:00 -l select=2 -l place=scatter</code></li> <li>Getting closer: <code>resv_nodes = (edtb-01[0]:ncpus=1)+(edtb-02[0]:ncpus=1)</code></li> <li>The <code>-l place=scatter</code> got me two different nodes, but edtb allows sharing, so I got one thread on each node, but there were actually jobs running on those nodes at the time. On Polaris, since the nodes are <code>force_exclhost</code> that wouldn't have been an issue.</li> <li><code>pbs_rsub -N rsub_test -R 2217 -D 05:00 -l select=2:ncpus=64 -l place=scatter:excl</code> This gave me what I wanted:<ul> <li><code>resv_nodes = (edtb-03[0]:ncpus=64)+(edtb-04[0]:ncpus=64)</code></li> <li>Leaving it to default to <code>ncpus=1</code> should work, but asking for them all isn't a bad idea.</li> </ul> </li> <li><code>pbs_rsub -N rsub_test -R 1200 -D 05:00 --hosts x3004c0s1b0n0 x3003c0s25b0n0...</code></li> <li>If you use <code>--hosts</code> it makes it a maintenance reservation.  You can't / don't need to add <code>-l select</code> or <code>-l place</code> on a maintenance reservation.  PBS will set it for you and will make it the entire host and exclusive access.  Nodes don't have to be up.  If jobs are running they will continue to run.  This will override any other reservation.</li> <li><code>pbs_ralter</code> You can use this to change attributes of the reservation (start time, end time, how many nodes, which users can access it, etc).  Works just like <code>qalter</code> for jobs. </li> <li><code>pbs_rdel &lt;reservation id&gt;</code>  This will kill all running jobs, delete the queue, meaning you lose any jobs that were in the queue, and release all the resources.</li> <li>NOTE: once the reservation queue is in place, you use all the normal jobs commands (qsub, qalter, qdel, etc.) to manipulate the jobs in the queue.  On the qsub you have to add <code>-q &lt;reservation queue name&gt;</code></li> </ul>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#giving-users-access-to-the-reservation","title":"Giving users access to the reservation","text":"<p>By default, only the person submitting the reservation will be able to submit jobs to the reservation queue.  You change this with the <code>-U +username@*,+username@*,...</code>.  You can add this to the initial <code>pbs_rsub</code> or use <code>pbs_ralter</code> after the fact.  The plus is basically ALLOW. We haven't tested it, but you can also theoretically use a minus for DENY.  You may also gate on group membership by setting <code>qmgr -c \"set queue &lt;reservation queue name&gt; acl_group_enable=True\"</code> and then adding groups to <code>acl_groups</code> on the reservation queue, using the same sort of syntax as you use for acl_users.  This is a bit of a hack, but if you want anyone to be able to run you can do <code>qmgr -c \"set queue &lt;reservation queue name&gt; acl_user_enable=False\"</code> </p> <p>WARNING: if you have both acl_users and acl_groups enabled, then the submitting user must be in the group and the user ACL list otherwise the job will be rejected! It is recommended that only one or the other be used on a queue.</p>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#mig-mode","title":"MIG mode","text":"<ul> <li>See the Nvidia Multi-Instance GPU User Guide for more details.</li> <li><code>sudo nvidia-smi mig -lgip</code> List GPU Instance Profiles;  This is how you find the magic numbers used to configure it below.</li> <li><code>sudo nvidia-smi mig -lgipp</code> list all the possible placements;  The syntax of the placement is <code>{&lt;index&gt;}:&lt;GPU Slice Count&gt;</code> </li> <li><code>nvidia-smi --query-gpu=mig.mode.current --format=csv,noheader</code> - check the status of all the GPUs on the node;  add <code>-i &lt;GPU number&gt;</code> to check a specific GPU</li> <li><code>systemctl stop nvidia-dcgm.service ; systemctl stop nvsm ; sleep 5 ; /usr/bin/nvidia-smi -mig 1</code> Put the node in MIG mode;  <code>-mig 0</code> will take it out of MIG mode.</li> <li><code>nvidia-smi mig -i 3 -cgi 19,19,19,19,19,19,19 -C</code> configure GPU #3 to have 7 instances.</li> <li><code>nvidia-smi mig --destroy-compute-instance; nvidia-smi mig --destroy-gpu-instance</code> Will free up the resources;  You have to do this before you can change the configuration.</li> </ul>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#polaris-rack-and-dragonfly-group-mappings","title":"Polaris Rack and Dragonfly group mappings","text":"<ul> <li>Racks contain (7) 6U chassis; Each chassis has 2 nodes for 14 nodes per rack</li> <li>The hostnames are of the form xRRPPc0sUUb[0|10]n0 where:<ul> <li>RR is the row {30, 31, 32}</li> <li>PP is the position in the row {30 goes 01-16, 31 and 32 go 01-12}</li> <li>c is chassis and is always 0 (I wish they would have counted up chasses, oh well)</li> <li>s stands for slot, but in this case is the RU in the rack. Values are {1,7,13,19,25,31,37}</li> <li>b is BMC controller and is 0 or 1 (each node has its own BMC)</li> <li>n is node, but is always 0 since there is only one node per BMC </li> </ul> </li> <li>So, 16+12+12 = 40 racks * 14 nodes per rack = 560 nodes.</li> <li>Note that in production group 9 (the last 4 racks) will be the designated on-demand racks</li> <li>The management racks are x3000 and X3100 and are dragonfly group 10</li> <li>The TDS rack is x3200 and is dragonfly group 11 </li> </ul> Group 0 Group 1 Group 2 Group 3 Group 4 Group 5 Group 6 Group 7 Group 8 Group 9 x3001-g0 x3005-g1 x3009-g2 x3013-g3 x3101-g4 x3105-g5 x3109-g6 x3201-g7 x3205-g8 x3209-g9 x3002-g0 x3006-g1 x3010-g2 x3014-g3 x3102-g4 x3106-g5 x3110-g6 x3202-g7 x3206-g8 x3210-g9 x3003-g0 x3007-g1 x3011-g2 x3015-g3 x3103-g4 x3107-g5 x3111-g6 x3203-g7 x3207-g8 x3211-g9 x3004-g0 x3008-g1 x3012-g2 x3016-g3 x3104-g4 x3108-g5 x3112-g6 x3204-g7 x3208-g8 x3212-g9"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#restricting-a-reservation-to-vnodes-with-specific-resources","title":"Restricting a Reservation to Vnodes With Specific Resources","text":"<p>You can restrict a reservation to particular resources in the select statement just like you can with job placement.  For instance, to restrict replacement to nodes that are not in the on-demand queue you can use <code>-l select=256:demand=False</code> in your select statement for a regular or repeating reservation.</p>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#removing-blocking-resources","title":"Removing Blocking Resources","text":"<p>There is a current behavior in PBS where reservations may inherit server defaults as restrictions and may not check other server values.  This may result in jobs running unexpectedly, or may cause a job to not be queued.  </p> <p>To fix jobs not being queued, some resources_max restrictions may have to be removed from the reservation queue, for example, you can clear filesystems and project_priority with the following: <code>gmgr -c \"unset queue &lt;reservation queue name&gt; resources_max.filesystems\"</code> <code>gmgr -c \"unset queue &lt;reservation queue name&gt; resources_max.project_priority\"</code></p> <p>If you need to add an additional restriction, you can likewise set a resource on the queue as a resources_max restrictions, for instance, to forbid eagle_fs from being used you can run: <code>qmgr -c \"set queue &lt;reservation queue name&gt; resources_max.eagle_fs=False\"</code> <code>qmgr -c \"set queue &lt;reservation queue name&gt; resources_mix.eagle_fs=False\"</code></p> <p>You can also set this as a part of the -l flag options at reservation creation.</p>"},{"location":"services/continuous-integration/","title":"Continuous Integration","text":""},{"location":"services/continuous-integration/#continuous-integration_1","title":"Continuous Integration","text":"<p>Continuous Integration (CI) in software development is the practice of committing code changes regularly to a version control system and having automated processes perform build, test, package, and deploy activities.</p> <p>The key concepts of CI include high frequency, repeatability, and automation in order to realize increased quality and ease of delivery. The main goal CI aims to achieve is the elimination of build and deployment issues, which in turn improves development cycles, provides a timely feedback loop with developers, and results in higher quality deliverables with reduced development time.</p> <p>CI usually describes the work that is done by a deployment or operations team to build and deploy code throughout an environment and make it available to the different interested teams involved in the SDLC. The steps that make up this process are referred to as a workflow or pipeline, which, when combined with automation, provides the mechanism for Continuous Integration.</p> <p>Today it is a common practice to use a CI tool for defining pipelines and executing the tasks required to take code from a source stored in a version control system to compiled and packaged artifacts executing in production. Two excellent examples of CI tools are Jenkins and GitLab.</p>"},{"location":"services/continuous-integration/#ci-tools-at-alcf","title":"CI Tools at ALCF","text":""},{"location":"services/continuous-integration/#gitlab-ci","title":"GitLab-CI","text":"<p>GitLab is an application that offers combined functionality as a git repository, issue tracker, and CI/CD platform. The ALCF implementation of the GitLab-CI environment leverages upstream GitLab runners combined with the ECP's Jacamar custom executor. As CI/CD is built directly into GitLab, it can allow for tighter DevOps processes. GitLab-CI is meant to provide CI/CD services for projects using GitLab-CI to store their git repositories. ALCF does not allow users to join their own private runners to our existing GitLab CI environment and provides runners on our supported systems.</p>"},{"location":"services/getting-started/","title":"ALCF Services","text":"<p>Below is a list of some of the services ALCF offers.</p> <ul> <li>JupyterHub: An interactive computing environment for Python and other languages.</li> <li>Continuous Integration: Automated processes to help build, test, package, and deploy on ALCF systems.</li> </ul>"},{"location":"services/gitlab-ci/","title":"Continuous Integration via GitLab-CI","text":""},{"location":"services/gitlab-ci/#gitlab-ci","title":"GitLab-CI","text":"<p>GitLab is an application that offers combined functionality as a git repository, issue tracker, and CI/CD platform. The ALCF implementation of the GitLab-CI environment leverages upstream GitLab runners combined with the ECP's Jacamar custom executor. As CI/CD is built directly into GitLab, it can allow for tighter DevOps processes.</p> <p>GitLab-CI is meant to provide CI/CD services for projects using GitLab-CI to store your git repositories and execute code on our HPC clusters. ALCF does not allow users to join their own private runners to our existing GitLab CI/CD environment and provides dedicated runners for our supported systems.</p> <p>Additional information, technical and user documentation, and community support can be found on GitLab's Runner website.</p> <p>Also see GitLab's CI/CD YAML syntax reference for the full list of keywords supported by GitLab CI.</p> <p>ALCF's GitLab-CI environment can be accessed by logging into the ALCF GitLab-CI web portal using your ALCF credentials (ALCF username and cryptocard token password).</p>"},{"location":"services/gitlab-ci/#quickstart","title":"Quickstart","text":"<ul> <li>A user emails ALCF Support requesting access for their ALCF Project for gitlab-ci.alcf.anl.gov.</li> <li>ALCF Support will add the ALCF Project to the appropriate system(s) via the Account and Project management system.</li> <li>ALCF will create a <code>GitLab Group/SubGroup</code> for the ALCF Project and map it to the appropriate LDAP group that maps to the ALCF Project.</li> <li>ALCF Support will reply back to the user and inform them that the project is created.</li> <li>User(s) will need to log in to gitlab-ci.alcf.anl.gov and configure their initial GitLab profile. Users will add an SSH key so they can pull/push code to the GitLab server.</li> <li>User will then need to create a <code>GitLab Project</code> in their assigned <code>GitLab Group/SubGroup</code>.</li> <li>CI/CD needs to be enabled for the GitLab Project.</li> <li>When ready to run CI/CD jobs, users will add a <code>.gitlab-ci.yml</code> file to their git repositories.</li> <li>They will need to set any ALCF specific variable(s).</li> </ul> <p>Example: <code>.gitlab-ci.yml</code> file <pre><code># this include allows us to reference defaults in anl/ci-resource/defaults\ninclude:\n  - project: 'anl/ci-resources/defaults'\n    ref: main\n    file:\n      - '/runners.yml'\n\nstages:\n  - polaris_batch # stages may have any name\n\n# the below submits a batch job to the scheduler\nsubmit_batch: # CI jobs may have any name\n  stage: polaris_batch  # from the stages list above\n  extends: .polaris-batch-runner # this includes the defaults provided in the 'anl/ci-resources/defaults' project\n  variables:  # scheduler parameters must be included, adjust the below to match your values\n    ANL_POLARIS_SCHEDULER_PARAMETERS: \"-A ProjectName -l select=1,walltime=10:00,filesystems=home -q myQueue\"\n  script:\n    - id\n    - hostname\n    - echo \"Running on $(hostname) with setuid shell runner\"\n</code></pre></p> <p>For a more complete example, see the .gitlab-ci.yml file in the large-example project.</p>"},{"location":"services/gitlab-ci/#glossary","title":"Glossary","text":"<ul> <li>Group - A collection of projects. Certain settings can be applied at the <code>Group</code> level and apply down to all child <code>SubGroups</code> and/or <code>Projects</code>. When an ALCF Project is allocated resources on the GitLab-CI environment, we will create a GitLab <code>Group</code> that will map to your ALCF Project allocation.</li> <li>Jacamar-CI - A Custom Executor we use that runs jobs as a given user on the shell and is capable of submitting jobs to schedulers like Cobalt and PBS.</li> <li>Job - An individual set of commands that are run. This is the lowest unit of GitLab-CI abstraction.</li> <li>Pipeline - GitLab organizes your jobs for each run into a <code>pipeline</code>.</li> <li>Project - GitLab Projects can be thought of as an individual git repository plus all services and features GitLab layers on top. This term is unrelated to the ALCF Project concept. ALCF Projects often map to LDAP groups and/or quotas and allocations.</li> <li>Stage - A collection of jobs in a pipeline. Jobs in the next stage will not start until the jobs in the current stage complete. If a job fails, the pipeline will not run the following stages by default.</li> <li>Triggering User - The user whose actions cause a CI/CD job to run and who the Jacamar-CI executor will run the jobs as. Examples include pushing commits up to the server, creating a merge request, and/or merging one branch into another branch.</li> </ul>"},{"location":"services/gitlab-ci/#projects-using-cicd","title":"Projects Using CI/CD","text":"<p>Any project with a git repository on the GitLab-CI environment has access to the CI/CD environment by default. In order to launch a shell job on a system, you must already have access to that system.</p>"},{"location":"services/gitlab-ci/#on-boarding-with-cicd","title":"On-Boarding with CI/CD","text":"<p>To gain access to the GitLab-CI environment, send an email to support@alcf.anl.gov requesting access for your project(s). Include with the request:</p> <ul> <li>That you are requesting access to the GitLab-CI environment at https://gitlab-ci.alcf.anl.gov</li> <li>The ALCF Project shortname</li> <li>The PI\u2019s name </li> </ul> <p>GitLab-CI jobs run as the triggering user on relevant systems. The triggering user's home directory will be used by Jacamar-CI to copy the git repository and cache files into <code>~/.jacamar-ci</code>. This job will run out of their home directory and consume filesystem quota. If you need more space, you should try to reference files in any ALCF Project allocations you have on shared filesystems. Unfortunately, the initial git clone must run out of <code>~/.jacamar-ci</code> in your home directory.</p> <p>The triggering user is defined as the user account who caused the CI/CD pipeline to execute, via scheduling a re-occurring job, pushing commits up to the server, creating a merge request, and/or merging a branch. When the CI/CD jobs run, they will run as that user on the relevant systems. For a job to succeed, the <code>triggering user</code> must have appropriate permissions and access to all relevant systems and files.</p>"},{"location":"services/gitlab-ci/#initial-login-and-profile-setup-of-gitlab-ci","title":"Initial Login and Profile Setup of GitLab-CI","text":"<ul> <li>Log in to gitlab-ci.alcf.anl.gov using your username and Cryptocard token.</li> <li>Once logged in, add your public key you already have or created earlier so that it can be associated with your account.</li> <li>Click the Profile icon on the upper right-hand corner, then click \"Edit Profile\"      GitLab Profile Dropdown screenshot </li> <li>Click \"SSH Keys\" on the left-hand menu.      GitLab Profile Add SSH Key screenshot </li> <li>Copy/Paste your SSH public key into the large text box under the word Key.<ul> <li>On Linux, Unix, and OSX-based systems using OpenSSH, your SSH public key is commonly found at <code>~/.ssh/id_rsa.pub</code>. If using Windows, you will need to consult your application's documentation on the location of your public key.</li> <li>Give it a descriptive title such as where the key resides; by default, it will extract the name from the end of the public key if possible.</li> </ul> </li> <li>Click the <code>Add Key</code> button. The button is disabled until you paste a key.</li> </ul>"},{"location":"services/gitlab-ci/#gitlab-projects-repositories","title":"GitLab Projects (repositories)","text":"<p>GitLab takes a git repository, adds additional functionality, and calls it a <code>GitLab Project</code>. This is the most common level you will be interacting with GitLab at. Please do not confuse ALCF Projects with <code>GitLab Projects</code> as they are two separate things. ALCF Projects more closely map to the <code>GitLab Group/SubGroup</code> concept, which we explain in the next section.  Once you are assigned access to a <code>GitLab Group/SubGroup</code>, you will be able to create arbitrary <code>GitLab Projects</code> underneath, configuring CI/CD jobs for each independently.</p> <p>To create a new <code>GitLab Project</code>:</p> <ul> <li>In the left pane, click \"Groups\", and then click the \"Explore groups\" link on the right.</li> </ul> <p> </p> GitLab Your Groups Page screenshot <ul> <li>From the list in the \"Explore groups\" page, click the group you were informed corresponds to your <code>ALCF Project</code>.</li> </ul> <p> </p> GitLab Explore Groups Page screenshot <ul> <li>Click the <code>New project</code> button near the upper right. If this is the first project you are creating, you will have two large square buttons near the middle of the screen to create <code>GitLab SubGroups</code> or <code>GitLab Projects</code>.</li> </ul> <p> </p> GitLab Empty Group Page screenshot <ul> <li>On the <code>Create new project</code> page, click <code>Create blank project</code>.</li> </ul> <p> </p> GitLab Create New Project screenshot <ul> <li>Fill in the <code>Project Name</code> field. The <code>Project slug</code> field will auto-populate based on the <code>Project Name</code>; do not change it. If you are pushing an existing repository, you MUST uncheck the default <code>Initialize repository with a README</code> option. Failure to uncheck this option will result in a merge conflict that you will need to resolve manually between your existing \"local\" git repository and the one you just created on the server.</li> </ul> <p> </p> GitLab Create New Project screenshot <ul> <li>Click the <code>Create project</code> button near the bottom.</li> <li>After creating the project, navigate to \"Settings\" &gt; \"General\", expand the \"Visibility, project features, permissions\" section, and enable the \"CI/CD\" toggle.</li> </ul>"},{"location":"services/gitlab-ci/#gitlab-groupssubgroups-folders","title":"GitLab Groups/SubGroups (Folders)","text":"<p>GitLab organizes <code>GitLab Projects</code> into \"folders\" called <code>Groups</code> or <code>SubGroups</code>. When an ALCF Project is granted access to GitLab-CI, a GitLab <code>Group</code> will be created with access for all members of that ALCF Project. Users will then be able to create arbitrary GitLab <code>Projects</code>. </p> <p>Each ALCF Project will have a top-level <code>Group</code> or <code>SubGroup</code> created with the ALCF Project\u2019s name. It is used for organization in the multi-project environment and is required for implementing the needed level of security. The <code>Group</code> folder is where all of your <code>GitLab Projects</code> are to be stored. You can additionally create new <code>SubGroups</code>, <code>Projects</code>, group variables, etc., within your designated <code>Group</code>, <code>SubGroups</code>, and/or <code>Projects</code>.</p> <p>To create a new <code>GitLab SubGroup</code>:</p> <ul> <li>In the left pane, click \"Groups\", and then click the \"Explore groups\" link on the right.</li> </ul> <p> </p> GitLab Your Groups Page screenshot <ul> <li>From the list in the \"Explore groups\" page, click the group you were informed corresponds to your <code>ALCF Project</code>.</li> </ul> <p> </p> GitLab Explore Groups Page screenshot <ul> <li>Click the <code>New subgroup</code> button near the upper right. If this is the first project you are creating, you will have two large square buttons near the middle of the screen to create <code>GitLab SubGroups</code> or <code>GitLab Projects</code>.</li> </ul> <p> </p> GitLab Empty Group Page screenshot <ul> <li>On the <code>Create subgroup</code> page, enter the <code>Subgroup name</code>. <code>Subgroup slug</code> will auto-populate; do not change it.</li> </ul> <p> </p> GitLab Create New SubGroup screenshot <ul> <li>Click the <code>Create subgroup</code> button near the bottom.</li> </ul>"},{"location":"services/gitlab-ci/#gitlab-runner-nodes","title":"GitLab Runner Nodes","text":"<p>Each system is assigned one or more GitLab runner node(s) that are shared by all users in GitLab-CI. Each runner is only capable of running one user's pipeline at a time, while multiple jobs in that pipeline may run in parallel.</p> <p>Each node will have two runners available, <code>shell</code> and <code>batch</code>. <code>shell</code> will run shell jobs directly on the runner node as the user. <code>batch</code> will submit the job to the HPC cluster's scheduler that is paired to that node. You will need to select the appropriate runner in your <code>.gitlab-ci.yml</code> file for the job to be executed properly. For more details on the <code>.gitlab-ci.yml</code> file, please see upstream docs.</p>"},{"location":"services/gitlab-ci/#gitlab-ciyml-configuration-sections","title":"<code>.gitlab-ci.yml</code> Configuration Sections","text":"<p>GitLab uses a per repository <code>.gitlab-ci.yml</code> file. On any commit, merge request, or merge, GitLab will attempt to trigger a CI/CD pipeline based on the contents of this file. Within the <code>.gitlab-ci.yml</code> file, you can limit jobs to only run under certain conditions. A common workflow is to have linting and validation happen on every commit to a non-master/non-main branch. Larger, more complex tasks are then performed when that branch is merged back into master/main. All jobs launched on a given event are organized into a <code>Pipeline</code>. You can watch the progress of your pipeline via the CI/CD pipeline page for your <code>Project</code>.</p> <p> </p> GitLab Group and Projects screenshot <p> </p> GitLab Group and Projects screenshot"},{"location":"services/gitlab-ci/#tags","title":"Tags","text":"<p>Tags are used to select which runner a job will be sent to. Improper tags can prevent your job from running and result in a failed job. Tags should be added by extending the defaults in the 'anl/ci-resources/defaults' runner.yml file. ALCF specific tags are described here in case overrides are needed.</p>"},{"location":"services/gitlab-ci/#alcf-specific-tags","title":"ALCF Specific tags","text":"<p>Two tags are necessary to run on our systems. One tag will select which cluster the jobs are sent to. The other will determine if the job is to be run locally on the GitLab runner host, or if it is to be submitted to a job scheduler on an HPC cluster.</p> <p>Cluster Tag(s)</p> Cluster tag Description Polaris polaris This tag will send jobs to the Polaris HPC runners <p>Job Type Tag(s)</p> tag Description shell This tag will execute the job locally on the GitLab runner host. batch This tag will submit the job to the HPC cluster's job scheduler."},{"location":"services/gitlab-ci/#variables","title":"Variables","text":"<p>Variables can be stored two ways: inline in the <code>.gitlab-ci.yml</code> file or as a setting in the GitLab <code>Group</code> or <code>Project</code> itself. Variables are exported as environment variables by gitlab-runner for each job and can be used inside the <code>.gitlab-ci.yml</code> file.</p> <p>GitLab also has a list of predefined variables available in every GitLab CI/CD pipeline.</p> <p>To set a variable directly in the <code>.gitlab-ci.yml</code> file, declare a <code>variables:</code> section with each <code>VariableName: \"VariableValue\"</code> being on its own line. <code>variables:</code> can be declared globally or in individual jobs.</p> <p>Example: Declaring variables <pre><code>variables:\n  GlobalVariable1: \"Global Value 1\"\n  GlobalVariable2: \"Global Value 2\"\n\njob:\n  variables:\n    LocalVariable: 'This is a local variable'\n  script:\n    - 'echo $LocalVariable'\n</code></pre></p> <p>To store variables in the <code>Group</code> or <code>Project</code> settings, in the left side menu, click <code>Settings&gt;CI/CD</code>. Expand the Variables option on the right side frame. You can then add variables by clicking <code>Add variable</code>.</p> <p>For more details, please see the upstream docs.</p> <p> </p> GitLab Group and Projects screenshot <p> </p> GitLab Group and Projects screenshot"},{"location":"services/gitlab-ci/#alcf-specific-variables","title":"ALCF Specific Variables","text":"<p>If you are planning to submit jobs to a scheduler, then you will need to specify a per system variable <code>ANL_${CLUSTER}_SCHEDULER_PARAMETERS</code>; where <code>${CLUSTER}</code> is the name of the cluster. This variable will contain any command line flags you would need to submit jobs as if you were on the command line/scripting. Please consult the below table for more info.</p> Cluster Scheduler Variable Name Support docs Polaris PBS ANL_POLARIS_SCHEDULER_PARAMETERS Polaris Getting Started <p>Example: Running a batch job <pre><code>include:\n  - project: 'anl/ci-resources/defaults'\n    ref: main\n    file:\n      - '/runners.yml'\n\nbatch_test:\n  extends: .polaris-batch-runner\n  variables:\n    ANL_POLARIS_SCHEDULER_PARAMETERS: \"-A ProjectName -l select=1,walltime=10:00,filesystems=home -q myQueue\"\n  script:\n    - echo \"Job start\"\n    - aprun -n 1 id\n    - aprun -n 1 hostname\n    - aprun -n 1 echo \"Running with setuid batch runner\"\n    - echo \"Job end\"\n</code></pre></p>"},{"location":"services/gitlab-ci/#stages","title":"Stages","text":"<p>Jobs can be organized into <code>stages</code>. Jobs in the next stage will not start until all dependencies in the previous stage have completed. This is often used if there are building and testing steps required before code may be run or packaged. These stages are assembled in a <code>Pipeline</code>, a directed graph of <code>stages</code>. By default, GitLab includes the following stages executed in the below order: <pre><code>.pre\nbuild\ntest\ndeploy\n.post\n</code></pre></p> <p>You may declare your own stages by first declaring a <code>stages:</code> array near the top of your <code>.gitlab-ci.yml</code> file. Stages will be processed in the order given in the array.</p> <p>Example: Declaring Stages <pre><code>stages:\n  - stage1\n  - stage2\n  - stage3\n</code></pre></p> <p>Example: Pipeline with custom stages <pre><code># this include allows us to reference defaults in anl/ci-resource/defaults\ninclude:\n  - project: 'anl/ci-resources/defaults'\n    ref: main\n    file:\n      - '/runners.yml'\n\nvariables:\n  ANL_POLARIS_SCHEDULER_PARAMETERS: \"-A ProjectName -l select=1,walltime=10:00,filesystems=home -q myQueue\"\nstages:\n  - stage1\n  - stage2\ntest1:\n  stage: stage1\n  extends: .polaris-shell-runner\n  script:\n    - export\n    - id\n    - hostname\n    - echo \"Running with setuid shell runner\" \n    - echo test &gt; test.txt\ntest2:\n  stage: stage2\n  extends: .polaris-batch-runner\n  script:\n    - echo \"Job 2 start\"\n    - aprun -n 1 id\n    - aprun -n 1 hostname\n    - aprun -n 1 echo \"Running with setuid batch runner\"\n    - echo \"Job 2 end\"\n</code></pre></p>"},{"location":"services/gitlab-ci/#rules","title":"Rules","text":"<p>GitLab allows for CI/CD jobs to be launched only if certain conditions are met. GitLab sets a series of variables in addition to any the user explicitly sets when a job launches. A job can check these variables and choose to run or not based on the results. This is often used to ensure certain jobs only run on commits, merge requests, and/or merges. By default, if any rule matches, it will run. You can override this behavior with commands like <code>when: never</code> when a conditional matches.</p> <p>For more details, please see the upstream docs.</p> <p>Rules can use the following conditional checks: <pre><code>if\nchanges\nexists\nallow_failure\nvariables\nwhen\n</code></pre></p> <p>Example: GitLab job designed to only run on merge requests <pre><code>test1:\n  rules:\n    - if: $CI_COMMIT_TAG                    # Do not execute jobs for tag context\n      when: never\n    - if: $CI_COMMIT_BRANCH == \"master\"     # Do not run on master, since will run on the merge request just prior\n      when: never\n    - if: $CI_MERGE_REQUEST_IID             # CI_MERGE_REQUEST_IID exists, so run job\n  stage: stage1\n  extends: .polaris-shell-runner\n  script:\n    - echo \"Run test 1\"\n</code></pre></p>"},{"location":"services/gitlab-ci/#job-scheduling","title":"Job Scheduling","text":"<p>GitLab provides pipeline scheduling functionality to support recurring pipelines, specified using a Cron-like syntax. Pipeline scheduling is available in the project sidebar under \"Build\" &gt; \"Pipeline schedules\". For more details, see the upstream documentation.</p>"},{"location":"services/gitlab-ci/#template-jobs","title":"Template Jobs","text":"<p>GitLab allows you to create <code>template jobs</code>, these are pieces of job specifications which can be included in jobs. Each <code>template job</code> name must begin with a period (.) and follow the same syntax as normal jobs. To instantiate a job based on the <code>template job</code>, use the keyword <code>extends</code>. If your specific job declares a key/value already in the template, the specific job will overwrite it.</p> <p>Example: Use a job template so two tests will only run on merge requests <pre><code>.MR_rules:\n  rules:\n    - if: $CI_COMMIT_TAG                    # Do not execute jobs for tag context\n      when: never\n    - if: $CI_COMMIT_BRANCH == \"master\"     # Do not run on master, otherwise runs everything from scratch on merge\n      when: never\n    - if: $CI_MERGE_REQUEST_IID\n    - if: '$CI_PIPELINE_SOURCE == \"merge_request_event\"'    \n\ntest1:\n  extends: .MR_rules\n  stage: stage1\n  extends: .polaris-shell-runner\n  script:\n    - echo \"Run test 1\"\ntest2:\n  extends: .MR_rules\n  stage: stage2\n  extends: .polaris-shell-runner\n  script:\n    - echo \"Run test 2\"\n</code></pre></p>"},{"location":"services/gitlab-ci/#console-output","title":"Console Output","text":"<p>To see the output of a job, click on it in the GUI, and it will show the STDOUT and STDERR from the job run. If the job did not launch successfully, it will have error messages from gitlab-runner or Jacamar-CI or both. Please be aware of any sensitive data you do not want exported or saved to the output console, such as passwords. Please do not output large amounts of data from your jobs to the stdout. If your CI/CD job outputs large amounts of text to STDOUT or STDERR, consider redirecting it into a job log.</p> <p> </p> GitLab Group Job Console"},{"location":"services/gitlab-ci/#storage-use-and-policy","title":"Storage Use and Policy","text":""},{"location":"services/gitlab-ci/#gitlab-project-quota","title":"GitLab Project Quota","text":"<p>Each repository has a default quota of 1GB. Quota increases may be requested by emailing Support. This quota is separate from the storage quotas allocated to ALCF Projects and ALCF Users on the HPC clusters and shared filesystems.</p>"},{"location":"services/gitlab-ci/#cicd-filesystem-usage","title":"CI/CD Filesystem Usage","text":"<p>CI/CD jobs will run out of your home directory by default. Each job will begin by cloning the repository into a path under <code>~/.jacamar-ci</code> and will continue to write there unless you reference other destinations in your CI/CD job. You will need to ensure that you have the minimum amount of space for this runner operation. If you do not, the job will fail to run. Each GitLab runner will create a new subdirectory under <code>~/.jacamar-ci</code> for itself; however, it will reuse space for subsequent pipelines launched for that project on that runner.</p> <p>It is recommended that if you need more space than your home directory can provide, you leverage any ALCF Project space you may have been allocated on a shared filesystem.</p>"},{"location":"services/gitlab-ci/#gitlab-ci-access-termination-policy","title":"GitLab-CI Access Termination Policy","text":"<p>Projects that have been inactive for at least 6 months will have their access disabled and their repositories deleted. Notification will be sent to the PI 30 days prior to the day of the action. </p> <p>Inactivity is defined as, but not limited to:</p> <ul> <li>No new projects created</li> <li>No new commits to an existing project</li> <li>Prolonged period of continuously failing CI/CD jobs (In the case of re-occurring scheduled jobs)</li> </ul>"},{"location":"services/jupyter-hub/","title":"JupyterHub","text":"<p>JupyterHub is an open-source service application that enables users to launch separate Jupyter instances on a remote server. ALCF JupyterHub provides access to Polaris with the same authentication protocol that is used to access these systems, but through a web interface rather than a terminal. On the ALCF JupyterHub home page, users can choose their desired system. Upon selection, they'll be directed to the sign-in page to enter their ALCF username and passcode token.</p> <p> </p> ALCF JupyterHub home page and sign-in screen <p>We describe below how to use JupyterHub on Polaris in more detail.</p>"},{"location":"services/jupyter-hub/#polaris","title":"Polaris","text":"<p>The Polaris JupyterHub server runs on a Polaris login node and launches individual users' environments on the compute nodes through the PBS job scheduler. After the authentication step, the user will be presented with the menu of the available job options to start the Jupyter instance.</p> <ul> <li>Select a job profile: This field lists the available profiles, which is limited to \u201cPolaris Compute Node\u201d at this time.</li> <li>Queue Name: This field provides a list of available queues on the system.</li> <li>Project List: This field displays the active projects associated with the user on Polaris.</li> <li>Number of Nodes: This field allows the user to select the number of compute nodes to be allocated.</li> <li>Runtime (minutes:seconds): This field allows the user to set the runtime of the job in minutes and seconds. The user should refer to the Polaris queue scheduling policy for minimum and maximum runtime allowed for the selected queue.</li> <li>File Systems: This field allows the user to select the file systems to be mounted. By default, all the file systems are selected.</li> </ul> <p> </p> Polaris Job options <p>Once the appropriate information is provided, the user will click the \u201cStart\u201d button and wait for the job to spawn. If there's an extended wait time due to a lengthy job queue, the interface might time out, leading to the job's removal from the queue. If not, the job kicks off and it begins to use up the user's allocation based on the chosen job options. It's crucial for users to shut down the server when resources are no longer required. Failing to do so will result in continued consumption of the allocated time until the predetermined runtime concludes.</p> <p> </p> Job queued <p>NOTE: If you would like to change your selection about where to run the Jupyter instance after the Notebook has started, you need to stop the server to be able to see the drop-down menu again.</p>"},{"location":"services/jupyter-hub/#known-issues","title":"Known Issues","text":""},{"location":"services/jupyter-hub/#spawn-failed-timeout","title":"Spawn Failed: Timeout","text":"<p>This happens when the queue is backed up. Since Jupyter is interactive, it expects an immediate connection. Therefore, it waits 5 minutes for your job to begin before throwing this error. You can monitor the queue usage with Gronk and submit when there isn't a wait.</p>"},{"location":"services/jupyter-hub/#additional-notes","title":"Additional Notes","text":""},{"location":"services/jupyter-hub/#custom-ipython-kernels","title":"Custom IPython Kernels","text":"<p>ALCF JupyterHub provides a set of pre-configured IPython kernels for the users to select. However, users may need custom kernels with additional packages installed. This can be achieved by first creating custom Python environments either through venv or conda. More information on creating custom Python environments can be found in our documentation for Polaris. After activating the custom environment, the <code>ipykernel</code> package needs to be installed with the following command:</p> <pre><code>pip install ipykernel\n</code></pre> <p>Once <code>ipykernel</code> is installed, the custom kernel can be added to the list of available kernels with the following command:</p> <pre><code>python -m ipykernel install --user --name custom_kernel_name\n</code></pre> <p>where <code>custom_kernel_name</code> is the name of the kernel that will appear in the kernel list. This name does not have to match the name of the environment, but should not contain spaces. If you want more flexibility in naming, you can add the <code>--display-name</code> argument as shown below.</p> <pre><code>python -m ipykernel install --user --name custom_kernel_name --display-name \"Polaris Python 3.11 Tensorflow 2.4.1\"\n</code></pre> <p>Note that you still need to provide <code>--name</code> with a simple name that does not contain spaces. Additionally, you can also set environment variables for the kernel with the <code>--env</code> argument, i.e:</p> <pre><code>python -m ipykernel install --user --name custom_kernel_name --env http_proxy http://proxy.alcf.anl.gov:3128 --env https_proxy http://proxy.alcf.anl.gov:3128\n</code></pre> <p>You can see the list of available kernels with the following command:</p> <pre><code>jupyter kernelspec list\n</code></pre> <p>By default, the kernels are installed in the user's home directory under <code>~/.local/share/jupyter/kernels/</code>. All the configuration is specified in the <code>kernel.json</code> file under the kernel directory. For the example above, the path for the json file will be <code>~/.local/share/jupyter/kernels/custom_kernel_name/kernel.json</code>. You can edit this file to add additional environment variables or change the display name.</p> <p>Once you've followed the steps above, your new kernel will be visible on JupyterHub. It's recommended to perform these steps in a terminal, ideally on the login node of the system you're using. After setting up a custom kernel, you can easily add more packages directly within JupyterHub. Simply create a new notebook using your custom kernel and use the <code>%pip</code> or <code>%conda</code> magic commands to install packages. If you're on a compute node, remember to enable internet access by configuring the <code>http_proxy</code> and <code>https_proxy</code> environment variables as previously mentioned.</p>"},{"location":"services/jupyter-hub/#accessing-project-folders","title":"Accessing Project Folders","text":"<p>The Jupyter file browser limits the user to view files and directories within their home directory. To access directories located outside of the user home directory, a symbolic link to the directory must be created within the user home directory. An example of this is:</p> <pre><code>ln -s /project/ABC ~/ABC_project_link\n</code></pre> <p>Please note that one can run any shell command directly on a Jupyter notebook by simply adding an exclamation mark, <code>!</code>, to the beginning of the command. For example, the above command can be run from a notebook cell as follows:</p> <pre><code>!ln -s /project/ABC ~/ABC_project_link\n</code></pre>"},{"location":"services/jupyter-hub/#ending-a-jupyter-notebook-running-on-a-compute-node","title":"Ending a Jupyter Notebook running on a compute node","text":"<p>Failing to correctly end a running Jupyter Notebook will continue to consume the selected project's allocation on the resource in question. When a user has completed their task in Jupyter, the user should stop the Jupyter instance running on the compute node before logging out. To stop the Notebook, click the \u201cControl Panel\u201d button in the top right, then click \u201cStop My Server\u201d.</p> <p> </p> Stop panel <p> </p> Stop server"},{"location":"services/jupyter-hub/#resources","title":"Resources","text":"<ul> <li>Jupyter Lab documentation.</li> <li>ALCF Hands-on HPC Workshop presentation on Python and Jupyter on Polaris: slides and video.</li> <li>ALCF webinar on JupyterHub: slides and video.</li> </ul>"},{"location":"sophia/getting-started/","title":"Getting Started on Sophia","text":""},{"location":"sophia/getting-started/#logging-into-sophia","title":"Logging Into Sophia","text":"<p>To log into Sophia: <pre><code>ssh &lt;username&gt;@sophia.alcf.anl.gov\n</code></pre> Then, type in the password from your CRYPTOCard/MobilePASS+ token. Once logged in, you land on one of the Sophia login nodes (sophia-login-01, sophia-login-02).</p>"},{"location":"sophia/getting-started/#hardware-overview","title":"Hardware Overview","text":"<p>An overview of the Sophia system, including details on the compute node architecture, is available on the Machine Overview page.</p>"},{"location":"sophia/getting-started/#compiling-applications","title":"Compiling Applications","text":"<p>For all code building and development, please use Sophia compute nodes. Please read through the Compiling and Linking Overview page and corresponding pages depending on the target compiler and programming model.</p>"},{"location":"sophia/getting-started/#accessing-additional-software","title":"Accessing Additional Software","text":"<p>ALCF installs additional software in <code>/soft</code>, which can be accessed via module commands by altering your <code>$MODULEPATH</code>: <pre><code>module use /soft/modulefiles\n</code></pre> The available software can then be queried with <code>module avail</code>.</p>"},{"location":"sophia/getting-started/#submitting-and-running-jobs","title":"Submitting and Running Jobs","text":"<p>Please read through the Running Jobs with PBS at the ALCF page for information on using the PBS scheduler and preparing job submission scripts.</p> <p>For more information on Sophia queues and job submission, visit: Running Jobs on Sophia.</p>"},{"location":"sophia/getting-started/#lustre-file-striping","title":"Lustre File Striping","text":"<p>In addition to the content above, here is a document on Lustre File Striping Basics.</p> <ul> <li>Lustre File Striping Basics</li> </ul>"},{"location":"sophia/getting-started/#proxy","title":"Proxy","text":"<p>If the node you are on doesn\u2019t have outbound network connectivity, add the following to your <code>~/.bash_profile</code> file to access the proxy host:</p> <pre><code># proxy settings\nexport HTTP_PROXY=\"http://proxy.alcf.anl.gov:3128\"\nexport HTTPS_PROXY=\"http://proxy.alcf.anl.gov:3128\"\nexport http_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport https_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport ftp_proxy=\"http://proxy.alcf.anl.gov:3128\"\n</code></pre>"},{"location":"sophia/getting-started/#getting-assistance","title":"Getting Assistance","text":"<p>Please direct all questions, requests, and feedback to support@alcf.anl.gov.</p>"},{"location":"sophia/compiling-and-linking/compiling-and-linking-overview/","title":"Compiling and Linking on Sophia","text":""},{"location":"sophia/compiling-and-linking/compiling-and-linking-overview/#overview","title":"Overview","text":"<p>Sophia has AMD processors on the login nodes (<code>sophia-login-01,02</code>) and AMD processors and NVIDIA A100 GPUs on the compute nodes (see Machine Overview page). The login nodes can be used to create containers and launch jobs.</p> <p>Must compile on a compute node</p> <p>Until the cross-compiling environment is set up or dedicated build nodes are added, the compute nodes will have to be used for compiling. Do not compile codes on the login nodes. To launch an interactive job and acquire a compute node for compiling, use:</p> <pre><code>qsub -I -l select=1 -l walltime=HH:MM:SS -q by-gpu -A &lt;myProjectName&gt; -l filesystems=home:eagle\n</code></pre> <p>The default programming environment on the Sophia compute nodes is the GNU compiler tools coupled with NVIDIA\u2019s CUDA toolkit.</p> <p>For non-GPU codes:</p> <ul> <li><code>gcc</code></li> <li><code>g++</code></li> <li><code>gfortran</code></li> </ul> <p>For CUDA codes, please note that there is a new driver (v470) and default CUDA toolkit (v12.4):</p> <ul> <li><code>nvcc</code></li> </ul> <p>The default Nvidia installed software will be in your PATH on compute nodes (not on login nodes).</p> <pre><code>which nvcc\n</code></pre> <p>NEEDS UPDATING: everything from here down:</p> <p>For MPI, the latest MPI is in <code>/usr/mpi/gcc/openmpi-4.1.5a1</code>:</p> <ul> <li><code>mpicc</code></li> <li><code>mpicxx</code>/<code>mpic++</code>/<code>mpiCC</code></li> <li><code>mpifort</code>/<code>mpif77</code>/<code>mpif90</code></li> </ul> <p>On the login nodes, GNU compilers are available.</p>"},{"location":"sophia/compiling-and-linking/compiling-and-linking-overview/#modules-on-sophia","title":"Modules on Sophia","text":"<p>Available modules can be listed via the command:</p> <pre><code>module avail\n</code></pre> <p>Loaded modules in your environment can be listed via the command:</p> <pre><code>module list\n</code></pre> <p>To load new modules, use:</p> <pre><code>module load &lt;module_name&gt;\n</code></pre> <p>Usage: csh and zsh users do not have to do anything special to their environments. Bash users, however, will need to add the following to any job scripts:</p> <pre><code>#!/bin/bash\n. /etc/profile\n</code></pre> <p>Bash users are also encouraged to modify their <code>~/.bashrc</code> to ensure the Ubuntu system <code>/etc/bash.bashrc</code> file is sourced properly:</p> <pre><code># Source global definitions\nif [ -f /etc/bashrc ]\nthen\n    . /etc/bashrc\nelif [ -f /etc/bash.bashrc ]\nthen\n    . /etc/bash.bashrc\nfi\n</code></pre>"},{"location":"sophia/containers/containers/","title":"Containers on Sophia","text":"<p>Sophia, powered by NVIDIA A100 GPUs, benefits from container-based workloads for seamless compatibility across NVIDIA systems. This guide details the use of containers on Sophia, including custom container creation, large-scale execution, and common pitfalls.</p>"},{"location":"sophia/containers/containers/#apptainer-setup","title":"Apptainer Setup","text":"<p>Sophia employs Apptainer (formerly known as Singularity) for container management. To set up Apptainer, run:</p> <pre><code>module use /soft/spack/base/0.7.1/install/modulefiles/Core/\nmodule load apptainer\napptainer version #1.3.3\n</code></pre> <p>The Apptainer version on Sophia is 1.3.3. Detailed user documentation is available here.</p>"},{"location":"sophia/containers/containers/#building-from-docker-or-argonne-github-container-registry","title":"Building from Docker or Argonne GitHub Container Registry","text":"<p>Containers on Sophia can be built by writing Dockerfiles on a local machine and then publishing the container to DockerHub, or by directly building them on an ALCF compute node by writing an Apptainer recipe file. If you prefer to use existing containers, you can pull them from various registries like DockerHub and run them on Sophia.</p> <p>Since Docker requires root privileges, which users do not have on Sophia, existing Docker containers must be converted to Apptainer. To build a Docker-based container on Sophia, use the following as an example:</p> <pre><code>qsub -I -A &lt;Project&gt; -l select=1:ngpus=8:ncpus=256 -l walltime=01:00:00 -l filesystems=home:eagle -l singularity_fakeroot=True -q by-node -k doe\nexport HTTP_PROXY=http://proxy.alcf.anl.gov:3128\nexport HTTPS_PROXY=http://proxy.alcf.anl.gov:3128\nexport http_proxy=http://proxy.alcf.anl.gov:3128\nexport https_proxy=http://proxy.alcf.anl.gov:3128\nmodule use /soft/spack/base/0.7.1/install/modulefiles/Core/\nmodule load apptainer\napptainer build --fakeroot pytorch:22.06-py3.sing docker://nvcr.io/nvidia/pytorch:22.06-py3\n</code></pre> <p>You can find the latest prebuilt NVIDIA PyTorch containers here. The TensorFlow containers are here (though note that LCF doesn't typically prebuild the TF-1 containers). You can search the full container registry here. For custom containers tailored for Sophia, visit ALCF's GitHub container registry.</p> <p>Note: Currently, container build and executions are only supported on the Sophia compute nodes.</p>"},{"location":"sophia/data-science/fine-tune-LLM-with-Autotrain/","title":"Autotrain","text":"<p>Autotrain, developed by Hugging Face, is a platform designed to simplify training cutting-edge models in various fields: NLP, LLM, CV, etc. Read more.</p>"},{"location":"sophia/data-science/fine-tune-LLM-with-Autotrain/#create-python-virtual-environment-for-autotrain","title":"Create Python Virtual Environment for Autotrain","text":"<p>Let's first create a virtual environment for Autotrain, built on top of the minimal system Python installation located at <code>/usr/bin/python</code>:</p> <pre><code>mkdir -p venv_autotrain\npython -m venv venv_autotrain --system-site-packages\nsource venv_autotrain/bin/activate\npip3 install autotrain-advanced\n</code></pre> <p>Note: If Autotrain doesn't work properly, you may have to reinstall <code>nvidia-ml-py</code>.</p> <pre><code>pip3 uninstall nvidia-ml-py3 pynvml\npip3 install --force-reinstall nvidia-ml-py==11.450.51\n</code></pre>"},{"location":"sophia/data-science/fine-tune-LLM-with-Autotrain/#train-dataset-format","title":"Train Dataset Format","text":"<p>The dataset should have a column \"text\" containing the data to be trained on. Example</p>"},{"location":"sophia/data-science/fine-tune-LLM-with-Autotrain/#config-file-for-fine-tuning-local-llm","title":"Config File for Fine-Tuning Local LLM","text":"<p>Here is an example to create a config file for supervised fine-tuning purposes:</p> <pre><code>task: llm-sft\nbase_model: meta-llama/Meta-Llama-3.1-8B-Instruct\nproject_name: Llama-3-1-FT\nlog: wandb\nbackend: local\ndata:\n  path: Path/to/the/training/dataset/folder\n  train_split: train\n  valid_split: null\n  chat_template: null\n  column_mapping:\n    text_column: text\nparams:\n  block_size: 1024\n  model_max_length: 8192\n  epochs: 800\n  batch_size: 2\n  lr: 1e-5\n  peft: true\n  quantization: null\n  target_modules: all-linear\n  padding: right\n  optimizer: paged_adamw_8bit\n  scheduler: cosine\n  gradient_accumulation: 8\n  mixed_precision: bf16\nhub:\n  username: ***\n  token: hf_***\n  push_to_hub: true\n</code></pre> <p>More details</p>"},{"location":"sophia/data-science/fine-tune-LLM-with-Autotrain/#run-autotrain-to-fine-tune-using-the-config-file","title":"Run Autotrain to Fine-Tune Using the Config File","text":"<pre><code>cd Path/to/save/the/adapter\nautotrain --config path/to/config.yaml\n</code></pre>"},{"location":"sophia/data-science/fine-tune-LLM-with-Autotrain/#merge-adapters-with-base-model-to-create-new-model","title":"Merge Adapters with Base Model to Create New Model","text":"<p>Adapters need to be merged with the base model in order to run. You can use the code below:</p> <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\nimport torch\nfrom peft import PeftModel\nimport os\n\nadapter = \"path/to/saved/adapters/\"\nmodel_name = \"project-name-from-config-file\"\nadapter_path = os.path.join(adapter, model_name)\nbase_model_path = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\ntarget_model_path = \"path/to/save/fine-tuned/models\" + model_name\n\nconfig = AutoConfig.from_pretrained(base_model_path)\nbase_model = AutoModelForCausalLM.from_pretrained(base_model_path)\n\nmerged_model = PeftModel.from_pretrained(base_model, adapter_path)\n\ntokenizer = AutoTokenizer.from_pretrained(adapter_path, trust_remote_code=True)\nmerged_model = merged_model.merge_and_unload()\n\nprint(\"Saving target model...\")\nmerged_model.save_pretrained(target_model_path)\ntokenizer.save_pretrained(target_model_path)\nconfig.save_pretrained(target_model_path)\n</code></pre>"},{"location":"sophia/data-science/python/","title":"Python","text":"<p>We provide prebuilt <code>conda</code> environments containing GPU-supported builds of <code>torch</code>, <code>tensorflow</code> (both with <code>horovod</code> support for multi-node calculations), <code>jax</code>, and many other commonly-used Python modules.</p> <p>Users can activate this environment by first loading the <code>conda</code> module, and then activating the base environment.</p> <p>Explicitly (either from an interactive job, or inside a job script):</p> <pre><code>module use /soft/modulefiles; module load conda; conda activate base\n</code></pre> <p>This will load and activate the base environment.</p>"},{"location":"sophia/data-science/python/#virtual-environments-via-venv","title":"Virtual environments via <code>venv</code>","text":"<p>To install additional packages that are missing from the <code>base</code> environment, we can build a <code>venv</code> on top of it.</p> <p>Conda <code>base</code> environment + <code>venv</code></p> <p>If you need a package that is not already installed in the <code>base</code> environment, this is generally the recommended approach.</p> <p>We can create a <code>venv</code> on top of the base Anaconda environment (with <code>--system-site-packages</code> to inherit the <code>base</code> packages):</p> <pre><code>module use /soft/modulefiles; module load conda; conda activate base\nCONDA_NAME=$(echo ${CONDA_PREFIX} | tr '\\/' '\\t' | sed -E 's/mconda3|\\/base//g' | awk '{print $NF}')\nVENV_DIR=\"$(pwd)/venvs/${CONDA_NAME}\"\nmkdir -p \"${VENV_DIR}\"\npython -m venv \"${VENV_DIR}\" --system-site-packages\nsource \"${VENV_DIR}/bin/activate\"\n</code></pre> <p>You can always retroactively change the <code>--system-site-packages</code> flag state for this virtual environment by editing <code>${VENV_DIR}/pyvenv.cfg</code> and changing the value of the line <code>include-system-site-packages=false</code>.</p> <p>To install a different version of a package that is already installed in the base environment, you can use:</p> <pre><code>python3 -m pip install --ignore-installed &lt;package&gt; # or -I\n</code></pre> <p>The shared base environment is not writable, so it is impossible to remove or uninstall packages from it. The packages installed with the above <code>pip</code> command should shadow those installed in the base environment.</p>"},{"location":"sophia/data-science/python/#cloning-the-base-anaconda-environment","title":"Cloning the base Anaconda environment","text":"<p>Warning</p> <p>This approach is generally not recommended as it can be quite slow and can use significant storage space.</p> <p>If you need more flexibility, you can clone the conda environment into a custom path, which would then allow for root-like installations via <code>conda install &lt;module&gt;</code> or <code>pip install &lt;module&gt;</code>.</p> <p>Unlike the <code>venv</code> approach, using a cloned Anaconda environment requires you to copy the entirety of the base environment, which can use significant storage space.</p> <p>To clone the <code>base</code> environment:</p> <pre><code>module load conda; conda activate base\nconda create --clone base --prefix /path/to/envs/base-clone\nconda activate /path/to/envs/base-clone\n</code></pre> <p>where <code>/path/to/envs/base-clone</code> should be replaced by a suitably chosen path.</p> <p>Note: The cloning process can be quite slow.</p>"},{"location":"sophia/data-science/python/#using-pip-install-user-not-recommended","title":"Using <code>pip install --user</code> (not recommended)","text":"<p>Danger</p> <p>This is typically not recommended.</p> <p>With the conda environment setup, one can install common Python modules using <code>python3 -m pip install --user '&lt;module-name&gt;'</code> which will install packages in <code>$PYTHONUSERBASE/lib/pythonX.Y/site-packages</code>.</p> <p>The <code>$PYTHONUSERBASE</code> environment variable is automatically set when you load the base conda module, and is equal to <code>/home/$USER/.local/polaris/conda/YYYY-MM-DD</code>.</p> <p>Note, Python modules installed this way that contain command line binaries will not have those binaries automatically added to the shell's <code>$PATH</code>. To manually add the path:</p> <pre><code>export PATH=\"$PYTHONUSERBASE/bin:$PATH\"\n</code></pre> <p>Be sure to remove this location from <code>$PATH</code> if you deactivate the base Anaconda environment or unload the module.</p> <p>Cloning the Anaconda environment, or using <code>venv</code> are both more flexible and transparent when compared to <code>--user</code> installs.</p>"},{"location":"sophia/data-science/python/#default-python-version","title":"Default Python Version","text":"<p>The default Python on Sophia is located at <code>/usr/bin/python</code> with version 3.9.18.</p>"},{"location":"sophia/data-science/python/#creating-a-jupyter-kernel","title":"Creating a Jupyter Kernel","text":"<p>If you would like to use your Python virtual environment on JupyterHub, you will need to create a Jupyter kernel for it.</p> <ol> <li> <p>Install <code>ipykernel</code>:     Ensure <code>ipykernel</code> is installed in your virtual environment:     <pre><code>pip install ipykernel\n</code></pre></p> </li> <li> <p>Create a Jupyter Kernel:     <pre><code>python -m ipykernel install --user --name=myenv --display-name \"Jupyter (myenv)\"\n</code></pre>     Replace <code>myenv</code> with the name of your virtual environment and <code>\"Jupyter (myenv)\"</code> with the display name you want for the kernel in JupyterHub.</p> </li> </ol>"},{"location":"sophia/hardware-overview/machine-overview/","title":"Sophia Machine Overview","text":"<p>Sophia is comprised of 24 NVIDIA DGX A100 nodes. Each DGX A100 node comprises eight NVIDIA A100 Tensor Core GPUs and two AMD Rome CPUs that provide 22 nodes with 320 GB of GPU memory and two nodes with 640 GB of GPU memory (8,320 GB in total) for training artificial intelligence (AI) datasets, while also enabling GPU-specific and -enhanced high-performance computing (HPC) applications for modeling and simulation.</p> <p>A 15-terabyte solid-state drive offers up to 25 gigabits per second in bandwidth. The dedicated compute fabric comprises 20 Mellanox QM9700 HDR200 40-port switches wired in a fat-tree topology.</p> <p>Table 1 summarizes the capabilities of a Sophia compute node:</p> COMPONENT COUNT PER NODE AGGREGATE AMD Rome 64-core CPU 2 48 DDR4 Memory 1 TB on 320 GB nodes &amp; 2 TB on 640 GB nodes 26 TB NVIDIA A100 GPU 8 192 GPU Memory 22 nodes w/ 320 GB &amp; 2 nodes w/ 640 GB 8,320 GB HDR200 Compute Ports 8 192 HDR200 Storage Ports 2 48 100GbE Ports 2 48 3.84 TB Gen4 NVME drives 4 96"},{"location":"sophia/not_in_nav/applications-and-libraries/applications/gromacs/","title":"Gromacs on ThetaGPU","text":""},{"location":"sophia/not_in_nav/applications-and-libraries/applications/gromacs/#what-is-gromacs","title":"What is Gromacs?","text":"<p>GROMACS is a versatile package to perform molecular dynamics, i.e., simulate the Newtonian equations of motion for systems with hundreds to millions of particles. It is primarily designed for biochemical molecules like proteins, lipids, and nucleic acids that have a lot of complicated bonded interactions. However, since GROMACS is extremely fast at calculating the nonbonded interactions (which usually dominate simulations), many groups are also using it for research on non-biological systems, e.g., polymers.</p>"},{"location":"sophia/not_in_nav/applications-and-libraries/applications/gromacs/#using-gromacs-at-alcf","title":"Using GROMACS at ALCF","text":"<p>ALCF offers assistance with building binaries and compiling instructions for GROMACS. For questions, contact us at support@alcf.anl.gov.</p>"},{"location":"sophia/not_in_nav/applications-and-libraries/applications/gromacs/#building-gromacs","title":"Building Gromacs","text":"<ol> <li>Download the latest source code: http://manual.gromacs.org/documentation/2022.1/download.html</li> <li>Extract the tarball:    <pre><code>tar -xzf gromacs-2022.1.tar.gz\n</code></pre></li> <li>Submit an interactive job to a ThetaGPU compute node from the Theta login node:    <pre><code>user@thetalogin4:~&gt; module load cobalt/cobalt-gpu\nuser@thetalogin4:~&gt; qsub -I -n 1 -t 60 -q single-gpu -A PROJECT --attrs filesystems=home\n</code></pre>    Wait for the job to start and open an interactive session:    <pre><code>Job routed to queue \"single-gpu\".\nWait for job 10108666 to start...\nOpening interactive session to thetagpu06-gpu0\n...\nuser@thetagpu06:~$\n</code></pre></li> <li>Change to the Gromacs directory:    <pre><code>cd gromacs-2022.1\n</code></pre></li> <li>Create a build directory:    <pre><code>mkdir build\n</code></pre></li> <li>Load the cmake module:    <pre><code>module load cmake\n</code></pre></li> <li>Configure the build with cmake:    <pre><code>cmake -DCMAKE_C_COMPILER=mpicc -DCMAKE_CXX_COMPILER=mpicxx \\\n      -DBUILD_SHARED_LIBS=OFF -DGMX_BUILD_OWN_FFTW=ON \\\n      -DCMAKE_INSTALL_PREFIX=/path-to/gromacs-2022.1/build \\\n      -DGMX_MPI=ON -DGMX_OPENMP=ON -DGMX_GPU=CUDA \\\n      -DCUDA_TOOLKIT_ROOT_DIR=/user/local/cuda-11.4\n</code></pre></li> <li>Compile the code:    <pre><code>make -j 16\n</code></pre></li> <li>Install the binaries:    <pre><code>make install\n</code></pre></li> <li>The installed binary is <code>build/bin/gmx_mpi</code>.</li> </ol>"},{"location":"sophia/not_in_nav/applications-and-libraries/applications/gromacs/#running-gromacs-on-thetagpu","title":"Running Gromacs on ThetaGPU","text":"<p>Prebuilt Gromacs binaries can be found in the directory <code>/soft/applications/gromacs/gromacs_cuda</code>.</p> <p>A sample qsub script follows that will run GROMACS on a full node using all eight GPUs available.</p> <pre><code>#!/bin/bash -l\n#COBALT -n 1\n#COBALT -t 30 \n#COBALT -q full-node \n#COBALT -project catalyst \n#COBALT --attrs filesystems=home,theta-fs0\n\nNODES=`cat $COBALT_NODEFILE | wc -l`\n\nmpirun -hostfile $COBALT_NODEFILE --np 8 \\\n      /soft/applications/gromacs/gromacs_cuda/gmx_mpi.2022.1 \\\n      mdrun -ntomp 8 -gputasks 01234567 -nb gpu -pme gpu -npme 1 \\\n      -dlb yes -resethway -pin on -v deffnm step5_1 -g test.log\n</code></pre> <p>We strongly suggest that users try combinations of different numbers of nodes, MPI ranks per node, number of GPU tasks/devices, GPU task decomposition between nonbonded and PME kernels, and OMP threads per rank to find the optimal throughput for their particular workload.</p> <p>The following is a representative benchmark for a system with 30,000 atoms generated on a single ThetaGPU node with the above example.</p> Core time (sec) Wall time (sec) (%) Time 691.769 10.810 6399.6 ns/day hour/ns Performance 399.661 0.060"},{"location":"sophia/not_in_nav/compiling-and-linking/continuous-integration/","title":"Continuous Integration on ThetaGPU","text":""},{"location":"sophia/not_in_nav/compiling-and-linking/continuous-integration/#overview","title":"Overview","text":"<p>Continuous Integration (CI) in software development is the practice of committing code changes regularly to a version control system and having automated processes perform build, test, package, and deploy activities. The key concepts of CI include high frequency, repeatability, and automation in order to realize increased quality and ease of delivery. The main goal CI aims to achieve is the elimination of build and deployment issues, which in turn improves development cycles, provides a timely feedback loop with developers, and results in higher quality deliverables with reduced development time.</p> <p>CI usually describes the work that is done by a deployment or operations team to build and deploy code throughout an environment and make it available to the different interested teams involved in the SDLC. The steps that make up this process are referred to as a workflow or pipeline, which, when combined with automation, provides the mechanism for Continuous Integration.</p> <p>Today it is a common practice to use a CI tool for defining pipelines and executing the tasks required to take code from a source stored in a version control system to compiled and packaged artifacts executing in production. Two excellent examples of CI tools are Jenkins and GitLab.</p>"},{"location":"sophia/not_in_nav/compiling-and-linking/continuous-integration/#ci-tools-at-alcf","title":"CI Tools at ALCF","text":"<p>The ALCF provides a tool for implementing CI processes named Jenkins. Using the Jenkins tool, ALCF projects can make use of CI functionality. The Jenkins CI tool enables projects to auto-compile their custom software code, automate testing cycles, provide a feedback loop, and submit jobs to HPC resources. The custom pipelines needed for each project can be defined in Jenkins by project users, and execution can be controlled through triggers.</p>"},{"location":"sophia/not_in_nav/compiling-and-linking/continuous-integration/#jenkins","title":"Jenkins","text":"<p>Jenkins \"is a self-contained, open-source automation server which can be used to automate all sorts of tasks relating to building, testing, and delivering or deploying software.\" Jenkins is the tool that provides CI/CD functionality for ALCF resources. Most importantly, it provides the mechanisms required for DevOps automation.</p> <p>Additional information, technical and user documentation, and community support can be found on the Jenkin's project website.</p>"},{"location":"sophia/not_in_nav/compiling-and-linking/continuous-integration/#alcf-jenkins","title":"ALCF Jenkins","text":"<p>Log in to the ALCF Jenkins web portal using your ALCF credentials (ALCF username and cryptocard token password).</p>"},{"location":"sophia/not_in_nav/compiling-and-linking/continuous-integration/#projects-using-ci","title":"Projects Using CI","text":"<p>Enabling a project to use CI requires some additional steps and configuration to get started. Once enabled for a project, users can access the Jenkins CI environment and configure CI jobs or pipelines for building and testing their project code.</p>"},{"location":"sophia/not_in_nav/compiling-and-linking/continuous-integration/#on-boarding-with-ci","title":"On-Boarding with CI","text":"<p>To enable CI for your project, send an email to the ALCF Service Desk requesting CI functionality for your project and include the ALCF project shortname and the PI\u2019s name with the request.</p> <p>The project\u2019s PI will get an email with details and a new CI account associated with the project. This is a service account that the Jenkins CI tool will use when executing tasks associated with your project. The CI account will be listed as a project member and added to the project\u2019s group for access controls.</p>"},{"location":"sophia/not_in_nav/compiling-and-linking/continuous-integration/#folders","title":"Folders","text":"<p>Each CI project will have a top-level \u2018folder\u2019 created with the project\u2019s name. Please do not delete the project folder: it is used for organization in the multi-project environment and is required for implementing the needed level of security. The project folder is where all of the project objects are stored, you can additionally create any subfolders, jobs, pipelines, etc. within your project folder to meet your CI needs.</p> <p>In the example below, we have a project named \u2018TestFromJanet2\u2019 with an associated folder.</p> <p> </p> CI folders screenshot"},{"location":"sophia/not_in_nav/compiling-and-linking/continuous-integration/#nodes","title":"Nodes","text":"<p>Each CI project will have an assigned node for execution. Nodes execute jobs defined within a project, typically on the target system\u2019s login node. Currently there are CI nodes configured for HPC systems Theta and Cooley, as well as non-HPC nodes with 32 cores (Intel Xeon Processor E5-2683 v4) and 128 GB RAM for generic x86 processing with access to the Mira shared filesystems.</p> <p>In the example below, the node for this project is named \u2018TestFromJanet2-Theta. Jobs and pipeline steps triggered from Jenkins will execute on the TestFromJanet2-Theta node which has been configured to use host: thetalogin1 and will use the project\u2019s CI user ID (provided during on-boarding) to execute scripts or code just as if the end user had logged into the thetalogin1 node and executed the same set of actions manually from the command line.</p> <p> </p> CI folders screenshot"},{"location":"sophia/not_in_nav/compiling-and-linking/continuous-integration/#job-configuration","title":"Job Configuration","text":"<p>When configuring any new job within a project there are some guidelines to follow for setting permissions and nodes. Project data is kept secure by setting up permissions at the project level and node selection controls where the job will execute.</p> <p>When creating jobs, enable project-based security, set the inheritance strategy, and add your project\u2019s group name to the permission matrix table. The example below has enabled project-based security, set the inheritance strategy to Do not inherit permission grants from other ACLs, and added the project\u2019s group name \u2018\u2018TestFromJanet2\u2019 to the permission matrix granting all rights to the group.</p> <p> </p> CI folders screenshot <p>To assign the node that the project will use to execute jobs, select the option Restrict where this project can be run and enter the project\u2019s assigned node. The example below has assigned the jobs to node: TestFromJanet2-Theta so that any time the job is executed, it runs on host: thetalogin1.</p> <p> </p> Execute"},{"location":"sophia/not_in_nav/compiling-and-linking/continuous-integration/#common-jenkins-features","title":"Common Jenkins Features","text":""},{"location":"sophia/not_in_nav/compiling-and-linking/continuous-integration/#version-control-features","title":"Version Control Features","text":"<p>Jenkins can connect to most common version control systems (VCS), including git/svn. The ALCF Jenkins instance can connect with local VCS hosted at at ANL as well as with external VCS, such as that hosted at Github.</p> <p>On the job configuration page, look for the section Source Code Management (SCM). If it is there already, add it to the job. The required fields for SCM are Repository URL and Credentials. The example below shows a connection to the ALCF internal Gitlab VCS and uses previously setup credentials.</p> <p> </p> Repository access <p>To use the new connection to the Git repository interactively, configure the job to be parameterized and add a a Git Parameter to the job. The example below shows the configuration to select a branch at build time.</p> <p> </p> Git Parameter <p>On the build screen, select from the drop-down menu the branch to be referenced during this job execution. The example below shows the list of available branches from the configured repository. It is automatically populated duing the Git connector configuration of the preceding steps. If a new branch is added to the Git repository, it will display in the populated list of avialable branches when the job runs in Jenkins.</p> <p> </p> Select branch"},{"location":"sophia/not_in_nav/compiling-and-linking/continuous-integration/#build-steps","title":"Build Steps","text":"<p>Build steps are where users define executable tasks and jobs do something interesting within an envionment. A core component of Jenkens, build steps can take a few different forms and are morst commonly configured to call remote scripts for code building and deployment. A build step can even contain the shell script contents to execute on the remote machine.</p> <p> </p> Select branch <p>The example below uses the Execute Shell build step type and codes the shell logic within the Jenkins portal.</p> <p> </p> Execute shell"},{"location":"sophia/not_in_nav/compiling-and-linking/continuous-integration/#pipelines","title":"Pipelines","text":"<p>Pipelines in Jenkins allow for more advanced execution logic and are written in Groovy. A pipeline can be added directly to your project as an object using the New Item link. More commonly, they are defined in a \"Jenkinsfile\" and stored in VCS along with the project code. The Jenkinsfile can be created and edited outside of the Jenkins system using any text editor.</p> <p>To add a pipeline manually, select Pipeline from the new New Item dialog box.</p> <p> </p> New Item dialog box <p>The pipeline can then be configured and edited from the project folder in the same way as jobs, as shown in the example below.</p> <p> </p> Pipeline configuration <p>To add a pipeline using a Jenkinsfile in SCM, add the pipeline object as shown below. On the pipeline configuration page, select Pipeline script from SCM and provide the SCM connection details along with the Script Path. The Script Path is the path-to and filename where the Jenkinsfile is located within the SCM repository. The example below uses a Jenkinsfile stored in the project source code from the ALCF Git repository, and the Jenkinsfile containing the Groovy code pipeline definition is located at scripts/Jenkinsfile from the repository root.</p> <p> </p> Pipeline script path"},{"location":"sophia/not_in_nav/compiling-and-linking/continuous-integration/#triggers","title":"Triggers","text":"<p>Triggers are events that intitiate tasks in Jenkins. Triggers can be called a few different ways, including directly by a user via the Build Now action (a time-based trigger similar to a Cron system), or based on commits made to source control.</p> <p>The example below shows a time-based configuration to run the job on a regular schedule. Details on the scheduling syntax can be found by clicking the blue question mark to the right of the Schedule field.</p> <p> </p> Build Triggers"},{"location":"sophia/not_in_nav/compiling-and-linking/continuous-integration/#console-output","title":"Console Output","text":"<p>Jenkins provides console output and saves this history for each job run. During job execution you can view the live output from the tasks in a display similar to what would be seen if the commands were run directly in an interactive console.</p> <p> </p> Console output"},{"location":"sophia/not_in_nav/compiling-and-linking/continuous-integration/#credentials","title":"Credentials","text":"<p>Credentials are stored in Jenkins and used when connecting to remote resources that require authentication in a non-interactive manner. Once defined, credentials can be used throughout the Jenkins system when configuring jobs, SCM connections, SSH connections, etc.</p> <p>To add a set of credentials, click on Credentials from the available options on the left-hand navigation menu. Then select System and click on the link for Global credentials.</p> <p> </p> Credentials <p>Click Add Credentials from the left-hand navigation menu and provide the required information. The example below configures a new credential set of type \"SSH Username with private key.\" Make sure Scope is set to \"Global.\" Provide the username, private key (copy and paste), and key passphrase, and then give a pertinent ID and detailed description to help identify and organize stored credentials in the system.</p> <p> </p> Add credentials"},{"location":"sophia/not_in_nav/compiling-and-linking/continuous-integration/#faqs","title":"FAQS","text":"<p>Why does my project's execution node say it is offline? Node services for executing project tasks are inititated when there is demand for the node. The process of starting the node services acan take up to one minute; the status change is displayed in the Jenkins web portal. When there is no longer demand for the node, the services will stop again after one minute of idle time.</p> <p>Why is my shell environment different when executing tasks on a Jenkins node? Since Jenkins uses SSH with no tty, any shell scripts need to have this at the top so that login scripts are run against the session:</p> <p><code>#!/bin/bash -1</code></p>"},{"location":"sophia/not_in_nav/compiling-and-linking/continuous-integration/#glossary","title":"Glossary","text":"<p>Continuous Integration (CI) - The process of automating the build and testing of code every time developers commit changes to version control.</p> <p>Pipeline - A CI pipeline is a list of tasks or jobs that are defined and executed as a procedure within a project. Pipeline is analogous to workflow.</p> <p>Source Control Management (SCM) - A term used in Jenkins to describe objects related to version control.</p> <p>Version Control System (VCS) - Software that manages access, storage, and revision history for a code respository.</p>"},{"location":"sophia/not_in_nav/compiling-and-linking/continuous-integration/#appendix","title":"Appendix","text":""},{"location":"sophia/not_in_nav/compiling-and-linking/continuous-integration/#abbreviated-setup","title":"Abbreviated Setup","text":"<ul> <li>Request CI capabilities for your project by emailing the ALCF Service Desk.</li> <li>Add jobs and pipelines to the project folder space to handle code compiling and testing.</li> <li>Configure jobs with credentials, SCM integrations, and trigger components depending on the intended behavior for your project.</li> <li>Execute jobs and pipelines by invoking the configured triggers.</li> </ul>"},{"location":"sophia/not_in_nav/data-science/building-python-packages/","title":"Building Python Packages","text":"<p>To build Python packages for ThetaGPU, there are two options: build on top of a bare-metal build or build on top of (and within) a Singularity container. Additionally, you can build a new container from NVIDIA's Docker images.</p>"},{"location":"sophia/not_in_nav/data-science/building-python-packages/#build-on-thetagpu-compute-using-conda","title":"Build on ThetaGPU compute using Conda","text":"<p>To build on ThetaGPU compute and install your own packages, log in to Theta and then submit an interactive job to log on to a ThetaGPU compute node.</p> <p>Please see Running PyTorch with Conda or Running TensorFlow with Conda for more information.</p>"},{"location":"sophia/not_in_nav/data-science/building-python-packages/#building-on-top-of-a-container","title":"Building on top of a container","text":"<p>At the moment, you will need two shells to do this: have one open on a login node (for example, <code>thetaloginN</code>), and one open on a compute node (<code>thetagpuN</code>). First, start the container in interactive mode:</p> <pre><code>singularity exec -B /lus:/lus --nv /lus/theta-fs0/projects/datascience/thetaGPU/containers/pytorch_20.08-py3.sif bash\n</code></pre> <p>From here, you can create a virtual environment for installation:</p> <pre><code>export VENV_LOCATION=/path/to/virtualenv # replace this with your path! \npython -m venv --system-site-packages $VENV_LOCATION\n</code></pre> <p>Note: Sometimes, the <code>venv</code> package is available, and if not, you can try <code>python -m virtualenv</code>. If neither are available, you can install it in your user directory:</p> <pre><code>pip install --user virtualenv\n</code></pre> <p>and it should work.</p> <p>Next time you log in, you'll have to start the container, and then run <code>source $VENV_LOCATION/bin/activate</code> to re-enable your installed packages.</p>"},{"location":"sophia/not_in_nav/data-science/building-python-packages/#reaching-the-outside-world-for-pip-packages","title":"Reaching the outside world for pip packages","text":"<p>You'll notice right away when you try to pip install you cannot, because the connection fails. You can, however, go through a proxy server for pip by enabling these variables:</p> <pre><code>export HTTP_PROXY=http://theta-proxy.tmi.alcf.anl.gov:3128\nexport HTTPS_PROXY=https://theta-proxy.tmi.alcf.anl.gov:3128\n</code></pre> <p>Now, you can pip install your favorite packages: <code>pip install mpi4py</code></p>"},{"location":"sophia/not_in_nav/data-science/building-python-packages/#building-custom-packages","title":"Building custom packages","text":"<p>Most packages (HDF5, for example, or Python packages) can be built and installed into your virtual environment. Here are two common examples that aren't currently part of the PyTorch container that may be useful.</p>"},{"location":"sophia/not_in_nav/data-science/building-python-packages/#hdf5","title":"HDF5","text":"<p>You can find the source code for HDF5 on their website https://www.hdfgroup.org/downloads/hdf5/source-code. When downloaded and un-tarred, <code>cd</code> to the directory and run:</p> <pre><code>./configure --prefix=$VENV_LOCATION # Add any other configuration arguments \nmake -j 64 \nmake install\n</code></pre> <p>This should get you HDF5! For example, after this:</p> <pre><code>(pytorch_20.08) Singularity&gt; which h5cc \n/home/cadams/ThetaGPU/venvs/pytorch_20.08/bin/h5cc # This is my virtualenv, success!\n</code></pre>"},{"location":"sophia/not_in_nav/data-science/building-python-packages/#horovod","title":"Horovod","text":"<p>Horovod is useful for distributed training. To use it, you need it enabled within the container.</p> <pre><code>git clone https://github.com/horovod/horovod.git \ncd horovod \ngit submodule update --init \npython setup.py build \npython setup.py install\n</code></pre> <p>This should install Horovod within your container.</p>"},{"location":"sophia/not_in_nav/data-science/data-science-software-availability/","title":"Data Science Software Availability","text":"<p>On ThetaGPU, we currently support the major deep learning frameworks through two paths: Singularity containers, based on NVIDIA's Docker containers, and through bare-metal source builds. The bare-metal builds are for TensorFlow 2.X and PyTorch. TensorFlow 1.X is supported only via NVIDIA's containers at this time.</p>"},{"location":"sophia/not_in_nav/data-science/data-science-software-availability/#containers","title":"Containers","text":"<p>As of now, the NVIDIA containers with TensorFlow 1, 2, and PyTorch built against <code>cuda11</code>, <code>cudnn8</code> are available in Singularity format here:</p> <pre><code>$ ls /lus/theta-fs0/projects/datascience/thetaGPU/containers/ \npytorch_20.08-py3.sif tf1_20.08-py3.sif tf2_20.08-py3.sif\n</code></pre> <p>Execute a container interactively like this:</p> <pre><code>$ singularity exec --nv -B /lus:/lus /lus/theta-fs0/projects/datascience/thetaGPU/containers/tf1_20.08-py3.sif bash\n</code></pre>"},{"location":"sophia/not_in_nav/data-science/gpu-monitoring/","title":"GPU Monitoring","text":"<p>Each node on ThetaGPU hosts 8 A100 GPUs. You can see information about these GPUs via the command <code>nvidia-smi</code>.</p> <p>Each GPU has 40 GB of on-GPU memory. When you run applications, you will know the GPU is in use when you see the memory increase, and the GPU Utilization will be non-zero.</p> <p>You can target a specific GPU with <code>nvidia-smi -i 0</code> for the first GPU, for example.</p>"},{"location":"sophia/not_in_nav/data-science/gpu-monitoring/#gpu-selection","title":"GPU Selection","text":"<p>In many application codes, you may want to specify which GPU is used. This is particularly important in node-sharing applications where each GPU is running its own code, which can be either in data-parallel model training, workflow-based throughput jobs, etc. You can control individual process launches with:</p> <pre><code># Specify to run only on GPU 4: \nexport CUDA_VISIBLE_DEVICES=4 \n\n# Let your application see GPUs 0, 1, and 7: \nexport CUDA_VISIBLE_DEVICES=\"0,1,7\"\n</code></pre> <p>In these cases, the GPU orderings will appear as a consecutive list starting with 0.</p> <p>From inside an application, many software frameworks have the ability to let you target specific GPUs, including TensorFlow and PyTorch:</p> <ul> <li>TensorFlow</li> <li>PyTorch</li> </ul>"},{"location":"sophia/not_in_nav/data-science/gpu-node-queue-and-policy/","title":"GPU Node Queue and Policy","text":"<p>Note: Users will need an allocation on ThetaGPU to utilize the GPU nodes. Request an allocation by filling out this form: Allocation Request.</p> <p>ThetaGPU is listed under Theta on the form.</p> <p>The GPU nodes are new, and we expect the workload to be significantly different than it is on the KNL nodes. This document describes the current state of affairs, but we will monitor usage and adjust the policies as necessary.</p>"},{"location":"sophia/not_in_nav/data-science/gpu-node-queue-and-policy/#nodes-vs-queue-vs-mig-mode","title":"Nodes vs Queue vs MIG mode","text":"<p>The GPU nodes are NVIDIA DGX A100 nodes, and each node contains eight (8) A100 GPUs.</p> <p>You may request either entire nodes or individual GPUs based on your job needs. What you will get is determined by the queue you submit to: - If it has \"node\" in the name, you will get nodes. - If it has \"GPU\" in the name, you will get GPUs.</p> <p>Note: The <code>-n</code> parameter in your <code>qsub</code> will match the resource type in the queue (<code>-n 2</code> in a node queue will get you two full nodes, <code>-n 2</code> in a GPU queue will get you two GPUs).</p> <p>Additionally, the NVIDIA A100 GPUs support a feature called \u201cMulti-Instance GPU\u201d (MIG) mode. This allows a single GPU to be shared by up to 7 different processes. We do not schedule at this level, but you may pass <code>\u2013attrs mig-mode=True</code> in with your qsub, and we will set the node to MIG mode, allowing you to take advantage of it in your job script.</p>"},{"location":"sophia/not_in_nav/data-science/gpu-node-queue-and-policy/#queues","title":"Queues","text":"<p>There will be two primary queues: - full-node: This is the general production queue for jobs that require full nodes. - single-gpu: This is the general production queue for jobs that operate best on individual GPUs.</p> <p>And two debug queues: - debug-node: Submit to this queue if you need an entire node for your testing (for instance, you are utilizing the NVLink). - debug-gpu: Submit to this queue if you need GPUs.</p> <p>Initially, we are relaxing our node restrictions to encourage early users. Please be courteous to your fellow users and do not monopolize the machine. We will tighten restrictions as required to manage the demonstrated workload.</p> <p>Here are the initial queue limits: - MinTime is 5 minutes. - MaxTime is 12 hours. - MaxRunning will be 2 full nodes or 16 individual GPUs.</p>"},{"location":"sophia/not_in_nav/data-science/gpu-node-queue-and-policy/#queue-restrictions","title":"Queue Restrictions","text":"<ul> <li>MaxQueued will be 100 jobs.</li> <li>You may have at most 1152 node-hours or 9216 GPU hours in the queue at any time.</li> <li>You may not violate either of these policies.</li> <li>You could not submit (1000) 1 node-hour jobs because that would violate the MaxQueued of 100 jobs, nor could you submit (2) 1000 node-hour jobs because that would violate the MaxNodeHours limit.</li> <li>The initial queue policy will be simple First-In-First-Out (FIFO) based on priority with EASY backfill.</li> </ul>"},{"location":"sophia/not_in_nav/data-science/pythonc-code-interoperability/","title":"Python/C++ Code Interoperability","text":"<p>These are the steps to build code that has Python/C++ code interoperability.</p>"},{"location":"sophia/not_in_nav/data-science/pythonc-code-interoperability/#login-to-a-thetagpu-head-node","title":"Login to a ThetaGPU head node","text":"<pre><code>ssh thetagpusn1\n</code></pre>"},{"location":"sophia/not_in_nav/data-science/pythonc-code-interoperability/#1-request-an-interactive-session-on-an-a100-gpu","title":"1. Request an interactive session on an A100 GPU","text":"<pre><code>qsub -n 1 -q default -A datascience -I -t 1:00:00\n</code></pre> <p>Following this, we need to execute a few commands to get set up with an appropriately optimized TensorFlow. These are:</p>"},{"location":"sophia/not_in_nav/data-science/pythonc-code-interoperability/#2-activate-the-tensorflow-22-singularity-container","title":"2. Activate the TensorFlow 2.2 Singularity container","text":"<pre><code>singularity exec -B /lus:/lus --nv /lus/theta-fs0/projects/datascience/thetaGPU/containers/tf2_20.08-py3.sif bash\n</code></pre>"},{"location":"sophia/not_in_nav/data-science/pythonc-code-interoperability/#3-setup-access-to-the-internet","title":"3. Setup access to the internet","text":"<pre><code>export HTTP_PROXY=http://theta-proxy.tmi.alcf.anl.gov:3128 \nexport HTTPS_PROXY=https://theta-proxy.tmi.alcf.anl.gov:3128\n</code></pre> <p>Now that we can access the internet, we need to set up a virtual environment in Python (these commands should only be run the first time).</p> <pre><code>python -m pip install --user virtualenv \nexport VENV_LOCATION=/home/rmaulik/THETAGPU_TF_ENV # Add your path here \npython -m virtualenv --system-site-packages $VENV_LOCATION \nsource $VENV_LOCATION/bin/activate \npython -m pip install cmake \npython -m pip install matplotlib \npython -m pip install sklearn\n</code></pre> <p><code>cmake</code> is required to build our C++ app and link to Python, and other packages may be pip installed as needed in your Python code. An example <code>MakeLists.txt</code> file for building with Python/C++ interoperability with examples can be found here.</p>"},{"location":"sophia/not_in_nav/data-science/containers/containers/","title":"Containers on Theta(GPU)","text":""},{"location":"sophia/not_in_nav/data-science/containers/containers/#building-using-docker","title":"Building using Docker","text":"<p>If you followed the <code>Dockerfile</code> instructions, using the Theta(GPU) specific <code>Dockerfile_thetagpu</code>, you can build your container for ThetaGPU using: <pre><code>singularity build &lt;image_name&gt; docker://&lt;username&gt;/&lt;repo_name&gt;:&lt;tag&gt;\n# using tutorial example\nsingularity build my_image.simg docker://jtchilders/alcf_cwp_example:thetagpu\n</code></pre></p> <p></p> <p>Then you can submit a job to Theta(GPU) using the job submission script.</p> <pre><code>module load cobalt/cobalt-gpu\nqsub -A &lt;project-name&gt; job_submission_thetagpu.sh ./my_image.simg\n</code></pre> <p>The output should look like this: <pre><code>C++ MPI\nHello world from processor thetagpu12, rank 4 out of 16 processors\nHello world from processor thetagpu12, rank 7 out of 16 processors\nHello world from processor thetagpu12, rank 1 out of 16 processors\nHello world from processor thetagpu12, rank 5 out of 16 processors\nHello world from processor thetagpu12, rank 6 out of 16 processors\nHello world from processor thetagpu12, rank 0 out of 16 processors\nHello world from processor thetagpu12, rank 2 out of 16 processors\nHello world from processor thetagpu12, rank 3 out of 16 processors\nHello world from processor thetagpu18, rank 14 out of 16 processors\nHello world from processor thetagpu18, rank 15 out of 16 processors\nHello world from processor thetagpu18, rank 13 out of 16 processors\nHello world from processor thetagpu18, rank 8 out of 16 processors\nHello world from processor thetagpu18, rank 9 out of 16 processors\nHello world from processor thetagpu18, rank 11 out of 16 processors\nHello world from processor thetagpu18, rank 12 out of 16 processors\nHello world from processor thetagpu18, rank 10 out of 16 processors\nPython MPI\nHello world from processor thetagpu18, rank 13 out of 16 processors\nHello world from processor thetagpu18, rank 8 out of 16 processors\nHello world from processor thetagpu18, rank 9 out of 16 processors\nHello world from processor thetagpu18, rank 14 out of 16 processors\nHello world from processor thetagpu18, rank 15 out of 16 processors\nHello world from processor thetagpu18, rank 11 out of 16 processors\nHello world from processor thetagpu18, rank 10 out of 16 processors\nHello world from processor thetagpu18, rank 12 out of 16 processors\nHello world from processor thetagpu12, rank 2 out of 16 processors\nHello world from processor thetagpu12, rank 5 out of 16 processors\nHello world from processor thetagpu12, rank 0 out of 16 processors\nHello world from processor thetagpu12, rank 6 out of 16 processors\nHello world from processor thetagpu12, rank 4 out of 16 processors\nHello world from processor thetagpu12, rank 1 out of 16 processors\nHello world from processor thetagpu12, rank 7 out of 16 processors\nHello world from processor thetagpu12, rank 3 out of 16 processors\n</code></pre></p>"},{"location":"sophia/not_in_nav/data-science/containers/containers/#building-using-singularity-recipes","title":"Building using Singularity Recipes","text":"<p>While building using Docker on your local machine tends to be the easier method, there are sometimes reasons to build in the environment of the supercomputer. In this case, one can build a Singularity container on ThetaGPU in an interactive session on a compute (or worker) node. First, a recipe file is needed. Below is an example Singularity definition file, which can also be found here.</p> <p>Detailed directions for recipe construction are available on the Singularity Recipe Page.</p>"},{"location":"sophia/not_in_nav/data-science/containers/containers/#example-singularity-definition-file","title":"Example Singularity definition file","text":"<p>Here we have defined the base image from which to <code>bootstrap</code> our container. We are using an image from Docker Hub, <code>ubuntu:20.04</code>.</p> <pre><code>Bootstrap: docker\nFrom: ubuntu:20.04\n</code></pre> <p>The <code>%files</code> section lists files to copy from the host system (left path) to the container filesystem (right path) prior to build time.</p> <pre><code>%files\n    ../Local/source/* /usr/source/\n    ../Local/submit.sh /usr/\n</code></pre> <p>The <code>%environment</code> section defines environment variables that will be available to the container at runtime.</p> <pre><code>%environment\n    export PATH=$PATH:/mpich/install/bin\n    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/mpich/install/lib\n</code></pre> <p>The <code>%post</code> section executes within the container at build time on top of our <code>ubuntu:20.04</code> operating system. The <code>%post</code> section is therefore the place to perform installations of custom apps with syntax similar to BASH.</p> <pre><code>%post\n    #### INSTALL BASE PACKAGES NEEDED FOR MPI APPLICATIONS AND PYTHON3 ####\n    DEBIAN_FRONTEND=noninteractive\n    apt-get update -y \\\n    &amp;&amp; DEBIAN_FRONTEND=noninteractive \\\n    &amp;&amp; apt-get install -y build-essential libfabric-dev libibverbs-dev gfortran wget \\\n    &amp;&amp; apt-get install -y python3 python3-distutils python3-pip gcc\n\n    #### DOWNLOAD AND INSTALL MPICH AND MPI4PY ####\n    # Source is available at http://www.mpich.org/static/downloads/\n    # See installation guide of target MPICH version\n    # Ex: https://www.mpich.org/static/downloads/4.0.2/mpich-4.0.2-installguide.pdf\n    # These options are passed to the steps below\n    OPENMPI_VERSION_A=\"4.0\"\n    OPENMPI_VERSION_B=\"4.0.5\"\n    OPENMPI_CONFIGURE_OPTIONS=\"--prefix=/openmpi/install --disable-wrapper-rpath --disable-wrapper-runpath\"\n    OPENMPI_MAKE_OPTIONS=\"-j\"\n    mkdir -p openmpi\n    cd /openmpi\n    wget https://download.open-mpi.org/release/open-mpi/v${OPENMPI_VERSION_A}/openmpi-${OPENMPI_VERSION_B}.tar.gz\n    tar xfz openmpi-${OPENMPI_VERSION_B}.tar.gz  --strip-components=1\n   ./configure ${OPENMPI_CONFIGURE_OPTIONS}\n   make install ${OPENMPI_MAKE_OPTIONS}\n\n    export PATH=$PATH:/openmpi/install/bin\n   export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/openmpi/install/lib\n\n    pip install mpi4py\n\n    #### BUILD FILES ####\n    chmod +x /usr/submit.sh\n    mpicc -o /usr/source/mpi_hello_world /usr/source/mpi_hello_world.c\n</code></pre> <p>The <code>%runscript</code> section defines actions for the container to take when it is executed using <code>singularity run &lt;container_name&gt;</code>.</p> <pre><code>%runscript\n    exec /usr/submit.sh \"$@\"\n</code></pre> <p>The <code>%labels</code> section allows for custom metadata to be added to the container.</p> <pre><code>%labels\n        MAINTAINER Aditya atanikanti@anl.gov\n</code></pre> <p>The <code>%help</code> section can be used to define how to build and run the container.</p> <pre><code>%help\n        This container is used to illustrate an MPI-based def file to build a container running Python and C programs. To build the container, use singularity build --fakeroot mpi.sif mpi.def\n</code></pre>"},{"location":"sophia/not_in_nav/data-science/containers/containers/#build-singularity-container-on-thetagpu-compute","title":"Build Singularity container on ThetaGPU compute","text":"<p>After logging on to Theta login nodes, launch an interactive job using the attrs <code>fakeroot=true</code>, <code>pubnet=true</code> and specifying the filesystems <code>filesystems=home,theta-fs0</code>.</p> <pre><code># on Theta login node, must load cobalt-gpu module to submit jobs to ThetaGPU\nmodule load cobalt/cobalt-gpu\nqsub -I -n 1 -t 01:00:00 -q single-gpu -A &lt;project_name&gt; --attrs fakeroot=true:pubnet=true:filesystems=home,theta-fs0\n</code></pre> <p>Before building the container, make sure the ThetaGPU compute nodes have access to external resources. This is achieved by setting the <code>http_proxy</code> and <code>https_proxy</code> variables. <pre><code># setup network proxy to reach outside world\nexport http_proxy=http://proxy.tmi.alcf.anl.gov:3128\nexport https_proxy=http://proxy.tmi.alcf.anl.gov:3128\n</code></pre></p> <p>Now build the container using <code>--fakeroot</code> where <code>&lt;def_filename&gt;.def</code> is the definition file we have defined in the example above and <code>&lt;image_name&gt;.sif</code> is the user-defined image file name. Using mpi.def example: <pre><code># important you run this in the proper path because the file copies in\n# the `%files` section of the recipe uses relative paths on the host.\ncd \nsingularity build --fakeroot &lt;image_name&gt;.sif &lt;def_filename&gt;.def \n</code></pre></p>"},{"location":"sophia/not_in_nav/data-science/containers/containers/#run-singularity-container-on-thetagpu-compute","title":"Run Singularity container on ThetaGPU compute","text":"<p>An example job submission script is here: job_submission_thetagpu.sh.</p> <p>First, we define our job, and our script takes the container name as an input parameter.</p> <pre><code>#!/bin/bash -l\n#COBALT -n 1\n#COBALT -t 00:10:00\n#COBALT -q single-gpu\n#COBALT --attrs filesystems=home,theta-fs0:pubnet=true\nCONTAINER=$1\n</code></pre> <p>Enable network access at runtime by setting the proxy.</p> <pre><code>export http_proxy=http://proxy.tmi.alcf.anl.gov:3128\nexport https_proxy=http://proxy.tmi.alcf.anl.gov:3128\n</code></pre> <p>Set up our MPI settings, figure out the number of nodes <code>NODES</code>, fix the number of processes per node <code>PPN</code>, and multiply to get total MPI ranks <code>PROCS</code>.</p> <pre><code>NODES=`cat $COBALT_NODEFILE | wc -l`\nPPN=8 # GPUs per NODE\nPROCS=$((NODES * PPN))\necho NODES=$NODES  PPN=$PPN  PROCS=$PROCS\n</code></pre> <p>The OpenMPI installed on ThetaGPU must be used for MPI to properly run across nodes. Here the library path is added to <code>SINGULARITYENV_LD_LIBRARY_PATH</code>, which will be used by Singularity to set the container's <code>LD_LIBRARY_PATH</code> and therefore tell our executables where to find the MPI libraries.</p> <pre><code>MPI_BASE=/lus/theta-fs0/software/thetagpu/openmpi-4.0.5/\nexport LD_LIBRARY_PATH=$MPI_BASE/lib:$LD_LIBRARY_PATH\nexport SINGULARITYENV_LD_LIBRARY_PATH=$LD_LIBRARY_PATH\necho mpirun=$(which mpirun)\n</code></pre> <p>Finally, the executable is launched. Notice on NVidia systems that the <code>singularity exec</code> or <code>singularity run</code> commands must use the <code>--nv</code> flag to pass important libraries/drivers from the host to the container environment.</p> <p><pre><code>mpirun -hostfile $COBALT_NODEFILE -n $PROCS -npernode $PPN singularity exec --nv -B $MPI_BASE $CONTAINER /usr/source/mpi_hello_world\n</code></pre> The job can be submitted using: <pre><code>qsub -A &lt;project-name&gt; job_submission_thetagpu.sh /path/to/my_image.sif\n</code></pre></p> <p>The output should look like this: <pre><code>C++ MPI\nHello world from processor thetagpu02, rank 12 out of 16 processors\nHello world from processor thetagpu02, rank 8 out of 16 processors\nHello world from processor thetagpu02, rank 10 out of 16 processors\nHello world from processor thetagpu02, rank 11 out of 16 processors\nHello world from processor thetagpu02, rank 13 out of 16 processors\nHello world from processor thetagpu02, rank 9 out of 16 processors\nHello world from processor thetagpu02, rank 14 out of 16 processors\nHello world from processor thetagpu02, rank 15 out of 16 processors\nHello world from processor thetagpu01, rank 0 out of 16 processors\nHello world from processor thetagpu01, rank 1 out of 16 processors\nHello world from processor thetagpu01, rank 2 out of 16 processors\nHello world from processor thetagpu01, rank 3 out of 16 processors\nHello world from processor thetagpu01, rank 4 out of 16 processors\nHello world from processor thetagpu01, rank 5 out of 16 processors\nHello world from processor thetagpu01, rank 6 out of 16 processors\nHello world from processor thetagpu01, rank 7 out of 16 processors\nPython MPI\nHello world from processor thetagpu02, rank 9 out of 16 processors\nHello world from processor thetagpu02, rank 10 out of 16 processors\nHello world from processor thetagpu02, rank 11 out of 16 processors\nHello world from processor thetagpu02, rank 15 out of 16 processors\nHello world from processor thetagpu02, rank 13 out of 16 processors\nHello world from processor thetagpu02, rank 8 out of 16 processors\nHello world from processor thetagpu02, rank 12 out of 16 processors\nHello world from processor thetagpu02, rank 14 out of 16 processors\nHello world from processor thetagpu01, rank 7 out of 16 processors\nHello world from processor thetagpu01, rank 3 out of 16 processors\nHello world from processor thetagpu01, rank 1 out of 16 processors\nHello world from processor thetagpu01, rank 4 out of 16 processors\nHello world from processor thetagpu01, rank 5 out of 16 processors\nHello world from processor thetagpu01, rank 6 out of 16 processors\nHello world from processor thetagpu01, rank 0 out of 16 processors\nHello world from processor thetagpu01, rank 2 out of 16 processors\n</code></pre></p>"},{"location":"sophia/not_in_nav/data-science/containers/containers/#pre-existing-images-for-deep-learning","title":"Pre-existing Images for Deep Learning","text":"<p>There are several containers on ThetaGPU that will help you get started with deep learning experiments that can efficiently use the A100 GPUs. We have different optimized containers for DL here <code>ls /lus/theta-fs0/software/thetagpu/nvidia-containers/</code>.</p> <p>The bootstrap.def gives an example of how these containers were created.</p> <p>The image is bootstrapped from an NVidia image, in this case from a PyTorch build. One can also use the TensorFlow build. At the time of this writing, the latest tag for the PyTorch image was <code>22.04-py3</code>, but users should select the version that best suits their needs.</p> <p><pre><code>Bootstrap: docker\nFrom: nvcr.io/nvidia/pytorch:22.04-py3\n</code></pre> Next, we need to install MPI support for cross-node parallel training.</p> <p><pre><code>%post\n\n    # Install mpi4py\n    CC=$(which mpicc) CXX=$(which mpicxx) pip install --no-cache-dir mpi4py\n\n    # Install horovod\n    CC=$(which mpicc) CXX=$(which mpicxx) HOROVOD_WITH_TORCH=1 pip install --no-cache-dir horovod\n</code></pre> Next, build the container on a ThetaGPU compute node, following the instructions in the previous section. Then an example job submission script is here: job_submission_thetagpudl.sh.</p>"},{"location":"sophia/not_in_nav/data-science/dl-frameworks/deepspeed/","title":"Deepspeed","text":"<pre><code># DeepSpeed\n\nThe base `#!bash conda` environment on ThetaGPU comes with Microsoft's\n[DeepSpeed](https://github.com/microsoft/DeepSpeed) pre-installed. Instructions\nfor using/cloning the base environment can be found\n&lt;!-- broken sophia link [here](docs/polaris/data-science-workflows/python.md).--&gt;\n\nWe describe below the steps needed to get started with DeepSpeed on ThetaGPU.\n\nWe focus on the `cifar` example provided in the\n[DeepSpeedExamples](https://github.com/microsoft/DeepSpeedExamples) repository,\nthough this approach should be generally applicable for running any model with\nDeepSpeed support.\n\n## Running DeepSpeed on ThetaGPU\n\n!!! note\n\n    The instructions below should be **run directly from a compute node**.\n    Explicitly, to request an interactive job (from `thetalogin`):\n\n    ```bash\n    qsub-gpu -A &lt;project&gt; -n 2 -t 01:00 -q full-node \\\n        --attrs=\"filesystems=home,eagle,theta-fs0:ssds=required\" \\\n        -I\n    ```\n\n    Refer to [GPU Node Queue and Policy](../gpu-node-queue-and-policy.md).\n\n1. Load `conda` module and activate base environment:\n    ```bash\n    module load conda ; conda activate base\n    ```\n\n2. Clone\n   [microsoft/DeepSpeedExamples](https://github.com/microsoft/DeepSpeedExamples)\n   and navigate into the directory:\n    ```bash\n    git clone https://github.com/microsoft/DeepSpeedExamples.git\n    cd DeepSpeedExamples/cifar\n    ```\n\n3. Our newer conda environments should come with DeepSpeed pre-installed, but\n   in the event your environment has no `deepspeed`, it can be\n   installed[^deepspeed] with `pip`:\n    ```bash\n    $ which deepspeed\n    deepspeed not found\n    $ python3 -m pip install --upgrade pip setuptools wheel\n    $ DS_BUILD_OPS=1 python3 -m pip install deepspeed\n    ```\n\n!!! example \"Launching DeepSpeed\"\n\n    === \"Launching with OpenMPI\"\n\n        1. Get total number of available GPUs:\n            1. Count number of lines in `#!bash $COBALT_NODEFILE` (1 host per line)\n            2. Count number of GPUs available on current host\n            3. `#!bash NGPUS = $((${NHOSTS}*${NGPU_PER_HOST}))`\n        ```bash\n        NHOSTS=$(wc -l &lt; \"${COBALT_NODEFILE}\")\n        NGPU_PER_HOST=$(nvidia-smi -L | wc -l)\n        NGPUS=\"$((${NHOSTS}*${NGPU_PER_HOST}))\"\n        ```\n\n        2. Launch with `mpirun`[^mpi]:\n        ```bash\n        mpirun \\\n            -n \"${NGPUS}\" \\\n            -npernode \"${NGPU_PER_HOST}\" \\\n            --hostfile \"${COBALT_NODEFILE}\" \\\n            -x PATH \\\n            -x LD_LIBRARY_PATH \\\n            -x PYTHONUSERBASE \\\n            -x http_proxy \\\n            -x https_proxy \\\n            python3 cifar10_deepspeed.py \\\n                --deepspeed_config ds_config-1.json\n        ```\n\n    === \"Launching with DeepSpeed\"\n\n        1. Create a DeepSpeed compliant `hostfile`, specifying the hostname and\n           number of GPUs (`slots`) for each of our available workers:\n        ```bash\n        cat $COBALT_NODEFILE &gt; hostfile\n        sed -e 's/$/ slots=4/' -i hostfile\n        ```\n\n        2. Create a `#!bash .deepspeed_env` containing the environment variables our\n           workers will need access to:\n        ```bash\n        echo \"PATH=${PATH}\" &gt;&gt; .deepspeed_env\n        echo \"LD_LIBRARY_PATH=${LD_LIBRARY_PATH}\" &gt;&gt; .deepspeed_env\n        echo \"http_proxy=${http_proxy}\" &gt;&gt; .deepspeed_env\n        echo \"https_proxy=${https_proxy}\" &gt;&gt; .deepspeed_env\n        ```\n\n        !!! warning\n\n            The `#!bash .deepspeed_env` file expects each line to be of the form\n            `KEY=VALUE`. Each of these will then be set as environment variables on\n            each available worker specified in our `hostfile`.\n\n        We can then run the `#!bash cifar10_deepspeed.py` module using DeepSpeed:\n        ```bash title=\"Launch with DeepSpeed\"\n        deepspeed --hostfile=hostfile cifar10_deepspeed.py \\\n            --deepspeed \\\n            --deepspeed_config ds_config.json\n        ```\n\n???- bug \"`AssertionError: Micro batch size per gpu: 0 has to be greater than 0`\"\n\n    Depending on the details of your specific job, it may be necessary to\n    modify the provided `#!bash ds_config.json`.\n\n    If you encounter an error:\n    ```\n    thetagpu23: AssertionError: Micro batch size per gpu: 0 has to be greater than 0\n    ```\n    you can modify the `#!json \"train_batch_size\": 16` variable in the provided\n    `#!bash ds_config.json`\n    to the (total) number of available GPUs, and explicitly set `#!json\n    \"gradient_accumulation_steps\": 1`, as shown below.\n    ```bash\n    $ export NHOSTS=$(wc -l &lt; \"${COBALT_NODEFILE}\")\n    $ export NGPU_PER_HOST=$(nvidia-smi -L | wc -l)\n    $ export NGPUS=\"$((${NHOSTS}*${NGPU_PER_HOST}))\"\n    $ echo $NHOSTS $NGPU_PER_HOST $NGPUS\n    2 8 16\n    $ # replace \"train_batch_size\" with $NGPUS in ds_config.json\n    $ # and write to `ds_config-polaris.json`\n    $ sed \\\n        \"s/$(cat ds_config.json| grep batch | cut -d ':' -f 2)/ ${NGPUS},/\" \\\n        ds_config.json \\\n        &gt; ds_config-polaris.json\n    $ cat ds_config-polaris.json\n    {\n        \"train_batch_size\": 16,\n        \"gradient_accumulation_steps\": 1,\n        ...\n    }\n    ```\n\n[^mpi]: \n      The flag `#!bash -x ENVIRONMENT_VARIABLE` ensures the `#!bash\n      $ENVIRONMENT_VARIABLE` will be set in the launched processes.\n\n[^deepspeed]:\n      Additional details for installing DeepSpeed can be found in their docs\n      from: [Installation\n      Details](https://www.deepspeed.ai/tutorials/advanced-install/)\n</code></pre>"},{"location":"sophia/not_in_nav/data-science/dl-frameworks/distributed-training-using-data-parallelism/","title":"Distributed Training on ThetaGPU Using Data Parallelism","text":"<p>There are two schemes for distributed learning:</p> <ol> <li> <p>Model parallelization: in this scheme, disjoint subsets of a neural network are assigned to different devices. Therefore, all the computations associated with the subsets are distributed. Communication happens between devices whenever there is data flow between two subsets. Model parallelization is suitable when the model is too large to be fitted into a single device (CPU/GPU) because of the memory capacity. However, partitioning the model into different subsets is not an easy task, and it might potentially introduce load imbalance issues limiting the scaling efficiency.</p> </li> <li> <p>Data parallelization: in this scheme, all the workers own a replica of the model. The global batch of data is split into multiple minibatches and processed by different workers. Each worker computes the corresponding loss and gradients with respect to the data it possesses. Before updating the parameters at each epoch, the loss and gradients are averaged among all the workers through a collective operation. This scheme is relatively simple to implement. MPI_Allreduce is the only communication operation needed.</p> </li> </ol> <p>Our recent presentation about data parallel training can be found here: https://youtu.be/930yrXjNkgM</p> <p>In this documentation, we would like to show how to do data parallel training on ThetaGPU.</p>"},{"location":"sophia/not_in_nav/data-science/dl-frameworks/distributed-training-using-data-parallelism/#software-environment-setup","title":"Software environment setup","text":"<p>We are still in the process of setting up the software stacks on ThetaGPU. Currently, one can get TensorFlow, PyTorch, and Horovod with the following setup script.</p> <pre><code>source /lus/theta-fs0/software/datascience/thetagpu/anaconda3/setup.sh\n</code></pre>"},{"location":"sophia/not_in_nav/data-science/dl-frameworks/distributed-training-using-data-parallelism/#tensorflow-with-horovod","title":"TensorFlow with Horovod","text":""},{"location":"sophia/not_in_nav/data-science/dl-frameworks/distributed-training-using-data-parallelism/#1-initialize-horovod","title":"1. Initialize Horovod","text":"<pre><code>import horovod.tensorflow as hvd\nhvd.init()\n</code></pre> <p>After this initialization, the rank ID and the number of processes can be referred to as <code>hvd.rank()</code> and <code>hvd.size()</code>. Besides, one can also call <code>hvd.local_rank()</code> to get the local rank ID within a node. This is useful when we are trying to assign GPUs to each rank.</p>"},{"location":"sophia/not_in_nav/data-science/dl-frameworks/distributed-training-using-data-parallelism/#2-assign-gpu-to-each-rank","title":"2. Assign GPU to each rank","text":"<pre><code>gpus = tf.config.experimental.list_physical_devices('GPU')\nfor gpu in gpus:\n    tf.config.experimental.set_memory_growth(gpu, True)\nif gpus:\n    tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')\n</code></pre> <p>In this case, we set one GPU per process: <code>ID=hvd.local_rank()</code></p>"},{"location":"sophia/not_in_nav/data-science/dl-frameworks/distributed-training-using-data-parallelism/#3-scale-the-learning-rate","title":"3. Scale the learning rate","text":"<p>Typically, since we use multiple workers, the global batch is usually increased n times (n is the number of workers). The learning rate should increase proportionally as follows (assuming that the learning rate initially is 0.01).</p> <pre><code>opt = tf.train.AdagradOptimizer(0.01 * hvd.size())\n</code></pre>"},{"location":"sophia/not_in_nav/data-science/dl-frameworks/distributed-training-using-data-parallelism/#4-wrap-the-optimizer-with-distributed-optimizer","title":"4. Wrap the optimizer with Distributed Optimizer","text":"<pre><code>opt = hvd.DistributedOptimizer(opt)\n</code></pre>"},{"location":"sophia/not_in_nav/data-science/dl-frameworks/distributed-training-using-data-parallelism/#5-broadcast-the-model-from-rank-0","title":"5. Broadcast the model from rank 0","text":"<p>This is to make sure that all the workers will have the same starting point.</p> <pre><code>hooks = [hvd.BroadcastGlobalVariablesHook(0)]\n</code></pre>"},{"location":"sophia/not_in_nav/data-science/dl-frameworks/distributed-training-using-data-parallelism/#6-loading-data-according-to-rank-id","title":"6. Loading data according to rank ID","text":"<p>TensorFlow has some functions for parallel distribution of data. But for specific applications, the user might have to write their own data loader.</p> <p>In general, one has two ways to deal with the data loading:</p> <ol> <li> <p>Each worker randomly selects one batch of data from the dataset at each step. In such a case, each worker can see the entire dataset. It is important to make sure that the different workers have different random seeds so that they will get different data at each step.</p> </li> <li> <p>Each worker accesses a subset of the dataset. One manually partitions the entire dataset into different partitions, and each rank accesses one of the partitions.</p> </li> </ol> <p>In both cases, the total number of steps per epoch is <code>nsamples / hvd.size()</code>.</p>"},{"location":"sophia/not_in_nav/data-science/dl-frameworks/distributed-training-using-data-parallelism/#7-checkpointing-on-root-rank","title":"7. Checkpointing on root rank","text":"<p>It is important to let only one process do the checkpointing I/O lest perhaps the file be corrupted.</p> <pre><code>if hvd.rank() == 0:\n    checkpoint.save(checkpoint_dir)\n</code></pre>"},{"location":"sophia/not_in_nav/data-science/dl-frameworks/distributed-training-using-data-parallelism/#8-average-metric-across-all-the-workers","title":"8. Average metric across all the workers","text":"<p>Notice that in the distributed training, any tensor is local to each worker. In order to get the global averaged value, one can use Horovod allreduce. Below is an example of how to do the average.</p> <pre><code>def tensor_average(val, name):\n    tensor = torch.tensor(val)\n    if with_hvd:\n        avg_tensor = hvd.allreduce(tensor, name=name)\n    else:\n        avg_tensor = tensor\n    return avg_tensor.item()\n</code></pre> <p>We provided some examples in: https://github.com/argonne-lcf/sdl_ai_workshop/blob/master/01_distributedDeepLearning/Horovod/tensorflow2_mnist.py</p>"},{"location":"sophia/not_in_nav/data-science/dl-frameworks/distributed-training-using-data-parallelism/#pytorch-with-ddp","title":"PyTorch with DDP","text":"<p>PyTorch has its own native parallelization library called DDP. We will provide more details on how to run this on ThetaGPU. The current PyTorch on ThetaGPU does not have DDP built-in. We will update our users once we have DDP.</p> <p>For now, please refer to https://pytorch.org/tutorials/intermediate/ddp_tutorial.html</p>"},{"location":"sophia/not_in_nav/data-science/dl-frameworks/distributed-training-using-data-parallelism/#mpi-profiling-for-data-parallel-training","title":"MPI Profiling for data parallel training","text":"<p>We support two ways for profiling the performance of data parallel training.</p> <ol> <li>mpitrace library MPI trace allows us to get a flat profiling of all the MPI function calls involved during the training. To enable this, one can set the environment variable</li> </ol> <pre><code>export LD_PRELOAD=/lus/theta-fs0/software/datascience/thetagpu/hpctw/lib/libmpitrace.so\n</code></pre> <p>Then run the application as usual. MPI profiling results will be generated after the run finishes <code>mpi_profile.XXXX.[rank_id]</code>.</p> <p>Below is an example output:</p> <pre><code>Data for MPI rank 0 of 8:\nTimes and statistics from MPI_Init() to MPI_Finalize().\n-----------------------------------------------------------------------\nMPI Routine #calls avg. bytes time(sec)\n-----------------------------------------------------------------------\nMPI_Comm_rank 3 0.0 0.000\nMPI_Comm_size 3 0.0 0.000\nMPI_Bcast 520 197140.6 0.518\nMPI_Allreduce 24561 208138.3 162.080\nMPI_Gather 126 4.0 0.363\nMPI_Gatherv 126 0.0 0.434\nMPI_Allgather 2 4.0 0.000\n-----------------------------------------------------------------\nMPI task 0 of 8 had the maximum communication time.\ntotal communication time = 163.396 seconds.\ntotal elapsed time = 187.298 seconds.\nuser cpu time = 4127.728 seconds.\nsystem time = 728.100 seconds.\nmax resident set size = 8403.938 MBytes.\n\nRank 0 reported the largest memory utilization : 8403.94 MBytes\nRank 0 reported the largest elapsed time : 187.30 sec\n-----------------------------------------------------------------\nMessage size distributions:\n                       MPI_Bcast      #calls   avg. bytes       time(sec)\n                                         126          4.0          0.008\n                                           1          8.0          0.000\n                                         121         25.0          0.006\n                                          30        251.5          0.002\n                                          32        512.0          0.002\n                                          64       1024.0          0.005\n                                          44       2048.0          0.003\n                                          29       4092.8          0.003\n                                          16       8192.0          0.032\n\n                       MPI_Allreduce   #calls  avg. bytes       time(sec)\n                                        19780         8.0         90.822\n                                         4576        24.0         18.239\n                                           43      4004.0          0.295\n                                            5   2780979.2          0.469\n                                           50   8160289.2         20.893\n                                            9  11803392.0          0.964\n                                           48  28060640.0          3.293\n                                           50  64731668.5         27.105\n\n                       MPI_Gather      #calls   avg. bytes      time(sec)\n                                          126         4.0          0.363\n</code></pre> <p>The useful information here is the message size distribution.</p> <ol> <li>Horovod Timeline To perform Horovod timeline analysis, one has to set the environment variable HOROVOD_TIMELINE which specifies the file for the output.</li> </ol> <pre><code>export HOROVOD_TIMELINE=timeline.json\n</code></pre> <p>This file is only recorded on rank 0, but it contains information about the activity of all workers. You can then open the timeline file using the chrome://tracing facility of the Chrome browser.</p> <p>More details: https://horovod.readthedocs.io/en/stable/timeline_include.html</p>"},{"location":"sophia/not_in_nav/data-science/dl-frameworks/running-pytorch-conda/","title":"Running PyTorch with Conda","text":"<p>Be aware that these builds use CUDA and will not work on login nodes, which do not have CUDA installed as there are no GPUs.</p> <p>One can test these software packages in an interactive session: <pre><code>qsub -I -q single-gpu -n 1 -t 30 -A &lt;project-name&gt; --attrs filesystems=&lt;list of filesystems&gt;\n</code></pre></p>"},{"location":"sophia/not_in_nav/data-science/dl-frameworks/running-pytorch-conda/#pytorch-master-build","title":"PyTorch (master build)","text":"<p>Users can find the latest builds via the <code>module avail</code> conda command, which will list available builds such as <code>conda/2021-06-26</code>, which is a module that was built on 2021-06-26. Use <code>module show conda/2021-06-26</code> or <code>module help conda/2021-06-26</code> to get high-level info on which versions of the key packages and libraries this particular module contains.</p> <p>This version can be used by: <pre><code>module load conda/2021-06-26 # loads conda into your environment, sets up appropriate CUDA libraries and environment variables\nconda activate # add entries to PATH for the environment and run any activation scripts that the environment may contain\n</code></pre></p> <p>This will set up a conda environment with the \"from scratch\" build of PyTorch.</p> <p>This package will also include builds of TensorFlow and Horovod tagged releases.</p>"},{"location":"sophia/not_in_nav/data-science/dl-frameworks/running-pytorch-conda/#installing-packages","title":"Installing Packages","text":""},{"location":"sophia/not_in_nav/data-science/dl-frameworks/running-pytorch-conda/#using-pip-install-user-not-recommended","title":"Using <code>pip install --user</code> (not recommended)","text":"<p>With the conda environment set up, one can install common Python modules using <code>pip install --user &lt;module-name&gt;</code>, which will install packages in <code>$PYTHONUSERBASE/lib/pythonX.Y/site-packages</code>. The <code>$PYTHONUSERBASE</code> environment variable is automatically set when you load the base conda module and is typically equal to <code>/home/$USER/.local/conda/YYYY-MM-DD</code> or <code>/home/$USER/.local/thetagpu/conda/YYYY-MM-DD</code>, depending on the date of the module.</p> <p>Note, Python modules installed this way that contain command-line binaries will not have those binaries automatically added to the shell's <code>$PATH</code>. To manually add the path: <pre><code>export PATH=$PYTHONUSERBASE/bin:$PATH\n</code></pre> Be sure to remove this location from <code>$PATH</code> if you deactivate the base Anaconda environment or unload the module.</p> <p>Cloning the Anaconda environment or using <code>venv</code> are both more flexible and transparent when compared to <code>--user</code> installs.</p>"},{"location":"sophia/not_in_nav/data-science/dl-frameworks/running-pytorch-conda/#using-conda-environments","title":"Using Conda Environments","text":"<p>If you need more flexibility, you can clone the conda environment into a custom path, which would then allow for root-like installations via <code>conda install &lt;module&gt;</code> or <code>pip install &lt;module&gt;</code>.</p> <ol> <li>Set up the conda environment you want to use as instructed above.</li> <li>Create/edit your <code>$HOME/.condarc</code> file to include these lines, replacing <code>&lt;project-name&gt;</code> with your project name. <code>&lt;path-to-your-project&gt;</code> is the path to the file system your project is on (e.g., <code>/lus/theta-fs0</code> or <code>/eagle</code>). By default, Conda will use your <code>$HOME/.conda/*</code> area for caching files.</li> </ol> <p>Note: Since home directories are limited to 100GB, this fills up quickly. This addition tells Conda to use your project space for cache storage instead.</p> <pre><code>pkgs_dirs: \n  - &lt;path-to-your-project&gt;/&lt;project-name&gt;/conda/pkgs \nenvs_dirs: \n  - &lt;path-to-your-project&gt;/&lt;project-name&gt;/conda/envs\n</code></pre> <ol> <li>Clone the environment into a local path to which you have write access: <pre><code>conda create --clone $CONDA_PREFIX -p &lt;path/to/env&gt;\n</code></pre></li> <li>Activate that environment: <pre><code>conda activate &lt;path/to/env&gt;\n</code></pre></li> </ol> <p>One should then be able to install modules natively.</p>"},{"location":"sophia/not_in_nav/data-science/dl-frameworks/running-tensorflow-conda/","title":"Running TensorFlow with Conda","text":"<p>Be aware that these builds use CUDA and will not work on login nodes, which do not have CUDA installed as there are no GPUs.</p> <p>One can test these software packages in an interactive session: <pre><code>qsub -I -q single-gpu -n 1 -t 30 -A &lt;project-name&gt; --attrs filesystems=&lt;list of filesystems&gt;\n</code></pre></p>"},{"location":"sophia/not_in_nav/data-science/dl-frameworks/running-tensorflow-conda/#tensorflow-master-build","title":"TensorFlow (master build)","text":"<p>Users can find the latest builds via the <code>module avail</code> conda command, which will list available builds such as <code>conda/2021-06-26</code>, a module that was built on 2021-06-26. Use <code>module show conda/2021-06-26</code> or <code>module help conda/2021-06-26</code> to get high-level info on which versions of the key packages and libraries this particular module contains.</p> <p>This version can be used by: <pre><code>module load conda/2021-06-26 # loads conda into your environment, sets up appropriate CUDA libraries and environment variables\nconda activate # add entries to PATH for the environment and run any activation scripts that the environment may contain\n</code></pre> This will set up a conda environment with the \"from scratch\" build of TensorFlow.</p> <p>This package will also include builds of PyTorch and Horovod tagged releases.</p>"},{"location":"sophia/not_in_nav/data-science/dl-frameworks/running-tensorflow-conda/#installing-packages","title":"Installing Packages","text":""},{"location":"sophia/not_in_nav/data-science/dl-frameworks/running-tensorflow-conda/#using-pip-install-user-not-recommended","title":"Using <code>pip install --user</code> (not recommended)","text":"<p>With the conda environment set up, one can install common Python modules using <code>pip install --user &lt;module-name&gt;</code>, which will install packages in <code>$PYTHONUSERBASE/lib/pythonX.Y/site-packages</code>. The <code>$PYTHONUSERBASE</code> environment variable is automatically set when you load the base conda module and is typically equal to <code>/home/$USER/.local/conda/YYYY-MM-DD</code> or <code>/home/$USER/.local/thetagpu/conda/YYYY-MM-DD</code>, depending on the date of the module.</p> <p>Note, Python modules installed this way that contain command-line binaries will not have those binaries automatically added to the shell's <code>$PATH</code>. To manually add the path: <pre><code>export PATH=$PYTHONUSERBASE/bin:$PATH\n</code></pre> Be sure to remove this location from <code>$PATH</code> if you deactivate the base Anaconda environment or unload the module.</p> <p>Cloning the Anaconda environment or using <code>venv</code> are both more flexible and transparent when compared to <code>--user</code> installs.</p>"},{"location":"sophia/not_in_nav/data-science/dl-frameworks/running-tensorflow-conda/#using-conda-environments","title":"Using Conda Environments","text":"<p>If you need more flexibility, you can clone the conda environment into a custom path, which would then allow for root-like installations via <code>conda install &lt;module&gt;</code> or <code>pip install &lt;module&gt;</code>.</p> <ol> <li> <p>Set up the conda environment you want to use as instructed above.</p> </li> <li> <p>Create/edit your <code>$HOME/.condarc</code> file to include these lines, replacing <code>&lt;project-name&gt;</code> with your project name. <code>&lt;path-to-your-project&gt;</code> is the path to the file system your project is on (e.g., <code>/lus/theta-fs0</code> or <code>/eagle</code>). By default, Conda will use your <code>$HOME/.conda/*</code> area for caching files.</p> </li> </ol> <p>Note: Since home directories are limited to 100GB, this fills up quickly. This addition tells Conda to use your project space for cache storage instead.</p> <pre><code>pkgs_dirs: \n   - /lus/theta-fs0/projects/&lt;project-name&gt;/conda/pkgs \nenvs_dirs: \n   - /lus/theta-fs0/projects/&lt;project-name&gt;/conda/envs\n</code></pre> <ol> <li> <p>Clone the environment into a local path to which you have write access. <pre><code>conda create --clone $CONDA_PREFIX -p &lt;path/to/env&gt;\n</code></pre></p> </li> <li> <p>Activate that environment. <pre><code>conda activate &lt;path/to/env&gt;\n</code></pre></p> </li> </ol> <p>One should then be able to install modules natively.</p>"},{"location":"sophia/not_in_nav/data-science/dl-frameworks/tensorboard-instructions/","title":"TensorBoard Instructions","text":"<p>If you are able to install TensorBoard on your local machine, it is often easiest to copy the requisite files from ALCF file systems (via <code>sftp</code>, <code>scp</code>, Globus, etc.) to your local machine and run TensorBoard there.</p> <p>However, if that is not possible, or if you have many and/or large files that TensorBoard needs to process located on ALCF file systems, there are several ways to run a TensorBoard server remotely.</p>"},{"location":"sophia/not_in_nav/data-science/dl-frameworks/tensorboard-instructions/#tensorboard-server-on-a-thetagpu-compute-node","title":"TensorBoard server on a ThetaGPU compute node","text":"<p>This approach can be useful to have TensorBoard analyze live training progress. After you have logged into ThetaGPU and have an interactive job running, you'll need to know the name of one of your worker nodes so you can SSH to it.</p> <pre><code>PORT0=9991 \nPORT1=9992 \nPORT3=9993 \n# Select a theta login node N where N=[1-6]\nssh -L $PORT0:localhost:$PORT1 $USER@thetaloginN.alcf.anl.gov \n\n# after reaching thetaloginN \n\n# Replace NN with your thetagpu worker node\nssh -L $PORT1:thetagpuNN:$PORT3 $USER@thetagpusn1 \n# after reaching thetagpusn1 \n\n# login to worker node \nssh thetagpuNN \n\n# now setup your tensorflow environment \n# for instance run the conda setup.sh script created during the install_tensorflow.sh script \n\n# now run tensorboard \ntensorboard --logdir &lt;/path/to/logs&gt; --port $PORT3 --bind_all\n</code></pre>"},{"location":"sophia/not_in_nav/data-science/dl-frameworks/tensorboard-instructions/#tensorboard-server-on-a-thetaknl-login-node","title":"TensorBoard server on a ThetaKNL login node","text":"<p>If you do not require the use of a GPU during analysis while TensorBoard runs, and you do not require a cutting-edge version of TensorBoard (this will load version 2.6.0), you can avoid additional SSH tunnel hops by running the TensorBoard server on a ThetaKNL login node:</p> <pre><code>ssh -D &lt;some-port-number&gt; theta.alcf.anl.gov\n\nmodule load conda/2021-09-22\nexport LD_LIBRARY_PATH=/soft/thetagpu/cuda/cuda_11.3.0_465.19.01_linux/lib64:$LD_LIBRARY_PATH\nexport LD_LIBRARY_PATH=/soft/thetagpu/cuda/cuda_11.3.0_465.19.01_linux/extras/CUPTI/lib64/:$LD_LIBRARY_PATH\n</code></pre>"},{"location":"sophia/not_in_nav/performance-tools/darshan/","title":"Darshan on ThetaGPU","text":""},{"location":"sophia/not_in_nav/performance-tools/darshan/#overview","title":"Overview","text":"<p>Darshan instrumentation on ThetaGPU is not automatically included in the binary like on Theta. The user must set the <code>LD_PRELOAD</code> variable as part of running the job.</p> <p>Logs will be generated in the directory: <code>/lus/eagle/logs/darshan/thetagpu/&lt;YEAR&gt;/&lt;MONTH&gt;/&lt;DAY&gt;</code></p> <p>To view a log, use the <code>darshan-parser</code> utility.</p> <pre><code>module load darshan \nmpirun ... -x LD_PRELOAD=$DARSHAN_PRELOAD\n</code></pre>"},{"location":"sophia/not_in_nav/performance-tools/darshan/#more-information","title":"More information","text":""},{"location":"sophia/not_in_nav/performance-tools/nvidia-nsight/","title":"NVIDIA Nsight","text":""},{"location":"sophia/not_in_nav/performance-tools/nvidia-nsight/#references","title":"References","text":"<ul> <li>NVIDIA Nsight Systems Documentation</li> <li>NVIDIA Nsight Compute Documentation</li> </ul>"},{"location":"sophia/not_in_nav/performance-tools/nvidia-nsight/#introduction","title":"Introduction","text":"<p>NVIDIA\u00ae Nsight\u2122 Systems provides developers with a system-wide visualization of an application's performance. Developers can optimize bottlenecks to scale efficiently across any number or size of CPUs and GPUs on ThetaGPU. For further optimizations to compute kernels, developers should use Nsight Compute.</p> <p>NVIDIA Nsight Compute is an interactive kernel profiler for CUDA applications. It provides detailed performance metrics and API debugging via a user interface and command-line tool.</p> <p>In addition, the baseline feature of this tool allows users to compare results within the tool. NVIDIA Nsight Compute provides a customizable and data-driven user interface, metric collection, and can be extended with analysis scripts for post-processing results.</p>"},{"location":"sophia/not_in_nav/performance-tools/nvidia-nsight/#step-by-step-guide","title":"Step-by-step guide","text":""},{"location":"sophia/not_in_nav/performance-tools/nvidia-nsight/#1-common-part-on-thetagpu","title":"1. Common part on ThetaGPU","text":"<p>Build your application for ThetaGPU and submit your job script to ThetaGPU or start an interactive job mode on ThetaGPU as follows:</p> <pre><code>$ module load cobalt/cobalt-gpu\n$ qsub -I -n 1 -t 30 -q full-node -A {your_project}\n</code></pre>"},{"location":"sophia/not_in_nav/performance-tools/nvidia-nsight/#2-nsight-systems","title":"2. Nsight Systems","text":"<p>Run your application with Nsight Systems as follows:</p> <pre><code>$ nsys profile -o {output_filename} --stats=true ./{your_application}\n</code></pre>"},{"location":"sophia/not_in_nav/performance-tools/nvidia-nsight/#3-nsight-compute","title":"3. Nsight Compute","text":"<p>Run your application with Nsight Compute.</p> <pre><code>$ ncu --set detailed -k {kernel_name} -o {output_filename} ./{your_application}\n</code></pre> <p>Note: Without the -o option, Nsight Compute provides performance data as standard output.</p>"},{"location":"sophia/not_in_nav/performance-tools/nvidia-nsight/#4-post-processing-the-profiled-data","title":"4. Post-processing the profiled data","text":""},{"location":"sophia/not_in_nav/performance-tools/nvidia-nsight/#post-processing-via-cli","title":"Post-processing via CLI","text":"<pre><code>$ nsys stats {output_filename}.qdrep\n$ ncu -i {output_filename}.ncu-rep\n</code></pre>"},{"location":"sophia/not_in_nav/performance-tools/nvidia-nsight/#post-processing-on-your-local-system-via-gui","title":"Post-processing on your local system via GUI","text":"<ul> <li>Install NVIDIA Nsight Systems and NVIDIA Nsight Compute after downloading both of them from the NVIDIA Developer Zone.</li> <li>Download nsys output files (i.e., ending with .qdrep and .sqlite) to your local system, and then open them with NVIDIA Nsight Systems on your local system.</li> <li>Download ncu output files (i.e., ending with .ncu-rep) to your local system, and then open them with NVIDIA Nsight Compute on your local system.</li> </ul> <p>For more options for performance analysis with Nsight Systems and Nsight Compute:</p> <pre><code>$ nsys --help\n$ ncu --help\n</code></pre>"},{"location":"sophia/not_in_nav/performance-tools/nvidia-nsight/#a-quick-example","title":"A quick example","text":""},{"location":"sophia/not_in_nav/performance-tools/nvidia-nsight/#nsight-systems","title":"Nsight Systems","text":""},{"location":"sophia/not_in_nav/performance-tools/nvidia-nsight/#running-a-stream-benchmark-with-nsight-systems","title":"Running a stream benchmark with Nsight Systems","text":"<pre><code>jkwack@thetagpu18:~/HPC_benchmarks/BabelStream/JK_thetaGPU$ nsys profile -o JKreport-nsys-BableStream --stats=true ./cuda-stream\n\nWarning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n\nCollecting data...\n\nBabelStream\n\nVersion: 3.4\n\nImplementation: CUDA\n\nRunning kernels 100 times\n\nPrecision: double\n\nArray size: 268.4 MB (=0.3 GB)\n\nTotal size: 805.3 MB (=0.8 GB)\n\nUsing CUDA device A100-SXM4-40GB\n\nDriver: 11000\n\nFunction    MBytes/sec  Min (sec)   Max         Average     \n\nCopy        1381210.283 0.00039     0.00040     0.00039     \n\nMul         1339635.322 0.00040     0.00041     0.00040     \n\nAdd         1357739.235 0.00059     0.00061     0.00060     \n\nTriad       1366533.461 0.00059     0.00061     0.00060     \n\nDot         1210611.093 0.00044     0.00047     0.00046     \n\nProcessing events...\n\nCapturing symbol files...\n\nSaving temporary \"/tmp/nsys-report-b948-7122-9b9a-feb1.qdstrm\" file to disk...\n\nCreating final output files...\n\nProcessing [==============================================================100%]\n\nSaved report file to \"/tmp/nsys-report-b948-7122-9b9a-feb1.qdrep\"\n\nExporting 7098 events: [==================================================100%]\n\nExported successfully to\n\n/tmp/nsys-report-b948-7122-9b9a-feb1.sqlite\n\nGenerating CUDA API Statistics...\n\nCUDA API Statistics (nanoseconds)\n\nTime(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n\n-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n\n   44.8       280504347           4      70126086.8         1050249       276881346  cudaMalloc                                                                      \n\n   31.4       196878210         401        490968.1          381542          600948  cudaDeviceSynchronize                                                           \n\n   22.4       140280462         103       1361946.2          436597        32339232  cudaMemcpy                                                                      \n\n    1.0         6263864           4       1565966.0         1236542         1884610  cudaFree                                                                        \n\n    0.4         2729558         501          5448.2            4970           36269  cudaLaunchKernel                                                                \n\nGenerating CUDA Kernel Statistics...\n\nCUDA Kernel Statistics (nanoseconds)\n\nTime(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                                                                                                                                                                                                                                                                                         \n\n-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------------------------------------------                                                                                                                                                                                                                         \n\n   24.7        58518170         100        585181.7          580347          594395  void add_kernel&lt;double&gt;(double const*, double const*, double*)                                                                                                                                                                                                                                                                               \n\n   24.6        58312184         100        583121.8          576987          595067  void triad_kernel&lt;double&gt;(double*, double const*, double const*)                                                                                                                                                                                                                                                                             \n\n   18.1        42942748         100        429427.5          419548          438333  void dot_kernel&lt;double&gt;(double const*, double const*, double*, int)                                                                                                                                                                                                                                                                          \n\n   16.5        39062588         100        390625.9          388733          392125  void mul_kernel&lt;double&gt;(double*, double const*)                                                                                                                                                                                                                                                                                              \n\n   16.0        37980930         100        379809.3          376541          392925  void copy_kernel&lt;double&gt;(double const*, double*)                                                                                                                                                                                                                                                                                             \n\n    0.2          521628           1        521628.0          521628          521628  void init_kernel&lt;double&gt;(double*, double*, double*, double, double, double)                                                                                                                                                                                                                                                                  \n\nGenerating CUDA Memory Operation Statistics...\n\nCUDA Memory Operation Statistics (nanoseconds)\n\nTime(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n\n-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n\n  100.0        94988808         103        922221.4            2335        32089877  [CUDA memcpy DtoH]                                                              \n\nCUDA Memory Operation Statistics (KiB)\n\n              Total      Operations              Average            Minimum              Maximum  Name                                                                            \n\n-------------------  --------------  -------------------  -----------------  -------------------  --------------------------------------------------------------------------------\n\n         786632.000             103             7637.204              2.000           262144.000  [CUDA memcpy DtoH]                                                              \n\nGenerating Operating System Runtime API Statistics...\n\nOperating System Runtime API Statistics (nanoseconds)\n\nTime(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n\n-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n\n   47.6      1184528854          22      53842220.6           10240       100064607  sem_timedwait                                                                   \n\n   36.2       901118663          20      45055933.1           16792       100119932  poll                                                                            \n\n   15.9       395394013        1456        271561.8            1012        17141907  ioctl                                                                           \n\n    0.2         4052477         105         38595.0            2064          111321  open64                                                                          \n\n    0.1         1716108          86         19954.7            1042          509715  mmap                                                                            \n\n    0.0          963824          51         18898.5            1363          771371  fopen                                                                           \n\n    0.0          208937           4         52234.3           42771           62549  pthread_create                                                                  \n\n    0.0          141128           3         47042.7           41178           58621  fgets                                                                           \n\n    0.0           52102          11          4736.5            1824           18145  munmap                                                                          \n\n    0.0           41950           6          6991.7            1783           19146  putc                                                                            \n\n    0.0           37641           5          7528.2            2444           13065  open                                                                            \n\n    0.0           32953          11          2995.7            1422            6402  write                                                                           \n\n    0.0           31581          23          1373.1            1042            1823  fclose                                                                          \n\n    0.0           16470           2          8235.0            1412           15058  sched_yield                                                                     \n\n    0.0            8927           2          4463.5            3437            5490  socket                                                                          \n\n    0.0            8586           1          8586.0            8586            8586  pipe2                                                                           \n\n    0.0            7324           3          2441.3            1593            3627  fwrite                                                                          \n\n    0.0            5782           2          2891.0            1844            3938  fread                                                                           \n\n    0.0            5751           1          5751.0            5751            5751  connect                                                                         \n\n    0.0            4369           2          2184.5            1714            2655  read                                                                            \n\n    0.0            3998           3          1332.7            1082            1603  fcntl                                                                           \n\n    0.0            1433           1          1433.0            1433            1433  fflush                                                                          \n\n    0.0            1252           1          1252.0            1252            1252  bind                                                                            \n\n\nGenerating NVTX Push-Pop Range Statistics...\n\nNVTX Push-Pop Range Statistics (nanoseconds)\n\nReport file moved to \"/gpfs/mira-home/jkwack/HPC_benchmarks/BabelStream/JK_thetaGPU/JKreport-nsys-BableStream.qdrep\"\n\nReport file moved to \"/gpfs/mira-home/jkwack/HPC_benchmarks/BabelStream/JK_thetaGPU/JKreport-nsys-BableStream.sqlite\"\n</code></pre>"},{"location":"sophia/not_in_nav/performance-tools/nvidia-nsight/#nsight-compute","title":"Nsight Compute","text":""},{"location":"sophia/not_in_nav/performance-tools/nvidia-nsight/#running-a-stream-benchmark-with-nsight-compute-for-triad_kernel","title":"Running a stream benchmark with Nsight Compute for triad_kernel","text":"<pre><code>jkwack@thetagpu18:~/HPC_benchmarks/BabelStream/JK_thetaGPU$ ncu --set detailed -k triad_kernel -o JKreport-ncu_detailed-triad_kernel-BableStream ./cuda-stream\n\nBabelStream\n\nVersion: 3.4\n\nImplementation: CUDA\n\nRunning kernels 100 times\n\nPrecision: double\n\nArray size: 268.4 MB (=0.3 GB)\n\nTotal size: 805.3 MB (=0.8 GB)\n\n==PROF== Connected to process 166971 (/gpfs/mira-home/jkwack/HPC_benchmarks/BabelStream/JK_thetaGPU/cuda-stream)\n\nUsing CUDA device A100-SXM4-40GB\n\nDriver: 11000\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\nFunction    MBytes/sec  Min (sec)   Max         Average     \n\nCopy        1336793.345 0.00040     0.00042     0.00041     \n\nMul         1307948.274 0.00041     0.00043     0.00042     \n\nAdd         1335561.797 0.00060     0.00062     0.00062     \n\nTriad       976.089     0.82503     1.00961     0.87930     \n\nDot         1081921.148 0.00050     0.00055     0.00053     \n\n==PROF== Disconnected from process 166971\n\n==PROF== Report: /gpfs/mira-home/jkwack/HPC_benchmarks/BabelStream/JK_thetaGPU/JKreport-ncu_detailed-triad_kernel-BableStream.ncu-rep\n</code></pre>"},{"location":"sophia/not_in_nav/programming-models/kokkos/","title":"Kokkos on ThetaGPU","text":""},{"location":"sophia/not_in_nav/programming-models/kokkos/#overview","title":"Overview","text":"<p>Kokkos implements a programming model in C++ for writing performance-portable applications targeting all major HPC platforms. It provides abstractions for both parallel execution of code and data management. Kokkos is designed to target complex node architectures with N-level memory hierarchies and multiple types of execution resources. It can use OpenMP, etc., as a backend programming model. For more information, please visit Kokkos GitHub.</p> <p>The Kokkos shared memory programming model is a C++ library that provides the necessary architecture-specific backends (e.g., OpenMP, CUDA, etc.). To begin with, it is important to note that the Kokkos programming model is usable only in C/C++ codes. Hence, for those with Fortran codes, Kokkos must first be encapsulated within C/C++ functions and called from the main Fortran code.</p> <p>The purpose of this document is to provide guidance on using Kokkos on ThetaGPU. Please see the following pages for tutorials and more information on Kokkos: Kokkos GitHub and Kokkos Tutorials.</p>"},{"location":"sophia/not_in_nav/programming-models/kokkos/#using-kokkos-at-alcf","title":"Using Kokkos at ALCF","text":"<p>ALCF provides assistance with build instructions, compiling executables, submitting jobs, and providing prebuilt binaries. For questions, contact us at support@alcf.anl.gov.</p>"},{"location":"sophia/not_in_nav/programming-models/kokkos/#building-kokkos-on-thetagpu","title":"Building Kokkos on ThetaGPU","text":"<p>Please follow the steps below to build Kokkos on ThetaGPU.</p>"},{"location":"sophia/not_in_nav/programming-models/kokkos/#step-1-clone-the-repository","title":"Step 1: Clone the repository","text":"<pre><code>git clone https://github.com/kokkos/kokkos.git \ncd kokkos \nexport KOKKOS_PATH=\"${PWD}\"\n</code></pre> <p>Note: The Kokkos Project strives to keep the master branch stable.</p>"},{"location":"sophia/not_in_nav/programming-models/kokkos/#step-2-make-sure-that-a-relatively-new-version-of-cmake-is-available","title":"Step 2: Make sure that a relatively new version of CMake is available","text":"<pre><code>module load cmake/3.14.5\n</code></pre>"},{"location":"sophia/not_in_nav/programming-models/kokkos/#step-3-create-a-build-directory","title":"Step 3: Create a build directory","text":"<pre><code>mkdir build &amp;&amp; cd build\n</code></pre>"},{"location":"sophia/not_in_nav/programming-models/kokkos/#step-4-generate-the-makefile","title":"Step 4: Generate the Makefile","text":"<pre><code>cmake ../kokkos \\\n    -DCMAKE_CXX_COMPILER=CC \\\n    -DCMAKE_INSTALL_PREFIX=${PWD}/kokkos-install \\\n    -DKokkos_ENABLE_OPENMP=On \\\n    -DKokkos_ENABLE_HWLOC=On \\\n    -DKokkos_HWLOC_DIR=/usr \\\n    -DHWLOC_LIBRARY=/usr/lib64/libhwloc.so\n</code></pre> <p>Note: The default compiler on Theta should be sufficient to build Kokkos.</p>"},{"location":"sophia/not_in_nav/programming-models/kokkos/#step-5-install-kokkos","title":"Step 5: Install Kokkos","text":"<pre><code>make install\n</code></pre> <p>Note: This will end up installing Kokkos in <code>${KOKKOS_PATH}/build/kokkos-install</code>.</p> <p>If you wish to install it in a different directory, change <code>CMAKE_INSTALL_PREFIX</code> in step 4.</p>"},{"location":"sophia/not_in_nav/programming-models/openmp/","title":"OpenMP on ThetaGPU","text":""},{"location":"sophia/not_in_nav/programming-models/openmp/#openmp-threading-on-cpu","title":"OpenMP threading on CPU","text":"<p>All the compilers available on ThetaGPU support OpenMP threading.</p>"},{"location":"sophia/not_in_nav/programming-models/openmp/#openmp-offload-on-a100-gpu","title":"OpenMP offload on A100 GPU","text":"<p>A few compilers that support OpenMP offload are accessible on ThetaGPU. They are made available via modules.</p> <pre><code>$ module avail llvm\n\n--------- /lus/theta-fs0/software/environment/thetagpu/lmod/modulefiles ----------\n   llvm/main-20210112    llvm/release-12.0.0 (D)\n\n$ module avail nvhpc\n\n--------- /lus/theta-fs0/software/environment/thetagpu/lmod/modulefiles ----------\n   nvhpc-byo-compiler/20.9 (D)    nvhpc-nompi/20.9 (D)    nvhpc/20.9 (D)\n   nvhpc-byo-compiler/21.2        nvhpc-nompi/21.2        nvhpc/21.2\n   nvhpc-byo-compiler/21.3        nvhpc-nompi/21.3        nvhpc/21.3\n</code></pre>"},{"location":"sophia/not_in_nav/programming-models/openmp/#llvm-clang-for-cc","title":"LLVM Clang for C/C++","text":"<ul> <li>Clang OpenMP offload features</li> <li>More details about the OpenMP runtime</li> </ul> <p>If there is an issue with the compiler, feel free to contact openmp-dev@lists.llvm.org</p>"},{"location":"sophia/not_in_nav/programming-models/openmp/#warning-message","title":"Warning message","text":"<pre><code>clang-12 warning: Unknown CUDA version. version.txt: 11.0.205. Assuming the latest supported version 10.1 [-Wunknown-cuda-version]\n</code></pre> <p>This means CUDA 11 language features are not supported. As long as these features are not used, the full CUDA 11.x toolchain works. This warning can be ignored or suppressed by the <code>-Wno-unknown-cuda-version</code> compiler option.</p>"},{"location":"sophia/not_in_nav/programming-models/openmp/#compiling-example","title":"Compiling example","text":"<pre><code>module load llvm/release-12.0.0\nclang++ -fopenmp -fopenmp-targets=nvptx64 your_source.cpp\n</code></pre>"},{"location":"sophia/not_in_nav/programming-models/openmp/#nvidia-hpc-sdk-for-ccfortran","title":"NVIDIA HPC SDK for C/C++/Fortran","text":"<p>OpenMP documentation</p> <p>For compiler bugs, please file bug reports at NVIDIA Developer after logging in.</p>"},{"location":"sophia/not_in_nav/programming-models/openmp/#compiling-example_1","title":"Compiling example","text":"<pre><code>module load nvhpc-sdk/nvhpc/21.3\nnvfortran -mp=gpu -gpu=cc80 your_source.f90\n</code></pre>"},{"location":"sophia/not_in_nav/programming-models/raja/","title":"RAJA ThetaGPU","text":""},{"location":"sophia/not_in_nav/programming-models/raja/#overview","title":"Overview","text":"<p>RAJA is a collection of C++ software abstractions, being developed at Lawrence Livermore National Laboratory (LLNL), that enable architecture portability for HPC applications. The overarching goals of RAJA are to:</p> <ul> <li>Make existing (production) applications portable with minimal disruption</li> <li>Provide a model for new applications so that they are portable from inception.</li> </ul> <p>RAJA targets portable, parallel loop execution by providing building blocks that extend the generally-accepted parallel for idiom.</p> <p>Additional information can be found at RAJA User Guide.</p>"},{"location":"sophia/not_in_nav/programming-models/raja/#using-raja","title":"Using RAJA","text":"<p>RAJA provides a project template for how to use RAJA in an application project that uses CMake or Make. This is located at RAJA Project Template.</p>"},{"location":"sophia/not_in_nav/programming-models/raja/#how-to-get-the-source-code","title":"How to get the source code","text":"<p>The RAJA source code is available at RAJA GitHub.</p> <p>It can be cloned with:</p> <pre><code>git clone --recursive https://github.com/llnl/raja.git\n</code></pre> <p>The recursive clone will also clone RAJA's dependencies in the proper locations.</p>"},{"location":"sophia/not_in_nav/programming-models/raja/#add-a-cmake-configuration-for-thetagpu","title":"Add a CMake configuration for ThetaGPU","text":"<pre><code>raja/host-configs/alcf-builds/thetagpu.cmake\n</code></pre> <pre><code>set(RAJA_COMPILER \"RAJA_COMPILER_GNU\" CACHE STRING \"\")\n\nset(CMAKE_CXX_COMPILER \"g++\" CACHE PATH \"\")\nset(CMAKE_C_COMPILER \"gcc\" CACHE PATH \"\")\n\nset(CMAKE_CXX_FLAGS_RELEASE \"-O3\" CACHE STRING \"\")\nset(CMAKE_CXX_FLAGS_RELWITHDEBINFO \"-O3\" CACHE STRING \"\")\nset(CMAKE_CXX_FLAGS_DEBUG \"-O0 -g\" CACHE STRING \"\")\n\nset(CUDA_COMMON_OPT_FLAGS -restrict; --gpu-architecture sm_80; -std c++11; --expt-extended-lambda)\nset(CUDA_COMMON_DEBUG_FLAGS -restrict; --gpu-architecture sm_80; -std c++11; --expt-extended-lambda)\n\nset(HOST_OPT_FLAGS -Xcompiler -O3 -Xcompiler -fopenmp)\n\nif(CMAKE_BUILD_TYPE MATCHES Release)\n  set(RAJA_NVCC_FLAGS -O3; ${CUDA_COMMON_OPT_FLAGS}; -ccbin; ${CMAKE_CXX_COMPILER} ; ${HOST_OPT_FLAGS} CACHE LIST \"\")\nelseif(CMAKE_BUILD_TYPE MATCHES RelWithDebInfo)\n  set(RAJA_NVCC_FLAGS -g; -G; -O3; ${CUDA_COMMON_OPT_FLAGS}; -ccbin; ${CMAKE_CXX_COMPILER} ; ${HOST_OPT_FLAGS} CACHE LIST \"\")\nelseif(CMAKE_BUILD_TYPE MATCHES Debug)\n  set(RAJA_NVCC_FLAGS -g; -G; -O0; ${CUDA_COMMON_DEBUG_FLAGS}; -ccbin; ${CMAKE_CXX_COMPILER} ; -Xcompiler -fopenmp CACHE LIST \"\")\nendif()\n\nset(RAJA_RANGE_ALIGN 4 CACHE INT \"\")\nset(RAJA_RANGE_MIN_LENGTH 32 CACHE INT \"\")\nset(RAJA_DATA_ALIGN 64 CACHE INT \"\")\n\nset(RAJA_HOST_CONFIG_LOADED On CACHE Bool \"\")\n</code></pre>"},{"location":"sophia/not_in_nav/programming-models/raja/#now-build-on-the-thetagpu-compute-node","title":"Now build on the ThetaGPU compute node","text":"<pre><code>git clone --recursive https://github.com/llnl/raja.git\n\nmkdir build_alcf-thetagpu &amp;&amp; cd build_alcf-thetagpu\ncmake \\\n  -DCMAKE_BUILD_TYPE=Release \\\n  -C ../host-configs/alcf-builds/thetagpu.cmake \\\n  -DENABLE_OPENMP=On \\\n  -DENABLE_CUDA=On \\\n  -DCUDA_ARCH=sm_80 \\\n  -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda \\\n  -DCMAKE_INSTALL_PREFIX=../install_${BUILD_SUFFIX} \\\n  \"$@\" \\\n  ..\n</code></pre>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/","title":"Job and Queue Scheduling on ThetaGPU","text":""},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#queues-and-job-scheduling","title":"Queues and Job Scheduling","text":""},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#nodes-vs-queue-vs-mig-mode","title":"Nodes vs Queue vs MIG mode","text":"<p>The GPU nodes are NVIDIA DGX A100 nodes, and each node contains eight (8) A100 GPUs. You may request either entire nodes or a single GPU based on your job needs. What you will get is determined by the queue you submit to (see the Queues section below). If it has \"node\" in the name, you will get nodes. If it has \"GPU\" in the name, you will get a single GPU. Note that if you need more than a single GPU, you should submit to the full-node queue.</p> <p>Additionally, the NVIDIA A100 GPUs support a feature called \u201cMulti-Instance GPU\u201d (MIG) mode. This allows a single GPU to be shared by up to 7 different processes. We do not schedule at this level, but to use MIG capabilities, you may pass <code>--attrs mig-mode=True</code> with your qsub and use the <code>nvidia-smi_mig</code> command (note the UNDERSCORE) just as you would call <code>nvidia-smi mig ...</code> directly. Attempts to call <code>nvidia-smi mig ...</code> (no underscore) directly will result in an error message. The single-GPU host will, by default, not create a MIG instance, and users will have direct access to the GPU. If you are not using MIG mode, your session will appear as if it were a normal full-node system with only one GPU. Note that as of 12/13/21, MIG mode is unavailable for full-node jobs.</p>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#queues","title":"Queues","text":"<p>There are three primary queues:</p> <ul> <li>full-node: This is the general production queue for jobs that require full nodes. The -n parameter in your qsub will match the resource type in this queue, i.e., -n 2 in the node queue will get you two full nodes.</li> <li>bigmem: 2 of the nodes have 640 GB of memory compared to the other 22 nodes with 320 GB. Use this queue to access these 2 nodes by specifying <code>-q bigmem</code> in your script. A max of 2 nodes (-n 2) can be requested in this queue.</li> <li>single-gpu: This is the general production queue for jobs that operate best on a single GPU. The -n parameter in your qsub should always be 1 as you can only submit to a single GPU. If you need more than 1 GPU, use the full-node queue.</li> </ul> <p>Here are the initial queue limits. You may not violate either of these policies.</p>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#full-node-queue","title":"full-node queue:","text":"<ul> <li>MinTime is 5 minutes</li> <li>MaxTime is 12 hours</li> <li>MaxQueued will be 20 jobs</li> <li>MaxRunning will be 10 jobs</li> </ul>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#bigmem-queue","title":"bigmem queue:","text":"<ul> <li>MinTime is 5 minutes</li> <li>MaxTime is 12 hours</li> <li>MaxQueued is 2 jobs</li> <li>MaxRunning is 1 job</li> </ul>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#single-gpu-queue","title":"single-gpu queue:","text":"<ul> <li>MinTime is 5 minutes</li> <li>MaxTime is 1 hour</li> <li>MaxQueued is 1 job</li> <li>MaxRunning is 1 job</li> </ul> <p>The initial queue policy will be simple First-In-First-Out (FIFO) based on priority with EASY backfill.</p>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#running-jobs-on-thetagpu","title":"Running Jobs On ThetaGPU","text":"<p>Note: Users will need an allocation on ThetaGPU to utilize the GPU nodes. Request an allocation by filling out this form: Allocation request. ThetaGPU is listed under Theta on the form.</p>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#running-on-multiple-gpu-nodes","title":"Running on multiple GPU nodes","text":"<p>Until there is tighter integration of Cobalt and mpirun on GPU nodes, the user will have to identify the nodes Cobalt assigned to their job and pass them as options to mpirun along with some other mpirun options. The following shows 2 different code snippets on how to get the hosts allocated to the job and pass them to mpirun.</p> <p>Option 1 - Simplest</p> <pre><code>mpirun -hostfile $COBALT_NODEFILE -n 16 -npernode 8 mpi-example-code\n</code></pre> <p>where <code>$COBALT_NODEFILE</code> is a file that the -hostfile option can use.</p> <p>Option 2 - A little more complicated</p> <pre><code>HOSTS=$(cat $COBALT_NODEFILE | sed ':a;N;$!ba;s/\\n/,/g')\nmpirun --np 16 --host $HOSTS --oversubscribe ./mpi-example-code\n</code></pre> <p>To specifically see how the MPI ranks were assigned, one could add <code>--display-map --display-allocation</code> to the mpirun options.</p>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#controlling-which-cobalt-instance-gpu-for-my-commands","title":"Controlling which Cobalt instance (GPU) for my commands","text":"<p>IF YOU ARE ONLY USING KNL NODES, NOTHING CHANGES, AND YOU CAN IGNORE THIS.</p> <p>Because of the difference in architectures and limitations in Cobalt V1, we are running two Cobalt instances: the existing one for the KNL nodes, which remains as is, and a second one for the GPU nodes. You need to be able to control which instance you are interacting with, and there are several ways to do so.</p> <ul> <li>As was true in the past, if you do nothing, the commands will default to the architecture associated with the host you are on when you issue it.</li> <li>If you are on the Theta login nodes, commands will default to the KNL instance.</li> <li>If you are on a GPU node, for instance, the build nodes, then commands will default to the GPU instance.</li> <li>You can set an environment variable to control which instance the default commands (qsub, qstat, etc.) will interact with. The primary use case here will be users who only use GPU nodes but are working from the Theta login nodes. To do so, you may:</li> <li><code>module load cobalt/cobalt-knl</code> which would make Cobalt commands interact with the original Cobalt instance and launch jobs on the KNL nodes.</li> <li><code>module load cobalt/cobalt-gpu</code> which would make Cobalt commands interact with the new Cobalt instance and launch jobs on the GPU nodes.</li> <li>You can also set <code>COBALT_CONFIG_FILES=&lt;path to cobalt config&gt;</code><ul> <li>KNL config: /etc/cobalt.knl.conf</li> <li>GPU config: /etc/cobalt.gpu.conf</li> </ul> </li> </ul> <p>You can use suffixed commands to explicitly control which instance you are interacting with. If you regularly use both types of nodes, this is the recommended path to avoid confusion and to prevent launching jobs on the wrong architecture.</p> <p>All the commands you are used to are there; they take the same command line parameters, etc., they just have either -knl or a -gpu suffix on them. For instance:</p> <ul> <li><code>qsub-knl &lt;parameters&gt;</code> would submit a job to the KNL nodes.</li> <li><code>qstat-gpu</code> would check the queue status for the GPU nodes.</li> </ul>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#requesting-dgx-nodes-or-individual-gpus","title":"Requesting DGX nodes or individual GPUs","text":"<p>The DGX nodes, which contain (8) A100 GPUs, are extremely powerful, and it can be very difficult for a single job to efficiently use an entire node. For this reason, you may request either full nodes (all 8 GPUs) or individual GPUs. What you are assigned (a node or a GPU) is dependent on the queue you submit to:</p> <ul> <li>If the queue name ends in -node, you will get full nodes (8 A100 GPUs).</li> <li>If the queue name ends in -gpu, you will get an individual GPU.</li> <li>The -n parameter on the qsub is the number of resources of the type in that queue. So, for example:</li> <li><code>qsub -n 2 -q full-node &lt;rest of command line&gt;</code> would get you two full DGX nodes, which would be a total of (16) A100 GPUs.</li> <li><code>qsub -n 2 -q single-gpu &lt;rest of command line&gt;</code> would get you two A100 GPUs.</li> </ul> <p>For reservations, you can only have one queue, and the resources in the queue need to be consistent, so your entire reservation must be in nodes or GPUs. If you need both, you will need two reservations, one for each type of resource.</p> <ul> <li>Node names are of the form thetagpu## where ## ranges from 01 to 24. This is an entire node (8 GPUs).</li> <li>GPU names are of the form thetagpu##-gpu# where the GPU numbers range from 0-7.</li> </ul>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#multi-instance-gpu-mig-mode","title":"Multi-Instance GPU (MIG) mode","text":"<p>The A100 GPUs have a capability known as Multi-Instance GPU (MIG). This allows a single A100 GPU to be reconfigured at a hardware level down to a maximum of 7 instances. The valid configurations are shown in a table on the MIG page referenced above. These instances appear as a GPU to the application. In order to use this feature, the GPU must be put into MIG mode, and this requires a reset of the GPU. At the current time, we are not supporting scheduling at the MIG level. However, a user can request that their GPU be put in MIG mode, and then they can reconfigure the GPU into a supported configuration from their job script.</p> <p>If you wish to have the resources you have requested put into MIG mode, you can add either of these to your qsub command line: <code>--attrs mig-mode=True</code>.</p>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#details-of-a-job-submission","title":"Details of a job submission","text":"<p>Details of the job submission are recorded in the <code>&lt;jobid&gt;.cobaltlog</code>. This file contains the qsub command and environment variables. The location of this file can be controlled with the <code>qsub --debuglog &lt;path&gt;</code> that defaults to the same place as the <code>.output</code> and <code>.error</code> files.</p>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#jobs-stuck-in-starting-state","title":"Jobs stuck in \"starting\" state","text":"<p>If you submit a job and qstat shows it in \"starting\" state for 5 minutes or more, most likely your memory/numa mode selection requires rebooting some or all of the nodes your job was assigned. This process takes about 15 minutes, during which your job appears to be in the \"starting\" phase. When no reboots are required, the \"starting\" phase only lasts a matter of seconds.</p>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#utime-and-stime","title":"\"utime\" and \"stime\"","text":"<p>At the bottom of a <code>&lt;jobid&gt;.output</code> file, there is usually a line like:</p> <pre><code>Application 3373484 resources: utime ~6s, stime ~6s, Rss ~5036, inblocks ~0, outblocks ~8\n</code></pre> <p>The \"utime\" and \"stime\" values are user CPU time and system CPU time from the aprun and getrusage commands. They are rounded aggregate numbers scaled by the number of resources used and are approximate. The aprun man page has more information about them.</p>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#cobalt-directives-on-the-second-line-of-job-script","title":"<code>#COBALT</code> directives on the second line of job script","text":"<p>If <code>#COBALT</code> directives are used inside a job submission script, then they must appear at the topmost lines of the script. <code>#COBALT</code> directives following a blank line will be ignored. Attempting to qsub the following example script will lead to the error message below.</p> <pre><code>&gt; cat submit.csh #!/bin/csh #COBALT -n 2 -t 2:00:00 -q full-node mpirun -np 20 -npernode 10 ./my_app &gt; qsub submit.csh Usage: qsub.py [options] [] Refer to man pages for JOBID EXPANSION and SCRIPT JOB DIRECTIVES. No required options provided\n</code></pre> <p>A correct submission script would look like the following with the blank line removed.</p> <pre><code>&gt; cat submit.csh\n\n#!/bin/csh\n\n#COBALT -n 2 -t 2:00:00 -q full-node\n\nmpirun -np 20 -npernode 10 ./my_app\n\n&gt; qsub submit.csh\n\n12345\n</code></pre>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#job-submission-on-thetagpu","title":"Job Submission on ThetaGPU","text":"<p>The queuing system used on ThetaGPU is Cobalt. On ThetaGPU, Cobalt jobs may run either as script jobs or interactive mode jobs.</p>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#script-method","title":"Script Method","text":"<p>In the script method, Cobalt will execute a user-supplied script when running a user\u2019s job. Following are the required flags to <code>qsub</code>, as well as some of the more common options. A complete list of options may be found as a part of the <code>qsub</code> manpage, available on any login node.</p>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#required-flags","title":"Required Flags","text":"<pre><code>-n NN - number of nodes (-n 16 for 16 nodes)\n-t time - running time (-t 30 for 30 minutes, -t 01:10:20 for 1 hr 10 min 20 sec) \n-A Project - project (-A YourProject)\n</code></pre>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#common-options","title":"Common options","text":"<pre><code>--attrs - you may specify additional attributes for your job. \n          Multiple attribute key-value pairs are colon-delimited. \n          The following are common on the KNL nodes: \n          - filesystem: a comma-separated list of filesystems used while running your job\n          - location: a comma-separated list of node ids. Ranges may be hyphenated. \n          - mcdram: The desired MCDRAM mode of a job (default: cache) \n          - numa: The desired NUMA mode of a job (default: quad)\n\n          The following are common on the GPU nodes:          \n          - location: a comma-separated list of node names (not IDs). Ranges may NOT be hyphenated on the GPU nodes. \n          - mig-mode: Should the GPUs be put in Multi Instance GPU (MIG) mode (default: False) \n          - pubnet: Enable public network connectivity from compute nodes\n\nSee: https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html\n\n\n--env VAR1=1:VAR2=2:\u2026 - specify required environment variables\n-i file - give a file name to be used for stdin \n-O Name - name your job and stdout/stderr (-O Job1) \n-q queue - queue name (full-node, single-gpu, bigmem) (default: full-node)\n</code></pre> <p>Note: Remember to give all options before the executable name.</p> <p>Users will need an allocation on ThetaGPU to utilize the GPU nodes. Request for an allocation on ThetaGPU through our Director's Discretionary award. ThetaGPU is listed under Theta on the form.</p> <p>Users will need to load the <code>cobalt/cobalt-gpu</code> module before issuing a <code>qsub</code> command to access ThetaGPU.</p>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#example-script","title":"Example Script","text":"<pre><code>module load cobalt/cobalt-gpu\n\nqsub -A YourProject -n 4 -t 30 -q full-node \\\n--env MYVAR=value1 -i inputdata -O Project1_out \\\n--attrs filesystems=home,eagle \\\nprogram.exe progarg1\n</code></pre> <p>The syntax for Cobalt scripting is slightly different than that of a PBS script. For more information, see Cobalt scripting.</p>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#interactive-method","title":"Interactive Method","text":"<p>To run an \u201cinteractive mode\u201d job on ALCF Cray resources, add the \u201c-I\u201d (uppercase \"i\", not a lowercase \"L\") flag or \u201c--mode interactive\u201d to your qsub line and omit any executable. Your qsub submission will then wait until nodes are allocated to your job, and Cobalt will start a shell on a job-launch node on your behalf. You may aprun against your assigned resources and run other interactive commands from this node. It is important to note that your shell is executed from a launch node and not from your compute head-node. Once your allocation ends, all apruns will be terminated, but your shell will remain for any cleanup actions that you choose to take.</p>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#ensemble-jobs","title":"Ensemble Jobs","text":"<p>Users may run an \u201censemble job\u201d and combine runs into a single script. This can provide major enhancements to throughput, especially for large ensemble jobs. Users may run multiple jobs in sequence or may use multiple backgrounded apruns to subset their resources among multiple backend executables. There is a system limitation of 1000 simultaneous apruns per Cobalt script job.</p>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#submitted-job-with-the-wrong-arguments","title":"Submitted Job with the Wrong Arguments","text":"<p>If you submit a job with the wrong arguments, you can modify it without deleting it and resubmitting it. Most settings can be changed using <code>qalter</code>.</p>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#example","title":"Example","text":"<pre><code>Usage: qalter [-d] [-v] -A &lt;project name&gt; -t &lt;time in minutes&gt;\n             --attrs filesystems=&lt;filesystem&gt;\n             -e &lt;error file path&gt; -o &lt;output file path&gt;\n             --dependencies &lt;jobid1&gt;:&lt;jobid2&gt;\n             -n &lt;number nodes of&gt; -h --proccount &lt;processor count&gt;\n             -M &lt;email address&gt; &lt;jobid1&gt; &lt;jobid2&gt;\n</code></pre> <p>Note: To change the queue, use <code>qmove</code>.</p> <pre><code>Usage: qmove &lt;queue name&gt; &lt;jobid&gt; &lt;jobid&gt;\n</code></pre>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#changing-executable-after-job-submission","title":"Changing Executable after Job Submission","text":"<p>When a job is submitted via qsub, Cobalt records the path to the executable or script, but it does not make a copy. As a result, if the executable or script is modified when there is a deletion or modification, it will affect any jobs already submitted that use that executable. To avoid confusion, it is generally best to avoid making changes after job submission.</p>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#holding-and-releasing-jobs","title":"Holding and Releasing Jobs","text":""},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#user-holds","title":"User Holds","text":"<p>To hold a job (prevent it from running), use <code>qhold</code>. This will put the job in the <code>user_hold</code> state.</p> <pre><code>qhold &lt;jobid&gt;\n</code></pre> <p>To release a job in a user hold (<code>user_hold</code>) state, use <code>qrls</code>.</p> <pre><code>qrls &lt;jobid&gt;\n</code></pre> <p>A job may also be put into a user hold immediately upon submission by passing <code>qsub</code> the <code>-h</code> flag.</p> <pre><code>qsub -n 8 -t 60 --attrs filesystems=home,eagle -A MyProject -h myExe\n</code></pre>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#dependency-holds","title":"Dependency Holds","text":"<p>For jobs in the <code>dep_hold</code> or <code>dep_fail</code> state, see the section on job dependencies.</p>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#admin-holds","title":"Admin Holds","text":"<p>Jobs in the state <code>admin_hold</code> may be released only by a system administrator.</p>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#maxrun-holds","title":"MaxRun Holds","text":"<p>Jobs may temporarily enter the state <code>maxrun_hold</code> if the user has reached the limit of per-user running jobs in a particular queue. No action is required; as running jobs complete, jobs in the <code>maxrun_hold</code> state will be automatically changed back to queued and eligible to run.</p>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#job-dependencies","title":"Job Dependencies","text":"<p>To submit a job that waits until another job or jobs have completed, use the dependencies argument to qsub. For example, to submit a job that depends on job 12345:</p> <pre><code>qsub -n 2 -t 10 --attrs filesystems=theta-fs0,eagle,home -A yourproject --dependencies 12345 a.out\n</code></pre> <p>For multiple dependencies, list and separate with colons.</p> <pre><code>qsub -n 2 -t 30 -A yourproject --attrs filesystems=theta-fs0,eagle,home --dependencies 12345:12346 a.out\n</code></pre> <p>Jobs submitted with dependencies will remain in the state <code>dep_hold</code> until all the dependencies are fulfilled, then will proceed to the state queued.</p> <p>Note: In the event any of the dependencies do not complete successfully (nonzero exit status), the job will instead go into the state <code>dep_fail</code>. To manually release a job that is in either <code>dep_hold</code> or <code>dep_fail</code>:</p> <pre><code>qrls --dependencies &lt;jobid&gt;\n</code></pre> <p>or alternatively change the job's dependencies setting to \"none\":</p> <pre><code>qalter --dependencies none &lt;jobid&gt;\n</code></pre>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#customizing-the-output-of-qstat","title":"Customizing the Output of <code>qstat</code>","text":"<p>Default fields displayed by the <code>qstat</code> command may be changed by setting the <code>QSTAT_HEADER</code> environment variable.</p> <pre><code>export QSTAT_HEADER=\"JobID:JobName:User:WallTime:RunTime:Nodes:State:attrs:Queue\"\nqstat\n\nJobID   JobName                           User      WallTime  RunTime   Nodes  State      attrs             Queue\n     =======================================================================================================================================\n     104927  N/A                               user1     02:00:00  01:20:45  128    running    {'numa': 'quad', 'mcdram': 'cache'}  backfill\n     104941  N/A                               user2     00:20:00  N/A       2048   queued     {'numa': 'quad', 'mcdram': 'flat'}   backfill\n     104934  xxxx.yyyyy                        user3     04:00:00  01:10:12  32     running    {'numa': 'quad', 'mcdram': 'cache'}  default\n     104948  Xxx-YY_ZZ                         user4     02:00:00  00:15:03  128    running    {'numa': 'quad', 'mcdram': 'cache'}  default\n     104919  aaaaa_0000_bbbb_c                 user5     06:00:00  01:50:21  64     running    {'numa': 'quad', 'mcdram': 'cache'}  default\n     104945  aaaa_bb_cccc-d_eee_f.hhhhhh.iiii  user6     06:00:00  00:18:25  100    running    {'numa': 'quad', 'mcdram': 'cache'}  default\n     104848  bbbbbb                            user7     01:00:00  N/A       3624   queued     {'numa': 'quad', 'mcdram': 'cache'}  default\n     ....\n</code></pre> <p>One may specify column headers via the <code>--header</code> flag to <code>qstat</code>.</p> <p>Available field names can be seen by entering <code>qstat -fl &lt;jobid&gt;</code> for any current jobid.</p>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#redirecting-standard-input","title":"Redirecting Standard Input","text":"<p>To redirect the standard input to a job, do not use the <code>&lt;</code> redirection operator on the <code>qsub</code> command line. This simply redirects standard input to <code>qsub</code>, not the job itself. Instead, use the qsub option <code>-i</code>.</p> <pre><code># WRONG\nqsub -t 10 -n 1 --attrs filesystems=home a.out &lt; my_input_file.dat\n\n# RIGHT\nqsub -t 10 -n 1 --attrs filesystems=home -i my_input_file.dat a.out\n</code></pre>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#project-names","title":"Project Names","text":"<p>You can find active project names that your account is associated with by running the command:</p> <pre><code>sbank allocations\n</code></pre> <p>If an account is associated with more than one project, a job must be submitted by using a specific project name using <code>-A</code> or by setting the environment variable <code>COBALT_PROJ</code>.</p>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#sbank","title":"Sbank","text":"<p>The sbank database is updated hourly. This means transactions against your account can take up to an hour before they show up.</p>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#submitting-into-backfill-nodes","title":"Submitting into Backfill Nodes","text":"<p>Sometimes the scheduler will try to clear up room for a large job. During these times, although not many jobs may be running, new jobs are not being scheduled as expected.</p> <p>At such times, backfill nodes may be available. While nodes are being drained for a larger job, other user jobs may be backfilled onto these resources, provided that their requested wall time is less than the remaining drain time of the set of resources. For instance, suppose that 16 nodes are being drained to allow a 16-node job to run. Of the 16 nodes, perhaps eight are empty, and the other eight are running an eight-node job that has 2 hours of wall time left. This allows the opportunity to run a 2-hour, eight-node job in the backfill here.</p> <p>To discover available backfill, run the nodelist command. This command can only be run on the service nodes (<code>thetagpusn1-2</code>), it cannot be run on the compute nodes.</p>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#example_1","title":"Example","text":"<pre><code>nodelist\nHost             Queue                                       State      Backfill\n==================================================================================\n[...]\nthetagpu16       CompBioAffin:backfill:full-node             down       -       \nthetagpu16-gpu0  single-gpu                                  idle       -       \nthetagpu16-gpu1  single-gpu                                  allocated  -       \nthetagpu16-gpu2  single-gpu                                  allocated  -       \nthetagpu16-gpu3  single-gpu                                  allocated  -       \nthetagpu16-gpu4  single-gpu                                  idle       -       \nthetagpu16-gpu5  single-gpu                                  idle       -       \nthetagpu16-gpu6  single-gpu                                  idle       -       \nthetagpu16-gpu7  single-gpu                                  idle       -       \nthetagpu17       CompBioAffin:backfill:full-node             allocated  -       \nthetagpu17-gpu0  single-gpu                                  down       -       \nthetagpu17-gpu1  single-gpu                                  down       -       \nthetagpu17-gpu2  single-gpu                                  down       -       \nthetagpu17-gpu3  single-gpu                                  down       -       \nthetagpu17-gpu4  single-gpu                                  down       -       \nthetagpu17-gpu5  single-gpu                                  down       -       \nthetagpu17-gpu6  single-gpu                                  down       -       \nthetagpu17-gpu7  single-gpu                                  down       -\n[...]\n</code></pre> <p>In this example, a four-node job with a maximum wall time of 4 hours and 59 minutes can be run during this backfill. The backfill times will not always be identical and will depend on the mix of jobs on the partitions that are being drained.</p>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#submitting-to-specific-nodes","title":"Submitting to Specific Nodes","text":"<p>In rare cases, there may be a need to target specific hardware. This may be accomplished using <code>--attrs location=</code>.</p>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#example_2","title":"Example","text":"<pre><code>qsub -t 10 -n 2 --attrs filesystems=eagle location=thetagpu01:thetagpu02 myprog.exe \n</code></pre> <p>This will force the job to run on those specific nodes. Should that location become unschedulable, for instance, due to a failed node, the job will not be allowed to run anywhere else, without resetting the location attribute. If more nodes are specified in the location field than are required to fill a job\u2019s requested node count, then the first n nodes available in the location set will be used.</p>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#running-with-a-group-of-users","title":"Running with a Group of Users","text":"<p>Sometimes it is useful to allow other users to run Cobalt commands on a given job such as <code>qhold</code>, <code>qrls</code>, or <code>qdel</code>. A list of users can be allowed to run commands on your job by submitting a list of users to <code>qsub</code>, <code>cqsub</code>, or <code>qalter</code> using the flag <code>--run_users</code>. Specified users need not be in the same project under which the job was submitted.</p>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#example_3","title":"Example","text":"<pre><code>qsub -t 10 -n 16 --attrs filesystems=eagle,eagle,home location=thetagpu01:thetagpu02 --run_users frodo:sam:pippin myprog.exe\n</code></pre> <p>As a convenience, all users belonging to the project under which a job was submitted can be added to a list of users that may control a job by using the <code>--run_project</code> flag.</p> <p>Users who have been added to the list can run any command that the job-submitter could run on a job. This includes <code>qhold</code>, <code>qrls</code>, <code>qalter</code>, and <code>qdel</code>.</p>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#group-running-and-file-system-groups","title":"Group Running and File System Groups","text":"<p>While setting this list of users allows any of the listed users to run Cobalt commands on a job, it does not do anything about the permissions of any files involved with the job. Those must be handled by the user(s) setting appropriate permissions on their directories to allow users in their group to read and write files as appropriate. If your project needs a group on the file system to share files or a user needs to be added, email User Support.</p>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#more-information","title":"More Information","text":"<p>For more information on Cobalt commands and their options, consult the manpages on the system. The same information may be found online in the Cobalt Command Reference.</p>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#using-the-job-resource-manager-commands-options-and-examples","title":"Using the Job Resource Manager: Commands, Options, and Examples","text":"<p>This document provides examples of how to submit jobs on our systems. It also provides examples of commands that can be used to query the status of jobs, what partitions are available, etc. For an introduction to using the job resource manager and running jobs on ThetaGPU, see Running Jobs on ThetaGPU.</p>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#submit-a-job-request","title":"Submit a Job Request","text":"<p>Use <code>qsub</code> to submit a job. Unlike jobs on the ALCF Blue Gene systems, all jobs on ThetaGPU are either script or interactive.</p> <p>Run the script <code>jobscript.sh</code> with 2 nodes for a maximum of 15 minutes:</p> <pre><code>qsub -n 2 -t 15 --attrs filesystems=theta-fs0 jobscript.sh\n</code></pre> <p>To submit jobs to a particular queue, use <code>qsub -q &lt;queue_name&gt;</code>.</p>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#charge-a-job-to-a-project","title":"Charge a Job to a Project","text":"<p>Use <code>qsub -A &lt;project_name&gt;</code> to charge a job to a particular project.</p> <p>To run <code>jobscript.sh</code> with 2 nodes for a maximum of 15 minutes and charge the job to MyProject:</p> <pre><code>qsub -n 2 -t 15 --attrs filesystems=eagle,home -A MyProject jobscript.sh\n</code></pre> <p>To see which projects you are a member of:</p> <pre><code>projects\n</code></pre> <p>You can use the environment variable <code>COBALT_PROJ</code> to set your default project. qsub -A takes precedence over <code>COBALT_PROJ</code>.</p>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#delete-a-job-from-the-queue","title":"Delete a Job from the Queue","text":"<p>To delete a job from the queue, use the qdel command. For example, for a job with ID of 34586.</p> <pre><code>qdel 34586\n</code></pre> <p>Depending on the stage of a job\u2019s lifetime, qdel may not complete immediately, especially if the delete is issued during startup on a job that is changing memory modes and rebooting a node. If the job does not ultimately terminate, contact support@alcf.anl.gov with the jobid so that an administrator can take appropriate cleanup actions and administratively terminate the job.</p>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#query-partition-availability","title":"Query Partition Availability","text":"<p>To determine which partitions are currently available to the scheduler, use the nodelist command. This command provides a list of node ids, names, queue, and state as well as any backfill windows. This command can only be run on the service nodes (<code>thetagpusn1-2</code>), it cannot be run on the compute nodes.</p> <p>For example, on <code>thetagpusnX</code>, it displays:</p> <pre><code>Host             Queue                                       State      Backfill\n================================================================================== \n[...]\nthetagpu16       CompBioAffin:backfill:full-node             down       -       \nthetagpu16-gpu0  single-gpu                                  idle       -       \nthetagpu16-gpu1  single-gpu                                  allocated  -       \nthetagpu16-gpu2  single-gpu                                  allocated  -       \nthetagpu16-gpu3  single-gpu                                  allocated  -       \nthetagpu16-gpu4  single-gpu                                  idle       -       \nthetagpu16-gpu5  single-gpu                                  idle       -       \nthetagpu16-gpu6  single-gpu                                  idle       -       \nthetagpu16-gpu7  single-gpu                                  idle       -       \nthetagpu17       CompBioAffin:backfill:full-node             allocated  -       \nthetagpu17-gpu0  single-gpu                                  down       -       \nthetagpu17-gpu1  single-gpu                                  down       -       \nthetagpu17-gpu2  single-gpu                                  down       -       \nthetagpu17-gpu3  single-gpu                                  down       -       \nthetagpu17-gpu4  single-gpu                                  down       -       \nthetagpu17-gpu5  single-gpu                                  down       -       \nthetagpu17-gpu6  single-gpu                                  down       -       \nthetagpu17-gpu7  single-gpu                                  down       -       \nthetagpu18       CompBioAffin:backfill:full-node             allocated  -       \n[...]\n</code></pre>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#specifying-filesystems","title":"Specifying Filesystems","text":"<p>On ThetaGPU and other systems running Cobalt at the ALCF, your job submission should specify which filesystems you will be using. In the event that a filesystem becomes unavailable, this information is used to preserve jobs that would use that filesystem while allowing other jobs that are not using an affected filesystem to proceed to run normally.</p> <p>You may specify your filesystem by adding <code>filesystems=&lt;list of filesystems&gt;</code> to the <code>--attrs</code> argument of qsub in Cobalt. Valid filesystems are <code>home</code>, <code>eagle</code>, and <code>theta-fs0</code>. The list is comma-delimited.</p> <p>For example, to request the <code>home</code> and <code>eagle</code> filesystems for your job, you would add <code>filesystems=home,eagle</code> to your <code>qsub</code> command. If this is not specified, a warning will be printed, and then the job will be tagged as requesting all filesystems and may be held unnecessarily if a filesystem is not currently available. The warnings are written to stderr of <code>qsub</code> and <code>qalter</code> commands that change the value of the <code>--attrs</code> flag. Scripts that are parsing stderr from these utilities may encounter errors from the additional warnings if filesystems are not specified in these commands.</p> <p>If a job is submitted while a filesystem it requested is marked down, the job will automatically be placed into a <code>user_hold</code> status, and a warning message will be printed, but the job will be otherwise queued. The job is also placed into <code>admin_hold</code> status by a sysadmin script. Once the affected filesystem has been returned to normal operation, the <code>admin_hold</code> is released. You are responsible for releasing the <code>user_hold</code> once you receive the message that the affected filesystem has been returned to normal operation. The job cannot run until both the holds are released.</p> <p>If a job requesting a filesystem that is marked down is already in the queue, it will be placed on <code>admin_hold</code> status and will be released once the filesystem is operational.</p> <pre><code>qsub -n 1 -t 30 -q full-node --attrs filesystems=home,eagle -A Project ./my_job.sh\n</code></pre> <p>To update the filesystems list for your job, use <code>qalter</code>. Note that <code>qalter --attrs</code> is a replace and not an update operation. This means that you should once again specify all the attributes that you had in the original <code>qsub</code> command.</p> <pre><code>qalter --attr filesystems=home,eagle:mig-mode=True &lt;jobid&gt;\n</code></pre> <p>To release user hold:</p> <pre><code>qrls &lt;jobid&gt;\n</code></pre>"},{"location":"sophia/not_in_nav/queueing-and-running-jobs/job-and-queue-scheduling/#references","title":"References","text":""},{"location":"sophia/queueing-and-running-jobs/running-jobs/","title":"Running Jobs on Sophia","text":""},{"location":"sophia/queueing-and-running-jobs/running-jobs/#nodes-vs-queue","title":"Nodes vs Queue","text":"<p>The Sophia nodes are NVIDIA DGX A100 nodes, and each node contains eight (8) A100 GPUs.  The majority of the nodes have the 40 GB A100 models, but two nodes contain the 80 GB A100 models (see below).  You may request resources by node (with 8 GPUs) or by individual GPUs based on your job needs.  What you will get is determined by the queue you submit to (see Queues section below).</p>"},{"location":"sophia/queueing-and-running-jobs/running-jobs/#queues","title":"Queues","text":"<p>There are three production queues you can target in your <code>qsub</code> command (<code>-q &lt;queue name&gt;</code>):</p> Queue Name Node/GPU Min Node/GPU Max Time Min Time Max <code>by-gpu</code> 1 GPU 8 GPUs (valid values are 1, 2, 4, and 8) 5 min 12 hr <code>by-node</code> 1 Node 8 Nodes 5 min 12 hr <code>bigmem</code> 1 Node 1 Node 5 min 12 hrs <p>!!!  note</p> <pre><code>For all Sophia queues, `MaxQueued` will be 20 queued or running jobs (per project) and `MaxRunning` will be 5 concurrent jobs (per project)\n</code></pre> <p>The initial queue policy will be simple First-In-First-Out (FIFO) based on priority with EASY backfill.  The <code>by-queue</code> and <code>by-gpu</code> queues target non-bigmem nodes.  The old <code>single-node</code> queue is now a routing queue (redirect) to the <code>by-node</code>, and the old <code>single-gpu</code> queue is now a routing queue (redirect) to the <code>by-gpu</code> queue.</p>"},{"location":"sophia/queueing-and-running-jobs/running-jobs/#queue-descriptions","title":"Queue Descriptions","text":""},{"location":"sophia/queueing-and-running-jobs/running-jobs/#by-gpu","title":"<code>by-gpu</code>","text":"<p>This is the default production queue<sup>1</sup> and is targeted at jobs that can utilize 1-8 GPUs.  The number of \"chunks\" you specify in your <code>qsub</code> (i.e., <code>-l select=4</code>) will be the number of GPUs you are allocated, and they will all be on the same node.  Valid values are 1, 2, 4, or 8 in your select statement.  These restrictions ensure you get a sane set of resources (RAM is in the same NUMA node as the cores, the GPU has minimal hops to the GPU, etc.).  If you specify a different value, your <code>qsub</code> will issue an error and fail.</p>"},{"location":"sophia/queueing-and-running-jobs/running-jobs/#by-node","title":"<code>by-node</code>","text":"<p>This queue is targeted at jobs that can utilize more than 8 GPUs.  The number of \"chunks\" you specify in your <code>qsub</code> (i.e., <code>-l select=4</code>) will be the number of Sophia DGX nodes (with 8 GPUs each) you are allocated.</p>"},{"location":"sophia/queueing-and-running-jobs/running-jobs/#bigmem","title":"<code>bigmem</code>","text":"<p>Two of the nodes have 80GB of RAM per GPU, while the other 22 have 40GB of RAM per GPU (640 GB of aggregate GPU memory per node vs. 320 aggregate GPU memory per node).  Use this queue to access the 2 nodes with more memory by specifying <code>-q bigmem</code> in your <code>qsub</code>.  A max of 1 node (<code>-l select=1</code>) can be requested in this queue.</p> <ol> <li> <p>The default queue is where your job will be submitted if you don't have <code>-q &lt;queue name&gt;</code> in your <code>qsub</code>.\u00a0\u21a9</p> </li> </ol>"},{"location":"sophia/visualization/paraview/","title":"ParaView on Sophia","text":"<p>The recommended way of running ParaView on Sophia is in client/server mode. This consists of running the ParaView client on your local resource and the ParaView server on the Sophia compute nodes. The ParaView client needs to first be installed on your local resource and needs to match the version that you run on Sophia.</p> <p>There may be multiple versions of ParaView installed on Sophia. To find the versions of ParaView currently available on Sophia, run the following command on a login node:  <pre><code>module use /soft/modulefiles\nmodule avail paraview\n</code></pre></p> <p>Binary and source packages of the ParaView client for Linux, macOS, and Windows are available from the ParaView Download Page.</p>"},{"location":"sophia/visualization/paraview/#connecting-to-the-paraview-server-on-sophia","title":"Connecting to the ParaView server on Sophia","text":"<p>This section describes how to launch the ParaView server on Sophia from a local ParaView client.</p>"},{"location":"sophia/visualization/paraview/#start-paraview-client","title":"Start ParaView Client","text":"<p>First, launch the ParaView client on your local resource. You will need to configure some server settings in the client. This initial setup should only need to be done once and can be reused each time you want to run ParaView on Sophia.</p>"},{"location":"sophia/visualization/paraview/#server-configuration","title":"Server Configuration","text":""},{"location":"sophia/visualization/paraview/#1-select-connect","title":"1. Select Connect","text":"<p>From the ParaView client, choose to connect to a server by either clicking on the \"Connect\" icon in the menu bar</p> <p></p> <p>or selecting File-&gt;Connect from the main menu.</p> <p></p>"},{"location":"sophia/visualization/paraview/#2-set-up-servers-first-time-only","title":"2. Set Up Servers (first time only)","text":"<p>The first time you want to run a server on Sophia and have it connect to your local ParaView client, you will need to set up a server. Once this server is set up, you can reuse it each time you run the ParaView client with the ParaView server on Sophia.</p> <p>Kitware, the developers of ParaView, maintain a database of server configurations which you can retrieve through the ParaView client.</p> <p>NOTE</p> <p>At this time, there are no specific files for Sophia available from Kitware. We will update this page when the files are available. In the meantime, you can download configuration files here and import them with the <code>Load Servers</code> option. Please use the <code>Save link as</code> option in your browser. Mac Windows</p>"},{"location":"sophia/visualization/paraview/#3-use-paraview","title":"3. Use ParaView","text":"<p>After the previous step, you can now select SOPHIA@ANL in the File-&gt;Connect menu and press Connect.</p> <p></p> <p>At this point, a new window will pop up.</p> <p></p> <p>There are a number of parameters that you must enter manually here:</p> <ul> <li>Xterm executable: The path of a terminal on your system. The figure shows the case of a Mac with XQuartz. You may need to change these values for Windows or Linux.</li> <li>SSH executable: The name of your ssh command. It may be different on Windows depending on the ssh client installed (e.g., PuTTY).</li> <li>Remote machine: Leave this value at sophia.alcf.anl.gov.</li> <li>Username: Your ALCF username.</li> <li>ParaView version: The version of ParaView that you want to use. Verify first that this version is installed on the system (as described at the top of this document). You will also need to add a <code>-EGL</code> suffix.</li> </ul> <p>Example: <pre><code>5.13.1-EGL\n</code></pre></p> <ul> <li>Client port: It is safe to use the default value.</li> <li>Server port: It is safe to use the default value.</li> <li>Number of nodes to reserve: Enter the number of Sophia compute nodes you want to use for your job.</li> <li>Number of ranks per node: Enter the number of ranks per node.</li> <li>Number of minutes to reserve: The duration of your job in minutes.</li> <li>Account: Enter here the name of your ALCF allocation.</li> <li>Queue: The name of the Sophia queue you would like to use (e.g., <code>by-gpu</code> or <code>by-node</code>). We recommend <code>by-gpu</code> for most jobs so that the nodes are used efficiently.</li> <li>File Systems: Enter here the file systems you need for your job, separated with colons, no spaces. Keep in mind that your job may not run if one of these file systems is not available at that time, so enter these values carefully.</li> <li>Job name: Safe to use the default value. The PBS scheduler will assign this name to your job.</li> </ul> <p>Now you can press OK to establish the connection with a ParaView server on Sophia.</p> <p>An SSH connection will be established with a Sophia login node, and a password will be requested in a terminal, similar to the process you normally use to connect and work on the system.</p> <p>After you enter your password, a job will be queued, and you will see a window like this:</p> <p></p> <p>When the job is launched on the compute nodes, the previous window will go away, and ParaView will show it is connected to Sophia in its Pipeline Browser:</p> <p></p> <p>At this point, you can open datasets stored on the ALCF file systems and use ParaView normally.</p>"},{"location":"sophia/visualization/paraview/#additional-information","title":"Additional Information","text":"<ul> <li>ParaView Documentation</li> <li>ParaView Community Support</li> </ul>"},{"location":"sophia/visualization/visualization/","title":"Visualization on Sophia","text":"<p>With its powerful NVIDIA GPUs, Sophia offers an excellent environment for visualization.</p> <p>Below is a list of available visualization tools with links to their respective documentation.</p> <p>ParaView: ParaView is an open-source visualization engine that seamlessly integrates with your existing tools and workflows. It allows you to construct visualization pipelines for quick data analysis. Whether interactively exploring large datasets in 3D or performing batch processing programmatically, ParaView provides versatile capabilities. For additional information, visit the Kitware website.</p>"}]}